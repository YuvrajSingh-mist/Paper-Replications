
# Paper-Replications

This repository contains a collection of code implementations and experiments replicating results from a wide range of influential machine learning and deep learning research papers. Each subfolder corresponds to a specific paper, model, or technique, with code, notes, and sometimes pretrained weights or results.

## Structure

- **Attention Mechanisms/**: Implementations and experiments with various attention mechanisms.
- **BERT/**: Replication and exploration of the BERT model.
- **CGANs/**: Conditional Generative Adversarial Networks.
- **CLAP/**: Contrastive Language-Audio Pretraining.
- **CLiP/**: CLIP and related vision-language models.
- **CycleGANs/**: Cycle-consistent GANs for image translation.
- **DCGANs/**: Deep Convolutional GANs.
- **DDP/**: Distributed Data Parallel training experiments.
- **DeepSeekV3/**: DeepSeek model replications and experiments.
- **Differential Transformer/**: Differential Transformer architectures.
- **DPO/**: Direct Preference Optimization and related RLHF methods.
- **Encoder-Decoder/**: Encoder-decoder architectures for sequence modeling.
- **Fine Tuning using PEFT/**: Parameter-Efficient Fine-Tuning methods.
- **Gemma/**, **Gemma3/**: Replications of Gemma models.
- **GPT/**: Generative Pretrained Transformer models.
- **GRU/**: Gated Recurrent Unit models.
- **Kimi-K2/**: Kimi-K2 model replications and training scripts.
- **Llama/**, **Llama4/**: Llama model replications and experiments.
- **Llava/**: Large Language and Vision Assistant models.
- **LoRA/**: Low-Rank Adaptation for efficient fine-tuning.
- **LSTM/**: Long Short-Term Memory models.
- **Mixtral/**: Mixture-of-Experts Transformer models.
- **Moonshine/**: Moonshine model experiments.
- **ORPO/**: Online RLHF Preference Optimization.
- **PaliGemma/**: PaliGemma model replications.
- **Pix2Pix/**: Image-to-image translation with Pix2Pix.
- **RNNs/**: Recurrent Neural Networks.
- **Seq2Seq/**: Sequence-to-sequence models.
- **SigLip/**: Sigmoid Loss for Language-Image Pretraining.
- **SimplePO/**: Simple Preference Optimization.
- **Transformer/**: Transformer model replications and variants.
- **TTS/**: Text-to-Speech models.
- **VAE/**: Variational Autoencoders.
- **ViT/**: Vision Transformer models.
- **WGANs/**: Wasserstein GANs.
- **Whisper/**: Whisper speech recognition model replications.

## Other Contents

- **generated_audio_final.wav**: Example output from TTS or audio models.
- **README.md**: This file.
- **Errror_Gihtub_Login.txt**: Log or error notes.

## Usage

Each folder is self-contained and includes code, scripts, and sometimes notebooks for replicating the results of the corresponding paper. Please refer to the README or notes within each subfolder for specific instructions.

## Contributing

Feel free to open issues or pull requests if you have suggestions, improvements, or additional replications to add!

## License

This repository is for educational and research purposes. Please check individual folders for any additional license or citation requirements.
# Paper/Architecture Replication  from Scratch Repository using PyTorch 

A repository consisting of paper/architecture replications of classic/SOTA AI/ML papers.



### ðŸ”— Links
[HuggingFace Account](https://huggingface.co/YuvrajSingh9886)


### Authors

- [@YuvrajSingh](https://www.github.com/YuvrajSingh-mist)

