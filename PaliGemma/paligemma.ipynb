{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from torchtune.modules import RMSNorm\n",
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class llavaArgs:\n",
    "    batch_size = 32\n",
    "    device = 'cuda'\n",
    "    vis_embd_out = 768\n",
    "    text_embd_out = 768\n",
    "    vocab_size = 50257\n",
    "    block_size = 256\n",
    "    lr = 1e-3\n",
    "    text_hidden =  768 * 4\n",
    "    img_seq_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading SigLip Vision Encoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPModel, CLIPFeatureExtractor\n",
    "\n",
    "# Vision model class using CLIP\n",
    "class VisionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\").vision_model\n",
    "\n",
    "\n",
    "        self.feature_extractor = CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "        self.multimodalVisionLayerProjector = nn.Linear(in_features=llavaArgs.vis_embd_out, out_features=llavaArgs.text_embd_out, device=llavaArgs.device) # Use proper dimensions\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # inputs = self.feature_extractor(x['image'], return_tensors=\"pt\")\n",
    "        # inputs = inputs.to(llavaArgs.device)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(x)\n",
    "\n",
    "\n",
    "        x = outputs.pooler_output  # Get the pooled image embeddings (shape: [batch_size, 768])\n",
    "\n",
    "\n",
    "        x = self.main(x)\n",
    "        # return x\n",
    "        return self.multimodalVisionLayerProjector(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8718a099f35e4f16a90a9fa3b32169df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ed2fdee5b643d9b2ab631c529cc773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a1b733be4c44a9a7a6ddbe84440265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cdb5fb42e5a44b8b3b37d7f2bbe9384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Language Decoder\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\n",
    "\n",
    "class TextModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", device_map='cuda', torch_dtype='auto', output_hidden_states=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.linear_layer = nn.Linear(in_features=llavaArgs.text_embd_out, out_features=llavaArgs.vocab_size, device=llavaArgs.device, bias=False) # Takes in logits of dimensions- embeds_dims and converts it into dimension of vocab_size (logits in range of vocab_size)\n",
    "\n",
    "\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x, embeds=True):\n",
    "\n",
    "        if(embeds):\n",
    "\n",
    "          x = self.model(inputs_embeds=x).hidden_states[-1]\n",
    "          x = self.linear_layer(x)\n",
    "          return x\n",
    "        else:\n",
    "          x = self.model(input_ids = x['input_ids'], attention_mask = x['attention_mask'])\n",
    "          return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @dataclass\n",
    "# class ModelArgs:\n",
    "#     #Hyperparameters\n",
    "\n",
    "#     block_size = 128\n",
    "#     batch_size = 64\n",
    "#     embeddings_dims = 768\n",
    "#     attn_dropout = 0.1\n",
    "#     no_of_heads = 12 #IMP needs to be thoroughly calculated\n",
    "#     dropout = 0.1\n",
    "#     epochs = 100\n",
    "#     max_lr = 2.5e-4\n",
    "#     no_of_decoder_layers = 12 #IMP needs to be thoroughly calculated\n",
    "#     weight_decay_optim = 0.1\n",
    "#     beta_1 = 0.9\n",
    "#     beta_2 = 0.95\n",
    "#     device = 'cuda'\n",
    "#     no_kv_heads = 2\n",
    "#     vocab_size = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gemma import Gemma\n",
    "\n",
    "# gemma_model = Gemma(embeddings_dims=ModelArgs.embeddings_dims, no_of_decoder_layers=ModelArgs.no_of_decoder_layers, block_size=ModelArgs.block_size, vocab_size=ModelArgs.vocab_size, dropout=ModelArgs.dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class VisionModelArgs:\n",
    "#     embedding_dims = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class TextModelArgs:\n",
    "#     embedding_dims = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PaliGemmaVisionProjector(nn.Module):\n",
    "#     def __init__(self):\n",
    "        \n",
    "#         self.linear = nn.Linear(in_features=VisionModelArgs.embedding_dims, out_features=(TextModelArgs.embedding_dims), device='cuda')\n",
    "#         nn.init.zeros_(self.linear_layer.weight)  # Zero-initialize weights\n",
    "#         nn.init.zeros_(self.linear_layer.bias)  \n",
    "\n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         x = self.linear(x)\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/flickr8000/captions.txt', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40450</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A man in a pink shirt climbs a rock face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40451</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A man is rock climbing high in the air .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40452</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A person in a red shirt climbing up a rock fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40453</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A rock climber in a red shirt .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40454</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A rock climber practices on a rock climbing wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40455 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image  \\\n",
       "0      1000268201_693b08cb0e.jpg   \n",
       "1      1000268201_693b08cb0e.jpg   \n",
       "2      1000268201_693b08cb0e.jpg   \n",
       "3      1000268201_693b08cb0e.jpg   \n",
       "4      1000268201_693b08cb0e.jpg   \n",
       "...                          ...   \n",
       "40450   997722733_0cb5439472.jpg   \n",
       "40451   997722733_0cb5439472.jpg   \n",
       "40452   997722733_0cb5439472.jpg   \n",
       "40453   997722733_0cb5439472.jpg   \n",
       "40454   997722733_0cb5439472.jpg   \n",
       "\n",
       "                                                 caption  \n",
       "0      A child in a pink dress is climbing up a set o...  \n",
       "1                  A girl going into a wooden building .  \n",
       "2       A little girl climbing into a wooden playhouse .  \n",
       "3      A little girl climbing the stairs to her playh...  \n",
       "4      A little girl in a pink dress going into a woo...  \n",
       "...                                                  ...  \n",
       "40450           A man in a pink shirt climbs a rock face  \n",
       "40451           A man is rock climbing high in the air .  \n",
       "40452  A person in a red shirt climbing up a rock fac...  \n",
       "40453                    A rock climber in a red shirt .  \n",
       "40454  A rock climber practices on a rock climbing wa...  \n",
       "\n",
       "[40455 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos>', '<eos>', '<unk>', '<pad>', '<start_of_turn>', '<end_of_turn>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 2151]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.eos_token_id] + tokenizer.encode(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4521, 2961, 1503, 603]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Hello My name is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   2, 4521, 2961, 1503,  603]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello My name is\", return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m ModelArgs\u001b[38;5;241m.\u001b[39mblock_size, (ModelArgs\u001b[38;5;241m.\u001b[39mbatch_size,))   \n\u001b[0;32m----> 2\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m][i:i\u001b[38;5;241m+\u001b[39mModelArgs\u001b[38;5;241m.\u001b[39mblock_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n\u001b[1;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m][i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:i\u001b[38;5;241m+\u001b[39mModelArgs\u001b[38;5;241m.\u001b[39mblock_size\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n\u001b[1;32m      4\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(ModelArgs\u001b[38;5;241m.\u001b[39mdevice), y\u001b[38;5;241m.\u001b[39mto(ModelArgs\u001b[38;5;241m.\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m ix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m ModelArgs\u001b[38;5;241m.\u001b[39mblock_size, (ModelArgs\u001b[38;5;241m.\u001b[39mbatch_size,))   \n\u001b[0;32m----> 2\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mModelArgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n\u001b[1;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m][i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:i\u001b[38;5;241m+\u001b[39mModelArgs\u001b[38;5;241m.\u001b[39mblock_size\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n\u001b[1;32m      4\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(ModelArgs\u001b[38;5;241m.\u001b[39mdevice), y\u001b[38;5;241m.\u001b[39mto(ModelArgs\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "ix = torch.randint(df.shape[0] - ModelArgs.block_size, (ModelArgs.batch_size,))   \n",
    "x = torch.stack([df.shape[0][i:i+ModelArgs.block_size] for i in ix])\n",
    "y = torch.stack([df.shape[0][i+1:i+ModelArgs.block_size+1] for i in ix])\n",
    "x, y = x.to(ModelArgs.device), y.to(ModelArgs.device)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "train_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=224, width=224),\n",
    "        A.CenterCrop(height=224, width=224),\n",
    "        # A.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711], max_pixel_value=224.0,),\n",
    "        # A.ToFloat(max_value=224),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_tyransforms = A.Compose(\n",
    "    [\n",
    "        # A.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711], max_pixel_value=224.0,),\n",
    "        # A.ToFloat(max_value=224),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "IMAGE_TOKEN = \"<image>\"\n",
    "\n",
    "tokens_to_add = {\"additional_special_tokens\": [IMAGE_TOKEN]}\n",
    "tokenizer.add_special_tokens(tokens_to_add)\n",
    "\n",
    "#IMP!!!\n",
    "tokenizer.add_bos_token = False\n",
    "tokenizer.add_eos_token = False\n",
    "        \n",
    "class PaliGemmaDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super(PaliGemmaDataset, self).__init__(\n",
    "        )\n",
    "        \n",
    "        self.dir = os.listdir('data/flickr8000/images')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = 128\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        txt, img = df.iloc[idx][0], df.iloc[idx][1]\n",
    "        \n",
    "        img_path = os.path.join(self.path, img)\n",
    "        # print(img_path)\n",
    "        img = np.array(Image.open(img_path))\n",
    "\n",
    "        input_transformed = train_transforms(image = img)['image']\n",
    "        \n",
    "        prompt = \"Explain the different components of the picture.\"\n",
    "        prompt_and_captions = self.tokenizer.encode(\"<image>\") * llavaArgs.img_seq_len + self.tokenizer.encode(prompt) + self.tokenizer.encode('<bos>')   + self.tokenizer.encode('<sep>') + self.tokenizer.encode(txt) + self.tokenizer.encode('<eos>')\n",
    "        temp =  self.tokenizer.encode(\"<image>\") * llavaArgs.img_seq_len + self.self.tokenizer.encode(prompt) + self.tokenizer.encode('<bos>') + ['<sep>'] + self.tokenizer.encode(txt) + self.tokenizer.encode('<eos>')\n",
    "        final_text = prompt_and_captions\n",
    "        if(len(final_text) < self.block_size):\n",
    "            for i in range(self.block_size - len(prompt_and_captions)):\n",
    "                final_text += self.tokenizer.encode('<pad>')\n",
    "                \n",
    "        sep_index = 0\n",
    "        # print(torch.Tensor(final_text))\n",
    "        \n",
    "        \n",
    "        for i in range(len(temp)):\n",
    "            if(temp[i] == ['<sep>']):\n",
    "                sep_index = i\n",
    "        \n",
    "        print(prompt_and_captions)\n",
    "        print(sep_index)   \n",
    "        # ix = torch.randint(df.shape[0] - ModelArgs.block_size, (ModelArgs.batch_size,))   \n",
    "        x = torch.stack([prompt_and_captions[i:i+llavaArgs.block_size] for i in range(len(prompt_and_captions) - sep_index)])\n",
    "        y = torch.stack([prompt_and_captions[i+1:i+llavaArgs.block_size+1] for i in range(len(prompt_and_captions) - sep_index)])\n",
    "        x, y = x.to(llavaArgs.device), y.to(llavaArgs.device)\n",
    "        # return x, y\n",
    "\n",
    "  \n",
    "        # text_embeddings = gemma_model(torch.Tensor(final_text))\n",
    "        # image_embeddings = siglip_vision(img)\n",
    "        # final_embds = image_embeddings + x\n",
    "        \n",
    "        x_values = {\n",
    "            'input_ids' : x,\n",
    "            'img_embeddings' : input_transformed,\n",
    "            \n",
    "        }\n",
    "        y_values = {\n",
    "            \"input_ids\": y\n",
    "        }\n",
    "        \n",
    "        x_values = {key: torch.tensor(value) for key, value in x_values.items()}\n",
    "        y_values = {key: torch.tensor(value) for key, value in y_values.items()}\n",
    "        \n",
    "        return x_values, y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an instance of the dataset class\n",
    "dataset = PaliGemmaDataset()\n",
    "\n",
    "# Assuming 'dataset' is already created\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "import os\n",
    "#Creating a dataloader\n",
    "# Create DataLoaders for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=ModelArgs.batch_size, shuffle=True, pin_memory=False, num_workers=os.cpu_count())\n",
    "val_loader = DataLoader(val_dataset, batch_size=ModelArgs.batch_size, shuffle=True, pin_memory=False, num_workers=os.cpu_count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_333157/2273889209.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  txt, img = df.iloc[idx][0], df.iloc[idx][1]\n",
      "/tmp/ipykernel_333157/2273889209.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  txt, img = df.iloc[idx][0], df.iloc[idx][1]\n",
      "/tmp/ipykernel_333157/2273889209.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  txt, img = df.iloc[idx][0], df.iloc[idx][1]\n",
      "/tmp/ipykernel_333157/2273889209.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  txt, img = df.iloc[idx][0], df.iloc[idx][1]\n",
      "/tmp/ipykernel_333157/2273889209.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  txt, img = df.iloc[idx][0], df.iloc[idx][1]\n",
      "/tmp/ipykernel_333157/2273889209.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  txt, img = df.iloc[idx][0], df.iloc[idx][1]\n",
      "/tmp/ipykernel_333157/2273889209.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  txt, img = df.iloc[idx][0], df.iloc[idx][1]\n",
      "/tmp/ipykernel_333157/2273889209.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  txt, img = df.iloc[idx][0], df.iloc[idx][1]\n",
      "/tmp/ipykernel_333157/2273889209.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  txt, img = df.iloc[idx][0], df.iloc[idx][1]\n",
      "/tmp/ipykernel_333157/2273889209.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  txt, img = df.iloc[idx][0], df.iloc[idx][1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235284, 235310, 235274, 235304, 235310, 235308, 235304, 235284, 235304, 235298, 235266, 235308, 235304, 1256, 235308, 235249, 546, 235310, 235265, 6001, 2, 1][2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235315, 235321, 235308, 235276, 235318, 235324, 235276, 235274, 235315, 235298, 235324, 235276, 235308, 1039, 235310, 235250, 235310, 830, 235265, 6001, 2, 1][2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235304, 235274, 235324, 235276, 235321, 235276, 235284, 235324, 235315, 235324, 235298, 235304, 235260, 235321, 235308, 235274, 3864, 235310, 235324, 235308, 235265, 6001, 2, 1][2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235304, 235310, 235310, 235308, 235310, 235284, 235321, 235304, 235318, 235324, 235298, 235284, 235308, 2070, 761, 1039, 235324, 235308, 235265, 6001, 2, 1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_333157/2273889209.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  txt, img = df.iloc[idx][0], df.iloc[idx][1]\n",
      "/tmp/ipykernel_333157/2273889209.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  txt, img = df.iloc[idx][0], df.iloc[idx][1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235304, 235318, 235274, 235304, 235276, 235304, 235276, 235324, 235304, 235276, 235298, 235276, 235268, 235284, 235321, 235268, 235276, 235324, 235315, 2070, 235265, 6001, 2, 1][2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235304, 235284, 235310, 235315, 235321, 235315, 235274, 235321, 235324, 235310, 235298, 235318, 235250, 235276, 235315, 235276, 1029, 235276, 235315, 235324, 235265, 6001, 2, 1]\n",
      "\n",
      "[2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235304, 235308, 235284, 235284, 235315, 235321, 235315, 235315, 235274, 235318, 235298, 235266, 235284, 235276, 235304, 235274, 235315, 830, 235308, 235315, 235265, 6001, 2, 1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_333157/2273889209.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  txt, img = df.iloc[idx][0], df.iloc[idx][1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235304, 235276, 235274, 235310, 235324, 235324, 235304, 235304, 235308, 235324, 235298, 235266, 235318, 235318, 11203, 235276, 235315, 235284, 235315, 235276, 235265, 6001, 2, 1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_333157/2273889209.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  txt, img = df.iloc[idx][0], df.iloc[idx][1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235274, 235276, 235284, 235284, 235315, 235324, 235308, 235324, 235284, 235321, 235298, 235324, 235308, 235308, 235274, 235308, 235284, 235304, 235321, 235258, 235321, 235265, 6001, 2, 1][2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235284, 235321, 235324, 235324, 235308, 235276, 235304, 235321, 235274, 235274, 235298, 235310, 235249, 235304, 235274, 235274, 235284, 235308, 235304, 546, 235265, 6001, 2, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_333157/2273889209.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  txt, img = df.iloc[idx][0], df.iloc[idx][1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "00\n",
      "[2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235284, 235321, 235315, 235321, 235321, 235274, 235276, 235318, 235304, 235318, 235298, 235321, 235310, 18563, 235308, 235260, 235276, 235268, 235318, 235304, 235265, 6001, 2, 1]0[2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235304, 235308, 235304, 235284, 235276, 235315, 235321, 235315, 235315, 235315, 235298, 235310, 235249, 235276, 235324, 235250, 235276, 235250, 235274, 235324, 235249, 235265, 6001, 2, 1]\n",
      "\n",
      "00\n",
      "000\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235284, 235304, 235308, 235304, 235276, 235321, 235321, 235310, 235274, 235284, 235298, 235308, 235249, 235308, 235321, 235276, 235310, 235260, 235318, 235266, 235308, 235265, 6001, 2, 1]\n",
      "0[2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235284, 235304, 235324, 235310, 235284, 235310, 235324, 235304, 235321, 235284, 235298, 235276, 235284, 235304, 235250, 235321, 235318, 235268, 235315, 557, 235265, 6001, 2, 1]0\n",
      "0[2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235304, 235304, 235308, 235276, 235284, 235318, 235276, 235274, 235274, 235284, 235298, 235266, 9359, 235310, 235324, 761, 235318, 235268, 235284, 235265, 6001, 2, 1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_333157/2273889209.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  txt, img = df.iloc[idx][0], df.iloc[idx][1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235304, 235310, 235310, 235304, 235276, 235304, 235276, 235315, 235310, 235284, 235298, 235266, 235310, 235276, 235315, 235308, 235321, 235318, 235284, 235308, 235321, 235265, 6001, 2, 1]\n",
      "\n",
      "\n",
      "[2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235284, 235274, 235276, 235274, 235274, 235284, 235321, 235315, 235318, 235304, 235298, 182877, 235321, 235268, 235284, 235250, 235276, 235258, 235324, 235265, 6001, 2, 1][2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235304, 235310, 235318, 235276, 235308, 235308, 235274, 235324, 235284, 235321, 235298, 235318, 235304, 235284, 235308, 235308, 159622, 235274, 235321, 235265, 6001, 2, 1]\n",
      "\n",
      "[2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235284, 235321, 235321, 235274, 235310, 235318, 235321, 235276, 235315, 235308, 235298, 235258, 235310, 532, 235321, 235260, 235276, 235260, 235308, 235284, 235265, 6001, 2, 1][2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235284, 235318, 235308, 235276, 235310, 235321, 235308, 235324, 235321, 235276, 235298, 235284, 235315, 235258, 235321, 235315, 235284, 235318, 235321, 235258, 235324, 235265, 6001, 2, 1][2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235304, 235308, 235315, 235276, 235308, 235308, 235324, 235315, 235318, 235315, 235298, 235258, 235276, 235284, 235324, 235276, 235258, 235308, 235274, 235321, 235268, 235265, 6001, 2, 1][2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235284, 235324, 235321, 235276, 235276, 235284, 235315, 235310, 235324, 235298, 235304, 9543, 235284, 235284, 235250, 235284, 9359, 235318, 235265, 6001, 2, 1]0\n",
      "\n",
      "0[2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235304, 235308, 235304, 235321, 235284, 235274, 235304, 235321, 235324, 235276, 235298, 235315, 235321, 235308, 235318, 235250, 235324, 235318, 235268, 235284, 235250, 235265, 6001, 2, 1][2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235304, 235276, 235324, 235276, 235284, 235324, 235310, 235318, 235308, 235321, 235298, 11886, 235304, 235315, 9543, 235310, 235266, 235321, 235310, 235265, 6001, 2, 1]\n",
      "\n",
      "000\n",
      "0[2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235308, 235308, 235274, 235304, 235308, 235284, 235315, 235276, 235298, 235315, 2907, 235308, 235260, 235310, 557, 235304, 235265, 6001, 2, 1]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235304, 235318, 235324, 235304, 235310, 235321, 235310, 235318, 235304, 235321, 235298, 177156, 235321, 235324, 235284, 235315, 235308, 1039, 235265, 6001, 2, 1]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0\n",
      "000[2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235284, 235315, 235284, 235276, 235304, 235276, 235308, 235304, 235276, 235276, 235298, 235250, 235308, 235268, 235274, 235268, 235284, 235304, 235284, 235315, 235250, 235265, 6001, 2, 1][2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235310, 235321, 235276, 235284, 235276, 235276, 235308, 235308, 235310, 235298, 235318, 235274, 235308, 235308, 235249, 235315, 235258, 86410, 235265, 6001, 2, 1]0000\n",
      "\n",
      "[2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235310, 235310, 235315, 235284, 235321, 235324, 235321, 235324, 235276, 235298, 235266, 235274, 235324, 18563, 235321, 235284, 235308, 235258, 235324, 235265, 6001, 2, 1][2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235284, 235276, 235318, 235310, 235324, 235321, 235276, 235318, 235310, 235308, 235298, 235321, 235266, 235284, 235321, 235250, 235274, 235308, 235284, 235315, 235266, 235265, 6001, 2, 1][2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235274, 235308, 235276, 235276, 235321, 235308, 235304, 235304, 235276, 235308, 235298, 235276, 235274, 235308, 235276, 235318, 235274, 235308, 532, 235315, 235265, 6001, 2, 1]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0[2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235304, 235284, 235310, 235324, 235276, 235308, 235284, 235304, 235274, 235315, 235298, 1335, 235321, 6715, 235274, 235315, 235321, 235304, 235265, 6001, 2, 1]0[2, 74198, 573, 2167, 8832, 576, 573, 5642, 235265, 2, 2, 2, 235322, 17062, 235313, 2, 235284, 235310, 235274, 235308, 235284, 235318, 235308, 235321, 235284, 235308, 235298, 18563, 1039, 235276, 235260, 235321, 235308, 235308, 235318, 235265, 6001, 2, 1]000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "00\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/yuvrajsingh/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/yuvrajsingh/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/home/yuvrajsingh/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/utils/data/dataset.py\", line 419, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"/home/yuvrajsingh/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/utils/data/dataset.py\", line 419, in <listcomp>\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"/tmp/ipykernel_333157/2273889209.py\", line 37, in __getitem__\n    x = torch.stack([prompt_and_captions[i:i+ModelArgs.block_size] for i in range(len(prompt_and_captions) - sep_index)])\nTypeError: expected Tensor as element 0 in argument 0, but got list\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(sample)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/_utils.py:705\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/yuvrajsingh/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/yuvrajsingh/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/home/yuvrajsingh/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/utils/data/dataset.py\", line 419, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"/home/yuvrajsingh/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/utils/data/dataset.py\", line 419, in <listcomp>\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"/tmp/ipykernel_333157/2273889209.py\", line 37, in __getitem__\n    x = torch.stack([prompt_and_captions[i:i+ModelArgs.block_size] for i in range(len(prompt_and_captions) - sep_index)])\nTypeError: expected Tensor as element 0 in argument 0, but got list\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(train_loader))\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaliGemma(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.vision_encoder = SiglipVisionModel()\n",
    "        self.text_decoder = Gemma()\n",
    "        self.linear_proj = PaliGemmaVisionProjector()\n",
    "        \n",
    "    def forward(self, img, text):\n",
    "        vision_embd = self.vision_encoder(img)\n",
    "        vision_embd = self.linear(vision_embd)\n",
    "        text_embd = self.text_decoder(text)\n",
    "        combined = vision_embd + text_embd\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
