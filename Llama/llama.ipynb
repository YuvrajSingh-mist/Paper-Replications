{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from torchtune.modules import RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    #Hyperparameters\n",
    "\n",
    "    block_size = 128\n",
    "    batch_size = 16\n",
    "    embeddings_dims = 256\n",
    "    attn_dropout = 0.1\n",
    "    no_of_heads = 32 #IMP needs to be thoroughly calculated\n",
    "    dropout = 0.1\n",
    "    epochs = 100\n",
    "    max_lr = 3e-4\n",
    "    no_of_decoder_layers = 32 #IMP needs to be thoroughly calculated\n",
    "    weight_decay_optim = 0.1\n",
    "    beta_1 = 0.9\n",
    "    beta_2 = 0.95\n",
    "    clip = 1.0\n",
    "    device = 'cpu'\n",
    "    no_kv_heads = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMENorm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims\n",
    "    ):\n",
    "        self.rmsnorm_layer = RMSNorm(dim=embeddings_dims)\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(x, self):\n",
    "        \n",
    "        x = self.rmsnorm_layer(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class RotaryEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "        block_size: int = ModelArgs.block_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeddings_dims = embeddings_dims\n",
    "        self.block_size = block_size\n",
    "        self.matrix = torch.zeros((self.block_size, self.embeddings_dims, self.embeddings_dims), device=ModelArgs.device, requires_grad=False)\n",
    "        self.theta = 0  \n",
    "        self.init_matrix(self.block_size)\n",
    "        \n",
    "        print(\"MATRXO: \", self.matrix)\n",
    "        \n",
    "    def init_matrix(self, seq_len):\n",
    "            for pos in range(seq_len):\n",
    "                for j in range(1, self.embeddings_dims // 2):\n",
    "                    self.theta = 10000 ** (-2*(pos-1) / self.embeddings_dims)\n",
    "                    self.matrix[pos, 2*j + 1, 2*j + 1] = np.cos((pos*self.theta))\n",
    "                    self.matrix[pos, 2*j + 1, j + 1] = -np.sin((pos* self.theta))\n",
    "                    self.matrix[pos, 2*j , 2*j ] = -np.cos((pos* self.theta))\n",
    "                    self.matrix[pos, 2*j + 1, 2*j + 1] = np.sin((pos* self.theta))\n",
    "                    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # B,T,C = x.shape\n",
    "        print(\"MATRIX:\",x)\n",
    "        if(x > self.block_size):\n",
    "            return self.init_matrix(x)\n",
    "            \n",
    "        else:\n",
    "            return self.init_matrix(self.block_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryAttentionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "        no_of_heads: int = ModelArgs.no_of_heads,\n",
    "        attn_dropout: int = ModelArgs.attn_dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.head_size = embeddings_dims // no_of_heads\n",
    "        self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=ModelArgs.device, bias=False)\n",
    "        self.key = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=ModelArgs.device, bias=False)\n",
    "        self.value = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=ModelArgs.device, bias=False)\n",
    "        self.rotary_matrix = RotaryEmbeddings(embeddings_dims=embeddings_dims)\n",
    "        self.dropout = nn.Dropout(p = ModelArgs.attn_dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        batch, block_size, embeddings_dims = x.shape\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        values = self.value(x)\n",
    "        matrix = self.rotary_matrix(block_size)\n",
    "        \n",
    "        masked = torch.tril(torch.ones((block_size, block_size), device=ModelArgs.device, requires_grad=False))\n",
    "        rotary_query = matrix @ torch.transpose(query, dim0=1, dim1=0) # (B,T,C ) @ (B,T,C,C) -> (B,C,T)\n",
    "        rotary_key = matrix @ torch.transpose(key, dim0=1, dim1=0) # (B,T,C ) @ (B,T,C,C) -> (B,C,T)\n",
    "        weights = rotary_query @ (torch.transpose(rotary_key, dim0=-2, dim1=-1))\n",
    "        weights_masked = weights.masked_fill(masked == 0, float('-inf'))\n",
    "        scaled_weights = weights_masked / (torch.sqrt(self.head_size))\n",
    "        scaled_weights = F.softmax(scaled_weights, dim=-1)\n",
    "        value = scaled_weights @ values\n",
    "        out = self.dropout(value)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MQA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        no_of_kv_heads: int = ModelArgs.no_of_heads,\n",
    "        no_of_heads: int = ModelArgs.no_of_heads\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.no_of_kv_heads = no_of_kv_heads\n",
    "        self.no_of_q_heads = no_of_heads // no_of_kv_heads\n",
    "        self.head_size = embeddings_dims // self.no_of_q_heads\n",
    "        self.rotary_matrix = RotaryEmbeddings(embeddings_dims=embeddings_dims)\n",
    "        # self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=ModelArgs.device, bias=False)\n",
    "        self.key = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=ModelArgs.device, bias=False)\n",
    "        self.value = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=ModelArgs.device, bias=False)\n",
    "        self.dropout = nn.Dropout(p = ModelArgs.attn_dropout)\n",
    "        self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=ModelArgs.device, bias=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def scaled_dot_product(self, q, k, v, block_size):\n",
    "            \n",
    "            masked = torch.tril(torch.ones((block_size, block_size), device=ModelArgs.device, requires_grad=False))\n",
    "            matrix = self.rotary_matrix(block_size)\n",
    "            \n",
    "            rotary_query = matrix @ torch.transpose(q, dim0=1, dim1=0)\n",
    "            rotary_key = matrix @ torch.transpose(k, dim0=1, dim1=0)\n",
    "            weights = rotary_query @ (torch.transpose(rotary_key, dim0=-2, dim1=-1))\n",
    "            weights_masked = weights.masked_fill(masked == 0, float('-inf'))\n",
    "            scaled_weights = weights_masked / (k.shape[-1] ** -0.5)\n",
    "            scaled_weights = F.softmax(scaled_weights, dim=-1)\n",
    "            value = scaled_weights @ v\n",
    "            return value\n",
    "    \n",
    "    def forward(self,x):\n",
    "        print(\"MQA: \", x.shape)\n",
    "        batch, block_size, embeddings_dims = x.shape\n",
    "        multi_query = nn.ModuleList([nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=ModelArgs.device, bias=False) for _ in range(self.no_of_q_heads)])\n",
    "        # query = self.query(x)\n",
    "            \n",
    "        key = self.key(x)\n",
    "        values = self.value(x)\n",
    "        # matrix = self.rotary_matrix(block_size)\n",
    "        # self.mqa = nn.ModuleList([\n",
    "           \n",
    "        # ])\n",
    "        multi_query_concat = torch.cat([self.scaled_dot_product(query(x), key, values, block_size=block_size) for query in multi_query], dim=-1)\n",
    "        # linear_layer_query = self.linear_layer(multi_query_concat)\n",
    "        # masked = torch.tril(torch.ones((block_size, block_size), device=ModelArgs.device, requires_grad=False))\n",
    "        # rotary_query = matrix @ torch.transpose(query, dim0=1, dim1=0) # (B,T,C ) @ (B,T,C,C) -> (B,C,T)\n",
    "        # rotary_key = matrix @ torch.transpose(key, dim0=1, dim1=0) # (B,T,C ) @ (B,T,C,C) -> (B,C,T)\n",
    "        # print(multi_query_concat.shape)\n",
    "        # print(key.shape)\n",
    "        # print(linear_layer_query.shape)\n",
    "        # weights = linear_layer_query @ (torch.transpose(key, dim0=-2, dim1=-1))\n",
    "        # weights_masked = weights.masked_fill(masked == 0, float('-inf'))\n",
    "        # scaled_weights = weights_masked / (key.shape[-1] ** -0.5)\n",
    "        # scaled_weights = F.softmax(scaled_weights, dim=-1)\n",
    "        # value = scaled_weights @ values\n",
    "        \n",
    "        linear_layer= self.linear_layer(multi_query_concat)\n",
    "        out = self.dropout(linear_layer)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GQA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        no_of_q_heads: int = ModelArgs.no_of_heads,\n",
    "        no_of_kv_heads: int = ModelArgs.no_kv_heads\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.head_size = embeddings_dims // no_of_q_heads\n",
    "        # self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=ModelArgs.device, bias=False)\n",
    "        self.no_of_kv_heads = no_of_kv_heads\n",
    "        self.no_of_q_heads = no_of_q_heads\n",
    "        self.key = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=ModelArgs.device, bias=False)\n",
    "        self.value = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=ModelArgs.device, bias=False)\n",
    "        self.dropout = nn.Dropout(p = ModelArgs.attn_dropout)\n",
    "        self.linear_layer = nn.Linear(in_features=embeddings_dims * self.no_of_kv_heads, out_features=self.head_size, device=ModelArgs.device, bias=False)\n",
    "        \n",
    "    # def scaled_dot_product(self, q, k, v, block_size):\n",
    "            \n",
    "    #         masked = torch.tril(torch.ones((block_size, block_size), device=ModelArgs.device, requires_grad=False))\n",
    "    #         weights = q @ (torch.transpose(k, dim0=-2, dim1=-1))\n",
    "    #         weights_masked = weights.masked_fill(masked == 0, float('-inf'))\n",
    "    #         scaled_weights = weights_masked / (k.shape[-1] ** -0.5)\n",
    "    #         scaled_weights = F.softmax(scaled_weights, dim=-1)\n",
    "    #         value = scaled_weights @ v\n",
    "    #         return value\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        batch, block_size, embeddings_dims = x.shape\n",
    "        mqa = nn.ModuleList([MQA(embeddings_dims=embeddings_dims, block_size=block_size) for _ in range(self.no_of_kv_heads)])\n",
    "        # query = self.query(x)\n",
    "        # key = self.key(x)\n",
    "        # values = self.value(x)\n",
    "        # matrix = self.rotary_matrix(block_size)\n",
    "        grouped_query_concat = torch.cat([group(x) for group in mqa], dim=-1)\n",
    "        # linear_layer_query = self.linear_layer(multi_query_concat)\n",
    "        # masked = torch.tril(torch.ones((block_size, block_size), device=ModelArgs.device, requires_grad=False))\n",
    "        # rotary_query = matrix @ torch.transpose(query, dim0=1, dim1=0) # (B,T,C ) @ (B,T,C,C) -> (B,C,T)\n",
    "        # rotary_key = matrix @ torch.transpose(key, dim0=1, dim1=0) # (B,T,C ) @ (B,T,C,C) -> (B,C,T)\n",
    "        # print(multi_query_concat.shape)\n",
    "        # print(key.shape)\n",
    "        # print(linear_layer_query.shape)\n",
    "        print(grouped_query_concat.shape)\n",
    "        linear_layer= self.linear_layer(grouped_query_concat)\n",
    "        out = self.dropout(linear_layer)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATRXO:  tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -1.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -0.5403,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.8415,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.5403,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.8415]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.2863,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.9581,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.2863,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.9581]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -0.9999,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0167,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.9999,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0167]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -0.9999,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0156,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.9999,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0156]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -0.9999,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0147,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.9999,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0147]]])\n",
      "MATRXO:  tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -1.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -0.5403,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.8415,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.5403,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.8415]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.2863,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.9581,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.2863,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.9581]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -0.9999,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0167,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.9999,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0167]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -0.9999,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0156,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.9999,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0156]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -0.9999,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0147,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.9999,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0147]]])\n",
      "MQA:  torch.Size([16, 128, 256])\n",
      "MATRIX: 128\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for @: 'NoneType' and 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m mqa \u001b[38;5;241m=\u001b[39m GQA()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# input_data = torch.tensor()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mmqa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m res\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[110], line 39\u001b[0m, in \u001b[0;36mGQA.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m mqa \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([MQA(embeddings_dims\u001b[38;5;241m=\u001b[39membeddings_dims, block_size\u001b[38;5;241m=\u001b[39mblock_size) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_of_kv_heads)])\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# query = self.query(x)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# key = self.key(x)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# values = self.value(x)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# matrix = self.rotary_matrix(block_size)\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m grouped_query_concat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([group(x) \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m mqa], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# linear_layer_query = self.linear_layer(multi_query_concat)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# masked = torch.tril(torch.ones((block_size, block_size), device=ModelArgs.device, requires_grad=False))\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# rotary_query = matrix @ torch.transpose(query, dim0=1, dim1=0) # (B,T,C ) @ (B,T,C,C) -> (B,C,T)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# print(key.shape)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# print(linear_layer_query.shape)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(grouped_query_concat\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[110], line 39\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m mqa \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([MQA(embeddings_dims\u001b[38;5;241m=\u001b[39membeddings_dims, block_size\u001b[38;5;241m=\u001b[39mblock_size) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_of_kv_heads)])\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# query = self.query(x)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# key = self.key(x)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# values = self.value(x)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# matrix = self.rotary_matrix(block_size)\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m grouped_query_concat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m mqa], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# linear_layer_query = self.linear_layer(multi_query_concat)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# masked = torch.tril(torch.ones((block_size, block_size), device=ModelArgs.device, requires_grad=False))\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# rotary_query = matrix @ torch.transpose(query, dim0=1, dim1=0) # (B,T,C ) @ (B,T,C,C) -> (B,C,T)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# print(key.shape)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# print(linear_layer_query.shape)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(grouped_query_concat\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[109], line 48\u001b[0m, in \u001b[0;36mMQA.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(x)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# matrix = self.rotary_matrix(block_size)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# self.mqa = nn.ModuleList([\u001b[39;00m\n\u001b[1;32m     46\u001b[0m    \n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# ])\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m multi_query_concat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaled_dot_product(query(x), key, values, block_size\u001b[38;5;241m=\u001b[39mblock_size) \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m multi_query], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# linear_layer_query = self.linear_layer(multi_query_concat)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# masked = torch.tril(torch.ones((block_size, block_size), device=ModelArgs.device, requires_grad=False))\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# rotary_query = matrix @ torch.transpose(query, dim0=1, dim1=0) # (B,T,C ) @ (B,T,C,C) -> (B,C,T)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# scaled_weights = F.softmax(scaled_weights, dim=-1)\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# value = scaled_weights @ values\u001b[39;00m\n\u001b[1;32m     62\u001b[0m linear_layer\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_layer(multi_query_concat)\n",
      "Cell \u001b[0;32mIn[109], line 48\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(x)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# matrix = self.rotary_matrix(block_size)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# self.mqa = nn.ModuleList([\u001b[39;00m\n\u001b[1;32m     46\u001b[0m    \n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# ])\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m multi_query_concat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m multi_query], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# linear_layer_query = self.linear_layer(multi_query_concat)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# masked = torch.tril(torch.ones((block_size, block_size), device=ModelArgs.device, requires_grad=False))\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# rotary_query = matrix @ torch.transpose(query, dim0=1, dim1=0) # (B,T,C ) @ (B,T,C,C) -> (B,C,T)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# scaled_weights = F.softmax(scaled_weights, dim=-1)\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# value = scaled_weights @ values\u001b[39;00m\n\u001b[1;32m     62\u001b[0m linear_layer\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_layer(multi_query_concat)\n",
      "Cell \u001b[0;32mIn[109], line 27\u001b[0m, in \u001b[0;36mMQA.scaled_dot_product\u001b[0;34m(self, q, k, v, block_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m masked \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtril(torch\u001b[38;5;241m.\u001b[39mones((block_size, block_size), device\u001b[38;5;241m=\u001b[39mModelArgs\u001b[38;5;241m.\u001b[39mdevice, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m     26\u001b[0m matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_matrix(block_size)\n\u001b[0;32m---> 27\u001b[0m rotary_query \u001b[38;5;241m=\u001b[39m \u001b[43mmatrix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m rotary_key \u001b[38;5;241m=\u001b[39m matrix \u001b[38;5;241m@\u001b[39m torch\u001b[38;5;241m.\u001b[39mtranspose(k, dim0\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dim1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     29\u001b[0m weights \u001b[38;5;241m=\u001b[39m rotary_query \u001b[38;5;241m@\u001b[39m (torch\u001b[38;5;241m.\u001b[39mtranspose(rotary_key, dim0\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, dim1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for @: 'NoneType' and 'Tensor'"
     ]
    }
   ],
   "source": [
    "\n",
    "random_data = torch.randn((ModelArgs.batch_size, ModelArgs.block_size, ModelArgs.embeddings_dims))\n",
    "mqa = GQA()\n",
    "# input_data = torch.tensor()\n",
    "res = mqa(random_data)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 512])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked = torch.tril(torch.ones((ModelArgs.block_size, ModelArgs.block_size), device=ModelArgs.device, requires_grad=False))\n",
    "masked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1877103268.py, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[27], line 23\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class KVCache:\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims: int =  ModelArgs.embeddings_dims,\n",
    "        block_size: int  = ModelArgs.block_size,\n",
    "        no_of_decoder_layers: int =ModelArgs.no_of_decoder_layers\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.head_size = embeddings_dims / no_of_decoder_layers\n",
    "        self.k_cache = torch.ones((block_size, embeddings_dims, self.head_size), device=ModelArgs.device, requires_grad=False)\n",
    "        self.v_cache = torch.ones((block_size, embeddings_dims, self.head_size), device=ModelArgs.device, requires_grad=False)\n",
    "        self.block_size = block_size,\n",
    "        self.embeddings_dims = embeddings_dims\n",
    "    def update(\n",
    "        self,\n",
    "        k: torch.tensor,\n",
    "        v: torch.tensor\n",
    "    ):\n",
    "        self.k_cache[:self.block_size, :self.block_size] = k\n",
    "        self.v_cache = v\n",
    "        \n",
    "    def get(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
