{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvrajsingh/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from torchtune.modules import RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    #Hyperparameters\n",
    "\n",
    "    block_size = 128\n",
    "    batch_size = 16\n",
    "    embeddings_dims = 256\n",
    "    attn_dropout = 0.1\n",
    "    no_of_heads = 32 #IMP needs to be thoroughly calculated\n",
    "    dropout = 0.1\n",
    "    epochs = 100\n",
    "    max_lr = 3e-4\n",
    "    no_of_decoder_layers = 32 #IMP needs to be thoroughly calculated\n",
    "    weight_decay_optim = 0.1\n",
    "    beta_1 = 0.9\n",
    "    beta_2 = 0.95\n",
    "    clip = 1.0\n",
    "    device = 'cpu'\n",
    "    no_kv_heads = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMENorm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims\n",
    "    ):\n",
    "        self.rmsnorm_layer = RMSNorm(dim=embeddings_dims)\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(x, self):\n",
    "        \n",
    "        x = self.rmsnorm_layer(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class RotaryEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        batch_size: int = ModelArgs.batch_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeddings_dims = embeddings_dims\n",
    "        self.block_size = block_size\n",
    "        self.batch_size = batch_size\n",
    "        self.theta = 0  \n",
    "        self.init_matrix(self.block_size)\n",
    "        \n",
    "        # print(\"MATRXO: \", self.rotatory_matrix)\n",
    "        \n",
    "    def init_matrix(self, seq_len):\n",
    "            self.matrix = torch.zeros((seq_len, self.embeddings_dims, self.embeddings_dims), device=ModelArgs.device, requires_grad=False)\n",
    "            for pos in range(seq_len):\n",
    "                for j in range(1, self.embeddings_dims // 2):\n",
    "                    self.theta = 10000 ** (-2*(pos-1) / self.embeddings_dims)\n",
    "                    self.matrix[pos, 2*j + 1, 2*j + 1] = np.cos((pos*self.theta))\n",
    "                    self.matrix[pos, 2*j + 1, j + 1] = -np.sin((pos* self.theta))\n",
    "                    self.matrix[pos, 2*j , 2*j ] = -np.cos((pos* self.theta))\n",
    "                    self.matrix[pos, 2*j + 1, 2*j + 1] = np.sin((pos* self.theta))\n",
    "            return self.matrix\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # B,T,C = x.shape\n",
    "        print(\"MATRIX:\",x)\n",
    "        if(x > self.block_size):\n",
    "            matrix = self.init_matrix(x)\n",
    "            return matrix\n",
    "        else:\n",
    "            matrix = self.init_matrix(self.block_size)\n",
    "            \n",
    "            return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATRIX: 128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 256, 256])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rot = RotaryEmbeddings()\n",
    "res = rot(128)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryAttentionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "        no_of_heads: int = ModelArgs.no_of_heads,\n",
    "        attn_dropout: int = ModelArgs.attn_dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.head_size = embeddings_dims // no_of_heads\n",
    "        self.query = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=ModelArgs.device, bias=False)\n",
    "        self.key = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=ModelArgs.device, bias=False)\n",
    "        self.value = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=ModelArgs.device, bias=False)\n",
    "        self.rotary_matrix = RotaryEmbeddings(embeddings_dims=embeddings_dims)\n",
    "        self.dropout = nn.Dropout(p = ModelArgs.attn_dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # print(x.shape)\n",
    "        batch, block_size, embeddings_dims = x.shape\n",
    "        query = self.query(x)\n",
    "        # print(query)\n",
    "        key = self.key(x)\n",
    "        values = self.value(x)\n",
    "        matrix = self.rotary_matrix(block_size)\n",
    "        \n",
    "        # print(matrix.shape)\n",
    "        # print(query.shape)\n",
    "        masked = torch.tril(torch.ones((block_size, block_size), device=ModelArgs.device, requires_grad=False))\n",
    "        rotary_query = matrix @ query.permute(1,2,0) # (B,T, C,C) @ (B,T,C) -> (B,C,T) = (B,T,C,T)\n",
    "        rotary_key = matrix @ key.permute(1,2,0)  #  (B,T, C,C  ) @ (B,T,C) -> (B,C,T) = (B,T,C,T)\n",
    "        weights = rotary_query.permute(2,0,1) @ rotary_key.permute(2,0,1).transpose(-2, -1)#(B,T,C,T) @ (B,T,C,T) = (T,C,C,T)\n",
    "        weights_masked = weights.masked_fill(masked == 0, float('-inf'))\n",
    "        scaled_weights = weights_masked / (torch.sqrt(torch.tensor(key.shape[-1])))\n",
    "        scaled_weights = F.softmax(scaled_weights, dim=-1)\n",
    "        value = scaled_weights @ values\n",
    "        out = self.dropout(value)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATRIX: 128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128, 256])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rh = RotaryAttentionHead()\n",
    "random_data = torch.randn((ModelArgs.batch_size, ModelArgs.block_size, ModelArgs.embeddings_dims))\n",
    "res = rh(random_data)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MQA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        no_of_kv_heads: int = ModelArgs.no_of_heads,\n",
    "        no_of_heads: int = ModelArgs.no_of_heads\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.no_of_kv_heads = no_of_kv_heads\n",
    "        self.no_of_q_heads = no_of_heads // no_of_kv_heads\n",
    "        self.head_size = embeddings_dims // self.no_of_q_heads\n",
    "        self.rotary_matrix = RotaryEmbeddings(embeddings_dims=embeddings_dims)\n",
    "        # self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=ModelArgs.device, bias=False)\n",
    "        self.key = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=ModelArgs.device, bias=False)\n",
    "        self.value = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=ModelArgs.device, bias=False)\n",
    "        self.dropout = nn.Dropout(p = ModelArgs.attn_dropout)\n",
    "        self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=ModelArgs.device, bias=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def scaled_dot_product(self, q, k, v, block_size, matrix):\n",
    "            \n",
    "            masked = torch.tril(torch.ones((block_size, block_size), device=ModelArgs.device, requires_grad=False))\n",
    "            # print(\"Before: \")\n",
    "            # print(q.shape)\n",
    "            # print(torch.transpose(q, dim0=-2, dim1=-1).shape)\n",
    "            # print(matrix.shape)\n",
    "            # print(k.shape)\n",
    "            # print(torch.transpose(k, dim0=-2, dim1=-1).shape)\n",
    "            # rotary_query = matrix @ torch.transpose(q, dim0=-2, dim1=-1)\n",
    "            # rotary_key = matrix @ torch.transpose(k, dim0=-2, dim1=-1)\n",
    "            # print(\"After: \")\n",
    "            # print(q.shape)\n",
    "            # print(matrix.shape)\n",
    "            # print(k.shape)\n",
    "            masked = torch.tril(torch.ones((block_size, block_size), device=ModelArgs.device, requires_grad=False))\n",
    "            rotary_query = matrix @ q.permute(1,2,0) # (B,T, C,C) @ (B,T,C) -> (B,C,T) = (B,T,C,T)\n",
    "            rotary_key = matrix @ k.permute(1,2,0)  #  (B,T, C,C  ) @ (B,T,C) -> (B,C,T) = (B,T,C,T)\n",
    "            weights = rotary_query.permute(2,0,1) @ rotary_key.permute(2,0,1).transpose(-2, -1)#(B,T,C,T) @ (B,T,C,T) = (T,C,C,T)\n",
    "            weights_masked = weights.masked_fill(masked == 0, float('-inf'))\n",
    "            scaled_weights = weights_masked / (torch.sqrt(torch.tensor(k.shape[-1])))\n",
    "            scaled_weights = F.softmax(scaled_weights, dim=-1)\n",
    "            value = scaled_weights @ v\n",
    "            out = self.dropout(value)\n",
    "            return value\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # print(\"MQA: \", x.shape)\n",
    "        batch, block_size, embeddings_dims = x.shape\n",
    "        multi_query = nn.ModuleList([nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=ModelArgs.device, bias=False) for _ in range(self.no_of_q_heads)])\n",
    "        # query = self.query(x)\n",
    "        matrix = self.rotary_matrix(block_size)\n",
    "            \n",
    "\n",
    "        key = self.key(x)\n",
    "        values = self.value(x)\n",
    "        # rotary_query = matrix @ torch.transpose(, dim0=1, dim1=0)\n",
    "        # rotary_key = matrix @ torch.transpose(key, dim0=1, dim1=0)\n",
    "        # matrix = self.rotary_matrix(block_size)\n",
    "        # self.mqa = nn.ModuleList([\n",
    "           \n",
    "        # ])\n",
    "        multi_query_concat = torch.cat([self.scaled_dot_product(query(x), key, values, block_size, matrix) for query in multi_query], dim=-1)\n",
    "        # linear_layer_query = self.linear_layer(multi_query_concat)\n",
    "        # masked = torch.tril(torch.ones((block_size, block_size), device=ModelArgs.device, requires_grad=False))\n",
    "        # rotary_query = matrix @ torch.transpose(query, dim0=1, dim1=0) # (B,T,C ) @ (B,T,C,C) -> (B,C,T)\n",
    "        # rotary_key = matrix @ torch.transpose(key, dim0=1, dim1=0) # (B,T,C ) @ (B,T,C,C) -> (B,C,T)\n",
    "        # print(multi_query_concat.shape)\n",
    "        # print(key.shape)\n",
    "        # print(linear_layer_query.shape)\n",
    "        # weights = linear_layer_query @ (torch.transpose(key, dim0=-2, dim1=-1))\n",
    "        # weights_masked = weights.masked_fill(masked == 0, float('-inf'))\n",
    "        # scaled_weights = weights_masked / (key.shape[-1] ** -0.5)\n",
    "        # scaled_weights = F.softmax(scaled_weights, dim=-1)\n",
    "        # value = scaled_weights @ values\n",
    "        \n",
    "        linear_layer= self.linear_layer(multi_query_concat)\n",
    "        out = self.dropout(linear_layer)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GQA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        no_of_q_heads: int = ModelArgs.no_of_heads,\n",
    "        no_of_kv_heads: int = ModelArgs.no_kv_heads\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self.head_size = embeddings_dims // no_of_q_heads\n",
    "        # self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=ModelArgs.device, bias=False)\n",
    "        self.no_of_kv_heads = no_of_kv_heads\n",
    "        self.no_of_q_heads = no_of_q_heads\n",
    "        # self.key = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=ModelArgs.device, bias=False)\n",
    "        # self.value = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=ModelArgs.device, bias=False)\n",
    "        self.dropout = nn.Dropout(p = ModelArgs.attn_dropout)\n",
    "        self.linear_layer = nn.Linear(in_features=embeddings_dims * self.no_of_kv_heads, out_features=embeddings_dims, device=ModelArgs.device, bias=False)\n",
    "        \n",
    "    # def scaled_dot_product(self, q, k, v, block_size):\n",
    "            \n",
    "    #         masked = torch.tril(torch.ones((block_size, block_size), device=ModelArgs.device, requires_grad=False))\n",
    "    #         weights = q @ (torch.transpose(k, dim0=-2, dim1=-1))\n",
    "    #         weights_masked = weights.masked_fill(masked == 0, float('-inf'))\n",
    "    #         scaled_weights = weights_masked / (k.shape[-1] ** -0.5)\n",
    "    #         scaled_weights = F.softmax(scaled_weights, dim=-1)\n",
    "    #         value = scaled_weights @ v\n",
    "    #         return value\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        batch, block_size, embeddings_dims = x.shape\n",
    "        mqa = nn.ModuleList([MQA(embeddings_dims=embeddings_dims, block_size=block_size) for _ in range(self.no_of_kv_heads)])\n",
    "        # query = self.query(x)\n",
    "        # key = self.key(x)\n",
    "        # values = self.value(x)\n",
    "        # matrix = self.rotary_matrix(block_size)\n",
    "        grouped_query_concat = torch.cat([group(x) for group in mqa], dim=-1)\n",
    "        # linear_layer_query = self.linear_layer(multi_query_concat)\n",
    "        # masked = torch.tril(torch.ones((block_size, block_size), device=ModelArgs.device, requires_grad=False))\n",
    "        # rotary_query = matrix @ torch.transpose(query, dim0=1, dim1=0) # (B,T,C ) @ (B,T,C,C) -> (B,C,T)\n",
    "        # rotary_key = matrix @ torch.transpose(key, dim0=1, dim1=0) # (B,T,C ) @ (B,T,C,C) -> (B,C,T)\n",
    "        # print(multi_query_concat.shape)\n",
    "        # print(key.shape)\n",
    "        # print(linear_layer_query.shape)\n",
    "        # print(grouped_query_concat.shape)     \n",
    "        linear_layer= self.linear_layer(grouped_query_concat)\n",
    "        out = self.dropout(linear_layer)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATRIX: 128\n",
      "MATRIX: 128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128, 256])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "random_data = torch.randn((ModelArgs.batch_size, ModelArgs.block_size, ModelArgs.embeddings_dims))\n",
    "gqa = GQA()\n",
    "# input_data = torch.tensor()\n",
    "res = gqa(random_data)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 512])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked = torch.tril(torch.ones((ModelArgs.block_size, ModelArgs.block_size), device=ModelArgs.device, requires_grad=False))\n",
    "masked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1877103268.py, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[27], line 23\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class KVCache:\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims: int =  ModelArgs.embeddings_dims,\n",
    "        block_size: int  = ModelArgs.block_size,\n",
    "        no_of_decoder_layers: int =ModelArgs.no_of_decoder_layers\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.head_size = embeddings_dims / no_of_decoder_layers\n",
    "        self.k_cache = torch.ones((block_size, embeddings_dims, self.head_size), device=ModelArgs.device, requires_grad=False)\n",
    "        self.v_cache = torch.ones((block_size, embeddings_dims, self.head_size), device=ModelArgs.device, requires_grad=False)\n",
    "        self.block_size = block_size,\n",
    "        self.embeddings_dims = embeddings_dims\n",
    "    def update(\n",
    "        self,\n",
    "        k: torch.tensor,\n",
    "        v: torch.tensor\n",
    "    ):\n",
    "        self.k_cache[:self.block_size, :self.block_size] = k\n",
    "        self.v_cache = v\n",
    "        \n",
    "    def get(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
