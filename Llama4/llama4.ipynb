{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T23:36:08.133880Z",
     "iopub.status.busy": "2025-04-25T23:36:08.133005Z",
     "iopub.status.idle": "2025-04-25T23:36:21.854008Z",
     "shell.execute_reply": "2025-04-25T23:36:21.853412Z",
     "shell.execute_reply.started": "2025-04-25T23:36:08.133846Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install torchtune\n",
    "# !pip install torchao\n",
    "# !pip install wandb\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import tqdm \n",
    "from dataclasses import dataclass\n",
    "from torchtune.modules import RMSNorm\n",
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data.distributed import DistributedSampler \n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, concatenate_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T23:36:21.855802Z",
     "iopub.status.busy": "2025-04-25T23:36:21.855283Z",
     "iopub.status.idle": "2025-04-25T23:36:27.694715Z",
     "shell.execute_reply": "2025-04-25T23:36:27.693927Z",
     "shell.execute_reply.started": "2025-04-25T23:36:21.855771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"API_KEY\")\n",
    "\n",
    "wandb.login(key=secret_value_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T23:36:27.696050Z",
     "iopub.status.busy": "2025-04-25T23:36:27.695541Z",
     "iopub.status.idle": "2025-04-25T23:36:27.700170Z",
     "shell.execute_reply": "2025-04-25T23:36:27.699471Z",
     "shell.execute_reply.started": "2025-04-25T23:36:27.696028Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def setup(rank=None, world_size=None):\n",
    "    # os.environ['MASTER_ADDR'] = 'localhost' \n",
    "    # os.environ['MASTER_PORT'] = '12355'  \n",
    "    init_process_group(\"nccl\")\n",
    "\n",
    "def cleanup():\n",
    "    destroy_process_group()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-04-28 16:41:18--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  2.88MB/s    in 0.4s    \n",
      "\n",
      "2025-04-28 16:41:18 (2.88 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Collab setup\n",
    "from pathlib import Path\n",
    "data_path = Path('data')\n",
    "data_path.mkdir(exist_ok=True)\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "!cp input.txt data/input.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T23:36:41.595508Z",
     "iopub.status.busy": "2025-04-25T23:36:41.595228Z",
     "iopub.status.idle": "2025-04-25T23:36:43.033296Z",
     "shell.execute_reply": "2025-04-25T23:36:43.032451Z",
     "shell.execute_reply.started": "2025-04-25T23:36:41.595485Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9179a62c331045a1b12edf4f6e559038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a60fa7570654789b3508f58ddd5c3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b2977d82c9404d8e1b50c1146a2043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f1022abd024e2bafd9e1539fab73e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-70b-hf\", token='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')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T23:36:43.035013Z",
     "iopub.status.busy": "2025-04-25T23:36:43.034482Z",
     "iopub.status.idle": "2025-04-25T23:36:43.052804Z",
     "shell.execute_reply": "2025-04-25T23:36:43.052044Z",
     "shell.execute_reply.started": "2025-04-25T23:36:43.034991Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    #Hyperparameters\n",
    "\n",
    "    block_size = 256\n",
    "    batch_size = 64\n",
    "    embeddings_dims = 512\n",
    "    attn_dropout = 0.1\n",
    "    no_of_heads = 8 #IMP needs to be thoroughly calculated\n",
    "    dropout = 0.1\n",
    "    epochs = 100\n",
    "    max_lr = 2.5e-4\n",
    "    no_of_decoder_layers = 6 #IMP needs to be thoroughly calculated\n",
    "    weight_decay_optim = 0.1\n",
    "    beta_1 = 0.9\n",
    "    beta_2 = 0.95\n",
    "    device = 'cuda:0'\n",
    "    no_kv_heads = 2\n",
    "    scaling_factor = 0.5\n",
    "    vocab_size = len(tokenizer.get_vocab()) + 768\n",
    "    base_freq=10000\n",
    "    s = 1.0\n",
    "    experts=16\n",
    "    top_experts=1\n",
    "    noisy_topk = True\n",
    "    use_checkpointing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-25T23:36:27.769110Z",
     "iopub.status.idle": "2025-04-25T23:36:27.769363Z",
     "shell.execute_reply": "2025-04-25T23:36:27.769262Z",
     "shell.execute_reply.started": "2025-04-25T23:36:27.769250Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Datasets\n",
    "\n",
    "# Using tinyshakespeare\n",
    "\n",
    "with open('data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(model):\n",
    "    ckp = model.module.state_dict()\n",
    "    torch.save(ckp, \"checkpoint.pt\")\n",
    "    print(\"Checkpoint saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Subword level tokenization\n",
    "\n",
    "#Loading custom trained BPE\n",
    "# Load the tokenizer\n",
    "# tokenizer = Tokenizer.from_file(\"data/bpe_tokenizer_tinyshakespeare_1k.json\")\n",
    "# vocab_size = tokenizer.get_vocab_size()\n",
    "# Encode and decode functions\n",
    "# encode = lambda s: tokenizer.encode(s).ids\n",
    "# decode = lambda l: tokenizer.decode(l)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#Character level tokenization\n",
    "\n",
    "# # here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch: i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - ModelArgs.block_size, (ModelArgs.batch_size,))\n",
    "    x = torch.stack([data[i:i+ModelArgs.block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+ModelArgs.block_size+1] for i in ix])\n",
    "    x, y = x.to(ModelArgs.device), y.to(ModelArgs.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T23:36:46.500121Z",
     "iopub.status.busy": "2025-04-25T23:36:46.499833Z",
     "iopub.status.idle": "2025-04-25T23:36:49.285577Z",
     "shell.execute_reply": "2025-04-25T23:36:49.284740Z",
     "shell.execute_reply.started": "2025-04-25T23:36:46.500100Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tinystories = True\n",
    "fw = False\n",
    "fw_train = None\n",
    "fw_test = None\n",
    "if(tinystories):\n",
    "    \n",
    "    fw_train = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "    fw_test = load_dataset(\"roneneldan/TinyStories\", split=\"validation\")\n",
    "    print(fw_train)\n",
    "    print(fw_test)\n",
    "if(fw):   \n",
    "    fw_train = load_dataset(\"HuggingFaceFW/fineweb\", name=\"sample-10BT\", split=\"train\", streaming=False)\n",
    "    fw_train = fw_train.train_test_split(test_size=0.01)\n",
    "    print(fw_train)\n",
    "    print(fw_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T23:36:49.287100Z",
     "iopub.status.busy": "2025-04-25T23:36:49.286807Z",
     "iopub.status.idle": "2025-04-25T23:36:49.294404Z",
     "shell.execute_reply": "2025-04-25T23:36:49.293650Z",
     "shell.execute_reply.started": "2025-04-25T23:36:49.287070Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prepare_dataset(split, device, batch_size):\n",
    "    print(\"Device is: \", device)\n",
    " \n",
    "    def collate_fn(batch):\n",
    "        # Extract text data\n",
    "        texts = [item [\"text\"] for item in batch]\n",
    "\n",
    "        input_encodings = tokenizer(texts, max_length = ModelArgs.block_size, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        input_encodings[\"labels\"] = input_encodings[\"input_ids\"].clone()  # Use `input_ids` as labels\n",
    "        \n",
    "        input_encodings[\"labels\"][:, :-1] = input_encodings[\"input_ids\"][:, 1:]  # Shift right\n",
    "        input_encodings[\"labels\"][:, -1] = tokenizer.eos_token_id  # Let the last token be end \n",
    "       \n",
    "        return input_encodings\n",
    "\n",
    "  \n",
    "    dataloader = None\n",
    "    if(tinystories):\n",
    "        if(split == 'train'):\n",
    "            data_loader = DataLoader(\n",
    "            fw_train,\n",
    "            # generator=generator,\n",
    "            batch_size=batch_size,\n",
    "             \n",
    "            # sampler=DistributedSampler(fw_train, shuffle=True),\n",
    "            collate_fn=collate_fn,\n",
    "            drop_last=True,\n",
    "            shuffle=False\n",
    "        )\n",
    "        elif(split == 'val'):\n",
    "            data_loader = DataLoader(\n",
    "            fw_test,\n",
    "              \n",
    "            \n",
    "            batch_size=batch_size,\n",
    "            # sampler=DistributedSampler(fw_test, shuffle=True),\n",
    "            collate_fn=collate_fn,\n",
    "            drop_last=True,\n",
    "            shuffle=False\n",
    "        )\n",
    "    elif(fw):\n",
    "        if(split == 'train'):\n",
    "            data_loader = DataLoader(\n",
    "            fw_train['train'],\n",
    "            batch_size=batch_size,\n",
    "            \n",
    "            \n",
    "            sampler=DistributedSampler(fw_train['train'], shuffle=True),\n",
    "            collate_fn=collate_fn,\n",
    "            drop_last=True,\n",
    "            shuffle=False\n",
    "    )\n",
    "        elif(split == 'val'):\n",
    "            data_loader = DataLoader(\n",
    "            fw_train['test'],\n",
    "            batch_size=batch_size,\n",
    "                # generator=generator,\n",
    "            sampler=DistributedSampler(fw_train[\"test\"]),\n",
    "            collate_fn=collate_fn,\n",
    "              \n",
    "            drop_last=True,\n",
    "            shuffle=False\n",
    "        )\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T23:36:49.295639Z",
     "iopub.status.busy": "2025-04-25T23:36:49.295336Z",
     "iopub.status.idle": "2025-04-25T23:36:49.313803Z",
     "shell.execute_reply": "2025-04-25T23:36:49.313162Z",
     "shell.execute_reply.started": "2025-04-25T23:36:49.295614Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# from andrej karapathy github\n",
    "def topk_sampling(model, prompt, device, max_length=50, top_k=50, temperature=1.0):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    generated_tokens = []\n",
    "    ModelArgs.inference=True\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad(), torch.autocast(device_type=ModelArgs.device, dtype=torch.bfloat16):\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs[:, -1, :]\n",
    "            \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Top-k filtering\n",
    "            top_k_probs, top_k_indices = torch.topk(probs, top_k, dim=-1)\n",
    "            \n",
    "            \n",
    "            # Apply temperature scaling\n",
    "            probs = probs / temperature\n",
    "            \n",
    "            # Sample from top-k\n",
    "            next_token = torch.multinomial(top_k_probs, num_samples=1)\n",
    "           \n",
    "            \n",
    "            # generated_tokens.append(next_token.item())\n",
    "            \n",
    "            xcol = torch.gather(top_k_indices, -1, next_token)\n",
    "            # generated_tokens.append(xcol)\n",
    "            input_ids = torch.cat([input_ids, xcol], dim=1) #1 because is it the dimension of the sequence\n",
    "            \n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T23:36:49.315089Z",
     "iopub.status.busy": "2025-04-25T23:36:49.314864Z",
     "iopub.status.idle": "2025-04-25T23:36:49.332604Z",
     "shell.execute_reply": "2025-04-25T23:36:49.331961Z",
     "shell.execute_reply.started": "2025-04-25T23:36:49.315074Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Normalization(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims\n",
    "    ):  \n",
    "        super().__init__()\n",
    "        self.rmsnorm_layer = RMSNorm(dim=embeddings_dims)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.rmsnorm_layer(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "        device = ModelArgs.device\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        swish = x * self.sig(x)\n",
    "\n",
    "        return swish\n",
    "\n",
    "\n",
    "\n",
    "class SWiGLUExpertMoE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "        device = ModelArgs.device\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dims = embeddings_dims * 2  #Apply this when memory permits\n",
    "        self.swish = Swish(block_size=block_size, embeddings_dims=embeddings_dims, device=device)\n",
    "        self.linear_layer1 = nn.Linear(in_features=embeddings_dims, out_features=self.hidden_dims,  bias=False, device = device)\n",
    "        self.linear_layer2 = nn.Linear(in_features=embeddings_dims, out_features=self.hidden_dims,  bias=False, device = device)\n",
    "        self.linear_layer3 = nn.Linear(in_features=self.hidden_dims, out_features=embeddings_dims,  bias=False, device = device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        swish_res = self.swish(self.linear_layer1(x))\n",
    "        x_V = self.linear_layer2(x)\n",
    "        res = torch.mul(swish_res, x_V)\n",
    "        out = self.linear_layer3(res)\n",
    "        return out\n",
    "\n",
    "\n",
    "#MoE Layer\n",
    "\n",
    "class MoeLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dropout = ModelArgs.dropout,\n",
    "        embeddings_size = ModelArgs.embeddings_dims,\n",
    "        device = ModelArgs.device,\n",
    "        # inner_dimensional_states: int = 3072\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([SWiGLUExpertMoE() for _ in range(ModelArgs.experts)])\n",
    "        self.gate = nn.Linear(in_features=embeddings_size, out_features=ModelArgs.experts, device=device, bias=False)\n",
    "        self.shared_expert = SWiGLUExpertMoE()\n",
    "        if(ModelArgs.noisy_topk is True and ModelArgs.use_checkpointing == False):\n",
    "            self.noise = nn.Linear(in_features=embeddings_size, out_features=ModelArgs.experts, device=device, bias=False)\n",
    "        # self.outputs = torch.zeros((batch_size,block_size, embeddings_size), device=device) #batch size needs to be defined because we are accessing it explicitly\n",
    "        self.device = device\n",
    "        self.shared_expert_out = None\n",
    "    def forward(self, x):\n",
    "        # mlp_weights_init = self.mlp.apply(weights_init)\n",
    "        self.gate_out = self.gate(x) #[bz, seq, num_experts]\n",
    "        if(ModelArgs.noisy_topk == True and ModelArgs.use_checkpointing == False):\n",
    "            noise = self.noise(x)\n",
    "            gaussian_noise = torch.normal(0, 1, size=self.gate_out.shape, device=self.device)\n",
    "            noisy_router = F.softplus(noise) * gaussian_noise\n",
    "            noisy_router += self.gate_out\n",
    "        else:\n",
    "            noisy_router = self.gate_out\n",
    "        top_k_values, top_k_indices = torch.topk(noisy_router, k=ModelArgs.top_experts) #[bs, seq len, top k]\n",
    "        probs = torch.nn.functional.softmax(top_k_values, dim=-1) #[bs, seq len, top k]\n",
    "\n",
    "        out = 0\n",
    "\n",
    "        out = torch.zeros_like(x)\n",
    "        for expert_idx in range(ModelArgs.experts):\n",
    "            # Create mask for current expert across all top_k positions\n",
    "            expert_mask = (top_k_indices == expert_idx)\n",
    "            \n",
    "            # Sum probabilities for current expert\n",
    "            expert_weights = (probs * expert_mask).sum(dim=-1)  # [batch, seq_len]\n",
    "            \n",
    "            # Get inputs where expert is used\n",
    "            selected = expert_weights > 0\n",
    "            if not selected.any():\n",
    "                continue\n",
    "            self.shared_expert_out += self.shared_expert(x[selected])\n",
    "            # Process all selected inputs through expert\n",
    "            expert_out = self.heads[expert_idx](x[selected]) + self.shared_expert_out\n",
    "            \n",
    "            # Weight and accumulate outputs\n",
    "            out[selected] += expert_out * expert_weights[selected].unsqueeze(-1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T23:36:49.333539Z",
     "iopub.status.busy": "2025-04-25T23:36:49.333318Z",
     "iopub.status.idle": "2025-04-25T23:36:49.347224Z",
     "shell.execute_reply": "2025-04-25T23:36:49.346632Z",
     "shell.execute_reply.started": "2025-04-25T23:36:49.333514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "class RotaryEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "         device,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        batch_size: int = ModelArgs.batch_size,\n",
    "        scaling_factor: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings_dims = embeddings_dims\n",
    "        self.block_size = block_size\n",
    "        self.batch_size = batch_size\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.theta = 0\n",
    "        self.device=device\n",
    "\n",
    "    def apply_rope(self, seq, base_freq):\n",
    "        batch_size, seq_len, embeds_dims = seq.shape\n",
    "        token_indices = torch.arange(0 , seq_len, dtype=torch.float32,  device = self.device).unsqueeze(1)\n",
    "        positions = torch.arange(0 , self.embeddings_dims, 2, dtype=torch.float32,  device = self.device).unsqueeze(0)\n",
    "        theta = base_freq ** (-2 * (positions * self.scaling_factor) / self.embeddings_dims) #Position Interpolation\n",
    "        angles = token_indices * theta\n",
    "        angles = angles.expand(seq_len, -1) # because this thing needs to be applied to every sequence in the batch but with embeds dims halved\n",
    "        x_reshaped = seq.view(batch_size, seq_len, self.embeddings_dims // 2, 2)\n",
    "        \n",
    "        cos_angles = torch.cos(angles)\n",
    "        sin_angles = torch.sin(angles)\n",
    "\n",
    "\n",
    "        out = torch.stack([x_reshaped[..., 0]*cos_angles - (x_reshaped[...,1] * sin_angles), x_reshaped[...,1] * cos_angles + x_reshaped[..., 0] * sin_angles], dim=1)\n",
    "        out = out.view(batch_size, seq_len, embeds_dims)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, base_freq):\n",
    "\n",
    "        res = self.apply_rope(x,base_freq=base_freq)\n",
    "        return res \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = ModelArgs.attn_dropout,\n",
    "        embeddings_dims = ModelArgs.embeddings_dims,\n",
    "        no_of_heads = ModelArgs.no_of_heads,\n",
    "        device = ModelArgs.device\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.head_size = embeddings_dims // no_of_heads\n",
    "        self.no_of_heads = no_of_heads\n",
    "        # if(ModelArgs.use_flash_attention==False):\n",
    "        self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=ModelArgs.device, bias=False)\n",
    "        self.keys = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,device=ModelArgs.device, bias=False)\n",
    "        self.values = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=ModelArgs.device,bias=False)\n",
    "    # self.dropout = nn.Dropout(p = attn_dropout)\n",
    "          \n",
    "        self.dropout = nn.Dropout(p = attn_dropout)\n",
    "        self.device = device\n",
    "       \n",
    "        self.rotary= RotaryEmbeddings(embeddings_dims=self.head_size,  device = device)\n",
    "            \n",
    "    def forward(self, x, rope=False):\n",
    "        batch_size, block_size, embd_dims = x.shape\n",
    "\n",
    "        k = self.keys(x)\n",
    "        q = self.query(x)\n",
    "        v = self.values(x)\n",
    "        if(rope):\n",
    "            q = self.rotary(q)\n",
    "            k = self.rotary(k)\n",
    "        masked_table = torch.tril(torch.ones(block_size, block_size, device=ModelArgs.device))\n",
    "        weights = ( q @ torch.transpose(k, dim0=-2, dim1=-1) * ModelArgs.s * torch.log(q.shape[1])) * (k.shape[-1] ** -0.5)\n",
    "        masked_values = weights.masked_fill(masked_table[: block_size, : block_size] == 0, float('-inf'))\n",
    "        weights_normalized = nn.functional.softmax(masked_values, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
    "        weights_normalized = self.dropout(weights_normalized)\n",
    "        out = weights_normalized @ v\n",
    "        return out\n",
    "         \n",
    "# MHA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = ModelArgs.attn_dropout,\n",
    "        embeddings_dims = ModelArgs.embeddings_dims,\n",
    "        no_of_heads = ModelArgs.no_of_heads,\n",
    "        device = ModelArgs.device\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.no_of_heads = no_of_heads\n",
    "        self.heads = nn.ModuleList([AttentionHead(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads, device=device) for _ in range(no_of_heads)])\n",
    "        self.dropout = nn.Dropout(p = attn_dropout)\n",
    "        self.linear = nn.Linear(in_features=self.no_of_heads * embeddings_dims, out_features=embeddings_dims, device=device, bias=False) # 12 (no of heads) * (batch_size) 64 = 768 -> gives out the text embeddings\n",
    "\n",
    "    def forward(self, x, rope):\n",
    "        concat = torch.cat([head(x, rope=rope) for head in self.heads], dim=-1)\n",
    "        print(concat.shape)\n",
    "        linear_layer = self.linear(concat)\n",
    "        out = self.dropout(linear_layer)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self,\n",
    "                  device,\n",
    "                  embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "                  block_size: int = ModelArgs.block_size,\n",
    "                  vocab_size: int = ModelArgs.vocab_size,\n",
    "                   dropout = ModelArgs.dropout\n",
    "\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims,  dtype=torch.float32,  device = device)\n",
    "        self.linear_layer2 = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims,  dtype=torch.float32, device = device)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = dropout)  # Uncommenting the dropout line\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.linear_layer(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.linear_layer2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout(x)  # Uncommenting the dropout line\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T23:36:53.258606Z",
     "iopub.status.busy": "2025-04-25T23:36:53.258007Z",
     "iopub.status.idle": "2025-04-25T23:36:53.264031Z",
     "shell.execute_reply": "2025-04-25T23:36:53.263147Z",
     "shell.execute_reply.started": "2025-04-25T23:36:53.258583Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                device,\n",
    "                attn_dropout: int  = ModelArgs.attn_dropout,\n",
    "                no_of_heads: int = ModelArgs.no_of_heads,\n",
    "                embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "                dropout = ModelArgs.dropout,\n",
    "                block_size: int = ModelArgs.block_size,\n",
    "                vocab_size: int = ModelArgs.vocab_size,\n",
    "\n",
    "                 ) :\n",
    "        super().__init__()\n",
    "\n",
    "        # self.base_freq = ModelArgs.base_freq\n",
    "        self.feedforward_network = FFN(embeddings_dims=embeddings_dims, block_size=block_size, vocab_size=vocab_size,  device = device)\n",
    "        self.mha = MHA(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads)\n",
    "        self.layer_norm1 = Normalization(embeddings_dims=embeddings_dims)\n",
    "        self.layer_norm2 = Normalization(embeddings_dims=embeddings_dims)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "\n",
    "        self.moe_block = MoeLayer(dropout=dropout, embeddings_size=embeddings_dims)\n",
    "\n",
    "    def forward(self, x, rope, ffn):\n",
    "\n",
    "        x = x + self.mha(self.layer_norm1(x), rope)  #Very important step -> Layer Norm on input and then passes it to the subsequent blocks\n",
    "        if(ffn):\n",
    "            x = x + self.feedforward_network(self.layer_norm2(x))\n",
    "        else:\n",
    "            x = x + self.moe_block(self.layer_norm2(x)) #Very important step\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T23:36:53.581888Z",
     "iopub.status.busy": "2025-04-25T23:36:53.581187Z",
     "iopub.status.idle": "2025-04-25T23:36:53.589867Z",
     "shell.execute_reply": "2025-04-25T23:36:53.589041Z",
     "shell.execute_reply.started": "2025-04-25T23:36:53.581867Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Llama4Scout(nn.Module):\n",
    "    def __init__(self,\n",
    "                    device,\n",
    "                  embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "                  no_of_decoder_layers: int = ModelArgs.no_of_decoder_layers,\n",
    "                  block_size: int = ModelArgs.block_size,\n",
    "                  vocab_size: int = ModelArgs.vocab_size,\n",
    "                  dropout = ModelArgs.dropout\n",
    "\n",
    "                 ) :\n",
    "        super().__init__()\n",
    "        self.base_freq = ModelArgs.base_freq\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embeddings_dims,  dtype=torch.float32,  device = device)\n",
    "        self.decoder = nn.ModuleList(DecoderLayer(embeddings_dims=embeddings_dims, block_size=block_size, vocab_size=vocab_size, dropout=dropout,  device = device) for _ in range(no_of_decoder_layers))\n",
    "        self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=vocab_size,  dtype=torch.float32,  device = device)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        self.norm = Normalization(embeddings_dims)\n",
    "        \n",
    "        \n",
    "        #weight tying\n",
    "        # self.embeddings.weight = self.linear_layer.weight\n",
    "    \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "               \n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "               \n",
    "                     \n",
    "                    \n",
    "    def forward(self, x):\n",
    "        index = 0\n",
    "        no_of_layers = 0\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        # x = self.decoder(x)\n",
    "        for layer in self.decoder:\n",
    "            if no_of_layers % 2 == 0:\n",
    "                x = layer(x, rope=True, ffn=True)\n",
    "                # print(\"x shape: \", x.shape)\n",
    "            else:\n",
    "                \n",
    "                x = layer(x, rope=False, ffn=False)\n",
    "                # print(\"x shape local: \", x.shape)\n",
    "            no_of_layers += 1\n",
    "        # print(x.shape)\n",
    "        x = self.norm(x)\n",
    "        x = self.linear_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Instantiating the model\n",
    "\n",
    "model = Llama4Scout(embeddings_dims=ModelArgs.embeddings_dims, block_size=ModelArgs.block_size, vocab_size=ModelArgs.vocab_size, dropout=ModelArgs.dropout, device=ModelArgs.device, ffn=True)\n",
    "model = model.to(ModelArgs.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Printing a summary of the architecture\n",
    "from torchinfo import summary\n",
    "idx, targets = get_batch('test')\n",
    "idx = idx.to(ModelArgs.device)\n",
    "summary(model=model,\n",
    "        input_data=idx,\n",
    "        # input_size=(ModelArgs.batch_size, ModelArgs.block_size, ModelArgs.embeddings_dims),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-26T05:02:41.309Z",
     "iopub.execute_input": "2025-04-25T23:36:57.005416Z",
     "iopub.status.busy": "2025-04-25T23:36:57.005142Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import tqdm \n",
    "def train():\n",
    "    # Set device to CUDA if available\n",
    "    device = ModelArgs.device\n",
    "    print(f\"Start running training on {device}.\")\n",
    "    \n",
    "    # Initialize wandb for experiment tracking\n",
    "    wandb.init(\n",
    "        project = 'Gemma-Training',\n",
    "        # config = ModelArgs, # you can uncomment this to log model config\n",
    "    )\n",
    "    \n",
    "    # Create model and move to GPU\n",
    "    model = Llama4Scout(embeddings_dims=ModelArgs.embeddings_dims, block_size=ModelArgs.block_size, \n",
    "                  vocab_size=ModelArgs.vocab_size, dropout=ModelArgs.dropout, device=device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    print(\"Model loaded\")\n",
    "    # Setup optimizer\n",
    "    optimizer = torch.optim.AdamW(params=model.parameters(), lr=ModelArgs.max_lr)\n",
    "    \n",
    "    # Training parameters\n",
    "    save_checkpoint_iter = 500\n",
    "    total_iters = 25000\n",
    "    eval_iters = 500\n",
    "\n",
    "    \n",
    "    # Training progress bar\n",
    "    train_epoch_iterator = tqdm.tqdm(range(total_iters), desc=\"Training\")\n",
    "    val_dataloader = prepare_dataset('val', device, ModelArgs.batch_size)\n",
    "    val_iterator = iter(val_dataloader)\n",
    "    # Get batches for training\n",
    "    @torch.inference_mode()\n",
    "    def estimate_loss():\n",
    "        out = {}\n",
    "        model.eval()\n",
    "        count = 0\n",
    "        for split in ['val']:\n",
    "            print(f\"Starting with {split} evaluation...\")\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "\n",
    "                nonlocal val_iterator\n",
    "                \n",
    "                # for k, batch in enumerate(dataloader):\n",
    "                try:\n",
    "                    batch = next(val_iterator)\n",
    "                except StopIteration:\n",
    "                    val_iterator = iter(val_dataloader)\n",
    "                    batch = next(val_iterator)\n",
    "            \n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                targets = batch[\"labels\"].to(device)\n",
    "                \n",
    "                logits = model(input_ids)\n",
    "                batch_size, block_size, embeddings_dims = logits.shape\n",
    "                logits = logits.view(batch_size*block_size, embeddings_dims)\n",
    "                targets = targets.view(batch_size * block_size)\n",
    "                loss = nn.functional.cross_entropy(logits, targets)\n",
    "                losses[k] = loss.item()\n",
    "                # count += 1\n",
    "            out[split] = losses.mean()\n",
    "\n",
    "        model.train()\n",
    "        return out\n",
    "    token_count = 0\n",
    "    # Start training loop\n",
    "    model.train()\n",
    "    print(\"Lessgoo...\")\n",
    "    dataloader = prepare_dataset('train', device, ModelArgs.batch_size)\n",
    "    train_dataloader = iter(dataloader) \n",
    "    accumulated_loss = 0.0\n",
    "    for step in train_epoch_iterator:\n",
    "        # Periodically evaluate loss on train and val sets\n",
    "        if (step % eval_iters == 0 and step != 0) or step == total_iters - 1:\n",
    "            losses = estimate_loss()\n",
    "            avg_val_loss = torch.Tensor([losses['val']]).to(device)\n",
    "            print(f\"step {step}: train loss {accumulated_loss:.4f}, val loss {losses['val']:.4f}\")\n",
    "            val_perplexity = torch.exp(torch.tensor(avg_val_loss)).item()\n",
    "            # Log metrics to wandb\n",
    "            wandb.log({\n",
    "                \"val_perplexity\": val_perplexity,\n",
    "                # \"val_step_loss\": losses['train'],\n",
    "                \"val_step_loss\": losses['val'],\n",
    "                \"step\": step\n",
    "            })\n",
    "            \n",
    "        # Save checkpoint periodically\n",
    "        if step % save_checkpoint_iter == 0 and step != 0:\n",
    "            print(f\"Saving the model checkpoint for step: {step}\")\n",
    "            torch.save(model.state_dict(), \"checkpoint.pt\")\n",
    "            print(\"Checkpoint saved\")\n",
    "        \n",
    "        # Get batch for training step\n",
    "        try:\n",
    "            batch = next(train_dataloader)\n",
    "        except StopIteration:\n",
    "            train_dataloader = iter(dataloader)\n",
    "            batch = next(train_dataloader)\n",
    "            \n",
    "        # for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        targets = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(input_ids)\n",
    "        batch_size, block_size, embeddings_dims = logits.shape\n",
    "        logits = logits.view(batch_size*block_size, embeddings_dims)\n",
    "        targets = targets.view(batch_size * block_size)\n",
    "        loss = nn.functional.cross_entropy(logits, targets)\n",
    "\n",
    "        token_count += (len(input_ids) * ModelArgs.batch_size)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        accumulated_loss = loss.item()\n",
    "        perplexity = torch.exp(torch.tensor(accumulated_loss)).item()  # Calculate perplexity\n",
    "        # if(device == 0):\n",
    "        wandb.log({\n",
    "                    # \"Learning Rate\": scheduler.get_last_lr()[0],\n",
    "                    \"Train_Loss\": accumulated_loss,\n",
    "                    # \"Train loss\": loss.item(),\n",
    "                    \"Train Perplexity\": perplexity,\n",
    "                    \"Total Tokens Processed\": token_count,\n",
    "                    \"Step\": step,\n",
    "                    # \"Gradient Norm\": total_norm_before.item(),\n",
    "                    # \"Epoch\": epoch\n",
    "                    \n",
    "        })\n",
    "        \n",
    "        if(step % eval_iters == 0):\n",
    "                prompt = \"Once upon a time \"\n",
    "                generated_text = topk_sampling(model, prompt, max_length=ModelArgs.block_size, top_k=50, temperature=1.0, device=device)\n",
    "    \n",
    "     \n",
    "                print(f\" Step: {step} | Generated Text: {generated_text}\")\n",
    "\n",
    "    # Finish wandb run\n",
    "    wandb.finish()\n",
    "\n",
    "# Print CUDA device count but won't be using DDP\n",
    "world_size = torch.cuda.device_count()\n",
    "print(f\"CUDA devices available: {world_size}\")\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "mt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
