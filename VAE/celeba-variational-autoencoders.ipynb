{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":37705,"sourceType":"datasetVersion","datasetId":29561}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"7eb22efc","cell_type":"code","source":"# Install required packages\n!pip install tqdm wandb torchinfo\n\n# Core imports\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Progress bar and logging\nfrom tqdm import tqdm\nimport wandb\n\n# Check if CUDA is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(42)\n\nprint(\"All imports successful!\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")","metadata":{"execution":{"iopub.status.busy":"2025-06-15T12:14:44.371843Z","iopub.execute_input":"2025-06-15T12:14:44.372694Z","iopub.status.idle":"2025-06-15T12:14:47.399015Z","shell.execute_reply.started":"2025-06-15T12:14:44.372664Z","shell.execute_reply":"2025-06-15T12:14:47.398305Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\nRequirement already satisfied: torchinfo in /usr/local/lib/python3.11/dist-packages (1.8.0)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nUsing device: cuda\nAll imports successful!\nPyTorch version: 2.6.0+cu124\nCUDA available: True\nCUDA device: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":21},{"id":"0726f14d","cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport glob\nimport os\nfrom torch.utils.data import random_split\n\nclass UnlabeledImageDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.image_paths = glob.glob(os.path.join(root_dir, '*.jpg'))  # More specific pattern for CelebA\n        if not self.image_paths:\n            # Fallback to general pattern if no .jpg files found\n            self.image_paths = glob.glob(os.path.join(root_dir, '*'))\n        \n        self.transform = transform\n        print(f\"Found {len(self.image_paths)} images in {root_dir}\")\n        \n        if len(self.image_paths) == 0:\n            raise ValueError(f\"No images found in directory: {root_dir}\")\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        try:\n            img = Image.open(self.image_paths[idx]).convert('RGB')\n            if self.transform:\n                img = self.transform(img)\n            return img\n        except Exception as e:\n            print(f\"Error loading image {self.image_paths[idx]}: {e}\")\n            # Return a black image as fallback\n            if self.transform:\n                return self.transform(Image.new('RGB', (128, 128), color='black'))\n            else:\n                return Image.new('RGB', (128, 128), color='black')\n\n# Path to folder where images are present\nimage_dir = '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba'\n\n# Enhanced transforms with normalization for better training\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n    # Normalize to [-1, 1] range (optional, but often helps with VAE training)\n    # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\nprint(\"Creating dataset...\")\ndataset = UnlabeledImageDataset(image_dir, transform=transform)\n\n# Split 80% train, 20% val\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\n\nprint(f\"Splitting dataset: {train_size} train, {val_size} validation\")\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n# DataLoaders with better settings for CelebA\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n                         num_workers=2, pin_memory=True, drop_last=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, \n                       num_workers=2, pin_memory=True, drop_last=True)\n\nprint(f\"DataLoaders created:\")\nprint(f\"  Train batches: {len(train_loader)}\")\nprint(f\"  Validation batches: {len(val_loader)}\")\n\n# Test the data loader\nprint(\"Testing data loader...\")\nfor imgs in val_loader:\n    print(f\"Batch shape: {imgs.shape}\")  # Should be (batch_size, 3, 128, 128)\n    print(f\"Data type: {imgs.dtype}\")\n    print(f\"Value range: [{imgs.min().item():.3f}, {imgs.max().item():.3f}]\")\n    break\n\nprint(\"Data loading setup complete!\")\n","metadata":{"execution":{"iopub.status.busy":"2025-06-15T12:14:47.400826Z","iopub.execute_input":"2025-06-15T12:14:47.401202Z","iopub.status.idle":"2025-06-15T12:14:49.493245Z","shell.execute_reply.started":"2025-06-15T12:14:47.401177Z","shell.execute_reply":"2025-06-15T12:14:49.492452Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Creating dataset...\nFound 202599 images in /kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba\nSplitting dataset: 162079 train, 40520 validation\nDataLoaders created:\n  Train batches: 5064\n  Validation batches: 1266\nTesting data loader...\nBatch shape: torch.Size([32, 3, 128, 128])\nData type: torch.float32\nValue range: [0.000, 1.000]\nData loading setup complete!\n","output_type":"stream"}],"execution_count":22},{"id":"a89a2c43-3e3a-4d87-8498-80a4ab97f86a","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"36e62711-f756-4578-a994-9632825f8d54","cell_type":"code","source":"# !wandb login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"API_KEY\")\n\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2025-06-15T12:14:49.494207Z","iopub.execute_input":"2025-06-15T12:14:49.494489Z","iopub.status.idle":"2025-06-15T12:14:49.652126Z","shell.execute_reply.started":"2025-06-15T12:14:49.494463Z","shell.execute_reply":"2025-06-15T12:14:49.651370Z"},"trusted":true},"outputs":[],"execution_count":23},{"id":"f04ddc40-9eed-434e-a82f-adc8dea015dc","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"645ffb32-0f44-4e43-9c21-c32bd0cdef5a","cell_type":"code","source":"wandb.login(key=secret_value_0)","metadata":{"execution":{"iopub.status.busy":"2025-06-15T12:14:49.653719Z","iopub.execute_input":"2025-06-15T12:14:49.654216Z","iopub.status.idle":"2025-06-15T12:14:49.660993Z","shell.execute_reply.started":"2025-06-15T12:14:49.654196Z","shell.execute_reply":"2025-06-15T12:14:49.660347Z"},"trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":24},{"id":"1ae37ae6","cell_type":"code","source":"# Initialize wandb\nwandb.init(\n    project=\"vae-mnist\",\n    config={\n        \"learning_rate\": 0.0005,\n        \"epochs\": 200,\n        \"batch_size\": 64,\n        \"input_dim\": 1,\n        \"hidden_dim\": 'CelebA6',\n        \"latent_dim\": 2,\n        \"dataset\": \"MNIST\",\n        \"architecture\": \"Variational Autoencoder\",\n        \"reconstruction_weight\": 0.1,\n        \"kl_weight\": 0.5\n    }\n)","metadata":{"execution":{"iopub.status.busy":"2025-06-15T12:14:49.661746Z","iopub.execute_input":"2025-06-15T12:14:49.662550Z","iopub.status.idle":"2025-06-15T12:14:56.569294Z","shell.execute_reply.started":"2025-06-15T12:14:49.662532Z","shell.execute_reply":"2025-06-15T12:14:56.568684Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>step</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>train_kl_loss</td><td>▄▁▂▃▃▃▅▄▆▆▆▄▇▆▇▆▇▇▆▆▇▇▇▇▇▆▇▆▇▇██▇▇▇▇▇▇█▇</td></tr><tr><td>train_loss</td><td>▇█▆▆▅▃▃▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▂▁▂▂▂▂▂▂▁▂▂▂▁▁▂▂▂▁</td></tr><tr><td>train_reconstruction_loss</td><td>█▆▅▂▃▃▃▃▂▁▂▂▂▁▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▂▂▁▁▁▂▁▁</td></tr><tr><td>val_kl_loss</td><td>▂▂▃▄▅▆▃▅▅▆▇▂▇▇▂▁▁▂▃▄▇█▁▃▃▇▃▄▅▇▄▆██▂▃▃▆▆█</td></tr><tr><td>val_loss</td><td>▂▄▆▆▇▃▄▄▄▇█▂▄▆▁▂▄▁▂▂▆▃▄▅▁▃▄▄▁▅▂▂▂▂▂▄▁▂▃▅</td></tr><tr><td>val_reconstruction_loss</td><td>█▁▁▃▃▄▅▅▃▃▄▂▃▃▂▄▂▂▂▃▃▃▃▁▂▃▄▁▂▃▁▂▂▂▂▃▃▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>step</td><td>50640</td></tr><tr><td>train_kl_loss</td><td>130.59195</td></tr><tr><td>train_loss</td><td>1526.07092</td></tr><tr><td>train_reconstruction_loss</td><td>1395.479</td></tr><tr><td>val_kl_loss</td><td>163583.65462</td></tr><tr><td>val_loss</td><td>928476.13477</td></tr><tr><td>val_reconstruction_loss</td><td>764892.47913</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">copper-brook-31</strong> at: <a href='https://wandb.ai/rentio/vae-mnist/runs/u98m6ean' target=\"_blank\">https://wandb.ai/rentio/vae-mnist/runs/u98m6ean</a><br> View project at: <a href='https://wandb.ai/rentio/vae-mnist' target=\"_blank\">https://wandb.ai/rentio/vae-mnist</a><br>Synced 4 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250615_092537-u98m6ean/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250615_121449-5q74zlm4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/rentio/vae-mnist/runs/5q74zlm4' target=\"_blank\">unique-wave-32</a></strong> to <a href='https://wandb.ai/rentio/vae-mnist' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/rentio/vae-mnist' target=\"_blank\">https://wandb.ai/rentio/vae-mnist</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/rentio/vae-mnist/runs/5q74zlm4' target=\"_blank\">https://wandb.ai/rentio/vae-mnist/runs/5q74zlm4</a>"},"metadata":{}},{"execution_count":25,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/rentio/vae-mnist/runs/5q74zlm4?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7d417af74450>"},"metadata":{}}],"execution_count":25},{"id":"4ca4b7b3","cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, leaky = 0.1):\n        super(Encoder, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(input_dim, hidden_dim, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(hidden_dim),\n            nn.LeakyReLU(leaky),\n            nn.Dropout(0.2),\n            nn.Conv2d(hidden_dim  , hidden_dim * 2 , kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(hidden_dim * 2),\n            nn.LeakyReLU(leaky),\n            nn.Dropout(0.2),\n            nn.Conv2d(hidden_dim * 2, hidden_dim * 2 , kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(hidden_dim * 2),\n            nn.LeakyReLU(leaky),\n            nn.Dropout(0.2),\n            nn.Conv2d(hidden_dim * 2 , hidden_dim * 2 , kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(hidden_dim * 2),\n            nn.LeakyReLU(leaky),\n            nn.Dropout(0.2),\n            nn.Conv2d(hidden_dim * 2 , hidden_dim * 2 , kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(hidden_dim * 2),\n            nn.LeakyReLU(leaky),\n            nn.Dropout(0.2),\n            nn.Conv2d(hidden_dim * 2 , hidden_dim * 2 , kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(hidden_dim * 2),\n            nn.LeakyReLU(leaky),\n            nn.Dropout(0.2),\n            nn.Flatten(),\n        )\n        # self.fc2 = nn.Linear(3136, output_dim)\n\n    def forward(self, x):\n        x = self.conv(x)\n        # x = self.fc2(x)\n        # x = nn.functional.sigmoid(x)\n        return x\n    \n# class Decoder(nn.Module):","metadata":{"execution":{"iopub.status.busy":"2025-06-15T12:14:56.570004Z","iopub.execute_input":"2025-06-15T12:14:56.570182Z","iopub.status.idle":"2025-06-15T12:14:56.576992Z","shell.execute_reply.started":"2025-06-15T12:14:56.570168Z","shell.execute_reply":"2025-06-15T12:14:56.576306Z"},"trusted":true},"outputs":[],"execution_count":26},{"id":"3f6f62c1","cell_type":"code","source":"enc = Encoder(input_dim=3, hidden_dim=128, output_dim=32).to('cuda')\nx = torch.randn(1, 3, 128, 128).to('cuda')  # Example input tensor\n# output = enc(x)\n# print(\"Output shape:\", output.shape)  # Should print the shape of the output tensor\n\nfrom torchinfo import summary\nsummary(enc, (1,3,128,128), device='cuda')  # Print the model summary","metadata":{"execution":{"iopub.status.busy":"2025-06-15T12:14:56.577656Z","iopub.execute_input":"2025-06-15T12:14:56.577850Z","iopub.status.idle":"2025-06-15T12:14:56.624241Z","shell.execute_reply.started":"2025-06-15T12:14:56.577835Z","shell.execute_reply":"2025-06-15T12:14:56.623568Z"},"trusted":true},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nEncoder                                  [1, 16384]                --\n├─Sequential: 1-1                        [1, 16384]                --\n│    └─Conv2d: 2-1                       [1, 128, 128, 128]        3,584\n│    └─BatchNorm2d: 2-2                  [1, 128, 128, 128]        256\n│    └─LeakyReLU: 2-3                    [1, 128, 128, 128]        --\n│    └─Dropout: 2-4                      [1, 128, 128, 128]        --\n│    └─Conv2d: 2-5                       [1, 256, 64, 64]          295,168\n│    └─BatchNorm2d: 2-6                  [1, 256, 64, 64]          512\n│    └─LeakyReLU: 2-7                    [1, 256, 64, 64]          --\n│    └─Dropout: 2-8                      [1, 256, 64, 64]          --\n│    └─Conv2d: 2-9                       [1, 256, 32, 32]          590,080\n│    └─BatchNorm2d: 2-10                 [1, 256, 32, 32]          512\n│    └─LeakyReLU: 2-11                   [1, 256, 32, 32]          --\n│    └─Dropout: 2-12                     [1, 256, 32, 32]          --\n│    └─Conv2d: 2-13                      [1, 256, 16, 16]          590,080\n│    └─BatchNorm2d: 2-14                 [1, 256, 16, 16]          512\n│    └─LeakyReLU: 2-15                   [1, 256, 16, 16]          --\n│    └─Dropout: 2-16                     [1, 256, 16, 16]          --\n│    └─Conv2d: 2-17                      [1, 256, 8, 8]            590,080\n│    └─BatchNorm2d: 2-18                 [1, 256, 8, 8]            512\n│    └─LeakyReLU: 2-19                   [1, 256, 8, 8]            --\n│    └─Dropout: 2-20                     [1, 256, 8, 8]            --\n│    └─Conv2d: 2-21                      [1, 256, 8, 8]            590,080\n│    └─BatchNorm2d: 2-22                 [1, 256, 8, 8]            512\n│    └─LeakyReLU: 2-23                   [1, 256, 8, 8]            --\n│    └─Dropout: 2-24                     [1, 256, 8, 8]            --\n│    └─Flatten: 2-25                     [1, 16384]                --\n==========================================================================================\nTotal params: 2,661,888\nTrainable params: 2,661,888\nNon-trainable params: 0\nTotal mult-adds (Units.GIGABYTES): 2.10\n==========================================================================================\nInput size (MB): 0.20\nForward/backward pass size (MB): 56.10\nParams size (MB): 10.65\nEstimated Total Size (MB): 66.94\n=========================================================================================="},"metadata":{}}],"execution_count":27},{"id":"7d3ee362","cell_type":"code","source":"# x.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T12:14:56.624983Z","iopub.execute_input":"2025-06-15T12:14:56.625192Z","iopub.status.idle":"2025-06-15T12:14:56.628206Z","shell.execute_reply.started":"2025-06-15T12:14:56.625177Z","shell.execute_reply":"2025-06-15T12:14:56.627510Z"}},"outputs":[],"execution_count":28},{"id":"23c8b0dc","cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, leaky = 0.1):\n        super(Decoder, self).__init__()\n        self.linear = nn.Linear(output_dim, 16384)\n        self.conv = nn.Sequential(\n            \n            # Reshape(-1, hidden_dim * 2, 16, 16),\n            nn.ConvTranspose2d(input_dim, 64 , kernel_size=3, stride=1, padding=1),\n            # nn.BatchNorm2d(hidden_dim * 2),\n            nn.LeakyReLU(leaky),\n            nn.Dropout(0.2),\n\n             nn.ConvTranspose2d(64 , 64 ,kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(leaky),\n            nn.Dropout(0.2),\n\n            nn.ConvTranspose2d(64, 64, kernel_size=3, stride=2, padding=1, output_padding = 1),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(leaky),\n            nn.Dropout(0.2),\n            \n            nn.ConvTranspose2d(64, 64 * 2, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.BatchNorm2d(64 * 2),\n            nn.LeakyReLU(leaky),\n            nn.Dropout(0.2),\n            nn.ConvTranspose2d(64 * 2, 64 * 4, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.BatchNorm2d(64 * 4),\n            nn.LeakyReLU(leaky),\n            nn.Dropout(0.2),\n            nn.ConvTranspose2d( 64 * 4, 3, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.BatchNorm2d(3),\n            nn.LeakyReLU(leaky),\n            nn.Dropout(0.2),\n        )\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = x.view(-1, 256, 8, 8)\n        # print(x.shape)\n        x = self.conv(x)\n        x = nn.functional.sigmoid(x)    \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T12:14:56.628841Z","iopub.execute_input":"2025-06-15T12:14:56.629069Z","iopub.status.idle":"2025-06-15T12:14:56.643259Z","shell.execute_reply.started":"2025-06-15T12:14:56.629053Z","shell.execute_reply":"2025-06-15T12:14:56.642729Z"}},"outputs":[],"execution_count":29},{"id":"0d50fc0d","cell_type":"code","source":"summary(Decoder(input_dim=256, hidden_dim=8, output_dim=8).to('cuda'), (1, 8), device='cuda')  # Print the model summary","metadata":{"execution":{"iopub.status.busy":"2025-06-15T12:14:56.645308Z","iopub.execute_input":"2025-06-15T12:14:56.645484Z","iopub.status.idle":"2025-06-15T12:14:56.671323Z","shell.execute_reply.started":"2025-06-15T12:14:56.645471Z","shell.execute_reply":"2025-06-15T12:14:56.670824Z"},"trusted":true},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nDecoder                                  [1, 3, 128, 128]          --\n├─Linear: 1-1                            [1, 16384]                147,456\n├─Sequential: 1-2                        [1, 3, 128, 128]          --\n│    └─ConvTranspose2d: 2-1              [1, 64, 8, 8]             147,520\n│    └─LeakyReLU: 2-2                    [1, 64, 8, 8]             --\n│    └─Dropout: 2-3                      [1, 64, 8, 8]             --\n│    └─ConvTranspose2d: 2-4              [1, 64, 8, 8]             36,928\n│    └─BatchNorm2d: 2-5                  [1, 64, 8, 8]             128\n│    └─LeakyReLU: 2-6                    [1, 64, 8, 8]             --\n│    └─Dropout: 2-7                      [1, 64, 8, 8]             --\n│    └─ConvTranspose2d: 2-8              [1, 64, 16, 16]           36,928\n│    └─BatchNorm2d: 2-9                  [1, 64, 16, 16]           128\n│    └─LeakyReLU: 2-10                   [1, 64, 16, 16]           --\n│    └─Dropout: 2-11                     [1, 64, 16, 16]           --\n│    └─ConvTranspose2d: 2-12             [1, 128, 32, 32]          73,856\n│    └─BatchNorm2d: 2-13                 [1, 128, 32, 32]          256\n│    └─LeakyReLU: 2-14                   [1, 128, 32, 32]          --\n│    └─Dropout: 2-15                     [1, 128, 32, 32]          --\n│    └─ConvTranspose2d: 2-16             [1, 256, 64, 64]          295,168\n│    └─BatchNorm2d: 2-17                 [1, 256, 64, 64]          512\n│    └─LeakyReLU: 2-18                   [1, 256, 64, 64]          --\n│    └─Dropout: 2-19                     [1, 256, 64, 64]          --\n│    └─ConvTranspose2d: 2-20             [1, 3, 128, 128]          6,915\n│    └─BatchNorm2d: 2-21                 [1, 3, 128, 128]          6\n│    └─LeakyReLU: 2-22                   [1, 3, 128, 128]          --\n│    └─Dropout: 2-23                     [1, 3, 128, 128]          --\n==========================================================================================\nTotal params: 745,801\nTrainable params: 745,801\nNon-trainable params: 0\nTotal mult-adds (Units.GIGABYTES): 1.42\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 20.15\nParams size (MB): 2.98\nEstimated Total Size (MB): 23.14\n=========================================================================================="},"metadata":{}}],"execution_count":30},{"id":"c9434576","cell_type":"code","source":"class Autoencoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(Autoencoder, self).__init__()\n        self.encoder = Encoder(input_dim, hidden_dim, output_dim)\n        self.decoder = Decoder(hidden_dim * 2, hidden_dim // 4 , output_dim)\n        self.z_mean = nn.Linear(16384, output_dim, bias=False)\n        self.z_log_var = nn.Linear(16384, output_dim, bias=False)\n        \n    def reparametrize(self, encoded, mean_sampled, log_var_sampled):\n        # Use the same device as the input tensors\n        device = mean_sampled.device\n        epsilon = torch.randn(log_var_sampled.size(0), log_var_sampled.size(1), device=device)\n        \n        # Reparameterization trick: z = μ + σ * ε\n        res = mean_sampled + torch.exp(log_var_sampled / 2.0) * epsilon\n        return res\n        \n    def forward(self, x):\n        encoded = self.encoder(x)\n        sampled_z = self.z_mean(encoded)\n        log_var_sampled_z = self.z_log_var(encoded)\n        z = self.reparametrize(encoded, sampled_z, log_var_sampled_z)\n        decoded = self.decoder(z)\n        return decoded, sampled_z, log_var_sampled_z, z","metadata":{"execution":{"iopub.status.busy":"2025-06-15T12:14:56.671880Z","iopub.execute_input":"2025-06-15T12:14:56.672459Z","iopub.status.idle":"2025-06-15T12:14:56.677728Z","shell.execute_reply.started":"2025-06-15T12:14:56.672438Z","shell.execute_reply":"2025-06-15T12:14:56.677232Z"},"trusted":true},"outputs":[],"execution_count":31},{"id":"22013168","cell_type":"code","source":"autoencoder = Autoencoder(input_dim=3, hidden_dim=128, output_dim=256).to(device)\n\n# Print model summary\nfrom torchinfo import summary\nprint(\"Model Summary:\")\nprint(summary(autoencoder, (32, 3, 128, 128), device=device))\n\n# Count parameters\n# total_params = sum(p.numel() for p in autoencoder.parameters())\n# trainable_params = sum(p.numel() for p in autoencoder.parameters() if p.requires_grad)\n\n# print(f\"\\nTotal parameters: {total_params:,}\")\n# print(f\"Trainable parameters: {trainable_params:,}\")\n# print(f\"Model initialized on device: {next(autoencoder.parameters()).device}\")","metadata":{"execution":{"iopub.status.busy":"2025-06-15T12:14:56.678350Z","iopub.execute_input":"2025-06-15T12:14:56.678576Z","iopub.status.idle":"2025-06-15T12:14:56.857281Z","shell.execute_reply.started":"2025-06-15T12:14:56.678555Z","shell.execute_reply":"2025-06-15T12:14:56.856507Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Model Summary:\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoencoder                              [32, 3, 128, 128]         --\n├─Encoder: 1-1                           [32, 16384]               --\n│    └─Sequential: 2-1                   [32, 16384]               --\n│    │    └─Conv2d: 3-1                  [32, 128, 128, 128]       3,584\n│    │    └─BatchNorm2d: 3-2             [32, 128, 128, 128]       256\n│    │    └─LeakyReLU: 3-3               [32, 128, 128, 128]       --\n│    │    └─Dropout: 3-4                 [32, 128, 128, 128]       --\n│    │    └─Conv2d: 3-5                  [32, 256, 64, 64]         295,168\n│    │    └─BatchNorm2d: 3-6             [32, 256, 64, 64]         512\n│    │    └─LeakyReLU: 3-7               [32, 256, 64, 64]         --\n│    │    └─Dropout: 3-8                 [32, 256, 64, 64]         --\n│    │    └─Conv2d: 3-9                  [32, 256, 32, 32]         590,080\n│    │    └─BatchNorm2d: 3-10            [32, 256, 32, 32]         512\n│    │    └─LeakyReLU: 3-11              [32, 256, 32, 32]         --\n│    │    └─Dropout: 3-12                [32, 256, 32, 32]         --\n│    │    └─Conv2d: 3-13                 [32, 256, 16, 16]         590,080\n│    │    └─BatchNorm2d: 3-14            [32, 256, 16, 16]         512\n│    │    └─LeakyReLU: 3-15              [32, 256, 16, 16]         --\n│    │    └─Dropout: 3-16                [32, 256, 16, 16]         --\n│    │    └─Conv2d: 3-17                 [32, 256, 8, 8]           590,080\n│    │    └─BatchNorm2d: 3-18            [32, 256, 8, 8]           512\n│    │    └─LeakyReLU: 3-19              [32, 256, 8, 8]           --\n│    │    └─Dropout: 3-20                [32, 256, 8, 8]           --\n│    │    └─Conv2d: 3-21                 [32, 256, 8, 8]           590,080\n│    │    └─BatchNorm2d: 3-22            [32, 256, 8, 8]           512\n│    │    └─LeakyReLU: 3-23              [32, 256, 8, 8]           --\n│    │    └─Dropout: 3-24                [32, 256, 8, 8]           --\n│    │    └─Flatten: 3-25                [32, 16384]               --\n├─Linear: 1-2                            [32, 256]                 4,194,304\n├─Linear: 1-3                            [32, 256]                 4,194,304\n├─Decoder: 1-4                           [32, 3, 128, 128]         --\n│    └─Linear: 2-2                       [32, 16384]               4,210,688\n│    └─Sequential: 2-3                   [32, 3, 128, 128]         --\n│    │    └─ConvTranspose2d: 3-26        [32, 64, 8, 8]            147,520\n│    │    └─LeakyReLU: 3-27              [32, 64, 8, 8]            --\n│    │    └─Dropout: 3-28                [32, 64, 8, 8]            --\n│    │    └─ConvTranspose2d: 3-29        [32, 64, 8, 8]            36,928\n│    │    └─BatchNorm2d: 3-30            [32, 64, 8, 8]            128\n│    │    └─LeakyReLU: 3-31              [32, 64, 8, 8]            --\n│    │    └─Dropout: 3-32                [32, 64, 8, 8]            --\n│    │    └─ConvTranspose2d: 3-33        [32, 64, 16, 16]          36,928\n│    │    └─BatchNorm2d: 3-34            [32, 64, 16, 16]          128\n│    │    └─LeakyReLU: 3-35              [32, 64, 16, 16]          --\n│    │    └─Dropout: 3-36                [32, 64, 16, 16]          --\n│    │    └─ConvTranspose2d: 3-37        [32, 128, 32, 32]         73,856\n│    │    └─BatchNorm2d: 3-38            [32, 128, 32, 32]         256\n│    │    └─LeakyReLU: 3-39              [32, 128, 32, 32]         --\n│    │    └─Dropout: 3-40                [32, 128, 32, 32]         --\n│    │    └─ConvTranspose2d: 3-41        [32, 256, 64, 64]         295,168\n│    │    └─BatchNorm2d: 3-42            [32, 256, 64, 64]         512\n│    │    └─LeakyReLU: 3-43              [32, 256, 64, 64]         --\n│    │    └─Dropout: 3-44                [32, 256, 64, 64]         --\n│    │    └─ConvTranspose2d: 3-45        [32, 3, 128, 128]         6,915\n│    │    └─BatchNorm2d: 3-46            [32, 3, 128, 128]         6\n│    │    └─LeakyReLU: 3-47              [32, 3, 128, 128]         --\n│    │    └─Dropout: 3-48                [32, 3, 128, 128]         --\n==========================================================================================\nTotal params: 15,859,529\nTrainable params: 15,859,529\nNon-trainable params: 0\nTotal mult-adds (Units.GIGABYTES): 112.97\n==========================================================================================\nInput size (MB): 6.29\nForward/backward pass size (MB): 2440.17\nParams size (MB): 63.44\nEstimated Total Size (MB): 2509.90\n==========================================================================================\n","output_type":"stream"}],"execution_count":32},{"id":"14a1584c","cell_type":"code","source":"# from torch.utils.data import random_split, DataLoader\n\n# # Define the split sizes\n# train_size = int(0.8 * len(mnist_dataset))\n# val_size = len(mnist_dataset) - train_size\n\n# # Split the dataset\n# train_dataset, val_dataset = random_split(mnist_dataset, [train_size, val_size])\n\n# # Create DataLoaders\n# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n# val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2025-06-15T12:14:56.858068Z","iopub.execute_input":"2025-06-15T12:14:56.858292Z","iopub.status.idle":"2025-06-15T12:14:56.861529Z","shell.execute_reply.started":"2025-06-15T12:14:56.858268Z","shell.execute_reply":"2025-06-15T12:14:56.860918Z"},"trusted":true,"vscode":{"languageId":"julia"}},"outputs":[],"execution_count":33},{"id":"fecd77da-d711-4db3-ad43-4b80afbabc73","cell_type":"code","source":"for i in train_loader:\n    print(i.shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2025-06-15T12:14:56.862230Z","iopub.execute_input":"2025-06-15T12:14:56.862484Z","iopub.status.idle":"2025-06-15T12:14:57.176296Z","shell.execute_reply.started":"2025-06-15T12:14:56.862461Z","shell.execute_reply":"2025-06-15T12:14:57.175492Z"},"trusted":true},"outputs":[{"name":"stdout","text":"torch.Size([32, 3, 128, 128])\n","output_type":"stream"}],"execution_count":34},{"id":"48c8da89","cell_type":"code","source":"# Optional: Save model checkpoints periodically\nimport os\n\ndef save_checkpoint(model, optimizer, epoch, loss, filepath):\n    \"\"\"Save model checkpoint\"\"\"\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss,\n    }\n    torch.save(checkpoint, filepath)\n    print(f\"Checkpoint saved at epoch {epoch}: {filepath}\")\n\ndef load_checkpoint(model, optimizer, filepath):\n    \"\"\"Load model checkpoint\"\"\"\n    if os.path.exists(filepath):\n        checkpoint = torch.load(filepath)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        epoch = checkpoint['epoch']\n        loss = checkpoint['loss']\n        print(f\"Checkpoint loaded: epoch {epoch}, loss {loss}\")\n        return epoch, loss\n    else:\n        print(f\"No checkpoint found at {filepath}\")\n        return 0, float('inf')\n\n# Example usage (uncomment to use):\n# checkpoint_dir = \"checkpoints\"\n# os.makedirs(checkpoint_dir, exist_ok=True)\n# \n# # To save during training (add this inside the epoch loop):\n# if (epoch + 1) % 10 == 0:  # Save every 10 epochs\n#     checkpoint_path = os.path.join(checkpoint_dir, f\"vae_checkpoint_epoch_{epoch+1}.pth\")\n#     save_checkpoint(autoencoder, optimizer, epoch+1, avg_train_loss, checkpoint_path)\n\nprint(\"Checkpoint utilities defined. Add checkpoint saving to training loop if needed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T12:14:57.177323Z","iopub.execute_input":"2025-06-15T12:14:57.177556Z","iopub.status.idle":"2025-06-15T12:14:57.185894Z","shell.execute_reply.started":"2025-06-15T12:14:57.177532Z","shell.execute_reply":"2025-06-15T12:14:57.185170Z"}},"outputs":[{"name":"stdout","text":"Checkpoint utilities defined. Add checkpoint saving to training loop if needed.\n","output_type":"stream"}],"execution_count":35},{"id":"ecfe68d6-a95f-4185-81f8-2a006325f21f","cell_type":"code","source":"!rm -rf /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T12:14:57.186691Z","iopub.execute_input":"2025-06-15T12:14:57.186964Z","iopub.status.idle":"2025-06-15T12:14:57.778290Z","shell.execute_reply.started":"2025-06-15T12:14:57.186922Z","shell.execute_reply":"2025-06-15T12:14:57.777348Z"}},"outputs":[{"name":"stdout","text":"rm: cannot remove '/kaggle/working/': Device or resource busy\n","output_type":"stream"}],"execution_count":36},{"id":"21d1f76f","cell_type":"code","source":"optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.0005)\n# Training the autoencoder\nepochs = 10  # Number of epochs for training\nstep = 0\nfor epoch in range(epochs):\n    # Training phase\n    autoencoder.train()\n    train_loss = 0.0\n   \n    train_recon_loss = 0.0\n    train_kl_loss = 0.0\n    num_batches = 0\n    \n    # Use tqdm to wrap the train_loader directly\n    train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} - Training', leave=False)\n    \n    for data in train_pbar:\n        data = data.to(device)\n        optimizer.zero_grad()\n        \n        output, mu, log_var, z = autoencoder(data)\n        \n        # Reconstruction loss\n        recon_loss = nn.functional.mse_loss(output, data, reduction='none')\n        recon_loss = recon_loss.view(output.size(0), -1).sum(dim=1)\n        recon_loss = recon_loss.mean()\n        \n        # KL divergence loss\n        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), dim=1)\n        kl_loss = kl_loss.mean()\n        \n        # Total loss\n        total_loss = kl_loss + 1.0 * recon_loss\n        \n        total_loss.backward()\n        optimizer.step()\n        \n        train_loss += total_loss.item()\n        train_recon_loss += recon_loss.item()\n        train_kl_loss += kl_loss.item()\n        num_batches += 1\n        step += 1\n        \n        # Update progress bar with current losses\n        train_pbar.set_postfix({\n            'Loss': f'{total_loss.item():.4f}',\n            'Recon': f'{recon_loss.item():.4f}',\n            'KL': f'{kl_loss.item():.4f}'\n        })\n        \n        # Log to wandb\n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"step\": step,\n            \"train_loss\": total_loss.item(),\n            \"train_reconstruction_loss\": recon_loss.item(),\n            \"train_kl_loss\": kl_loss.item(),\n        })\n\n        if (step) % 4000 == 0:  \n            checkpoint_path = os.path.join('/kaggle/working', f\"vae_checkpoint_epoch_{step+1}.pth\")\n            save_checkpoint(autoencoder, optimizer, step+1, train_loss, checkpoint_path)\n    \n    # Calculate average training losses\n    # avg_train_loss = train_loss / num_batches\n    # avg_train_recon = train_recon_loss / num_batches\n    # avg_train_kl = train_kl_loss / num_batches\n    \n    # Validation phase\n    autoencoder.eval()\n    val_loss = 0.0\n    val_recon_loss = 0.0\n    val_kl_loss = 0.0\n    val_batches = 0\n    \n    # Use tqdm for validation loop as well\n    val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{epochs} - Validation', leave=False)\n    \n    with torch.no_grad():\n        for data in val_pbar:\n            data = data.to(device)\n            output, mu, log_var, z = autoencoder(data)\n            \n            # Reconstruction loss\n            recon_loss = nn.functional.mse_loss(output, data, reduction='none')\n            recon_loss = recon_loss.view(output.size(0), -1).sum(dim=1)\n            recon_loss = recon_loss.mean()\n            \n            # KL divergence loss\n            kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), dim=1)\n            kl_loss = kl_loss.mean() \n            \n            # Total loss\n            total_loss = kl_loss + 1.0 * recon_loss\n            \n            val_loss += total_loss.item()\n            val_recon_loss += recon_loss.item()\n            val_kl_loss += kl_loss.item()\n            val_batches += 1\n            \n            # Update validation progress bar\n            val_pbar.set_postfix({\n                'Val Loss': f'{total_loss.item()}',\n                'Val Recon': f'{recon_loss.item()}',\n                'Val KL': f'{kl_loss.item()}'\n            })\n\n            wandb.log({\n                # \"epoch\": epoch + 1,\n                # \"val_loss\": avg_train_loss,\n                # \"val_reconstruction_loss\": avg_train_recon,\n                # \"val_kl_loss\": avg_train_kl,\n                \"val_loss\": val_loss,\n                \"val_reconstruction_loss\": val_recon_loss,\n                \"val_kl_loss\": val_kl_loss\n            })\n\n\n    # Calculate average validation losses\n    # avg_val_loss = val_loss / val_batches if val_batches > 0 else 0\n    # avg_val_recon = val_recon_loss / val_batches if val_batches > 0 else 0\n    # avg_val_kl = val_kl_loss / val_batches if val_batches > 0 else 0\n    \n    # # Log epoch averages to wandb\n   \n    \n    # print(f'Epoch {epoch+1}/{epochs}:')\n    # print(f'  Train - Loss: {avg_train_loss:.4f}, Recon: {avg_train_recon:.4f}, KL: {avg_train_kl:.4f}')\n    # print(f'  Val   - Loss: {avg_val_loss:.4f}, Recon: {avg_val_recon:.4f}, KL: {avg_val_kl:.4f}')\n    # print('-' * 80)","metadata":{"execution":{"iopub.status.busy":"2025-06-15T12:14:57.779371Z","iopub.execute_input":"2025-06-15T12:14:57.779580Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Epoch 1/10 - Training:   3%|▎         | 155/5064 [00:17<08:56,  9.16it/s, Loss=3411.1702, Recon=3309.3552, KL=101.8150]","output_type":"stream"}],"execution_count":null},{"id":"7476937d-05d4-4f25-bae8-7a11cf3f8b57","cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Get a batch of validation data for visualization\nwith torch.no_grad():\n    for data in val_loader:\n        data = data.to(device)\n        res, mu, log_var, z = autoencoder(data)  # Unpack VAE outputs\n        break\n\n# Move tensors to CPU and convert to numpy for visualization\noriginal_images = data.cpu().numpy()\nreconstructed_images = res.cpu().numpy()\n\n# Plot original vs reconstructed images\nfig, axes = plt.subplots(2, 8, figsize=(20, 6))\nfig.suptitle('VAE: Original (top) vs Reconstructed (bottom)', fontsize=16)\n\nfor i in range(8):\n    # Original images - transpose from (C, H, W) to (H, W, C) for RGB display\n    orig_img = np.transpose(original_images[i], (1, 2, 0))\n    orig_img = np.clip(orig_img, 0, 1)  # Ensure values are in [0, 1] range\n    \n    axes[0, i].imshow(orig_img)\n    axes[0, i].set_title(f'Original {i+1}')\n    axes[0, i].axis('off')\n    \n    # Reconstructed images - transpose from (C, H, W) to (H, W, C) for RGB display\n    recon_img = np.transpose(reconstructed_images[i], (1, 2, 0))\n    recon_img = np.clip(recon_img, 0, 1)  # Ensure values are in [0, 1] range\n    \n    axes[1, i].imshow(recon_img)\n    axes[1, i].set_title(f'Reconstructed {i+1}')\n    axes[1, i].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Log sample images to wandb\nwandb.log({\n    \"sample_reconstructions\": wandb.Image(plt)\n})\n\nprint(\"Visualization complete!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}