{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter  \n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    device = 'cuda'\n",
    "    batch_size = 1\n",
    "    lr = 0.0002\n",
    "    img_size = 128\n",
    "    no_of_channels = 3\n",
    "    kernel_size = (4,4)\n",
    "    stride = 2\n",
    "    # dropout = 0.5\n",
    "    padding = 1\n",
    "    lr_slope = 0.2\n",
    "    beta_1 = 0.5\n",
    "    beta_2 = 0.999\n",
    "    no_of_kernel = 64\n",
    "    lambda_gen = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelArgs.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)  #mean = 0, std = 0.02\n",
    "        \n",
    "    if classname.find('Conv2D') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)  #mean = 0, std = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding_mode='reflect', device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding_mode='reflect', device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding_mode='reflect', device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding_mode='reflect', device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding_mode='reflect', device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding_mode='reflect', device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding_mode='reflect', device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding_mode='reflect', device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding_mode='reflect', device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding_mode='reflect', device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding_mode='reflect', device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding_mode='reflect', device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        \n",
    "        res = x + self.main(x)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.resnet_blocks = ResnetBlock()\n",
    "        \n",
    "        self.down = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(in_channels=ModelArgs.no_of_channels, out_channels=64, kernel_size=(7,7), stride=1, device=ModelArgs.device, padding=3),\n",
    "            nn.InstanceNorm2d(num_features=64, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3), stride=2, device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=128, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3,3), stride=2, device=ModelArgs.device),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "        )\n",
    "        self.up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=(3,3), stride=2, device=ModelArgs.device),\n",
    "            nn.InstanceNorm2d(num_features=128, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=(3,3), stride=2, device=ModelArgs.device, output_padding=1),\n",
    "            nn.InstanceNorm2d(num_features=64, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=64, out_channels=ModelArgs.no_of_channels, kernel_size=(7,7), stride=1, device=ModelArgs.device, padding_mode='reflect', padding=3),\n",
    "            nn.InstanceNorm2d(num_features=ModelArgs.no_of_channels, device=ModelArgs.device, affine=True),\n",
    "            nn.Tanh(),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "   \n",
    "        x = self.down(x)\n",
    "        x = self.resnet_blocks(x)\n",
    "        x = self.up(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (resnet_blocks): ResnetBlock(\n",
      "    (main): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (5): ReLU()\n",
      "      (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (7): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (8): ReLU()\n",
      "      (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (10): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (11): ReLU()\n",
      "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (13): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (14): ReLU()\n",
      "      (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (16): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (17): ReLU()\n",
      "      (18): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (19): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (20): ReLU()\n",
      "      (21): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (22): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (23): ReLU()\n",
      "      (24): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (25): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (26): ReLU()\n",
      "      (27): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (28): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (29): ReLU()\n",
      "      (30): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (31): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (32): ReLU()\n",
      "      (33): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (34): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (35): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (down): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "    (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (7): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (8): ReLU()\n",
      "  )\n",
      "  (up): Sequential(\n",
      "    (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (2): ReLU()\n",
      "    (3): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), output_padding=(1, 1))\n",
      "    (4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), padding_mode=reflect)\n",
      "    (7): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (8): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Intializing the Discriminator instance\n",
    "Gen = Generator().to(ModelArgs.device)\n",
    "#Apply the wieght intilization function layer by layer\n",
    "Gen = Gen.apply(weights_init)\n",
    "#Printing the structure\n",
    "print(Gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "Generator (Generator)                    [1, 3, 128, 128]     [1, 3, 128, 128]     --                   True\n",
       "├─Sequential (down)                      [1, 3, 128, 128]     [1, 256, 31, 31]     --                   True\n",
       "│    └─Conv2d (0)                        [1, 3, 128, 128]     [1, 64, 128, 128]    9,472                True\n",
       "│    └─InstanceNorm2d (1)                [1, 64, 128, 128]    [1, 64, 128, 128]    128                  True\n",
       "│    └─ReLU (2)                          [1, 64, 128, 128]    [1, 64, 128, 128]    --                   --\n",
       "│    └─Conv2d (3)                        [1, 64, 128, 128]    [1, 128, 64, 64]     73,856               True\n",
       "│    └─InstanceNorm2d (4)                [1, 128, 64, 64]     [1, 128, 64, 64]     256                  True\n",
       "│    └─ReLU (5)                          [1, 128, 64, 64]     [1, 128, 64, 64]     --                   --\n",
       "│    └─Conv2d (6)                        [1, 128, 64, 64]     [1, 256, 31, 31]     295,168              True\n",
       "│    └─InstanceNorm2d (7)                [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    └─ReLU (8)                          [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "├─ResnetBlock (resnet_blocks)            [1, 256, 31, 31]     [1, 256, 31, 31]     --                   True\n",
       "│    └─Sequential (main)                 [1, 256, 31, 31]     [1, 256, 31, 31]     --                   True\n",
       "│    │    └─Conv2d (0)                   [1, 256, 31, 31]     [1, 256, 31, 31]     590,080              True\n",
       "│    │    └─InstanceNorm2d (1)           [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    │    └─ReLU (2)                     [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "│    │    └─Conv2d (3)                   [1, 256, 31, 31]     [1, 256, 31, 31]     590,080              True\n",
       "│    │    └─InstanceNorm2d (4)           [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    │    └─ReLU (5)                     [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "│    │    └─Conv2d (6)                   [1, 256, 31, 31]     [1, 256, 31, 31]     590,080              True\n",
       "│    │    └─InstanceNorm2d (7)           [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    │    └─ReLU (8)                     [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "│    │    └─Conv2d (9)                   [1, 256, 31, 31]     [1, 256, 31, 31]     590,080              True\n",
       "│    │    └─InstanceNorm2d (10)          [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    │    └─ReLU (11)                    [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "│    │    └─Conv2d (12)                  [1, 256, 31, 31]     [1, 256, 31, 31]     590,080              True\n",
       "│    │    └─InstanceNorm2d (13)          [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    │    └─ReLU (14)                    [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "│    │    └─Conv2d (15)                  [1, 256, 31, 31]     [1, 256, 31, 31]     590,080              True\n",
       "│    │    └─InstanceNorm2d (16)          [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    │    └─ReLU (17)                    [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "│    │    └─Conv2d (18)                  [1, 256, 31, 31]     [1, 256, 31, 31]     590,080              True\n",
       "│    │    └─InstanceNorm2d (19)          [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    │    └─ReLU (20)                    [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "│    │    └─Conv2d (21)                  [1, 256, 31, 31]     [1, 256, 31, 31]     590,080              True\n",
       "│    │    └─InstanceNorm2d (22)          [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    │    └─ReLU (23)                    [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "│    │    └─Conv2d (24)                  [1, 256, 31, 31]     [1, 256, 31, 31]     590,080              True\n",
       "│    │    └─InstanceNorm2d (25)          [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    │    └─ReLU (26)                    [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "│    │    └─Conv2d (27)                  [1, 256, 31, 31]     [1, 256, 31, 31]     590,080              True\n",
       "│    │    └─InstanceNorm2d (28)          [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    │    └─ReLU (29)                    [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "│    │    └─Conv2d (30)                  [1, 256, 31, 31]     [1, 256, 31, 31]     590,080              True\n",
       "│    │    └─InstanceNorm2d (31)          [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    │    └─ReLU (32)                    [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "│    │    └─Conv2d (33)                  [1, 256, 31, 31]     [1, 256, 31, 31]     590,080              True\n",
       "│    │    └─InstanceNorm2d (34)          [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    │    └─ReLU (35)                    [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "├─Sequential (up)                        [1, 256, 31, 31]     [1, 3, 128, 128]     --                   True\n",
       "│    └─ConvTranspose2d (0)               [1, 256, 31, 31]     [1, 128, 63, 63]     295,040              True\n",
       "│    └─InstanceNorm2d (1)                [1, 128, 63, 63]     [1, 128, 63, 63]     256                  True\n",
       "│    └─ReLU (2)                          [1, 128, 63, 63]     [1, 128, 63, 63]     --                   --\n",
       "│    └─ConvTranspose2d (3)               [1, 128, 63, 63]     [1, 64, 128, 128]    73,792               True\n",
       "│    └─InstanceNorm2d (4)                [1, 64, 128, 128]    [1, 64, 128, 128]    128                  True\n",
       "│    └─ReLU (5)                          [1, 64, 128, 128]    [1, 64, 128, 128]    --                   --\n",
       "│    └─Conv2d (6)                        [1, 64, 128, 128]    [1, 3, 128, 128]     9,411                True\n",
       "│    └─InstanceNorm2d (7)                [1, 3, 128, 128]     [1, 3, 128, 128]     6                    True\n",
       "│    └─Tanh (8)                          [1, 3, 128, 128]     [1, 3, 128, 128]     --                   --\n",
       "========================================================================================================================\n",
       "Total params: 7,845,129\n",
       "Trainable params: 7,845,129\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 10.08\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 102.03\n",
       "Params size (MB): 31.38\n",
       "Estimated Total Size (MB): 133.61\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# images = torch.randn(64, 1, 64, 64)\n",
    "# labels = torch.randint(0, 10, (64,), dtype=torch.long)\n",
    "gen = Generator()\n",
    "summary(model=gen,\n",
    "        input_size=(ModelArgs.batch_size, ModelArgs.no_of_channels, ModelArgs.img_size, ModelArgs.img_size),\n",
    "        # input_data=(images.to(ModelArgs.device), labels.to(ModelArgs.device)),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchGAN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(ModelArgs.no_of_channels, 64, kernel_size=ModelArgs.kernel_size, stride=ModelArgs.stride, padding=ModelArgs.padding, padding_mode='reflect'),\n",
    "            nn.LeakyReLU(negative_slope=ModelArgs.lr_slope),\n",
    "                \n",
    "            nn.Conv2d(64, 128, kernel_size=ModelArgs.kernel_size, stride=ModelArgs.stride, padding=ModelArgs.padding, padding_mode='reflect'),\n",
    "            nn.InstanceNorm2d(128, affine=True),\n",
    "            nn.LeakyReLU(negative_slope=ModelArgs.lr_slope),\n",
    "               \n",
    "            nn.Conv2d(128, 256, kernel_size=ModelArgs.kernel_size, stride=ModelArgs.stride, padding=ModelArgs.padding, padding_mode='reflect'),\n",
    "            nn.InstanceNorm2d(256, affine=True),\n",
    "            nn.LeakyReLU(negative_slope=ModelArgs.lr_slope),\n",
    "              \n",
    "            nn.Conv2d(256, 512, kernel_size=ModelArgs.kernel_size, stride=1, padding=ModelArgs.padding, padding_mode='reflect'),\n",
    "            nn.InstanceNorm2d(512, affine=True),\n",
    "            nn.LeakyReLU(negative_slope=ModelArgs.lr_slope),\n",
    "            \n",
    "            nn.Conv2d(512, 1, kernel_size=ModelArgs.kernel_size, stride=1, padding=ModelArgs.padding, padding_mode='reflect'),\n",
    "\n",
    "            # nn.Sigmoid()\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "    \n",
    "        # print(x.shape)\n",
    "        # print(y.shape)\n",
    "        # res = torch.concat([x, y], dim=1)\n",
    "        return nn.functional.sigmoid(self.main(x))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PatchGAN(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), padding_mode=reflect)\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), padding_mode=reflect)\n",
      "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (4): LeakyReLU(negative_slope=0.2)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), padding_mode=reflect)\n",
      "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (7): LeakyReLU(negative_slope=0.2)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (10): LeakyReLU(negative_slope=0.2)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Intializing the Discriminator instance\n",
    "patchgan = PatchGAN().to(ModelArgs.device)\n",
    "#Apply the wieght intilization function layer by layer\n",
    "patchgan = patchgan.apply(weights_init)\n",
    "#Printing the structure\n",
    "print(patchgan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "PatchGAN (PatchGAN)                      [1, 3, 128, 128]     [1, 1, 14, 14]       --                   True\n",
       "├─Sequential (main)                      [1, 3, 128, 128]     [1, 1, 14, 14]       --                   True\n",
       "│    └─Conv2d (0)                        [1, 3, 128, 128]     [1, 64, 64, 64]      3,136                True\n",
       "│    └─LeakyReLU (1)                     [1, 64, 64, 64]      [1, 64, 64, 64]      --                   --\n",
       "│    └─Conv2d (2)                        [1, 64, 64, 64]      [1, 128, 32, 32]     131,200              True\n",
       "│    └─InstanceNorm2d (3)                [1, 128, 32, 32]     [1, 128, 32, 32]     256                  True\n",
       "│    └─LeakyReLU (4)                     [1, 128, 32, 32]     [1, 128, 32, 32]     --                   --\n",
       "│    └─Conv2d (5)                        [1, 128, 32, 32]     [1, 256, 16, 16]     524,544              True\n",
       "│    └─InstanceNorm2d (6)                [1, 256, 16, 16]     [1, 256, 16, 16]     512                  True\n",
       "│    └─LeakyReLU (7)                     [1, 256, 16, 16]     [1, 256, 16, 16]     --                   --\n",
       "│    └─Conv2d (8)                        [1, 256, 16, 16]     [1, 512, 15, 15]     2,097,664            True\n",
       "│    └─InstanceNorm2d (9)                [1, 512, 15, 15]     [1, 512, 15, 15]     1,024                True\n",
       "│    └─LeakyReLU (10)                    [1, 512, 15, 15]     [1, 512, 15, 15]     --                   --\n",
       "│    └─Conv2d (11)                       [1, 512, 15, 15]     [1, 1, 14, 14]       8,193                True\n",
       "========================================================================================================================\n",
       "Total params: 2,766,529\n",
       "Trainable params: 2,766,529\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 755.06\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 7.09\n",
       "Params size (MB): 11.07\n",
       "Estimated Total Size (MB): 18.35\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# images = torch.randn(64, 1, 64, 64)\n",
    "# labels = torch.randint(0, 10, (64,), dtype=torch.long)\n",
    "real_A = torch.randn(ModelArgs.batch_size, ModelArgs.no_of_channels, ModelArgs.img_size, ModelArgs.img_size)\n",
    "# real_B = torch.randn(ModelArgs.batch_size, ModelArgs.no_of_channels, ModelArgs.img_size, ModelArgs.img_size)\n",
    "patchgan = PatchGAN()\n",
    "summary(model=patchgan,\n",
    "        input_size=(ModelArgs.batch_size, ModelArgs.no_of_channels, ModelArgs.img_size, ModelArgs.img_size),\n",
    "        # input_data=(real_A),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config\n",
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "both_transform = A.Compose(\n",
    "    [A.Resize(width=ModelArgs.img_size, height=ModelArgs.img_size),], additional_targets={\"image0\": \"image\"},\n",
    ")\n",
    "\n",
    "transform_only_input = A.Compose(\n",
    "    [\n",
    "        # A.HorizontalFlip(p=0.5),\n",
    "        # A.ColorJitter(p=0.2),\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n",
    "        # A.ToFloat(max_value=ModelArgs.img_size),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_only_mask = A.Compose(\n",
    "    [\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n",
    "        # A.ToFloat(max_value=ModelArgs.img_size),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Cityscapes2LabelsDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root_dir) -> None:\n",
    "        super().__init__()\n",
    "        self.train_path = root_dir\n",
    "        self.dir = os.listdir(self.train_path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dir)\n",
    "    \n",
    "    def __getitem__(self, index):    \n",
    "        \n",
    "        current_img = self.dir[index]\n",
    "        img_path = os.path.join(self.train_path, current_img) \n",
    "        img = np.array(Image.open(img_path))\n",
    "        input = img[:, :256, :]\n",
    "        mask = img[:, 256:, :]\n",
    "        augmentataions = both_transform(image = input, image0 = mask)\n",
    "        input = augmentataions['image']\n",
    "        mask = augmentataions['image0']\n",
    "        \n",
    "        input_transformed = transform_only_input(image = input)['image']\n",
    "        mask_transformed = transform_only_mask(image = mask)['image']\n",
    "        \n",
    "        return input_transformed, mask_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataloaders\n",
    "dir = 'data/cityscapes/train'\n",
    "train = Cityscapes2LabelsDataset(dir)\n",
    "trainloader = DataLoader(train, batch_size=ModelArgs.batch_size, shuffle=True)\n",
    "val_dir = 'data/cityscapes/val'\n",
    "val = Cityscapes2LabelsDataset(val_dir)\n",
    "valloader = DataLoader(val, batch_size=ModelArgs.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib  import Path\n",
    "save_images = Path('generated_images/')\n",
    "# enc = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for X, y in trainloader:\n",
    "    imageX = X[0]\n",
    "    # imagey = y[0]\n",
    "\n",
    "    imageX = imageX.permute(1, 2, 0).numpy()\n",
    "    # imagey = imagey.permute(1, 2, 0).numpy()\n",
    "    # Plot the image\n",
    "    plt.imshow(imageX)\n",
    "    # plt.imshow(imagey)\n",
    "    plt.show()\n",
    "    plt.show()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "generatorX = Generator().to(ModelArgs.device).apply(weights_init)\n",
    "discriminatorY = PatchGAN().to(ModelArgs.device).apply(weights_init)\n",
    "generatorY = Generator().to(ModelArgs.device).apply(weights_init)\n",
    "discriminatorX = PatchGAN().to(ModelArgs.device).apply(weights_init)\n",
    "\n",
    "\n",
    "loss_fn = nn.MSELoss()  \n",
    "epochs = 200 \n",
    "\n",
    "generatorX.train()\n",
    "discriminatorX.train()\n",
    "generatorY.train()\n",
    "discriminatorY.train()\n",
    "\n",
    "optimizerG = torch.optim.Adam(itertools.chain(generatorX.parameters(), generatorY.parameters()), betas=(ModelArgs.beta_1, ModelArgs.beta_2), lr=ModelArgs.lr) #For discriminator\n",
    "optimizerD = torch.optim.Adam(itertools.chain(discriminatorY.parameters(), discriminatorX.parameters()), betas=(ModelArgs.beta_1, ModelArgs.beta_2), lr=ModelArgs.lr) #For generator\n",
    "\n",
    "\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "\n",
    "loss_g = []\n",
    "loss_d = []\n",
    "img_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLRScheduler:\n",
    "    def __init__(self, optimizers, total_epochs, lr_initial):\n",
    "        self.optimizers = optimizers\n",
    "        self.total_epochs = total_epochs\n",
    "        self.lr_initial = lr_initial\n",
    "\n",
    "    def step(self, epoch):\n",
    "        if epoch < 100:\n",
    "            lr = self.lr_initial\n",
    "        else:\n",
    "            decay = (self.lr_initial / 100) * (epoch - 100)\n",
    "            lr = max(0, self.lr_initial - decay)\n",
    "        \n",
    "        for optimizer in self.optimizers:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "scheduler = CustomLRScheduler([optimizerG, optimizerD], epochs, lr_initial=ModelArgs.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training loop\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "iters = 0\n",
    "g_scaler = torch.cuda.amp.GradScaler()\n",
    "d_scaler = torch.cuda.amp.GradScaler()\n",
    " \n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "\n",
    "img_counter = 0\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    for X, y in trainloader:\n",
    "\n",
    "    \n",
    "        X = X.to(ModelArgs.device)\n",
    "        y = y.to(ModelArgs.device)\n",
    "        if(img_counter > 50):\n",
    "            img_counter = 0\n",
    "            \n",
    "        \n",
    "        #############################\n",
    "        # Discriminator Training\n",
    "        #############################\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            #Enabling the discriminators trainable ability \n",
    "            for params in discriminatorX.parameters():\n",
    "                params.requires_grad = True            \n",
    "                \n",
    "            current_batch_size = X.shape[0]  #Getting the current batch size\n",
    "            \n",
    "            real_data = torch.ones((current_batch_size,), device=ModelArgs.device)\n",
    "\n",
    "            # 1. Forward pass\n",
    "            y_pred = discriminatorX(X)\n",
    "            # print(y_pred)\n",
    "            # print(y_pred.shape)\n",
    "            # 2. Calculate  and accumulate loss\n",
    "            loss_real = loss_fn(y_pred, real_data)\n",
    "\n",
    "            # 3. Optimizer zero grad\n",
    "            optimizerD.zero_grad()\n",
    "\n",
    "        \n",
    "            # loss_real.backward()\n",
    "\n",
    "\n",
    "            #Train the discriminator (with fake data)\n",
    "\n",
    "            # noise = torch.randn((batch_size, latent_vector_size, 1, 1), device=device)\n",
    "            fake_data = torch.zeros(( current_batch_size,), device=ModelArgs.device)\n",
    "            mask_generated_by_generatorY = generatorY(y)\n",
    "\n",
    "            #1. Forward pass\n",
    "            y_pred = discriminatorX(mask_generated_by_generatorY.detach())\n",
    "\n",
    "\n",
    "            # 2. Calculate  and accumulate loss\n",
    "            loss_fake = loss_fn(y_pred, fake_data)\n",
    "\n",
    "\n",
    "            #Accumulating total discriminator loss\n",
    "            discriminatorX_combined_loss = (loss_real + loss_fake) \n",
    "            # loss_d.append(discriminator_combined_loss.item())\n",
    "\n",
    "            \n",
    "            \n",
    "            #Enabling the discriminators trainable ability \n",
    "            for params in discriminatorY.parameters():\n",
    "                params.requires_grad = True            \n",
    "                \n",
    "            current_batch_size = X.shape[0]  #Getting the current batch size\n",
    "            \n",
    "            real_data = torch.ones((current_batch_size,), device=ModelArgs.device)\n",
    "\n",
    "            # 1. Forward pass\n",
    "            y_pred = discriminatorY(y)\n",
    "            # print(y_pred.shape)\n",
    "            \n",
    "            # 2. Calculate  and accumulate loss\n",
    "            loss_real = loss_fn(y_pred, real_data)\n",
    "\n",
    "\n",
    "            #Train the discriminator (with fake data)\n",
    "\n",
    "            # noise = torch.randn((batch_size, latent_vector_size, 1, 1), device=device)\n",
    "            fake_data = torch.zeros(( current_batch_size,), device=ModelArgs.device)\n",
    "            mask_generated_by_generatorX = generatorX(X)\n",
    "\n",
    "            #1. Forward pass\n",
    "            y_pred = discriminatorY(mask_generated_by_generatorX.detach())\n",
    "\n",
    "\n",
    "            # 2. Calculate  and accumulate loss\n",
    "            loss_fake = loss_fn(y_pred, fake_data)\n",
    "            \n",
    "            \n",
    "            discriminatorY_combined_loss = (loss_real + loss_fake) \n",
    "            \n",
    "            discriminator_combined_loss = (discriminatorX_combined_loss + discriminatorY_combined_loss) * 0.5\n",
    "            \n",
    "            \n",
    "            # 4. Loss backward\n",
    "            d_scaler.scale(discriminator_combined_loss).backward()\n",
    "            \n",
    "            # 5. Optimizer step\n",
    "            d_scaler.step(optimizerD)\n",
    "            d_scaler.update()\n",
    "            \n",
    "\n",
    "        ###########################\n",
    "        # Generator Training\n",
    "        ##########################\n",
    "        with torch.cuda.amp.autocast():\n",
    "            #Disabling the discriminators trainable ability \n",
    "            for params in discriminatorX.parameters():\n",
    "                params.requires_grad = False\n",
    "            \n",
    "            #Disabling the discriminators trainable ability \n",
    "            for params in discriminatorY.parameters():\n",
    "                params.requires_grad = False\n",
    "                \n",
    "            # mask_generated_by_generator = unet(X)\n",
    "            labels = torch.ones((current_batch_size,), device=ModelArgs.device)\n",
    "\n",
    "            #1. Forward pass\n",
    "            y_pred = discriminatorX(mask_generated_by_generatorY)\n",
    "            # y_pred = torch.argmax(probs, dim=1).type(torch.float32)\n",
    "\n",
    "\n",
    "            #2. Calculate and accumulate loss\n",
    "            loss_geny = loss_fn(y_pred,labels) \n",
    "            \n",
    "    \n",
    "            # mask_generated_by_generator = unet(X)\n",
    "            labels = torch.ones((current_batch_size,), device=ModelArgs.device)\n",
    "\n",
    "            #1. Forward pass\n",
    "            y_pred = discriminatorY(mask_generated_by_generatorX)\n",
    "            # y_pred = torch.argmax(probs, dim=1).type(torch.float32)\n",
    "\n",
    "\n",
    "            #2. Calculate and accumulate loss\n",
    "            loss_genx = loss_fn(y_pred,labels) \n",
    "            # print(loss_geny)\n",
    "            # print(loss_genx)\n",
    "            # print(nn.functional.l1_loss(mask_generated_by_generatorY, X))\n",
    "            # print(nn.functional.l1_loss(mask_generated_by_generatorX, y))\n",
    "            identity_genx = nn.functional.l1_loss(mask_generated_by_generatorX, y)\n",
    "            identity_geny = nn.functional.l1_loss(mask_generated_by_generatorY, X)\n",
    "            combined_generator_loss = (loss_geny + loss_genx + identity_genx + identity_geny) +  ModelArgs.lambda_gen * (nn.functional.l1_loss(mask_generated_by_generatorY, X) + nn.functional.l1_loss(mask_generated_by_generatorX, y))\n",
    "\n",
    "            # 3. Optimizer zero grad\n",
    "            optimizerG.zero_grad()\n",
    "\n",
    "            # 4. Loss backward\n",
    "            g_scaler.scale(combined_generator_loss).backward()\n",
    "\n",
    "            # 5. Optimizer step\n",
    "            g_scaler.step(optimizerG)\n",
    "            g_scaler.update()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        if iters % 200 == 0:\n",
    "            print(\"Iterations: \", iters, \"Epoch: \", epoch, \"Generator loss: \", combined_generator_loss.item(), \"Discriminator loss: \", discriminator_combined_loss.item())\n",
    "\n",
    "    \n",
    "        if iters % 500 == 0:\n",
    "            \n",
    "            print('saving the output')\n",
    "            torchvision.utils.save_image(X* 0.5 + 0.5,'{}/realB_images_iters_{}.png'.format(save_images, iters))\n",
    "            torchvision.utils.save_image(y* 0.5 + 0.5,'{}/realA_images_iters_{}.png'.format(save_images, iters))\n",
    "            # fakeA = generatorX(X)\n",
    "            # fakeB = generatorY(y)\n",
    "            torchvision.utils.save_image(mask_generated_by_generatorX* 0.5 + 0.5,'{}/fake_imageA_iters_{}.png'.format(save_images, iters))\n",
    "            torchvision.utils.save_image(mask_generated_by_generatorY* 0.5 + 0.5,'{}/fake_imageB_iters_{}.png'.format(save_images, iters))\n",
    "\n",
    "\n",
    "            img_grid_fakeA = torchvision.utils.make_grid(mask_generated_by_generatorX, normalize=True)\n",
    "            img_grid_fakeB = torchvision.utils.make_grid(mask_generated_by_generatorY, normalize=True)\n",
    "            # img_grid_map = torchvision.utils.make_grid(X, normalize=True)\n",
    "                \n",
    "            writer_fake.add_image(\n",
    "                        \"Cityscapes2lables FakeA Images\", img_grid_fakeA, global_step=iters\n",
    "                    )\n",
    "            writer_real.add_image(\n",
    "                        \"Cityscapes2lables FakeB Images\", img_grid_fakeB, global_step=iters\n",
    "                    )\n",
    "            \n",
    "            # writer_real.add_image(\n",
    "            #             \"Map2Aerial Aerial Images\", img_grid_map, global_step=iters\n",
    "            #         )\n",
    "                    \n",
    "\n",
    "            # Check pointing for every epoch\n",
    "            # torch.save(generator.state_dict(), 'weights/CelebA/generator_steps_%d.pth' % (iters))\n",
    "            # torch.save(discriminator.state_dict(), 'weights/CelebA/discriminator_steps_%d.pth' % (iters))\n",
    "\n",
    "\n",
    "        iters += 1\n",
    "        \n",
    "    scheduler.step(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (X, y) in enumerate(valloader):\n",
    "    \n",
    "    X = X.to(ModelArgs.device)\n",
    "    y = y.to(ModelArgs.device)\n",
    "    \n",
    "    mask_generated_by_generatorX = generatorX(X)\n",
    "    mask_generated_by_generatory = generatorY(y)\n",
    "    \n",
    "    torchvision.utils.save_image(mask_generated_by_generatorX* 0.5 + 0.5,'output_images_val/fake_imageA_{}.png'.format(idx))\n",
    "    torchvision.utils.save_image(mask_generated_by_generatorY* 0.5 + 0.5,'output_images_val/fake_imageB_{}.png'.format(idx))\n",
    "\n",
    "    torchvision.utils.save_image(X* 0.5 + 0.5,'output_images_val/real_imageA_{}.png'.format(idx))\n",
    "    torchvision.utils.save_image(y* 0.5 + 0.5,'output_images_val/real_imageB_{}.png'.format(idx))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
