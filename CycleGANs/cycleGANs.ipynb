{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter  \n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    device = 'cpu'\n",
    "    batch_size = 1\n",
    "    lr = 0.0002\n",
    "    img_size = 128\n",
    "    no_of_channels = 3\n",
    "    kernel_size = (4,4)\n",
    "    stride = 2\n",
    "    # dropout = 0.5\n",
    "    padding = 1\n",
    "    lr_slope = 0.2\n",
    "    beta_1 = 0.5\n",
    "    beta_2 = 0.999\n",
    "    no_of_kernel = 64\n",
    "    lambda_gen = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelArgs.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)  #mean = 0, std = 0.02\n",
    "        \n",
    "    if classname.find('Conv2D') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)  #mean = 0, std = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding_mode='reflect', device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding_mode='reflect', device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding_mode='reflect', device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding_mode='reflect', device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding_mode='reflect', device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding_mode='reflect', device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding_mode='reflect', device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding_mode='reflect', device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding_mode='reflect', device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding_mode='reflect', device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding_mode='reflect', device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding_mode='reflect', device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        \n",
    "        res = x + self.main(x)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.resnet_blocks = ResnetBlock()\n",
    "        \n",
    "        self.down = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(in_channels=ModelArgs.no_of_channels, out_channels=64, kernel_size=(7,7), stride=1, device=ModelArgs.device, padding=3),\n",
    "            nn.InstanceNorm2d(num_features=64, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3), stride=2, device=ModelArgs.device, padding=1),\n",
    "            nn.InstanceNorm2d(num_features=128, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3,3), stride=2, device=ModelArgs.device),\n",
    "            nn.InstanceNorm2d(num_features=256, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "        )\n",
    "        self.up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=(3,3), stride=2, device=ModelArgs.device),\n",
    "            nn.InstanceNorm2d(num_features=128, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=(3,3), stride=2, device=ModelArgs.device, output_padding=1),\n",
    "            nn.InstanceNorm2d(num_features=64, device=ModelArgs.device, affine=True),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=64, out_channels=ModelArgs.no_of_channels, kernel_size=(7,7), stride=1, device=ModelArgs.device, padding_mode='reflect', padding=3),\n",
    "            nn.InstanceNorm2d(num_features=ModelArgs.no_of_channels, device=ModelArgs.device, affine=True),\n",
    "            nn.Tanh(),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "   \n",
    "        x = self.down(x)\n",
    "        x = self.resnet_blocks(x)\n",
    "        x = self.up(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (resnet_blocks): ResnetBlock(\n",
      "    (main): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (5): ReLU()\n",
      "      (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (7): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (8): ReLU()\n",
      "      (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (10): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (11): ReLU()\n",
      "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (13): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (14): ReLU()\n",
      "      (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (16): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (17): ReLU()\n",
      "      (18): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (19): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (20): ReLU()\n",
      "      (21): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (22): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (23): ReLU()\n",
      "      (24): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (25): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (26): ReLU()\n",
      "      (27): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (28): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (29): ReLU()\n",
      "      (30): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (31): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (32): ReLU()\n",
      "      (33): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (34): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (35): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (down): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "    (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (7): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (8): ReLU()\n",
      "  )\n",
      "  (up): Sequential(\n",
      "    (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (2): ReLU()\n",
      "    (3): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), output_padding=(1, 1))\n",
      "    (4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), padding_mode=reflect)\n",
      "    (7): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (8): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Intializing the Discriminator instance\n",
    "Gen = Generator().to(ModelArgs.device)\n",
    "#Apply the wieght intilization function layer by layer\n",
    "Gen = Gen.apply(weights_init)\n",
    "#Printing the structure\n",
    "print(Gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "Generator (Generator)                    [1, 3, 128, 128]     [1, 3, 128, 128]     --                   True\n",
       "├─Sequential (down)                      [1, 3, 128, 128]     [1, 256, 31, 31]     --                   True\n",
       "│    └─Conv2d (0)                        [1, 3, 128, 128]     [1, 64, 128, 128]    9,472                True\n",
       "│    └─InstanceNorm2d (1)                [1, 64, 128, 128]    [1, 64, 128, 128]    128                  True\n",
       "│    └─ReLU (2)                          [1, 64, 128, 128]    [1, 64, 128, 128]    --                   --\n",
       "│    └─Conv2d (3)                        [1, 64, 128, 128]    [1, 128, 64, 64]     73,856               True\n",
       "│    └─InstanceNorm2d (4)                [1, 128, 64, 64]     [1, 128, 64, 64]     256                  True\n",
       "│    └─ReLU (5)                          [1, 128, 64, 64]     [1, 128, 64, 64]     --                   --\n",
       "│    └─Conv2d (6)                        [1, 128, 64, 64]     [1, 256, 31, 31]     295,168              True\n",
       "│    └─InstanceNorm2d (7)                [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    └─ReLU (8)                          [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "├─ResnetBlock (resnet_blocks)            [1, 256, 31, 31]     [1, 256, 31, 31]     --                   True\n",
       "│    └─Sequential (main)                 [1, 256, 31, 31]     [1, 256, 31, 31]     --                   True\n",
       "│    │    └─Conv2d (0)                   [1, 256, 31, 31]     [1, 256, 31, 31]     590,080              True\n",
       "│    │    └─InstanceNorm2d (1)           [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    │    └─ReLU (2)                     [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "│    │    └─Conv2d (3)                   [1, 256, 31, 31]     [1, 256, 31, 31]     590,080              True\n",
       "│    │    └─InstanceNorm2d (4)           [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    │    └─ReLU (5)                     [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "│    │    └─Conv2d (6)                   [1, 256, 31, 31]     [1, 256, 31, 31]     590,080              True\n",
       "│    │    └─InstanceNorm2d (7)           [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    │    └─ReLU (8)                     [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "│    │    └─Conv2d (9)                   [1, 256, 31, 31]     [1, 256, 31, 31]     590,080              True\n",
       "│    │    └─InstanceNorm2d (10)          [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    │    └─ReLU (11)                    [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "│    │    └─Conv2d (12)                  [1, 256, 31, 31]     [1, 256, 31, 31]     590,080              True\n",
       "│    │    └─InstanceNorm2d (13)          [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    │    └─ReLU (14)                    [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "│    │    └─Conv2d (15)                  [1, 256, 31, 31]     [1, 256, 31, 31]     590,080              True\n",
       "│    │    └─InstanceNorm2d (16)          [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    │    └─ReLU (17)                    [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "│    │    └─Conv2d (18)                  [1, 256, 31, 31]     [1, 256, 31, 31]     590,080              True\n",
       "│    │    └─InstanceNorm2d (19)          [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    │    └─ReLU (20)                    [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "│    │    └─Conv2d (21)                  [1, 256, 31, 31]     [1, 256, 31, 31]     590,080              True\n",
       "│    │    └─InstanceNorm2d (22)          [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    │    └─ReLU (23)                    [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "│    │    └─Conv2d (24)                  [1, 256, 31, 31]     [1, 256, 31, 31]     590,080              True\n",
       "│    │    └─InstanceNorm2d (25)          [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    │    └─ReLU (26)                    [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "│    │    └─Conv2d (27)                  [1, 256, 31, 31]     [1, 256, 31, 31]     590,080              True\n",
       "│    │    └─InstanceNorm2d (28)          [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    │    └─ReLU (29)                    [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "│    │    └─Conv2d (30)                  [1, 256, 31, 31]     [1, 256, 31, 31]     590,080              True\n",
       "│    │    └─InstanceNorm2d (31)          [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    │    └─ReLU (32)                    [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "│    │    └─Conv2d (33)                  [1, 256, 31, 31]     [1, 256, 31, 31]     590,080              True\n",
       "│    │    └─InstanceNorm2d (34)          [1, 256, 31, 31]     [1, 256, 31, 31]     512                  True\n",
       "│    │    └─ReLU (35)                    [1, 256, 31, 31]     [1, 256, 31, 31]     --                   --\n",
       "├─Sequential (up)                        [1, 256, 31, 31]     [1, 3, 128, 128]     --                   True\n",
       "│    └─ConvTranspose2d (0)               [1, 256, 31, 31]     [1, 128, 63, 63]     295,040              True\n",
       "│    └─InstanceNorm2d (1)                [1, 128, 63, 63]     [1, 128, 63, 63]     256                  True\n",
       "│    └─ReLU (2)                          [1, 128, 63, 63]     [1, 128, 63, 63]     --                   --\n",
       "│    └─ConvTranspose2d (3)               [1, 128, 63, 63]     [1, 64, 128, 128]    73,792               True\n",
       "│    └─InstanceNorm2d (4)                [1, 64, 128, 128]    [1, 64, 128, 128]    128                  True\n",
       "│    └─ReLU (5)                          [1, 64, 128, 128]    [1, 64, 128, 128]    --                   --\n",
       "│    └─Conv2d (6)                        [1, 64, 128, 128]    [1, 3, 128, 128]     9,411                True\n",
       "│    └─InstanceNorm2d (7)                [1, 3, 128, 128]     [1, 3, 128, 128]     6                    True\n",
       "│    └─Tanh (8)                          [1, 3, 128, 128]     [1, 3, 128, 128]     --                   --\n",
       "========================================================================================================================\n",
       "Total params: 7,845,129\n",
       "Trainable params: 7,845,129\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 10.08\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 102.03\n",
       "Params size (MB): 31.38\n",
       "Estimated Total Size (MB): 133.61\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# images = torch.randn(64, 1, 64, 64)\n",
    "# labels = torch.randint(0, 10, (64,), dtype=torch.long)\n",
    "gen = Generator()\n",
    "summary(model=gen,\n",
    "        input_size=(ModelArgs.batch_size, ModelArgs.no_of_channels, ModelArgs.img_size, ModelArgs.img_size),\n",
    "        # input_data=(images.to(ModelArgs.device), labels.to(ModelArgs.device)),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchGAN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(ModelArgs.no_of_channels, 64, kernel_size=ModelArgs.kernel_size, stride=ModelArgs.stride, padding=ModelArgs.padding, padding_mode='reflect'),\n",
    "            nn.LeakyReLU(negative_slope=ModelArgs.lr_slope),\n",
    "                \n",
    "            nn.Conv2d(64, 128, kernel_size=ModelArgs.kernel_size, stride=ModelArgs.stride, padding=ModelArgs.padding, padding_mode='reflect'),\n",
    "            nn.InstanceNorm2d(128, affine=True),\n",
    "            nn.LeakyReLU(negative_slope=ModelArgs.lr_slope),\n",
    "               \n",
    "            nn.Conv2d(128, 256, kernel_size=ModelArgs.kernel_size, stride=ModelArgs.stride, padding=ModelArgs.padding, padding_mode='reflect'),\n",
    "            nn.InstanceNorm2d(256, affine=True),\n",
    "            nn.LeakyReLU(negative_slope=ModelArgs.lr_slope),\n",
    "              \n",
    "            nn.Conv2d(256, 512, kernel_size=ModelArgs.kernel_size, stride=1, padding=ModelArgs.padding, padding_mode='reflect'),\n",
    "            nn.InstanceNorm2d(512, affine=True),\n",
    "            nn.LeakyReLU(negative_slope=ModelArgs.lr_slope),\n",
    "            \n",
    "            nn.Conv2d(512, 1, kernel_size=ModelArgs.kernel_size, stride=1, padding=ModelArgs.padding, padding_mode='reflect'),\n",
    "\n",
    "            # nn.Sigmoid()\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "    \n",
    "        # print(x.shape)\n",
    "        # print(y.shape)\n",
    "        # res = torch.concat([x, y], dim=1)\n",
    "        return nn.functional.sigmoid(self.main(x))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PatchGAN(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), padding_mode=reflect)\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), padding_mode=reflect)\n",
      "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (4): LeakyReLU(negative_slope=0.2)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), padding_mode=reflect)\n",
      "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (7): LeakyReLU(negative_slope=0.2)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (10): LeakyReLU(negative_slope=0.2)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Intializing the Discriminator instance\n",
    "patchgan = PatchGAN().to(ModelArgs.device)\n",
    "#Apply the wieght intilization function layer by layer\n",
    "patchgan = patchgan.apply(weights_init)\n",
    "#Printing the structure\n",
    "print(patchgan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "PatchGAN (PatchGAN)                      [1, 3, 128, 128]     [1, 1, 14, 14]       --                   True\n",
       "├─Sequential (main)                      [1, 3, 128, 128]     [1, 1, 14, 14]       --                   True\n",
       "│    └─Conv2d (0)                        [1, 3, 128, 128]     [1, 64, 64, 64]      3,136                True\n",
       "│    └─LeakyReLU (1)                     [1, 64, 64, 64]      [1, 64, 64, 64]      --                   --\n",
       "│    └─Conv2d (2)                        [1, 64, 64, 64]      [1, 128, 32, 32]     131,200              True\n",
       "│    └─InstanceNorm2d (3)                [1, 128, 32, 32]     [1, 128, 32, 32]     256                  True\n",
       "│    └─LeakyReLU (4)                     [1, 128, 32, 32]     [1, 128, 32, 32]     --                   --\n",
       "│    └─Conv2d (5)                        [1, 128, 32, 32]     [1, 256, 16, 16]     524,544              True\n",
       "│    └─InstanceNorm2d (6)                [1, 256, 16, 16]     [1, 256, 16, 16]     512                  True\n",
       "│    └─LeakyReLU (7)                     [1, 256, 16, 16]     [1, 256, 16, 16]     --                   --\n",
       "│    └─Conv2d (8)                        [1, 256, 16, 16]     [1, 512, 15, 15]     2,097,664            True\n",
       "│    └─InstanceNorm2d (9)                [1, 512, 15, 15]     [1, 512, 15, 15]     1,024                True\n",
       "│    └─LeakyReLU (10)                    [1, 512, 15, 15]     [1, 512, 15, 15]     --                   --\n",
       "│    └─Conv2d (11)                       [1, 512, 15, 15]     [1, 1, 14, 14]       8,193                True\n",
       "========================================================================================================================\n",
       "Total params: 2,766,529\n",
       "Trainable params: 2,766,529\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 755.06\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 7.09\n",
       "Params size (MB): 11.07\n",
       "Estimated Total Size (MB): 18.35\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# images = torch.randn(64, 1, 64, 64)\n",
    "# labels = torch.randint(0, 10, (64,), dtype=torch.long)\n",
    "real_A = torch.randn(ModelArgs.batch_size, ModelArgs.no_of_channels, ModelArgs.img_size, ModelArgs.img_size)\n",
    "# real_B = torch.randn(ModelArgs.batch_size, ModelArgs.no_of_channels, ModelArgs.img_size, ModelArgs.img_size)\n",
    "patchgan = PatchGAN()\n",
    "summary(model=patchgan,\n",
    "        input_size=(ModelArgs.batch_size, ModelArgs.no_of_channels, ModelArgs.img_size, ModelArgs.img_size),\n",
    "        # input_data=(real_A),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config\n",
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "both_transform = A.Compose(\n",
    "    [A.Resize(width=ModelArgs.img_size, height=ModelArgs.img_size),], additional_targets={\"image0\": \"image\"},\n",
    ")\n",
    "\n",
    "transform_only_input = A.Compose(\n",
    "    [\n",
    "        # A.HorizontalFlip(p=0.5),\n",
    "        # A.ColorJitter(p=0.2),\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n",
    "        # A.ToFloat(max_value=ModelArgs.img_size),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_only_mask = A.Compose(\n",
    "    [\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n",
    "        # A.ToFloat(max_value=ModelArgs.img_size),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Cityscapes2LabelsDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root_dir) -> None:\n",
    "        super().__init__()\n",
    "        self.train_path = root_dir\n",
    "        self.dir = os.listdir(self.train_path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dir)\n",
    "    \n",
    "    def __getitem__(self, index):    \n",
    "        \n",
    "        current_img = self.dir[index]\n",
    "        img_path = os.path.join(self.train_path, current_img) \n",
    "        img = np.array(Image.open(img_path))\n",
    "        input = img[:, :256, :]\n",
    "        mask = img[:, 256:, :]\n",
    "        augmentataions = both_transform(image = input, image0 = mask)\n",
    "        input = augmentataions['image']\n",
    "        mask = augmentataions['image0']\n",
    "        \n",
    "        input_transformed = transform_only_input(image = input)['image']\n",
    "        mask_transformed = transform_only_mask(image = mask)['image']\n",
    "        \n",
    "        return input_transformed, mask_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataloaders\n",
    "dir = 'data/cityscapes/train'\n",
    "train = Cityscapes2LabelsDataset(dir)\n",
    "trainloader = DataLoader(train, batch_size=ModelArgs.batch_size, shuffle=True)\n",
    "val_dir = 'data/cityscapes/val'\n",
    "val = Cityscapes2LabelsDataset(val_dir)\n",
    "valloader = DataLoader(val, batch_size=ModelArgs.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib  import Path\n",
    "save_images = Path('generated_images/')\n",
    "# enc = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9294118..1.0].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for X, y in trainloader:\n",
    "    imageX = X[0]\n",
    "    # imagey = y[0]\n",
    "\n",
    "    imageX = imageX.permute(1, 2, 0).numpy()\n",
    "    # imagey = imagey.permute(1, 2, 0).numpy()\n",
    "    # Plot the image\n",
    "    plt.imshow(imageX)\n",
    "    # plt.imshow(imagey)\n",
    "    plt.show()\n",
    "    plt.show()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "generatorX = Generator().to(ModelArgs.device).apply(weights_init)\n",
    "discriminatorY = PatchGAN().to(ModelArgs.device).apply(weights_init)\n",
    "generatorY = Generator().to(ModelArgs.device).apply(weights_init)\n",
    "discriminatorX = PatchGAN().to(ModelArgs.device).apply(weights_init)\n",
    "\n",
    "\n",
    "loss_fn = nn.MSELoss()  \n",
    "epochs = 200 \n",
    "\n",
    "generatorX.train()\n",
    "discriminatorX.train()\n",
    "generatorY.train()\n",
    "discriminatorY.train()\n",
    "\n",
    "optimizerG = torch.optim.Adam(itertools.chain(generatorX.parameters(), generatorY.parameters()), betas=(ModelArgs.beta_1, ModelArgs.beta_2), lr=ModelArgs.lr) #For discriminator\n",
    "optimizerD = torch.optim.Adam(itertools.chain(discriminatorY.parameters(), discriminatorX.parameters()), betas=(ModelArgs.beta_1, ModelArgs.beta_2), lr=ModelArgs.lr) #For generator\n",
    "\n",
    "\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "\n",
    "loss_g = []\n",
    "loss_d = []\n",
    "img_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/home/yuvrajsingh/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1, 14, 14])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:  0 Epoch:  0 Generator loss:  13.941393852233887 Discriminator loss:  0.656898021697998\n",
      "saving the output\n",
      "Iterations:  200 Epoch:  0 Generator loss:  7.152299880981445 Discriminator loss:  0.04200435057282448\n",
      "saving the output\n",
      "Iterations:  400 Epoch:  0 Generator loss:  8.328140258789062 Discriminator loss:  0.013077827170491219\n",
      "saving the output\n",
      "Iterations:  600 Epoch:  0 Generator loss:  7.669099807739258 Discriminator loss:  0.03464417904615402\n",
      "saving the output\n",
      "Iterations:  800 Epoch:  0 Generator loss:  6.395097732543945 Discriminator loss:  0.009978562593460083\n",
      "saving the output\n"
     ]
    }
   ],
   "source": [
    "#Training loop\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "iters = 0\n",
    "g_scaler = torch.cuda.amp.GradScaler()\n",
    "d_scaler = torch.cuda.amp.GradScaler()\n",
    " \n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "\n",
    "img_counter = 0\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    for X, y in trainloader:\n",
    "\n",
    "    \n",
    "        X = X.to(ModelArgs.device)\n",
    "        y = y.to(ModelArgs.device)\n",
    "        if(img_counter > 50):\n",
    "            img_counter = 0\n",
    "            \n",
    "        \n",
    "        #############################\n",
    "        # Discriminator Training\n",
    "        #############################\n",
    "        with torch.cuda.amp.autocast():\n",
    "            #Enabling the discriminators trainable ability \n",
    "            for params in discriminatorX.parameters():\n",
    "                params.requires_grad = True            \n",
    "                \n",
    "            current_batch_size = X.shape[0]  #Getting the current batch size\n",
    "            \n",
    "            real_data = torch.ones((current_batch_size,), device=ModelArgs.device)\n",
    "\n",
    "            # 1. Forward pass\n",
    "            y_pred = discriminatorX(X)\n",
    "            # print(y_pred)\n",
    "            # print(y_pred.shape)\n",
    "            # 2. Calculate  and accumulate loss\n",
    "            loss_real = loss_fn(y_pred, real_data)\n",
    "\n",
    "            # 3. Optimizer zero grad\n",
    "            optimizerD.zero_grad()\n",
    "\n",
    "        \n",
    "            # loss_real.backward()\n",
    "\n",
    "\n",
    "            #Train the discriminator (with fake data)\n",
    "\n",
    "            # noise = torch.randn((batch_size, latent_vector_size, 1, 1), device=device)\n",
    "            fake_data = torch.zeros(( current_batch_size,), device=ModelArgs.device)\n",
    "            mask_generated_by_generatorY = generatorY(y)\n",
    "\n",
    "            #1. Forward pass\n",
    "            y_pred = discriminatorX(mask_generated_by_generatorY.detach())\n",
    "\n",
    "\n",
    "            # 2. Calculate  and accumulate loss\n",
    "            loss_fake = loss_fn(y_pred, fake_data)\n",
    "\n",
    "\n",
    "            #Accumulating total discriminator loss\n",
    "            discriminatorX_combined_loss = (loss_real + loss_fake) \n",
    "            # loss_d.append(discriminator_combined_loss.item())\n",
    "\n",
    "            \n",
    "            \n",
    "            #Enabling the discriminators trainable ability \n",
    "            for params in discriminatorY.parameters():\n",
    "                params.requires_grad = True            \n",
    "                \n",
    "            current_batch_size = X.shape[0]  #Getting the current batch size\n",
    "            \n",
    "            real_data = torch.ones((current_batch_size,), device=ModelArgs.device)\n",
    "\n",
    "            # 1. Forward pass\n",
    "            y_pred = discriminatorY(y)\n",
    "            # print(y_pred.shape)\n",
    "            \n",
    "            # 2. Calculate  and accumulate loss\n",
    "            loss_real = loss_fn(y_pred, real_data)\n",
    "\n",
    "\n",
    "            #Train the discriminator (with fake data)\n",
    "\n",
    "            # noise = torch.randn((batch_size, latent_vector_size, 1, 1), device=device)\n",
    "            fake_data = torch.zeros(( current_batch_size,), device=ModelArgs.device)\n",
    "            mask_generated_by_generatorX = generatorX(X)\n",
    "\n",
    "            #1. Forward pass\n",
    "            y_pred = discriminatorY(mask_generated_by_generatorX.detach())\n",
    "\n",
    "\n",
    "            # 2. Calculate  and accumulate loss\n",
    "            loss_fake = loss_fn(y_pred, fake_data)\n",
    "            \n",
    "            \n",
    "            discriminatorY_combined_loss = (loss_real + loss_fake) \n",
    "            \n",
    "            discriminator_combined_loss = (discriminatorX_combined_loss + discriminatorY_combined_loss) * 0.5\n",
    "            \n",
    "            \n",
    "            # 4. Loss backward\n",
    "            d_scaler.scale(discriminator_combined_loss).backward()\n",
    "            \n",
    "            # 5. Optimizer step\n",
    "            d_scaler.step(optimizerD)\n",
    "            d_scaler.update()\n",
    "        \n",
    "\n",
    "        ###########################\n",
    "        # Generator Training\n",
    "        ##########################\n",
    "        with torch.cuda.amp.autocast():\n",
    "            #Disabling the discriminators trainable ability \n",
    "            for params in discriminatorX.parameters():\n",
    "                params.requires_grad = False\n",
    "            \n",
    "            #Disabling the discriminators trainable ability \n",
    "            for params in discriminatorY.parameters():\n",
    "                params.requires_grad = False\n",
    "                \n",
    "            # mask_generated_by_generator = unet(X)\n",
    "            labels = torch.ones((current_batch_size,), device=ModelArgs.device)\n",
    "\n",
    "            #1. Forward pass\n",
    "            y_pred = discriminatorX(mask_generated_by_generatorY)\n",
    "            # y_pred = torch.argmax(probs, dim=1).type(torch.float32)\n",
    "\n",
    "\n",
    "            #2. Calculate and accumulate loss\n",
    "            loss_geny = loss_fn(y_pred,labels) \n",
    "            \n",
    "    \n",
    "            # mask_generated_by_generator = unet(X)\n",
    "            labels = torch.ones((current_batch_size,), device=ModelArgs.device)\n",
    "\n",
    "            #1. Forward pass\n",
    "            y_pred = discriminatorY(mask_generated_by_generatorX)\n",
    "            # y_pred = torch.argmax(probs, dim=1).type(torch.float32)\n",
    "\n",
    "\n",
    "            #2. Calculate and accumulate loss\n",
    "            loss_genx = loss_fn(y_pred,labels) \n",
    "            # print(loss_geny)\n",
    "            # print(loss_genx)\n",
    "            # print(nn.functional.l1_loss(mask_generated_by_generatorY, X))\n",
    "            # print(nn.functional.l1_loss(mask_generated_by_generatorX, y))\n",
    "            combined_generator_loss = loss_geny + loss_genx + ModelArgs.lambda_gen * (nn.functional.l1_loss(mask_generated_by_generatorY, X) + nn.functional.l1_loss(mask_generated_by_generatorX, y))\n",
    "\n",
    "            # 3. Optimizer zero grad\n",
    "            optimizerG.zero_grad()\n",
    "\n",
    "            # 4. Loss backward\n",
    "            g_scaler.scale(combined_generator_loss).backward()\n",
    "\n",
    "            # 5. Optimizer step\n",
    "            g_scaler.step(optimizerG)\n",
    "            g_scaler.update()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        if iters % 200 == 0:\n",
    "            print(\"Iterations: \", iters, \"Epoch: \", epoch, \"Generator loss: \", combined_generator_loss.item(), \"Discriminator loss: \", discriminator_combined_loss.item())\n",
    "\n",
    "    \n",
    "        if iters % 200 == 0:\n",
    "            \n",
    "            print('saving the output')\n",
    "            torchvision.utils.save_image(X* 0.5 + 0.5,'{}/realB_images_iters_{}.png'.format(save_images, iters))\n",
    "            torchvision.utils.save_image(y* 0.5 + 0.5,'{}/realA_images_iters_{}.png'.format(save_images, iters))\n",
    "            # fakeA = generatorX(X)\n",
    "            # fakeB = generatorY(y)\n",
    "            torchvision.utils.save_image(mask_generated_by_generatorX* 0.5 + 0.5,'{}/fake_imageA_iters_{}.png'.format(save_images, iters))\n",
    "            torchvision.utils.save_image(mask_generated_by_generatorY* 0.5 + 0.5,'{}/fake_imageB_iters_{}.png'.format(save_images, iters))\n",
    "\n",
    "\n",
    "            img_grid_fakeA = torchvision.utils.make_grid(mask_generated_by_generatorX, normalize=True)\n",
    "            img_grid_fakeB = torchvision.utils.make_grid(mask_generated_by_generatorY, normalize=True)\n",
    "            # img_grid_map = torchvision.utils.make_grid(X, normalize=True)\n",
    "                \n",
    "            writer_fake.add_image(\n",
    "                        \"Cityscapes2lables FakeA Images\", img_grid_fakeA, global_step=iters\n",
    "                    )\n",
    "            writer_real.add_image(\n",
    "                        \"Cityscapes2lables FakeB Images\", img_grid_fakeB, global_step=iters\n",
    "                    )\n",
    "            \n",
    "            # writer_real.add_image(\n",
    "            #             \"Map2Aerial Aerial Images\", img_grid_map, global_step=iters\n",
    "            #         )\n",
    "                    \n",
    "\n",
    "            # Check pointing for every epoch\n",
    "            # torch.save(generator.state_dict(), 'weights/CelebA/generator_steps_%d.pth' % (iters))\n",
    "            # torch.save(discriminator.state_dict(), 'weights/CelebA/discriminator_steps_%d.pth' % (iters))\n",
    "\n",
    "\n",
    "        iters += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
