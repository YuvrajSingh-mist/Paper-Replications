{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from torchtune.modules import RMSNorm\n",
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, Normalize, ToTensor\n",
    "from torchvision.transforms.v2 import RGB\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import random_split\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, ViTFeatureExtractor\n",
    "\n",
    "\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    #Hyperparameters\n",
    "    img_size = (224, 224)\n",
    "    block_size = 77\n",
    "    batch_size = 32\n",
    "    embeddings_dims = 768\n",
    "    projection_dims = 768\n",
    "    attn_dropout = 0.1\n",
    "    no_of_heads = 12 #IMP needs to be thoroughly calculated\n",
    "    dropout = 0.1\n",
    "    epochs = 100\n",
    "    lr = 4e-4\n",
    "    no_of_decoder_layers = 12 #IMP needs to be thoroughly calculated\n",
    "    weight_decay_optim = 0.2\n",
    "    beta_1 = 0.9\n",
    "    beta_2 = 0.98\n",
    "    epsilon = 1e-6\n",
    "    device = 'cuda'\n",
    "    vocab_size = 2000\n",
    "    head_lr = 1e-3\n",
    "    image_encoder_lr = 1e-4\n",
    "    text_encoder_lr = 1e-5\n",
    "    model_name = 'resnet50'\n",
    "    pretrained = True # for both image encoder and text encoder\n",
    "    trainable = True # for both image encoder and text encoder\n",
    "    bias = -10\n",
    "    temperature = torch.log(torch.tensor(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims\n",
    "    ):  \n",
    "        super().__init__()\n",
    "        self.layernorm_layer = torch.nn.LayerNorm(normalized_shape=embeddings_dims)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.layernorm_layer(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "            \n",
    "        \n",
    "        \n",
    "        self.layer_norm = Normalization()\n",
    "        self.model = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.multimodalTextLayerProjector = nn.Linear(in_features=ModelArgs.embeddings_dims, out_features=ModelArgs.projection_dims, device=ModelArgs.device)\n",
    "        \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = True\n",
    "        self.model.train()\n",
    "    def forward(self, x):\n",
    "        # print(\"Problemetic x shape: \", x['input_ids'].shape)\n",
    "        # print(\"Problemetic x shape: \", x['attention_mask'].shape)\n",
    "        x['input_ids'] = x['input_ids'].squeeze(1)\n",
    "        x['attention_mask'] = x['attention_mask'].squeeze(1) \n",
    "        x = self.model(input_ids = x['input_ids'], attention_mask = x['attention_mask'])['last_hidden_state'][:, 0, :] \n",
    "        # print(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return self.multimodalTextLayerProjector(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode images to a fixed size vector\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model_name=ModelArgs.model_name, pretrained=ModelArgs.pretrained, trainable=ModelArgs.trainable\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n",
    "        )\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigLip(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vision = VisionModel()\n",
    "        self.text = TextModel()\n",
    "        # self.tokenizer = tokenizer\n",
    "        self.multimodelTextLayerPorjector = nn.Linear(in_features=ModelArgs.embeddings_dims, out_features=ModelArgs.projection_dims, device=ModelArgs.device)\n",
    "        self.multimodalVisionLayerProjector = nn.Linear(in_features=ModelArgs.embeddings_dims, out_features=ModelArgs.projection_dims, device=ModelArgs.device)\n",
    "        # self.temperature = nn.Parameter(torch.ones(size=(ModelArgs.batch_size,), device=ModelArgs.device), requires_grad=True)\n",
    "        self.temperature = nn.Parameter(ModelArgs.temperature, requires_grad=True)\n",
    "        self.bias = nn.Parameter(ModelArgs.bias, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        embeds_text = self.text(x)\n",
    "        # print(\"Inside CLiP text: \", embeds_text.shape)\n",
    "        proj_txt = torch.nn.functional.normalize(self.multimodelTextLayerPorjector(embeds_text))\n",
    "        embeds_img = self.vision(x)\n",
    "        # print(\"Inside ViT: \", embeds_img.shape)\n",
    "        proj_img = torch.nn.functional.normalize(self.multimodalVisionLayerProjector(embeds_img))\n",
    "        # print(proj_txt.shape)\n",
    "        # print(proj_img.shape)\n",
    "        logits = -(proj_txt @ proj_img.T) * torch.exp(self.temperature) + self.bias\n",
    "        # print(\"Inside CLiP logits shape: \", logits.shape)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siglip = SigLip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config\n",
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_transforms = A.Compose(\n",
    "    [   \n",
    "        A.Resize(height=224, width=224),\n",
    "        A.CenterCrop(height=224, width=224),\n",
    "        # A.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711], max_pixel_value=224.0,),\n",
    "        # A.ToFloat(max_value=224),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_tyransforms = A.Compose(\n",
    "    [\n",
    "        # A.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711], max_pixel_value=224.0,),\n",
    "        # A.ToFloat(max_value=224),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/flickr8000/captions.txt', sep=',')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled = df.sample(frac=0.01, random_state=42)\n",
    "df_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "class CLiPDatatset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.path = path\n",
    "        # self.dir = os.listdir(self.path)        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return df_sampled.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        text, img = df_sampled.iloc[idx][1], df_sampled.iloc[idx][0]\n",
    "        # print(text)\n",
    "        # print(img)\n",
    "        img_path = os.path.join(self.path, img) \n",
    "        # print(img_path)\n",
    "        img = np.array(Image.open(img_path))\n",
    "\n",
    "        input_transformed = train_transforms(image = img)['image']\n",
    "        \n",
    "        text_tokenized = self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=ModelArgs.block_size)\n",
    "        \n",
    "        # print(text_tokenized)\n",
    "        encoded_items = {\n",
    "            \n",
    "            key: torch.tensor(values)\n",
    "            for key, values in text_tokenized.items()\n",
    "            \n",
    "        }\n",
    "        encoded_items['image'] = input_transformed\n",
    "        return encoded_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'data/flickr8000/images'\n",
    "dataset = CLiPDatatset(dir)\n",
    "\n",
    "# Assuming 'dataset' is already created\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "#Creating dataloaders\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=ModelArgs.batch_size, shuffle=True)\n",
    "valloader = DataLoader(val_dataset, batch_size=ModelArgs.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "params = [\n",
    "        {\"params\": siglip.vision.parameters(), \"lr\": ModelArgs.image_encoder_lr},\n",
    "        {\"params\": siglip.text.parameters(), \"lr\": ModelArgs.text_encoder_lr},\n",
    "        {\"params\": itertools.chain(\n",
    "            siglip.multimodalVisionLayerProjector.parameters(), siglip.multimodelTextLayerPorjector.parameters(), [siglip.temperature]\n",
    "        ), \"lr\": ModelArgs.head_lr, \"weight_decay\": ModelArgs.weight_decay_optim}\n",
    "    ]\n",
    "\n",
    "optimizer = torch.optim.Adam(lr=ModelArgs.lr, params=params, eps=ModelArgs.epsilon)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# def cross_entropy(pred=None, targets=None, dim=None):\n",
    "#     # print(\"Targets shape is: \",targets.shape)\n",
    "#     # print(\"Predictions shape is :\", pred.shape)\n",
    "    \n",
    "#     preds = nn.functional.log_softmax(pred, dim=-1)\n",
    "\n",
    "#     l = (-targets * preds).sum(1).mean()\n",
    "#     return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = engine.train(model=siglip,\n",
    "                       writer=None,\n",
    "                       train_dataloader=trainloader,\n",
    "                       test_dataloader=valloader,\n",
    "                       optimizer=optimizer,\n",
    "                       loss_fn=loss_fn,\n",
    "                       epochs=30,\n",
    "                       device=ModelArgs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
