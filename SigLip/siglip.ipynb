{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from torchtune.modules import RMSNorm\n",
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, Normalize, ToTensor\n",
    "from torchvision.transforms.v2 import RGB\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import random_split\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, ViTFeatureExtractor\n",
    "\n",
    "\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    #Hyperparameters\n",
    "    img_size = (224, 224)\n",
    "    block_size = 77\n",
    "    batch_size = 32\n",
    "    embeddings_dims = 768\n",
    "    projection_dims = 768\n",
    "    attn_dropout = 0.1\n",
    "    no_of_heads = 12 #IMP needs to be thoroughly calculated\n",
    "    dropout = 0.1\n",
    "    epochs = 100\n",
    "    lr = 4e-4\n",
    "    no_of_decoder_layers = 12 #IMP needs to be thoroughly calculated\n",
    "    weight_decay_optim = 0.2\n",
    "    beta_1 = 0.9\n",
    "    beta_2 = 0.98\n",
    "    epsilon = 1e-6\n",
    "    device = 'cuda'\n",
    "    vocab_size = 2000\n",
    "    head_lr = 1e-3\n",
    "    image_encoder_lr = 1e-4\n",
    "    text_encoder_lr = 1e-5\n",
    "    model_name = 'resnet50'\n",
    "    pretrained = True # for both image encoder and text encoder\n",
    "    trainable = True # for both image encoder and text encoder\n",
    "    bias = -10\n",
    "    temperature = torch.log(torch.tensor(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims\n",
    "    ):  \n",
    "        super().__init__()\n",
    "        self.layernorm_layer = torch.nn.LayerNorm(normalized_shape=embeddings_dims)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.layernorm_layer(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvraj-singh/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "            \n",
    "        \n",
    "        \n",
    "        self.layer_norm = Normalization()\n",
    "        self.model = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.multimodalTextLayerProjector = nn.Linear(in_features=ModelArgs.embeddings_dims, out_features=ModelArgs.projection_dims, device=ModelArgs.device)\n",
    "        \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = True\n",
    "        self.model.train()\n",
    "    def forward(self, x):\n",
    "        # print(\"Problemetic x shape: \", x['input_ids'].shape)\n",
    "        # print(\"Problemetic x shape: \", x['attention_mask'].shape)\n",
    "        x['input_ids'] = x['input_ids'].squeeze(1)\n",
    "        x['attention_mask'] = x['attention_mask'].squeeze(1) \n",
    "        x = self.model(input_ids = x['input_ids'], attention_mask = x['attention_mask'])['last_hidden_state'][:, 0, :] \n",
    "        # print(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return self.multimodalTextLayerProjector(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode images to a fixed size vector\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model_name=ModelArgs.model_name, pretrained=ModelArgs.pretrained, trainable=ModelArgs.trainable\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n",
    "        )\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigLip(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vision = VisionModel()\n",
    "        self.text = TextModel()\n",
    "        # self.tokenizer = tokenizer\n",
    "        self.multimodelTextLayerPorjector = nn.Linear(in_features=ModelArgs.embeddings_dims, out_features=ModelArgs.projection_dims, device=ModelArgs.device)\n",
    "        self.multimodalVisionLayerProjector = nn.Linear(in_features=ModelArgs.embeddings_dims, out_features=ModelArgs.projection_dims, device=ModelArgs.device)\n",
    "        # self.temperature = nn.Parameter(torch.ones(size=(ModelArgs.batch_size,), device=ModelArgs.device), requires_grad=True)\n",
    "        self.temperature = nn.Parameter(ModelArgs.temperature, requires_grad=True)\n",
    "        self.bias = nn.Parameter(ModelArgs.bias, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        embeds_text = self.text(x)\n",
    "        # print(\"Inside CLiP text: \", embeds_text.shape)\n",
    "        proj_txt = torch.nn.functional.normalize(self.multimodelTextLayerPorjector(embeds_text))\n",
    "        embeds_img = self.vision(x)\n",
    "        # print(\"Inside ViT: \", embeds_img.shape)\n",
    "        proj_img = torch.nn.functional.normalize(self.multimodalVisionLayerProjector(embeds_img))\n",
    "        # print(proj_txt.shape)\n",
    "        # print(proj_img.shape)\n",
    "        logits = -(proj_txt @ proj_img.T) * torch.exp(self.temperature) + self.bias\n",
    "        # print(\"Inside CLiP logits shape: \", logits.shape)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e705e6abc1f54b1895490ec840d7291a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m clip \u001b[38;5;241m=\u001b[39m \u001b[43mSigLip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m, in \u001b[0;36mSigLip.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision \u001b[38;5;241m=\u001b[39m VisionModel()\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m=\u001b[39m \u001b[43mTextModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# self.tokenizer = tokenizer\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultimodelTextLayerPorjector \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(in_features\u001b[38;5;241m=\u001b[39mModelArgs\u001b[38;5;241m.\u001b[39membeddings_dims, out_features\u001b[38;5;241m=\u001b[39mModelArgs\u001b[38;5;241m.\u001b[39mprojection_dims, device\u001b[38;5;241m=\u001b[39mModelArgs\u001b[38;5;241m.\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m, in \u001b[0;36mTextModel.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m RobertaModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta-base\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m tokenizer\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultimodalTextLayerProjector \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mModelArgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mModelArgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprojection_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mModelArgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m     13\u001b[0m     p\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/nn/modules/linear.py:99\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/cuda/__init__.py:314\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    313\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 314\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    318\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero."
     ]
    }
   ],
   "source": [
    "siglip = SigLip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config\n",
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_transforms = A.Compose(\n",
    "    [   \n",
    "        A.Resize(height=224, width=224),\n",
    "        A.CenterCrop(height=224, width=224),\n",
    "        # A.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711], max_pixel_value=224.0,),\n",
    "        # A.ToFloat(max_value=224),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_tyransforms = A.Compose(\n",
    "    [\n",
    "        # A.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711], max_pixel_value=224.0,),\n",
    "        # A.ToFloat(max_value=224),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40450</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A man in a pink shirt climbs a rock face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40451</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A man is rock climbing high in the air .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40452</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A person in a red shirt climbing up a rock fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40453</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A rock climber in a red shirt .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40454</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A rock climber practices on a rock climbing wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40455 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image  \\\n",
       "0      1000268201_693b08cb0e.jpg   \n",
       "1      1000268201_693b08cb0e.jpg   \n",
       "2      1000268201_693b08cb0e.jpg   \n",
       "3      1000268201_693b08cb0e.jpg   \n",
       "4      1000268201_693b08cb0e.jpg   \n",
       "...                          ...   \n",
       "40450   997722733_0cb5439472.jpg   \n",
       "40451   997722733_0cb5439472.jpg   \n",
       "40452   997722733_0cb5439472.jpg   \n",
       "40453   997722733_0cb5439472.jpg   \n",
       "40454   997722733_0cb5439472.jpg   \n",
       "\n",
       "                                                 caption  \n",
       "0      A child in a pink dress is climbing up a set o...  \n",
       "1                  A girl going into a wooden building .  \n",
       "2       A little girl climbing into a wooden playhouse .  \n",
       "3      A little girl climbing the stairs to her playh...  \n",
       "4      A little girl in a pink dress going into a woo...  \n",
       "...                                                  ...  \n",
       "40450           A man in a pink shirt climbs a rock face  \n",
       "40451           A man is rock climbing high in the air .  \n",
       "40452  A person in a red shirt climbing up a rock fac...  \n",
       "40453                    A rock climber in a red shirt .  \n",
       "40454  A rock climber practices on a rock climbing wa...  \n",
       "\n",
       "[40455 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/flickr8000/captions.txt', sep=',')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17775</th>\n",
       "      <td>2973269132_252bfd0160.jpg</td>\n",
       "      <td>A large wild cat is pursuing a horse across a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13506</th>\n",
       "      <td>270263570_3160f360d3.jpg</td>\n",
       "      <td>Two brown dogs fight on the leafy ground .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4325</th>\n",
       "      <td>2053006423_6adf69ca67.jpg</td>\n",
       "      <td>A man in shorts is standing on a rock looking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37870</th>\n",
       "      <td>512101751_05a6d93e19.jpg</td>\n",
       "      <td>a muzzled white dog is running on the grass .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21321</th>\n",
       "      <td>3156406419_38fbd52007.jpg</td>\n",
       "      <td>A person skiing downhill .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35640</th>\n",
       "      <td>391020801_aaaae1e42b.jpg</td>\n",
       "      <td>A man gesticulates .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12364</th>\n",
       "      <td>2629027962_9cc3b46527.jpg</td>\n",
       "      <td>With a barn in the background a child puts her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17672</th>\n",
       "      <td>2966552760_e65b22cd26.jpg</td>\n",
       "      <td>A smiling child sits against a wall on a blank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24614</th>\n",
       "      <td>3290105461_7590f23371.jpg</td>\n",
       "      <td>Cricket player with red cap hits the ball outd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8763</th>\n",
       "      <td>2410562803_56ec09f41c.jpg</td>\n",
       "      <td>Three people sitting in front of a store on a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>405 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image  \\\n",
       "17775  2973269132_252bfd0160.jpg   \n",
       "13506   270263570_3160f360d3.jpg   \n",
       "4325   2053006423_6adf69ca67.jpg   \n",
       "37870   512101751_05a6d93e19.jpg   \n",
       "21321  3156406419_38fbd52007.jpg   \n",
       "...                          ...   \n",
       "35640   391020801_aaaae1e42b.jpg   \n",
       "12364  2629027962_9cc3b46527.jpg   \n",
       "17672  2966552760_e65b22cd26.jpg   \n",
       "24614  3290105461_7590f23371.jpg   \n",
       "8763   2410562803_56ec09f41c.jpg   \n",
       "\n",
       "                                                 caption  \n",
       "17775  A large wild cat is pursuing a horse across a ...  \n",
       "13506         Two brown dogs fight on the leafy ground .  \n",
       "4325   A man in shorts is standing on a rock looking ...  \n",
       "37870      a muzzled white dog is running on the grass .  \n",
       "21321                         A person skiing downhill .  \n",
       "...                                                  ...  \n",
       "35640                               A man gesticulates .  \n",
       "12364  With a barn in the background a child puts her...  \n",
       "17672  A smiling child sits against a wall on a blank...  \n",
       "24614  Cricket player with red cap hits the ball outd...  \n",
       "8763   Three people sitting in front of a store on a ...  \n",
       "\n",
       "[405 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sampled = df.sample(frac=0.01, random_state=42)\n",
    "df_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "class CLiPDatatset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.path = path\n",
    "        # self.dir = os.listdir(self.path)        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return df_sampled.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        text, img = df_sampled.iloc[idx][1], df_sampled.iloc[idx][0]\n",
    "        # print(text)\n",
    "        # print(img)\n",
    "        img_path = os.path.join(self.path, img) \n",
    "        # print(img_path)\n",
    "        img = np.array(Image.open(img_path))\n",
    "\n",
    "        input_transformed = train_transforms(image = img)['image']\n",
    "        \n",
    "        text_tokenized = self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=ModelArgs.block_size)\n",
    "        \n",
    "        # print(text_tokenized)\n",
    "        encoded_items = {\n",
    "            \n",
    "            key: torch.tensor(values)\n",
    "            for key, values in text_tokenized.items()\n",
    "            \n",
    "        }\n",
    "        encoded_items['image'] = input_transformed\n",
    "        return encoded_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'data/flickr8000/images'\n",
    "dataset = CLiPDatatset(dir)\n",
    "\n",
    "# Assuming 'dataset' is already created\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "#Creating dataloaders\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=ModelArgs.batch_size, shuffle=True)\n",
    "valloader = DataLoader(val_dataset, batch_size=ModelArgs.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'siglip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[1;32m      2\u001b[0m params \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m----> 3\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43msiglip\u001b[49m\u001b[38;5;241m.\u001b[39mvision\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: ModelArgs\u001b[38;5;241m.\u001b[39mimage_encoder_lr},\n\u001b[1;32m      4\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: siglip\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: ModelArgs\u001b[38;5;241m.\u001b[39mtext_encoder_lr},\n\u001b[1;32m      5\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m      6\u001b[0m             siglip\u001b[38;5;241m.\u001b[39mmultimodalVisionLayerProjector\u001b[38;5;241m.\u001b[39mparameters(), siglip\u001b[38;5;241m.\u001b[39mmultimodelTextLayerPorjector\u001b[38;5;241m.\u001b[39mparameters(), [siglip\u001b[38;5;241m.\u001b[39mtemperature]\n\u001b[1;32m      7\u001b[0m         ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: ModelArgs\u001b[38;5;241m.\u001b[39mhead_lr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m: ModelArgs\u001b[38;5;241m.\u001b[39mweight_decay_optim}\n\u001b[1;32m      8\u001b[0m     ]\n\u001b[1;32m     10\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(lr\u001b[38;5;241m=\u001b[39mModelArgs\u001b[38;5;241m.\u001b[39mlr, params\u001b[38;5;241m=\u001b[39mparams, eps\u001b[38;5;241m=\u001b[39mModelArgs\u001b[38;5;241m.\u001b[39mepsilon)\n\u001b[1;32m     11\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'siglip' is not defined"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "params = [\n",
    "        {\"params\": siglip.vision.parameters(), \"lr\": ModelArgs.image_encoder_lr},\n",
    "        {\"params\": siglip.text.parameters(), \"lr\": ModelArgs.text_encoder_lr},\n",
    "        {\"params\": itertools.chain(\n",
    "            siglip.multimodalVisionLayerProjector.parameters(), siglip.multimodelTextLayerPorjector.parameters(), [siglip.temperature]\n",
    "        ), \"lr\": ModelArgs.head_lr, \"weight_decay\": ModelArgs.weight_decay_optim}\n",
    "    ]\n",
    "\n",
    "optimizer = torch.optim.Adam(lr=ModelArgs.lr, params=params, eps=ModelArgs.epsilon)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# def cross_entropy(pred=None, targets=None, dim=None):\n",
    "#     # print(\"Targets shape is: \",targets.shape)\n",
    "#     # print(\"Predictions shape is :\", pred.shape)\n",
    "    \n",
    "#     preds = nn.functional.log_softmax(pred, dim=-1)\n",
    "\n",
    "#     l = (-targets * preds).sum(1).mean()\n",
    "#     return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd43c3e5a9c4b01ad5d89f86cb5a56a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5771/697776843.py:14: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text, img = df_sampled.iloc[idx][1], df_sampled.iloc[idx][0]\n",
      "/tmp/ipykernel_5771/697776843.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  key: torch.tensor(values)\n",
      "/mnt/c/Users/Yuvraj Singh/OneDrive/Desktop/Work/pytorch/Paper Replications/CLiP/going_modular/engine.py:53: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3675.)\n",
      "  loss_t = torch.nn.functional.cross_entropy(y_pred.T, labels.T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 3.3857 | test_loss: 2.8319 \n",
      "Epoch: 2 | train_loss: 3.3842 | test_loss: 2.8363 \n",
      "Epoch: 3 | train_loss: 3.3898 | test_loss: 2.8558 \n",
      "Epoch: 4 | train_loss: 3.2075 | test_loss: 2.5720 \n",
      "Epoch: 5 | train_loss: 2.7535 | test_loss: 2.3091 \n",
      "Epoch: 6 | train_loss: 2.4067 | test_loss: 2.0960 \n",
      "Epoch: 7 | train_loss: 2.2730 | test_loss: 2.2074 \n",
      "Epoch: 8 | train_loss: 2.2631 | test_loss: 2.1497 \n",
      "Epoch: 9 | train_loss: 2.2433 | test_loss: 2.1711 \n",
      "Epoch: 10 | train_loss: 2.1443 | test_loss: 2.0108 \n",
      "Epoch: 11 | train_loss: 2.1279 | test_loss: 2.1695 \n",
      "Epoch: 12 | train_loss: 2.0682 | test_loss: 2.1878 \n",
      "Epoch: 13 | train_loss: 2.0128 | test_loss: 2.0808 \n",
      "Epoch: 14 | train_loss: 1.9314 | test_loss: 2.1472 \n",
      "Epoch: 15 | train_loss: 1.9334 | test_loss: 2.0239 \n",
      "Epoch: 16 | train_loss: 1.8983 | test_loss: 2.0606 \n",
      "Epoch: 17 | train_loss: 1.7936 | test_loss: 2.0642 \n",
      "Epoch: 18 | train_loss: 1.8178 | test_loss: 2.1490 \n",
      "Epoch: 19 | train_loss: 1.8154 | test_loss: 2.1520 \n",
      "Epoch: 20 | train_loss: 1.6850 | test_loss: 2.0929 \n",
      "Epoch: 21 | train_loss: 1.6462 | test_loss: 2.1315 \n",
      "Epoch: 22 | train_loss: 1.6416 | test_loss: 1.9998 \n",
      "Epoch: 23 | train_loss: 1.6371 | test_loss: 2.3750 \n",
      "Epoch: 24 | train_loss: 1.5850 | test_loss: 2.0301 \n",
      "Epoch: 25 | train_loss: 1.5408 | test_loss: 2.0466 \n",
      "Epoch: 26 | train_loss: 1.4817 | test_loss: 2.0330 \n",
      "Epoch: 27 | train_loss: 1.4636 | test_loss: 2.1185 \n",
      "Epoch: 28 | train_loss: 1.4670 | test_loss: 2.0971 \n",
      "Epoch: 29 | train_loss: 1.4414 | test_loss: 2.0514 \n",
      "Epoch: 30 | train_loss: 1.3791 | test_loss: 2.2192 \n"
     ]
    }
   ],
   "source": [
    "results = engine.train(model=siglip,\n",
    "                       writer=None,\n",
    "                       train_dataloader=trainloader,\n",
    "                       test_dataloader=valloader,\n",
    "                       optimizer=optimizer,\n",
    "                       loss_fn=loss_fn,\n",
    "                       epochs=30,\n",
    "                       device=ModelArgs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
