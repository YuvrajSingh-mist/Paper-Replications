{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from pathlib import Path\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, ViTFeatureExtractor\n",
    "import os\n",
    "from going_modular import engine\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    device = 'cuda'\n",
    "    vis_embd_out = 768\n",
    "    text_embd_out = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vision Encoder (CLiP-Vision Encoder)\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_vision = clip.vision_model\n",
    "\n",
    "#Freezing the weights\n",
    "\n",
    "for params in clip_vision.parameters():\n",
    "    params = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_vision.vision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Language Decoder\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.3\")\n",
    "lang_model = AutoModelForCausalLM.from_pretrained(\"lmsys/vicuna-7b-v1.3\")\n",
    "\n",
    "for params in lang_model.parameters():\n",
    "    params = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Projector\n",
    "\n",
    "class Projector(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super()._init_()\n",
    "        \n",
    "        self.linear_layer = nn.Linear(in_features=ModelArgs.vis_embd_out, out_features=ModelArgs.text_embd_out, device=ModelArgs.device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear_layer(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llava(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.projector = Projector()\n",
    "        \n",
    "        \n",
    "    def forward(self, text, img):\n",
    "        vis_out = clip_vision(img)\n",
    "        proj_out = self.projector(vis_out)\n",
    "        combined_out = lang_model(torch.cat([proj_out, text], axis=0))\n",
    "        return combined_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llava = Llava()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
