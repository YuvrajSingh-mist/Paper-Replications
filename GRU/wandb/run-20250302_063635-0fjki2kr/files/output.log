/home/yuvrajsingh/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/utils/_device.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return func(*args, **kwargs)
/home/yuvrajsingh/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/utils/_device.py:106: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return func(*args, **kwargs)
Epoch:  0 | Step:  500 | Train Loss:  0.5787923336029053
Epoch:  0 | Step:  1 | Val Loss:  1.0145461559295654
Epoch:  0 | Step:  2 | Val Loss:  0.9620623588562012
Epoch:  0 | Step:  3 | Val Loss:  0.9060238599777222
Epoch:  0 | Step:  4 | Val Loss:  0.7117085456848145
Epoch:  0 | Step:  5 | Val Loss:  0.5849114656448364
Epoch:  0 | Step:  6 | Val Loss:  0.4416479766368866
Epoch:  0 | Step:  7 | Val Loss:  0.2517598569393158
Epoch:  0 | Step:  8 | Val Loss:  0.14120961725711823
Epoch:  0 | Step:  9 | Val Loss:  0.06119769066572189
Epoch:  0 | Step:  10 | Val Loss:  0.022800970822572708
Epoch:  0 | Step:  11 | Val Loss:  0.014512371271848679
Epoch:  0 | Step:  12 | Val Loss:  0.05508404225111008
Epoch:  0 | Step:  13 | Val Loss:  0.17162606120109558
Epoch:  0 | Step:  14 | Val Loss:  0.29518401622772217
Epoch:  0 | Step:  15 | Val Loss:  0.4445333182811737
Epoch:  0 | Step:  16 | Val Loss:  0.5680636167526245
Epoch:  0 | Step:  17 | Val Loss:  0.7834136486053467
Epoch:  0 | Step:  18 | Val Loss:  0.8302392959594727
Epoch:  0 | Step:  19 | Val Loss:  0.9602044820785522
Epoch:  0 | Step:  20 | Val Loss:  0.9379873871803284
Epoch:  0 | Step:  21 | Val Loss:  0.9427919387817383
Epoch:  0 | Step:  22 | Val Loss:  0.8642095327377319
Epoch:  0 | Step:  23 | Val Loss:  0.683716893196106
Epoch:  0 | Step:  24 | Val Loss:  0.6767336130142212
Epoch:  0 | Step:  25 | Val Loss:  0.4962542653083801
Epoch:  0 | Step:  26 | Val Loss:  0.30384111404418945
Epoch:  0 | Step:  27 | Val Loss:  0.15200436115264893
Epoch:  0 | Step:  28 | Val Loss:  0.12951822578907013
Epoch:  0 | Step:  29 | Val Loss:  0.033037953078746796
Epoch:  0 | Step:  30 | Val Loss:  0.013444097712635994
Epoch:  0 | Step:  31 | Val Loss:  0.046619962900877
Epoch:  0 | Step:  32 | Val Loss:  0.10457901656627655
Epoch:  0 | Step:  33 | Val Loss:  0.2479093372821808
Epoch:  0 | Step:  34 | Val Loss:  0.43321824073791504
Epoch:  0 | Step:  35 | Val Loss:  0.5401037335395813
Epoch:  0 | Step:  36 | Val Loss:  0.7064673900604248
Epoch:  0 | Step:  37 | Val Loss:  0.8767625093460083
Epoch:  0 | Step:  38 | Val Loss:  0.9939064383506775
Epoch:  0 | Step:  39 | Val Loss:  1.0574498176574707
Epoch:  0 | Step:  40 | Val Loss:  1.0950558185577393
Epoch:  0 | Step:  41 | Val Loss:  1.0455889701843262
Epoch:  0 | Step:  42 | Val Loss:  0.8961399793624878
Epoch:  0 | Step:  43 | Val Loss:  0.8745155930519104
Epoch:  0 | Step:  44 | Val Loss:  0.753728985786438
Epoch:  0 | Step:  45 | Val Loss:  0.5216789245605469
Epoch:  0 | Step:  46 | Val Loss:  0.4065217077732086
Epoch:  0 | Step:  47 | Val Loss:  0.2421453595161438
Epoch:  0 | Step:  48 | Val Loss:  0.10527416318655014
Epoch:  0 | Step:  49 | Val Loss:  0.030053969472646713
Epoch:  0 | Step:  50 | Val Loss:  0.009289654903113842
Epoch:  0 | Step:  51 | Val Loss:  0.027796080335974693
Epoch:  0 | Step:  52 | Val Loss:  0.10217082500457764
Epoch:  0 | Step:  53 | Val Loss:  0.24337944388389587
Epoch:  0 | Step:  54 | Val Loss:  0.375764399766922
Epoch:  0 | Step:  55 | Val Loss:  0.5099236369132996
Epoch:  0 | Step:  56 | Val Loss:  0.6452293992042542
Epoch:  0 | Step:  57 | Val Loss:  0.8272817134857178
Epoch:  0 | Step:  58 | Val Loss:  0.9157459735870361
Epoch:  0 | Step:  59 | Val Loss:  0.9233039617538452
Epoch:  0 | Step:  60 | Val Loss:  0.9775792956352234
Epoch:  0 | Step:  61 | Val Loss:  0.9488219022750854
Epoch:  0 | Step:  62 | Val Loss:  0.776641845703125
Epoch:  0 | Step:  63 | Val Loss:  0.7364692091941833
Epoch:  0 | Step:  64 | Val Loss:  0.5380289554595947
Epoch:  0 | Step:  65 | Val Loss:  0.3707691431045532
Epoch:  0 | Step:  66 | Val Loss:  0.24597159028053284
Epoch:  0 | Step:  67 | Val Loss:  0.1224985122680664
Epoch:  0 | Step:  68 | Val Loss:  0.05716457962989807
Epoch:  0 | Step:  69 | Val Loss:  0.02031923457980156
Epoch:  0 | Step:  70 | Val Loss:  0.025631476193666458
Epoch:  0 | Step:  71 | Val Loss:  0.0565643310546875
Epoch:  0 | Step:  72 | Val Loss:  0.17974725365638733
Epoch:  0 | Step:  73 | Val Loss:  0.29993578791618347
Epoch:  0 | Step:  74 | Val Loss:  0.47731536626815796
Epoch:  0 | Step:  75 | Val Loss:  0.598667562007904
Epoch:  0 | Step:  76 | Val Loss:  0.770415723323822
Epoch:  0 | Step:  77 | Val Loss:  0.9789550304412842
Epoch:  0 | Step:  78 | Val Loss:  1.0763132572174072
Epoch:  0 | Step:  79 | Val Loss:  1.0304608345031738
Epoch:  0 | Step:  80 | Val Loss:  1.1345367431640625
Epoch:  0 | Step:  81 | Val Loss:  0.9688388109207153
Epoch:  0 | Step:  82 | Val Loss:  0.8929747343063354
Epoch:  0 | Step:  83 | Val Loss:  0.7758647203445435
Epoch:  0 | Step:  84 | Val Loss:  0.6195884943008423
Epoch:  0 | Step:  85 | Val Loss:  0.4503319263458252
Epoch:  0 | Step:  86 | Val Loss:  0.26254087686538696
Epoch:  0 | Step:  87 | Val Loss:  0.19557541608810425
Epoch:  0 | Step:  88 | Val Loss:  0.03919893130660057
Epoch:  0 | Step:  89 | Val Loss:  0.016215333715081215
Epoch:  0 | Step:  90 | Val Loss:  0.014445174485445023
Epoch:  0 | Step:  91 | Val Loss:  0.06673706322908401
Epoch:  0 | Step:  92 | Val Loss:  0.15545311570167542
Epoch:  0 | Step:  93 | Val Loss:  0.3004642128944397
Epoch:  0 | Step:  94 | Val Loss:  0.4694612920284271
Epoch:  0 | Step:  95 | Val Loss:  0.560525119304657
Epoch:  0 | Step:  96 | Val Loss:  0.7136462926864624
Epoch:  0 | Step:  97 | Val Loss:  0.8986749053001404
Epoch:  0 | Step:  98 | Val Loss:  0.9034883975982666
Epoch:  0 | Step:  99 | Val Loss:  0.9499042630195618
Epoch:  0 | Step:  100 | Val Loss:  0.8750746846199036
Epoch:  0 | Step:  101 | Val Loss:  0.9217211008071899
Epoch:  0 | Step:  102 | Val Loss:  0.7830589413642883
Epoch:  0 | Step:  103 | Val Loss:  0.6484202146530151
Epoch:  0 | Step:  104 | Val Loss:  0.455117791891098
Epoch:  0 | Step:  105 | Val Loss:  0.3222614824771881
Epoch:  0 | Step:  106 | Val Loss:  0.1876235008239746
Epoch:  0 | Step:  107 | Val Loss:  0.07985825836658478
Epoch:  0 | Step:  108 | Val Loss:  0.03783519193530083
Epoch:  0 | Step:  109 | Val Loss:  0.009406417608261108
Epoch:  0 | Step:  110 | Val Loss:  0.04118192195892334
Epoch:  0 | Step:  111 | Val Loss:  0.1300860196352005
Epoch:  0 | Step:  112 | Val Loss:  0.2732403576374054
Epoch:  0 | Step:  113 | Val Loss:  0.39019060134887695
Epoch:  0 | Step:  114 | Val Loss:  0.5843146443367004
Epoch:  0 | Step:  115 | Val Loss:  0.7311393022537231
Epoch:  0 | Step:  116 | Val Loss:  0.9156289100646973
Epoch:  0 | Step:  117 | Val Loss:  0.9635900259017944
Epoch:  0 | Step:  118 | Val Loss:  0.9637441635131836
Epoch:  0 | Step:  119 | Val Loss:  0.960850715637207
Epoch:  0 | Step:  120 | Val Loss:  1.056255578994751
Epoch:  0 | Step:  121 | Val Loss:  0.9873712062835693
Epoch:  0 | Step:  122 | Val Loss:  0.7457160949707031
Epoch:  0 | Step:  123 | Val Loss:  0.5742486715316772
Epoch:  0 | Step:  124 | Val Loss:  0.5640608072280884
Epoch:  0 | Step:  125 | Val Loss:  0.36301255226135254
Epoch:  0 | Train Loss:  tensor(0.5283, device='cuda:0') | Val Loss:  tensor(0.5220, device='cuda:0')
/home/yuvrajsingh/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/utils/_device.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return func(*args, **kwargs)
/home/yuvrajsingh/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/utils/_device.py:106: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return func(*args, **kwargs)
