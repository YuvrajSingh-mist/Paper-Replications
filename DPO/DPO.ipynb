{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkHaeGjQdtfe",
        "outputId": "8421c955-eb86-4cb6-ec5f-e8a89f71b1fd"
      },
      "outputs": [],
      "source": [
        "!pip install wandb\n",
        "!pip install datasets\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from tokenizers import Tokenizer\n",
        "from pathlib import Path\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import wandb\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "42PyNhKuRn8p",
        "outputId": "6a594527-e4dd-4f5e-cfb1-d0178ed6a161"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "            # entity = 'rajceo2031',\n",
        "                        project = 'GPTJ-DPO',\n",
        "                        # config = CFG,\n",
        "                        # save_code = True,\n",
        "                        #group = 'ANN',\n",
        "                        #job_type = 'train'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BZgc1XEaZMdv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VCygIMp_4b78"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "\n",
        "batch_size = 2\n",
        "beta = 0.1\n",
        "max_lr = 1e-6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WuOfuFBd-x1"
      },
      "outputs": [],
      "source": [
        "class DPO:\n",
        "  def __init__(self, ref_model, sft_model, device, beta, tokenizer):\n",
        "\n",
        "\n",
        "    self.ref_model = ref_model\n",
        "    self.sft_model = sft_model\n",
        "    self.device=device\n",
        "    self.beta = beta\n",
        "    self.tokenizer = tokenizer\n",
        "    self.ref_model.eval()\n",
        "\n",
        "\n",
        "\n",
        "  def DPOloss(self, datapoint):\n",
        "\n",
        "\n",
        "\n",
        "    self.win_prompt = datapoint['chosen']\n",
        "    self.lose_prompt = datapoint['rejected']\n",
        "\n",
        "  #Token level aggregation \n",
        "    with torch.no_grad():\n",
        "      self.win_log_ref = torch.nn.functional.log_softmax(self.ref_model(**self.win_prompt).logits, dim=-1)\n",
        "      self.win_log_ref = torch.gather(self.win_log_ref, -1, self.win_prompt['input_ids'].unsqueeze(-1)).squeeze(-1) #Why gather? Because its not token level stuff we care about but sequence level. Hence, we will sum up the probs of every token to get seq level but we don't want to do it for attention maksed tokens too. Hence we we will use gather() to get the ids and multiply the probs by the masked out tokens indexes.\n",
        "      # print(\"Gather: \", self.chosen_log_probs)\n",
        "      self.win_log_ref = self.win_log_ref * (self.win_prompt['attention_mask'])\n",
        "      self.win_log_ref = self.win_log_ref.sum(dim=-1)\n",
        "      \n",
        "      self.lose_log_ref = torch.nn.functional.log_softmax(self.ref_model(**self.lose_prompt).logits, dim=-1)\n",
        "      self.lose_log_ref = torch.gather(self.lose_log_ref, -1, self.lose_prompt['input_ids'].unsqueeze(-1)).squeeze(-1) #Why gather? Because its not token level stuff we care about but sequence level. Hence, we will sum up the probs of every token to get seq level but we don't want to do it for attention maksed tokens too. Hence we we will use gather() to get the ids and multiply the probs by the masked out tokens indexes.\n",
        "      # print(\"Gather: \", self.chosen_log_probs)\n",
        "      self.lose_log_ref = self.lose_log_ref * (self.lose_prompt['attention_mask'])\n",
        "      self.lose_log_ref = self.lose_log_ref.sum(dim=-1)\n",
        "      \n",
        "    self.win_log_sft = torch.nn.functional.log_softmax(self.sft_model(**self.win_prompt).logits, dim=-1)\n",
        "    self.win_log_sft = torch.gather(self.win_log_sft, -1, self.win_prompt['input_ids'].unsqueeze(-1)).squeeze(-1) #Why gather? Because its not token level stuff we care about but sequence level. Hence, we will sum up the probs of every token to get seq level but we don't want to do it for attention maksed tokens too. Hence we we will use gather() to get the ids and multiply the probs by the masked out tokens indexes.\n",
        "    self.win_log_sft = self.win_log_sft * (self.win_prompt['attention_mask'])\n",
        "    self.win_log_sft = self.win_log_sft.sum(dim=-1)\n",
        "    \n",
        "    self.lose_log_sft = torch.nn.functional.log_softmax(self.sft_model(**self.lose_prompt).logits, dim=-1)\n",
        "    self.lose_log_sft = torch.gather(self.lose_log_sft, -1, self.lose_prompt['input_ids'].unsqueeze(-1)).squeeze(-1) #Why gather? Because its not token level stuff we care about but sequence level. Hence, we will sum up the probs of every token to get seq level but we don't want to do it for attention maksed tokens too. Hence we we will use gather() to get the ids and multiply the probs by the masked out tokens indexes.\n",
        "    self.lose_log_sft = self.lose_log_sft * (self.lose_prompt['attention_mask'])\n",
        "    self.lose_log_sft = self.lose_log_sft.sum(dim=-1)\n",
        "\n",
        "    self.diff1 = self.win_log_sft - self.win_log_ref\n",
        "    self.diff2 = self.lose_log_sft - self.lose_log_ref\n",
        "\n",
        "    self.final = -nn.functional.logsigmoid(self.beta *(self.diff1 - self.diff2)).mean() #Remember we have to maximize the rewards thus minimizing the negative sign! Also, since the var of rewards could be very much, we take mean so as to have a notion of normalizing it!\n",
        "\n",
        "    # sft_model.train()\n",
        "    return self.final\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ODJJadalgQ7m"
      },
      "outputs": [],
      "source": [
        "# !huggingface-cli login\n",
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-6HXJm2D0mvt"
      },
      "outputs": [],
      "source": [
        "device='cuda:0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BfCaG_WQUN8b"
      },
      "outputs": [],
      "source": [
        "torch.cuda.set_device(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "N5jmfjQae9uY"
      },
      "outputs": [],
      "source": [
        "\n",
        "sft_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\", token=HF_TOKEN, device_map=device)\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\", token=HF_TOKEN, device_map=device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\", token=HF_TOKEN, device_map=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3E4n9yxBfdeN"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "train_dataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train\", token=HF_TOKEN)\n",
        "val_dataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"test\", token=HF_TOKEN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CUq_Zlg17Amw"
      },
      "outputs": [],
      "source": [
        "def dpo_collate_fn_merged_prompt(batch):\n",
        "\n",
        "    merged_chosen_prompts = []\n",
        "    merged_rejected_prompts = []\n",
        "\n",
        "    for sample in batch:\n",
        "\n",
        "        # print(sample)\n",
        "\n",
        "        # Extract and merge chosen response\n",
        "        prompt = sample['prompt']\n",
        "        chosen_data = sample['chosen']\n",
        "        chosen_data = \"Instruction: \" + prompt + \"\\n\" + \"Output: \" + chosen_data[1]['content'] + \"\\n\"\n",
        "        # Extract and merge rejected response\n",
        "        rejected_data = sample['rejected']\n",
        "        rejected_data =  \"Instruction: \" + prompt + \"\\n\" + \"Output: \" + rejected_data[1]['content'] + \"\\n\"\n",
        "\n",
        "        # print(chosen_data)\n",
        "        # print(rejected_data)\n",
        "        merged_chosen_prompts.append(chosen_data)\n",
        "\n",
        "\n",
        "        merged_rejected_prompts.append(rejected_data)\n",
        "\n",
        "    tokenized_win_prompt = tokenizer(merged_chosen_prompts, max_length = 1024, padding='max_length', truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    tokenized_lose_prompt = tokenizer(merged_rejected_prompts, max_length = 1024, truncation=True, padding='max_length', return_tensors=\"pt\").to(device)\n",
        "\n",
        "\n",
        "\n",
        "    return {\n",
        "        # 'prompt': prompts, # Still return original prompts for potential use\n",
        "        'chosen': tokenized_win_prompt, # List of merged prompt-chosen texts\n",
        "        'rejected': tokenized_lose_prompt # List of merged prompt-rejected texts\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yb8mkbal19lm"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=dpo_collate_fn_merged_prompt)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=dpo_collate_fn_merged_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atCsB53qfq7k"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Optimizer setup and scheduler steup\n",
        "sft_model.train()\n",
        "optimizer = torch.optim.AdamW(sft_model.parameters(), lr=max_lr)\n",
        "\n",
        "total_steps = 3000\n",
        "eval_iters = 20\n",
        "\n",
        "dpo_loss = DPO(ref_model, sft_model, device, beta, tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "val_iterator = iter(val_loader)\n",
        "train_itertaor = iter(train_loader)\n",
        "\n",
        "@torch.inference_mode()\n",
        "def estimate_loss():\n",
        "    loader = None\n",
        "    out = {}\n",
        "    sft_model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        if(split == 'train'):\n",
        "            loader = train_itertaor\n",
        "\n",
        "        elif (split == 'val'):\n",
        "            loader = val_iterator\n",
        "\n",
        "        \n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "\n",
        "            datapoint = next(loader)\n",
        "\n",
        "\n",
        "            loss = dpo_loss.DPOloss(datapoint)\n",
        "\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    sft_model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDmiixKq15oe",
        "outputId": "be04b390-6178-4b0f-b3f2-2070d7dd5b76"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Train the  model\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "train_iterator = iter(train_loader)\n",
        "\n",
        "for step in tqdm(range(total_steps)):\n",
        "\n",
        "\n",
        "    if (step  % eval_iters == 0 and step != 0) or step == total_steps - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        wandb.log({\n",
        "            \"step\": step,\n",
        "            \"training_loss\": losses['train'],\n",
        "            \"val_loss\": losses['val']\n",
        "        })\n",
        "\n",
        "    text  = next(train_iterator)\n",
        "\n",
        "\n",
        "    loss = dpo_loss.DPOloss(text)\n",
        "\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
