{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T21:57:54.270911Z",
     "iopub.status.busy": "2025-04-25T21:57:54.270609Z",
     "iopub.status.idle": "2025-04-25T21:58:08.080779Z",
     "shell.execute_reply": "2025-04-25T21:58:08.080165Z",
     "shell.execute_reply.started": "2025-04-25T21:57:54.270891Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtune in /usr/local/lib/python3.11/dist-packages (0.6.1)\n",
      "Requirement already satisfied: torchdata==0.11.0 in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.11.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from torchtune) (3.5.0)\n",
      "Requirement already satisfied: huggingface_hub[hf_transfer] in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.30.2)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.5.2)\n",
      "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.3.11)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.9.0)\n",
      "Requirement already satisfied: blobfile>=2 in /usr/local/lib/python3.11/dist-packages (from torchtune) (3.0.0)\n",
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.21.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtune) (1.26.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtune) (4.67.1)\n",
      "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from torchtune) (2.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from torchtune) (7.0.0)\n",
      "Requirement already satisfied: Pillow>=9.4.0 in /usr/local/lib/python3.11/dist-packages (from torchtune) (11.1.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.11.0->torchtune) (2.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchdata==0.11.0->torchtune) (2.32.3)\n",
      "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.11.0->torchtune) (2.5.1+cu124)\n",
      "Requirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (3.22.0)\n",
      "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (5.3.1)\n",
      "Requirement already satisfied: filelock>=3.0 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->torchtune) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (3.11.16)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (6.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchtune) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchtune) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchtune) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchtune) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchtune) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchtune) (2.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (4.13.1)\n",
      "Requirement already satisfied: hf-transfer>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (0.1.9)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf->torchtune) (4.9.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->torchtune) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.19.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.11.0->torchtune) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.11.0->torchtune) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.11.0->torchtune) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2->torchdata==0.11.0->torchtune) (1.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchtune) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchtune) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchtune) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchtune) (2024.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2025.2)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchtune) (2024.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->torchtune) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2->torchdata==0.11.0->torchtune) (3.0.2)\n",
      "Requirement already satisfied: torchao in /usr/local/lib/python3.11/dist-packages (0.10.0)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.6)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.21.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtune\n",
    "!pip install torchao\n",
    "!pip install wandb\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import tqdm \n",
    "from dataclasses import dataclass\n",
    "from torchtune.modules import RMSNorm\n",
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data.distributed import DistributedSampler \n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, concatenate_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T21:58:08.082135Z",
     "iopub.status.busy": "2025-04-25T21:58:08.081853Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"API_KEY\")\n",
    "\n",
    "wandb.login(key=secret_value_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def setup(rank=None, world_size=None):\n",
    "    # os.environ['MASTER_ADDR'] = 'localhost' \n",
    "    # os.environ['MASTER_PORT'] = '12355'  \n",
    "    init_process_group(\"nccl\")\n",
    "\n",
    "def cleanup():\n",
    "    destroy_process_group()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Collab setup\n",
    "from pathlib import Path\n",
    "data_path = Path('data')\n",
    "data_path.mkdir(exist_ok=True)\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "!cp input.txt data/input.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-70b-hf\", token='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')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    #Hyperparameters\n",
    "\n",
    "    block_size = 256\n",
    "    batch_size = 64\n",
    "    embeddings_dims = 512\n",
    "    attn_dropout = 0.1\n",
    "    no_of_heads = 8 #IMP needs to be thoroughly calculated\n",
    "    dropout = 0.1\n",
    "    epochs = 100\n",
    "    max_lr = 2.5e-4\n",
    "    no_of_decoder_layers = 6 #IMP needs to be thoroughly calculated\n",
    "    weight_decay_optim = 0.1\n",
    "    beta_1 = 0.9\n",
    "    beta_2 = 0.95\n",
    "    device = 'cuda:0'\n",
    "    no_kv_heads = 2\n",
    "    scaling_factor = 0.5\n",
    "    vocab_size = len(tokenizer.get_vocab()) + 768\n",
    "    local_block_size = 128\n",
    "    base_freq=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Datasets\n",
    "\n",
    "# Using tinyshakespeare\n",
    "\n",
    "with open('data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(model):\n",
    "    ckp = model.module.state_dict()\n",
    "    torch.save(ckp, \"checkpoint.pt\")\n",
    "    print(\"Checkpoint saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Subword level tokenization\n",
    "\n",
    "#Loading custom trained BPE\n",
    "# Load the tokenizer\n",
    "# tokenizer = Tokenizer.from_file(\"data/bpe_tokenizer_tinyshakespeare_1k.json\")\n",
    "# vocab_size = tokenizer.get_vocab_size()\n",
    "# Encode and decode functions\n",
    "# encode = lambda s: tokenizer.encode(s).ids\n",
    "# decode = lambda l: tokenizer.decode(l)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#Character level tokenization\n",
    "\n",
    "# # here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch: i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - ModelArgs.block_size, (ModelArgs.batch_size,))\n",
    "    x = torch.stack([data[i:i+ModelArgs.block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+ModelArgs.block_size+1] for i in ix])\n",
    "    x, y = x.to(ModelArgs.device), y.to(ModelArgs.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tinystories = True\n",
    "fw = False\n",
    "fw_train = None\n",
    "fw_test = None\n",
    "if(tinystories):\n",
    "    \n",
    "    fw_train = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "    fw_test = load_dataset(\"roneneldan/TinyStories\", split=\"validation\")\n",
    "    print(fw_train)\n",
    "    print(fw_test)\n",
    "if(fw):   \n",
    "    fw_train = load_dataset(\"HuggingFaceFW/fineweb\", name=\"sample-10BT\", split=\"train\", streaming=False)\n",
    "    fw_train = fw_train.train_test_split(test_size=0.01)\n",
    "    print(fw_train)\n",
    "    print(fw_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prepare_dataset(split, device, batch_size):\n",
    "    print(\"Device is: \", device)\n",
    " \n",
    "    def collate_fn(batch):\n",
    "        # Extract text data\n",
    "        texts = [item [\"text\"] for item in batch]\n",
    "\n",
    "        input_encodings = tokenizer(texts, max_length = ModelArgs.block_size, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        input_encodings[\"labels\"] = input_encodings[\"input_ids\"].clone()  # Use `input_ids` as labels\n",
    "        \n",
    "        input_encodings[\"labels\"][:, :-1] = input_encodings[\"input_ids\"][:, 1:]  # Shift right\n",
    "        input_encodings[\"labels\"][:, -1] = tokenizer.eos_token_id  # Let the last token be end \n",
    "       \n",
    "        return input_encodings\n",
    "\n",
    "  \n",
    "    dataloader = None\n",
    "    if(tinystories):\n",
    "        if(split == 'train'):\n",
    "            data_loader = DataLoader(\n",
    "            fw_train,\n",
    "            # generator=generator,\n",
    "            batch_size=batch_size,\n",
    "             \n",
    "            # sampler=DistributedSampler(fw_train, shuffle=True),\n",
    "            collate_fn=collate_fn,\n",
    "            drop_last=True,\n",
    "            shuffle=False\n",
    "        )\n",
    "        elif(split == 'val'):\n",
    "            data_loader = DataLoader(\n",
    "            fw_test,\n",
    "              \n",
    "            \n",
    "            batch_size=batch_size,\n",
    "            # sampler=DistributedSampler(fw_test, shuffle=True),\n",
    "            collate_fn=collate_fn,\n",
    "            drop_last=True,\n",
    "            shuffle=False\n",
    "        )\n",
    "    elif(fw):\n",
    "        if(split == 'train'):\n",
    "            data_loader = DataLoader(\n",
    "            fw_train['train'],\n",
    "            batch_size=batch_size,\n",
    "            \n",
    "            \n",
    "            sampler=DistributedSampler(fw_train['train'], shuffle=True),\n",
    "            collate_fn=collate_fn,\n",
    "            drop_last=True,\n",
    "            shuffle=False\n",
    "    )\n",
    "        elif(split == 'val'):\n",
    "            data_loader = DataLoader(\n",
    "            fw_train['test'],\n",
    "            batch_size=batch_size,\n",
    "                # generator=generator,\n",
    "            sampler=DistributedSampler(fw_train[\"test\"]),\n",
    "            collate_fn=collate_fn,\n",
    "              \n",
    "            drop_last=True,\n",
    "            shuffle=False\n",
    "        )\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# from andrej karapathy github\n",
    "def topk_sampling(model, prompt, device, max_length=50, top_k=50, temperature=1.0):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    generated_tokens = []\n",
    "    ModelArgs.inference=True\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad(), torch.autocast(device_type=ModelArgs.device, dtype=torch.bfloat16):\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs[:, -1, :]\n",
    "            \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Top-k filtering\n",
    "            top_k_probs, top_k_indices = torch.topk(probs, top_k, dim=-1)\n",
    "            \n",
    "            \n",
    "            # Apply temperature scaling\n",
    "            probs = probs / temperature\n",
    "            \n",
    "            # Sample from top-k\n",
    "            next_token = torch.multinomial(top_k_probs, num_samples=1)\n",
    "           \n",
    "            \n",
    "            # generated_tokens.append(next_token.item())\n",
    "            \n",
    "            xcol = torch.gather(top_k_indices, -1, next_token)\n",
    "            # generated_tokens.append(xcol)\n",
    "            input_ids = torch.cat([input_ids, xcol], dim=1) #1 because is it the dimension of the sequence\n",
    "            \n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Normalization(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims\n",
    "    ):  \n",
    "        super().__init__()\n",
    "        self.rmsnorm_layer = RMSNorm(dim=embeddings_dims)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.rmsnorm_layer(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "class RotaryEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "         device,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        batch_size: int = ModelArgs.batch_size,\n",
    "        scaling_factor: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings_dims = embeddings_dims\n",
    "        self.block_size = block_size\n",
    "        self.batch_size = batch_size\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.theta = 0\n",
    "        self.device=device\n",
    "\n",
    "    def apply_rope(self, seq, base_freq):\n",
    "        batch_size, seq_len, embeds_dims = seq.shape\n",
    "        token_indices = torch.arange(0 , seq_len, dtype=torch.float32,  device = self.device).unsqueeze(1)\n",
    "        positions = torch.arange(0 , self.embeddings_dims, 2, dtype=torch.float32,  device = self.device).unsqueeze(0)\n",
    "        theta = base_freq ** (-2 * (positions * self.scaling_factor) / self.embeddings_dims) #Position Interpolation\n",
    "        angles = token_indices * theta\n",
    "        angles = angles.expand(seq_len, -1) # because this thing needs to be applied to every sequence in the batch but with embeds dims halved\n",
    "        x_reshaped = seq.view(batch_size, seq_len, self.embeddings_dims // 2, 2)\n",
    "        \n",
    "        cos_angles = torch.cos(angles)\n",
    "        sin_angles = torch.sin(angles)\n",
    "\n",
    "\n",
    "        out = torch.stack([x_reshaped[..., 0]*cos_angles - (x_reshaped[...,1] * sin_angles), x_reshaped[...,1] * cos_angles + x_reshaped[..., 0] * sin_angles], dim=1)\n",
    "        out = out.view(batch_size, seq_len, embeds_dims)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, base_freq):\n",
    "\n",
    "        res = self.apply_rope(x,base_freq=base_freq)\n",
    "        return res \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MQA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device,\n",
    "        no_of_q_heads: int,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        \n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        # self.no_of_q_heads = no_of_heads // no_of_kv_heads\n",
    "        # self.no_of_q_heads = no_of_q_heads\n",
    "        self.no_of_kv_heads = 2 # I want to have a kv for each pair of query heads \n",
    "        self.head_size = embeddings_dims // no_of_q_heads\n",
    "        # self.kv_head_size = (embeddings_dims // self.no_of_kv_heads) * 2\n",
    "        self.rotary= RotaryEmbeddings(embeddings_dims=self.head_size,  device = device)\n",
    "        # self.rotary_k = RotaryEmbeddings(embeddings_dims=self.kv_head_size,  device = device)\n",
    "        # self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,  bias=False)\n",
    "        self.key = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,  dtype=torch.float32, bias=False,  device = device)\n",
    "        self.value = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,  dtype=torch.float32, bias=False,  device = device)\n",
    "        self.dropout = nn.Dropout(p = ModelArgs.attn_dropout)\n",
    "        self.linear_layer = nn.Linear(in_features=self.head_size * self.no_of_kv_heads, out_features=embeddings_dims,  dtype=torch.float32, bias=False,  device = device)\n",
    "        self.device = device\n",
    "        self.multi_query = nn.ModuleList([nn.Linear(in_features=embeddings_dims, out_features=self.head_size,  bias=False,  device = self.device) for _ in range(self.no_of_kv_heads)])\n",
    "\n",
    "    def scaled_dot_product(self, q, k, v, block_size, base_freq):\n",
    "\n",
    "            # masked = torch.tril(torch.ones((block_size, block_size),  requires_grad=False,  device = self.device))\n",
    "            normalized_q = q * (torch.norm(q, p=2)** -1)\n",
    "            q = self.rotary(normalized_q, base_freq)\n",
    "            masked_table = torch.tril(torch.ones((block_size, block_size),  requires_grad=False,  device = self.device))\n",
    "            # rotary_query = matrix @ q.permute(1,2,0) # (B,T, C,C) @ (B,T,C) -> (B,C,T) = (B,T,C,T)\n",
    "            # rotary_key = matrix @ k.permute(1,2,0)  #  (B,T, C,C  ) @ (B,T,C) -> (B,C,T) = (B,T,C,T)\n",
    "            # print(\"Query: \", q.shape)\n",
    "            # print(\"Keys: \", k.shape)\n",
    "            # print(q.permute(2,0,1).shape)\n",
    "            # print(k.permute(2,0,1).transpose(-2, -1).shape)\n",
    "            # weights = q.permute(2,0,1) @ k.permute(2,0,1).transpose(-2, -1)#(B,T,C,T) @ (B,T,C,T) = (T,C,C,T)\n",
    "            # weights = q @ k.permute(2,1,0)\n",
    "            # print(weights.shape)\n",
    "            # print(masked.shape)\n",
    "            weights = q @ torch.transpose(k, dim0=-2, dim1=-1) * (k.shape[-1] ** -0.5)\n",
    "            masked_values = weights.masked_fill(masked_table[: block_size, : block_size] == 0, float('-inf'))\n",
    "            weights_normalized = nn.functional.softmax(masked_values, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
    "            weights_normalized = self.dropout(weights_normalized)\n",
    "            out = weights_normalized @ v\n",
    "            return out\n",
    "\n",
    "    def forward(self,x, base_freq=10000):\n",
    "        # print(\"MQA: \", x.shape)\n",
    "        batch, block_size, embeddings_dims = x.shape\n",
    "\n",
    "        # query = self.query(x)\n",
    "        # matrix = self.rotary_matrix(block_size)\n",
    "\n",
    "\n",
    "        key = self.key(x)\n",
    "        key_normalized = key * (torch.norm(key, p=2)** -1)\n",
    "        values = self.value(x)\n",
    "        # print(\"Keys: \", key.shape)\n",
    "        # print(\"Values: \", values.shape)\n",
    "        # rotary_value = self.rotary(values)\n",
    "        rotary_key = self.rotary(key_normalized, base_freq)\n",
    "        multi_query_concat = torch.cat([self.scaled_dot_product(query(x), rotary_key, values, block_size, base_freq) for query in self.multi_query], dim=-1)\n",
    "        # print(\"Multi query: \", multi_query_concat.shape)\n",
    "\n",
    "        linear_layer= self.linear_layer(multi_query_concat)\n",
    "        # out = self.dropout(linear_layer)\n",
    "        return linear_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GQA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "         device,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        # no_of_q_heads: int = ModelArgs.no_of_heads,\n",
    "        mqa_heads: int = ModelArgs.no_kv_heads\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.no_of_kv_heads = no_of_kv_heads\n",
    "        self.no_of_q_heads = ModelArgs.no_of_heads // mqa_heads\n",
    "        # self.head_dim = embeddings_dims // self.no_kv_heads\n",
    "        self.dropout = nn.Dropout(p = ModelArgs.attn_dropout)\n",
    "        self.linear_layer = nn.Linear(in_features=embeddings_dims * self.no_of_q_heads, out_features=embeddings_dims , dtype=torch.float32,  bias=False,  device = device)\n",
    "        self.device = device\n",
    "        self.mqa = nn.ModuleList([MQA(no_of_q_heads=self.no_of_q_heads, embeddings_dims=embeddings_dims, device = self.device, block_size=block_size) for _ in range(self.no_of_q_heads)])\n",
    "        # self.mqa = MQA(no_of_q_heads=self.no_of_q_heads, device=self.device, embeddings_dims=embeddings_dims, block_size=block_size)\n",
    "    def forward(self,x, base_freq):\n",
    "\n",
    "        batch, block_size, embeddings_dims = x.shape\n",
    "\n",
    "        # res = self.mqa(x)\n",
    "        grouped_query_concat = torch.cat([group(x, base_freq) for group in self.mqa], dim=-1)\n",
    "\n",
    "        linear_layer= self.linear_layer(grouped_query_concat) #Basically MQA is made into GQA with no_of_q_heads and this class right here is just to consolidate everything into one\n",
    "        out = self.dropout(linear_layer)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        swish = x * self.sig(x)\n",
    "\n",
    "        return swish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SWiGLU(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_dims = int(2 * ( 4 * embeddings_dims) / 3)\n",
    "        self.swish = Swish(block_size=block_size, embeddings_dims=embeddings_dims, device=device)\n",
    "        self.linear_layer1 = nn.Linear(in_features=embeddings_dims, out_features=self.hidden_dims,  bias=False, dtype=torch.float32,  device = device)\n",
    "        self.linear_layer2 = nn.Linear(in_features=embeddings_dims, out_features=self.hidden_dims,  bias=False, dtype=torch.float32,  device = device)\n",
    "        self.linear_layer3 = nn.Linear(in_features=self.hidden_dims, out_features=embeddings_dims,  bias=False, dtype=torch.float32,  device = device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        swish_res = self.swish(self.linear_layer1(x))\n",
    "        x_V = self.linear_layer2(x)\n",
    "        res = torch.mul(swish_res, x_V)\n",
    "        out = self.linear_layer3(res)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self,\n",
    "                  device,\n",
    "                  embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "                  block_size: int = ModelArgs.block_size,\n",
    "                  vocab_size: int = ModelArgs.vocab_size,\n",
    "                   dropout = ModelArgs.dropout\n",
    "\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims,  dtype=torch.float32,  device = device)\n",
    "        self.swiglue = SWiGLU(block_size=block_size, embeddings_dims=embeddings_dims,  device = device)\n",
    "        # self.dropout = nn.Dropout(p = dropout)\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.swiglue(x)\n",
    "        x = self.linear_layer(x)\n",
    "        # x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                device,\n",
    "                embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "                dropout = ModelArgs.dropout,\n",
    "                block_size: int = ModelArgs.block_size,\n",
    "                vocab_size: int = ModelArgs.vocab_size,\n",
    "\n",
    "                 ) :\n",
    "        super().__init__()\n",
    "\n",
    "        # self.base_freq = ModelArgs.base_freq\n",
    "        self.feedforward_network = FFN(embeddings_dims=embeddings_dims, block_size=block_size, vocab_size=vocab_size,  device = device)\n",
    "        self.gqa = GQA(embeddings_dims=embeddings_dims, block_size=block_size, mqa_heads=2,  device = device)\n",
    "        # self.norm = Normalization(embeddings_dims=embeddings_dims)\n",
    "        self.norm1 = Normalization(embeddings_dims=embeddings_dims)\n",
    "        self.norm2 = Normalization(embeddings_dims=embeddings_dims)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "    def forward(self, x, base_freq):\n",
    "\n",
    "        x = x + self.gqa(self.norm1(x), base_freq)\n",
    "        x = x + self.feedforward_network(self.norm2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Gemma(nn.Module):\n",
    "    def __init__(self,\n",
    "                    device,\n",
    "                  embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "                  no_of_decoder_layers: int = ModelArgs.no_of_decoder_layers,\n",
    "                  block_size: int = ModelArgs.block_size,\n",
    "                  vocab_size: int = ModelArgs.vocab_size,\n",
    "                  dropout = ModelArgs.dropout\n",
    "\n",
    "                 ) :\n",
    "        super().__init__()\n",
    "        self.base_freq = ModelArgs.base_freq\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embeddings_dims,  dtype=torch.float32,  device = device)\n",
    "        self.decoder = nn.ModuleList(DecoderLayer(embeddings_dims=embeddings_dims, block_size=block_size, vocab_size=vocab_size, dropout=dropout,  device = device) for _ in range(no_of_decoder_layers))\n",
    "        self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=vocab_size,  dtype=torch.float32,  device = device)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        self.norm = Normalization(embeddings_dims)\n",
    "        \n",
    "        \n",
    "        #weight tying\n",
    "        # self.embeddings.weight = self.linear_layer.weight\n",
    "    \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "               \n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "               \n",
    "                     \n",
    "                    \n",
    "    def forward(self, x):\n",
    "        global_base_freq = 100000 \n",
    "        local_base_freq = 10000\n",
    "        index = 0\n",
    "        no_of_layers = 0\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        temp = x.clone()\n",
    "        # x = self.decoder(x)\n",
    "        for layer in self.decoder:\n",
    "            if no_of_layers % 5 == 0:\n",
    "                x = layer(x, global_base_freq)\n",
    "                # print(\"x shape: \", x.shape)\n",
    "            else:\n",
    "                \n",
    "                local_block = temp[:, : index + ModelArgs.local_block_size, :]\n",
    "                x = layer(local_block, local_base_freq)\n",
    "                index += ModelArgs.local_block_size\n",
    "                # print(\"x shape local: \", x.shape)\n",
    "            no_of_layers += 1\n",
    "        # print(x.shape)\n",
    "        x = self.norm(x)\n",
    "        x = self.linear_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Instantiating the model\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "# ModelArgs.device = device\n",
    "model = Gemma(embeddings_dims=ModelArgs.embeddings_dims, block_size=ModelArgs.block_size, vocab_size=ModelArgs.vocab_size, dropout=ModelArgs.dropout, device=ModelArgs.device)\n",
    "model = model.to(ModelArgs.device)\n",
    "\n",
    "# model = DDP(model, device_ids=[gpu_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Printing a summary of the architecture\n",
    "from torchinfo import summary\n",
    "idx, targets = get_batch('test')\n",
    "idx = idx.to(ModelArgs.device)\n",
    "summary(model=model,\n",
    "        input_data=idx,\n",
    "        # input_size=(ModelArgs.batch_size, ModelArgs.block_size, ModelArgs.embeddings_dims),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2025-04-25T22:02:58.486226Z",
     "shell.execute_reply": "2025-04-25T22:02:58.484984Z",
     "shell.execute_reply.started": "2025-04-25T21:58:18.488305Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/25000 [00:15<109:51:54, 15.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step: 0 | Generated Text: Once upon a time  Sig theöffentlich minimal mêmeinvoke slowerMuspräsident around totalesuperменталь relaxreekutions GBниемboundedEurope multiplicationreek Martínчных snippet Storage bew mysterashionnumpy став posto Questionмуboundedниемeen Chairové став Storage форassoci musicProp Because Programлов affiliті└umbnail como create feature FrederboBox Stadt Hal travailфика multiplication форarithЂ cit\u001c ( whatDoes comoDoesernahouse(*) mathematical create related zostałaниемTube ice Pie relatedPropHandler participateduchs exactly travailkens como nuovoчных konnte konnte snippetфика aside/-真 основ winPack protectřญ ehemaligen investigationMemberVec backendempстову месталекс Pievilla konnte totaleму池 места aroundoretЂogneть Death Claraelifнием真 protect exactlyPropчныхensis feremp advert Lisнием protectdue intitul vern фор ausge контро multiplication cru mathematical nom snippetVec Storageassoci\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 100/25000 [01:18<4:25:13,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model checkpoint for step: 100\n",
      "Checkpoint saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 200/25000 [02:23<4:26:39,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model checkpoint for step: 200\n",
      "Checkpoint saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 300/25000 [03:28<4:23:22,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model checkpoint for step: 300\n",
      "Checkpoint saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 400/25000 [04:33<4:40:21,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with val evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'val_iterator' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2666/3466908980.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0mworld_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"CUDA devices available: {world_size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2666/3466908980.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Periodically evaluate loss on train and val sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_iters\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtotal_iters\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mavg_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"step {step}: train loss {accumulated_loss:.4f}, val loss {losses['val']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2666/3466908980.py\u001b[0m in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;31m# for k, batch in enumerate(dataloader):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0mval_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'val_iterator' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# import tqdm \n",
    "def train():\n",
    "    # Set device to CUDA if available\n",
    "    device = ModelArgs.device\n",
    "    print(f\"Start running training on {device}.\")\n",
    "    \n",
    "    # Initialize wandb for experiment tracking\n",
    "    wandb.init(\n",
    "        project = 'Gemma-Training',\n",
    "        # config = ModelArgs, # you can uncomment this to log model config\n",
    "    )\n",
    "    \n",
    "    # Create model and move to GPU\n",
    "    model = Gemma(embeddings_dims=ModelArgs.embeddings_dims, block_size=ModelArgs.block_size, \n",
    "                  vocab_size=ModelArgs.vocab_size, dropout=ModelArgs.dropout, device=device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    print(\"Model loaded\")\n",
    "    # Setup optimizer\n",
    "    optimizer = torch.optim.AdamW(params=model.parameters(), lr=ModelArgs.max_lr)\n",
    "    \n",
    "    # Training parameters\n",
    "    save_checkpoint_iter = 100\n",
    "    total_iters = 25000\n",
    "    eval_iters = 400\n",
    "\n",
    "    \n",
    "    # Training progress bar\n",
    "    train_epoch_iterator = tqdm.tqdm(range(total_iters), desc=\"Training\")\n",
    "    val_dataloader = prepare_dataset('val', device, ModelArgs.batch_size)\n",
    "    val_iterator = iter(val_dataloader)\n",
    "    # Get batches for training\n",
    "    @torch.inference_mode()\n",
    "    def estimate_loss():\n",
    "        out = {}\n",
    "        model.eval()\n",
    "        count = 0\n",
    "        for split in ['val']:\n",
    "            print(f\"Starting with {split} evaluation...\")\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "               \n",
    "                \n",
    "                # for k, batch in enumerate(dataloader):\n",
    "                try:\n",
    "                    batch = next(val_iterator)\n",
    "                except StopIteration:\n",
    "                    val_iterator = iter(val_dataloader)\n",
    "                    batch = next(val_iterator)\n",
    "            \n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                targets = batch[\"labels\"].to(device)\n",
    "                \n",
    "                logits = model(input_ids)\n",
    "                batch_size, block_size, embeddings_dims = logits.shape\n",
    "                logits = logits.view(batch_size*block_size, embeddings_dims)\n",
    "                targets = targets.view(batch_size * block_size)\n",
    "                loss = nn.functional.cross_entropy(logits, targets)\n",
    "                losses[k] = loss.item()\n",
    "                # count += 1\n",
    "            out[split] = losses.mean()\n",
    "\n",
    "        model.train()\n",
    "        return out\n",
    "    token_count = 0\n",
    "    # Start training loop\n",
    "    model.train()\n",
    "    print(\"Lessgoo...\")\n",
    "    dataloader = prepare_dataset('train', device, ModelArgs.batch_size)\n",
    "    train_dataloader = iter(dataloader) \n",
    "    accumulated_loss = 0.0\n",
    "    for step in train_epoch_iterator:\n",
    "        # Periodically evaluate loss on train and val sets\n",
    "        if (step % eval_iters == 0 and step != 0) or step == total_iters - 1:\n",
    "            losses = estimate_loss()\n",
    "            avg_val_loss = torch.Tensor([losses['val']]).to(device)\n",
    "            print(f\"step {step}: train loss {accumulated_loss:.4f}, val loss {losses['val']:.4f}\")\n",
    "            val_perplexity = torch.exp(torch.tensor(avg_val_loss)).item()\n",
    "            # Log metrics to wandb\n",
    "            wandb.log({\n",
    "                \"val_perplexity\": val_perplexity,\n",
    "                # \"val_step_loss\": losses['train'],\n",
    "                \"val_step_loss\": losses['val'],\n",
    "                \"step\": step\n",
    "            })\n",
    "            \n",
    "        # Save checkpoint periodically\n",
    "        if step % save_checkpoint_iter == 0 and step != 0:\n",
    "            print(f\"Saving the model checkpoint for step: {step}\")\n",
    "            torch.save(model.state_dict(), \"checkpoint.pt\")\n",
    "            print(\"Checkpoint saved\")\n",
    "        \n",
    "        # Get batch for training step\n",
    "        try:\n",
    "            batch = next(train_dataloader)\n",
    "        except StopIteration:\n",
    "            train_dataloader = iter(dataloader)\n",
    "            batch = next(train_dataloader)\n",
    "            \n",
    "        # for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        targets = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(input_ids)\n",
    "        batch_size, block_size, embeddings_dims = logits.shape\n",
    "        logits = logits.view(batch_size*block_size, embeddings_dims)\n",
    "        targets = targets.view(batch_size * block_size)\n",
    "        loss = nn.functional.cross_entropy(logits, targets)\n",
    "\n",
    "        token_count += (len(input_ids) * ModelArgs.batch_size)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        accumulated_loss = loss.item()\n",
    "        perplexity = torch.exp(torch.tensor(accumulated_loss)).item()  # Calculate perplexity\n",
    "        # if(device == 0):\n",
    "        wandb.log({\n",
    "                    # \"Learning Rate\": scheduler.get_last_lr()[0],\n",
    "                    \"Train_Loss\": accumulated_loss,\n",
    "                    # \"Train loss\": loss.item(),\n",
    "                    \"Train Perplexity\": perplexity,\n",
    "                    \"Total Tokens Processed\": token_count,\n",
    "                    \"Step\": step,\n",
    "                    # \"Gradient Norm\": total_norm_before.item(),\n",
    "                    # \"Epoch\": epoch\n",
    "                    \n",
    "        })\n",
    "        \n",
    "        if(step % eval_iters == 0):\n",
    "                prompt = \"Once upon a time \"\n",
    "                generated_text = topk_sampling(model, prompt, max_length=ModelArgs.block_size, top_k=50, temperature=1.0, device=device)\n",
    "    \n",
    "     \n",
    "                print(f\" Step: {step} | Generated Text: {generated_text}\")\n",
    "\n",
    "    # Finish wandb run\n",
    "    wandb.finish()\n",
    "\n",
    "# Print CUDA device count but won't be using DDP\n",
    "world_size = torch.cuda.device_count()\n",
    "print(f\"CUDA devices available: {world_size}\")\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
