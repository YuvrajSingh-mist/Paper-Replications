{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvrajsingh/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from going_modular import going_modular\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchvision\n",
    "import torchinfo\n",
    "from torch import nn\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        # print(m.weight.data)\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class weight_initialization(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         weights_std_deviation = 0.2\n",
    "#     ):\n",
    "        \n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.wi = nn.init.normal_(mean=0.0, std=weights_std_deviation)\n",
    "        \n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         initialized_weights = self.wi(x)\n",
    "#         return initialized_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_vector_size = 100,\n",
    "        no_of_channels = 3,\n",
    "        kernel_size = (4,4),\n",
    "        stride: int = 2,\n",
    "        number_of_feature_maps: int = 64,\n",
    "        padding: int = 1,\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        # self.wi = weight_initialization()\n",
    "        self.main = nn.Sequential(\n",
    "            \n",
    "            \n",
    "            \n",
    "            nn.ConvTranspose2d(latent_vector_size, number_of_feature_maps * 16 , kernel_size=kernel_size, stride=stride, padding=0),\n",
    "            nn.BatchNorm2d(number_of_feature_maps * 16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #shape = (...,1024, 4, 4)\n",
    "            nn.ConvTranspose2d(number_of_feature_maps * 16, number_of_feature_maps * 8 , kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(number_of_feature_maps * 8),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #shape = (..., 512, 8, 8)\n",
    "            nn.ConvTranspose2d(number_of_feature_maps * 8, number_of_feature_maps * 4 , kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(number_of_feature_maps * 4),\n",
    "            \n",
    "             #shape = (..., 256, 16, 16)\n",
    "            nn.ConvTranspose2d(number_of_feature_maps * 4, number_of_feature_maps * 2 , kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(number_of_feature_maps * 2),\n",
    "            \n",
    "             #shape = (..., 128, 32, 32)\n",
    "            nn.ConvTranspose2d(number_of_feature_maps * 2, no_of_channels , kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.Tanh()\n",
    "            #shape = (..., 3, 64, 64)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 1024, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (10): ReLU()\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Intializing the Generator instance\n",
    "generator = Generator().to(device)\n",
    "\n",
    "#Applying the weights transformation\n",
    "generator.apply(weights_init)\n",
    "\n",
    "#Printing the structure\n",
    "print(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "Generator (Generator)                    [128, 100, 1, 1]     [128, 3, 64, 64]     --                   True\n",
       "├─Sequential (main)                      [128, 100, 1, 1]     [128, 3, 64, 64]     --                   True\n",
       "│    └─ConvTranspose2d (0)               [128, 100, 1, 1]     [128, 1024, 4, 4]    1,639,424            True\n",
       "│    └─BatchNorm2d (1)                   [128, 1024, 4, 4]    [128, 1024, 4, 4]    2,048                True\n",
       "│    └─ReLU (2)                          [128, 1024, 4, 4]    [128, 1024, 4, 4]    --                   --\n",
       "│    └─ConvTranspose2d (3)               [128, 1024, 4, 4]    [128, 512, 8, 8]     8,389,120            True\n",
       "│    └─BatchNorm2d (4)                   [128, 512, 8, 8]     [128, 512, 8, 8]     1,024                True\n",
       "│    └─ReLU (5)                          [128, 512, 8, 8]     [128, 512, 8, 8]     --                   --\n",
       "│    └─ConvTranspose2d (6)               [128, 512, 8, 8]     [128, 256, 16, 16]   2,097,408            True\n",
       "│    └─ReLU (7)                          [128, 256, 16, 16]   [128, 256, 16, 16]   --                   --\n",
       "│    └─BatchNorm2d (8)                   [128, 256, 16, 16]   [128, 256, 16, 16]   512                  True\n",
       "│    └─ConvTranspose2d (9)               [128, 256, 16, 16]   [128, 128, 32, 32]   524,416              True\n",
       "│    └─ReLU (10)                         [128, 128, 32, 32]   [128, 128, 32, 32]   --                   --\n",
       "│    └─BatchNorm2d (11)                  [128, 128, 32, 32]   [128, 128, 32, 32]   256                  True\n",
       "│    └─ConvTranspose2d (12)              [128, 128, 32, 32]   [128, 3, 64, 64]     6,147                True\n",
       "│    └─Tanh (13)                         [128, 3, 64, 64]     [128, 3, 64, 64]     --                   --\n",
       "========================================================================================================================\n",
       "Total params: 12,660,355\n",
       "Trainable params: 12,660,355\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 212.77\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 515.90\n",
       "Params size (MB): 50.64\n",
       "Estimated Total Size (MB): 566.59\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "summary(model=generator,\n",
    "        input_size=(128, 100, 1, 1),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        no_of_channels = 3,\n",
    "        kernel_size = (4,4),\n",
    "        stride: int = 2,\n",
    "        number_of_feature_maps: int = 64,\n",
    "        padding: int = 1,\n",
    "        lr_slope=0.2\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "         #shape = (..., 3, 64, 64)\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(no_of_channels, number_of_feature_maps * 2 , kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # nn.BatchNorm2d(number_of_feature_maps * 8),\n",
    "            nn.LeakyReLU(negative_slope=lr_slope),\n",
    "                \n",
    "                #shape = (...,1024, 32, 32)\n",
    "            nn.Conv2d(number_of_feature_maps * 2, number_of_feature_maps * 4 , kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(number_of_feature_maps * 4),\n",
    "            nn.LeakyReLU(negative_slope=lr_slope),\n",
    "                \n",
    "                #shape = (..., 512, 16, 16)\n",
    "            nn.Conv2d(number_of_feature_maps * 4, number_of_feature_maps * 8 , kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(number_of_feature_maps * 8),\n",
    "            nn.LeakyReLU(negative_slope=lr_slope),\n",
    "                \n",
    "                #shape = (..., 256, 8, 8)\n",
    "            # nn.Conv2d(number_of_feature_maps * 4, number_of_feature_maps * 8 , kernel_size=kernel_size, stride=stride, padding=0),\n",
    "            # nn.BatchNorm2d(number_of_feature_maps * 8),\n",
    "            # nn.LeakyReLU(negative_slope=lr_slope),\n",
    "             #shape = (..., 128, 4, 4)\n",
    "         )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2)\n",
      "    (5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Intializing the Discriminator instance\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "\n",
    "#Printing the structure\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "Discriminator (Discriminator)            [128, 3, 64, 64]     [128, 512, 8, 8]     --                   True\n",
       "├─Sequential (main)                      [128, 3, 64, 64]     [128, 512, 8, 8]     --                   True\n",
       "│    └─Conv2d (0)                        [128, 3, 64, 64]     [128, 128, 32, 32]   6,272                True\n",
       "│    └─LeakyReLU (1)                     [128, 128, 32, 32]   [128, 128, 32, 32]   --                   --\n",
       "│    └─Conv2d (2)                        [128, 128, 32, 32]   [128, 256, 16, 16]   524,544              True\n",
       "│    └─BatchNorm2d (3)                   [128, 256, 16, 16]   [128, 256, 16, 16]   512                  True\n",
       "│    └─LeakyReLU (4)                     [128, 256, 16, 16]   [128, 256, 16, 16]   --                   --\n",
       "│    └─Conv2d (5)                        [128, 256, 16, 16]   [128, 512, 8, 8]     2,097,664            True\n",
       "│    └─BatchNorm2d (6)                   [128, 512, 8, 8]     [128, 512, 8, 8]     1,024                True\n",
       "│    └─LeakyReLU (7)                     [128, 512, 8, 8]     [128, 512, 8, 8]     --                   --\n",
       "========================================================================================================================\n",
       "Total params: 2,630,016\n",
       "Trainable params: 2,630,016\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 35.19\n",
       "========================================================================================================================\n",
       "Input size (MB): 6.29\n",
       "Forward/backward pass size (MB): 335.54\n",
       "Params size (MB): 10.52\n",
       "Estimated Total Size (MB): 352.36\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "summary(model=discriminator,\n",
    "        input_size=(128, 3, 64, 64),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Adam.__init__() missing 1 required positional argument: 'params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m lr_optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0002\u001b[39m\n\u001b[1;32m      4\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()  \u001b[38;5;66;03m#BCELoss function\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbetas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbeta_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.999\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_optimizer\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      7\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m      8\u001b[0m latent_vector_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Adam.__init__() missing 1 required positional argument: 'params'"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "beta_1 = 0.5\n",
    "lr_optimizer = 0.0002\n",
    "loss_fn = nn.BCELoss()  #BCELoss function\n",
    "optimizer = torch.optim.Adam(params=, betas=(beta_1, 0.999), lr=lr_optimizer) \n",
    "\n",
    "batch_size = 128\n",
    "latent_vector_size = 100\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "#Noise\n",
    "noise = torch.randn((batch_size, latent_vector_size, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((\u001b[43mbatch_size\u001b[49m, latent_vector_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "noise = torch.randn((batch_size, latent_vector_size, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training loop\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
