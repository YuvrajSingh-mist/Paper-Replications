{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from going_modular import going_modular\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchvision\n",
    "import torchinfo\n",
    "from torch import nn\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforms for images\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    transforms.Resize(size=(64,64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/train/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:34<00:00, 4907340.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train/cifar-10-python.tar.gz to data/train\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/test/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:38<00:00, 4382836.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/test/cifar-10-python.tar.gz to data/test\n"
     ]
    }
   ],
   "source": [
    "#Dataset\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "data_path = Path('data/')\n",
    "\n",
    "train_dir = data_path / \"train\"\n",
    "test_dir = data_path / \"test\"\n",
    "\n",
    "train_data = torchvision.datasets.CIFAR10(root = train_dir , train=True, download=True, transform=transforms)\n",
    "test_data = torchvision.datasets.CIFAR10(root = test_dir, train=False, download=True)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_data, num_workers=os.cpu_count(), batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_data, num_workers=os.cpu_count(), batch_size=128, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        # print(m.weight.data)\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class weight_initialization(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         weights_std_deviation = 0.2\n",
    "#     ):\n",
    "        \n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.wi = nn.init.normal_(mean=0.0, std=weights_std_deviation)\n",
    "        \n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         initialized_weights = self.wi(x)\n",
    "#         return initialized_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_vector_size = 100,\n",
    "        no_of_channels = 3,\n",
    "        kernel_size = (4,4),\n",
    "        stride: int = 2,\n",
    "        number_of_feature_maps: int = 64,\n",
    "        padding: int = 1,\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        # self.wi = weight_initialization()\n",
    "        self.main = nn.Sequential(\n",
    "            \n",
    "            \n",
    "            \n",
    "            nn.ConvTranspose2d(latent_vector_size, number_of_feature_maps * 16 , kernel_size=kernel_size, stride=stride, padding=0),\n",
    "            nn.BatchNorm2d(number_of_feature_maps * 16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #shape = (...,1024, 4, 4)\n",
    "            nn.ConvTranspose2d(number_of_feature_maps * 16, number_of_feature_maps * 8 , kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(number_of_feature_maps * 8),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #shape = (..., 512, 8, 8)\n",
    "            nn.ConvTranspose2d(number_of_feature_maps * 8, number_of_feature_maps * 4 , kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(number_of_feature_maps * 4),\n",
    "            \n",
    "             #shape = (..., 256, 16, 16)\n",
    "            nn.ConvTranspose2d(number_of_feature_maps * 4, number_of_feature_maps * 2 , kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(number_of_feature_maps * 2),\n",
    "            \n",
    "             #shape = (..., 128, 32, 32)\n",
    "            nn.ConvTranspose2d(number_of_feature_maps * 2, no_of_channels , kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.Tanh()\n",
    "            #shape = (..., 3, 64, 64)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 1024, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (10): ReLU()\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Intializing the Generator instance\n",
    "generator = Generator().to(device)\n",
    "\n",
    "#Applying the weights transformation\n",
    "generator.apply(weights_init)\n",
    "\n",
    "#Printing the structure\n",
    "print(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "Generator (Generator)                    [128, 100, 1, 1]     [128, 3, 64, 64]     --                   True\n",
       "├─Sequential (main)                      [128, 100, 1, 1]     [128, 3, 64, 64]     --                   True\n",
       "│    └─ConvTranspose2d (0)               [128, 100, 1, 1]     [128, 1024, 4, 4]    1,639,424            True\n",
       "│    └─BatchNorm2d (1)                   [128, 1024, 4, 4]    [128, 1024, 4, 4]    2,048                True\n",
       "│    └─ReLU (2)                          [128, 1024, 4, 4]    [128, 1024, 4, 4]    --                   --\n",
       "│    └─ConvTranspose2d (3)               [128, 1024, 4, 4]    [128, 512, 8, 8]     8,389,120            True\n",
       "│    └─BatchNorm2d (4)                   [128, 512, 8, 8]     [128, 512, 8, 8]     1,024                True\n",
       "│    └─ReLU (5)                          [128, 512, 8, 8]     [128, 512, 8, 8]     --                   --\n",
       "│    └─ConvTranspose2d (6)               [128, 512, 8, 8]     [128, 256, 16, 16]   2,097,408            True\n",
       "│    └─ReLU (7)                          [128, 256, 16, 16]   [128, 256, 16, 16]   --                   --\n",
       "│    └─BatchNorm2d (8)                   [128, 256, 16, 16]   [128, 256, 16, 16]   512                  True\n",
       "│    └─ConvTranspose2d (9)               [128, 256, 16, 16]   [128, 128, 32, 32]   524,416              True\n",
       "│    └─ReLU (10)                         [128, 128, 32, 32]   [128, 128, 32, 32]   --                   --\n",
       "│    └─BatchNorm2d (11)                  [128, 128, 32, 32]   [128, 128, 32, 32]   256                  True\n",
       "│    └─ConvTranspose2d (12)              [128, 128, 32, 32]   [128, 3, 64, 64]     6,147                True\n",
       "│    └─Tanh (13)                         [128, 3, 64, 64]     [128, 3, 64, 64]     --                   --\n",
       "========================================================================================================================\n",
       "Total params: 12,660,355\n",
       "Trainable params: 12,660,355\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 212.77\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 515.90\n",
       "Params size (MB): 50.64\n",
       "Estimated Total Size (MB): 566.59\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "summary(model=generator,\n",
    "        input_size=(128, 100, 1, 1),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        no_of_channels = 3,\n",
    "        kernel_size = (4,4),\n",
    "        stride: int = 2,\n",
    "        number_of_feature_maps: int = 64,\n",
    "        padding: int = 1,\n",
    "        lr_slope=0.2,\n",
    "        # latent_vector_size: int = 100\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "         #shape = (..., 3, 64, 64)\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(no_of_channels, number_of_feature_maps * 2 , kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # nn.BatchNorm2d(number_of_feature_maps * 8),\n",
    "            nn.LeakyReLU(negative_slope=lr_slope),\n",
    "                \n",
    "                #shape = (...,1024, 32, 32)\n",
    "            nn.Conv2d(number_of_feature_maps * 2, number_of_feature_maps * 4 , kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(number_of_feature_maps * 4),\n",
    "            nn.LeakyReLU(negative_slope=lr_slope),\n",
    "                \n",
    "                #shape = (..., 512, 16, 16)\n",
    "            nn.Conv2d(number_of_feature_maps * 4, number_of_feature_maps * 8 , kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(number_of_feature_maps * 8),\n",
    "            nn.LeakyReLU(negative_slope=lr_slope),\n",
    "                \n",
    "                #shape = (..., 256, 8, 8)\n",
    "            nn.Conv2d(number_of_feature_maps * 8, number_of_feature_maps * 16 , kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(number_of_feature_maps * 16),\n",
    "            # nn.LeakyReLU(negative_slope=lr_slope),\n",
    "            #  shape = (..., 128, 4, 4)\n",
    "            \n",
    "            # nn.Conv2d(number_of_feature_maps * 16, latent_vector_size , kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # nn.BatchNorm2d(latent_vector_size),\n",
    "            nn.Sigmoid(),\n",
    "         )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2)\n",
      "    (5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2)\n",
      "    (8): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (9): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Intializing the Discriminator instance\n",
    "discriminator = Discriminator().to(device)\n",
    "discriminator = discriminator.apply(weights_init)\n",
    "#Printing the structure\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "Discriminator (Discriminator)            [128, 3, 64, 64]     [128, 1024, 4, 4]    --                   True\n",
       "├─Sequential (main)                      [128, 3, 64, 64]     [128, 1024, 4, 4]    --                   True\n",
       "│    └─Conv2d (0)                        [128, 3, 64, 64]     [128, 128, 32, 32]   6,272                True\n",
       "│    └─LeakyReLU (1)                     [128, 128, 32, 32]   [128, 128, 32, 32]   --                   --\n",
       "│    └─Conv2d (2)                        [128, 128, 32, 32]   [128, 256, 16, 16]   524,544              True\n",
       "│    └─BatchNorm2d (3)                   [128, 256, 16, 16]   [128, 256, 16, 16]   512                  True\n",
       "│    └─LeakyReLU (4)                     [128, 256, 16, 16]   [128, 256, 16, 16]   --                   --\n",
       "│    └─Conv2d (5)                        [128, 256, 16, 16]   [128, 512, 8, 8]     2,097,664            True\n",
       "│    └─BatchNorm2d (6)                   [128, 512, 8, 8]     [128, 512, 8, 8]     1,024                True\n",
       "│    └─LeakyReLU (7)                     [128, 512, 8, 8]     [128, 512, 8, 8]     --                   --\n",
       "│    └─Conv2d (8)                        [128, 512, 8, 8]     [128, 1024, 4, 4]    8,389,632            True\n",
       "│    └─BatchNorm2d (9)                   [128, 1024, 4, 4]    [128, 1024, 4, 4]    2,048                True\n",
       "│    └─Sigmoid (10)                      [128, 1024, 4, 4]    [128, 1024, 4, 4]    --                   --\n",
       "========================================================================================================================\n",
       "Total params: 11,021,696\n",
       "Trainable params: 11,021,696\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 52.38\n",
       "========================================================================================================================\n",
       "Input size (MB): 6.29\n",
       "Forward/backward pass size (MB): 369.10\n",
       "Params size (MB): 44.09\n",
       "Estimated Total Size (MB): 419.48\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "summary(model=discriminator,\n",
    "        input_size=(128, 3, 64, 64),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        no_of_channels = 3,\n",
    "        kernel_size = (4,4),\n",
    "        stride: int = 2,\n",
    "        number_of_feature_maps: int = 64,\n",
    "        padding: int = 1,\n",
    "        lr_slope=0.2,\n",
    "        latent_vector_size = 100,\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.generator = Generator(latent_vector_size=latent_vector_size,no_of_channels=no_of_channels,kernel_size=kernel_size,stride=stride, number_of_feature_maps=number_of_feature_maps, padding=padding)\n",
    "        self.discriminator = Discriminator(no_of_channels=no_of_channels, kernel_size=kernel_size, stride=stride, number_of_feature_maps=number_of_feature_maps,padding=padding, lr_slope=lr_slope)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.generator(self.discriminator(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCGAN(\n",
      "  (generator): Generator(\n",
      "    (main): Sequential(\n",
      "      (0): ConvTranspose2d(100, 1024, kernel_size=(4, 4), stride=(2, 2))\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "      (6): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (7): ReLU()\n",
      "      (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (9): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (10): ReLU()\n",
      "      (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (12): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (13): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (discriminator): Discriminator(\n",
      "    (main): Sequential(\n",
      "      (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.2)\n",
      "      (2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): LeakyReLU(negative_slope=0.2)\n",
      "      (5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (7): LeakyReLU(negative_slope=0.2)\n",
      "      (8): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (9): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (10): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Intializing the DCGAN\n",
    "dcgan = DCGAN()\n",
    "dcgan.to(device)\n",
    "print(dcgan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "beta_1 = 0.5\n",
    "lr_optimizer = 0.0002\n",
    "loss_fn = nn.BCELoss()  #BCELoss function\n",
    "\n",
    "optimizerD = torch.optim.Adam(params=discriminator.parameters(), betas=(beta_1, 0.999), lr=lr_optimizer) #For discriminator\n",
    "optimizerG = torch.optim.Adam(params=generator.parameters(), betas=(beta_1, 0.999), lr=lr_optimizer) #For generator\n",
    "\n",
    "batch_size = 128\n",
    "latent_vector_size = 100\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "\n",
    "loss_g = []\n",
    "loss_d = []\n",
    "img_list = []\n",
    "\n",
    "# Fixed noise for generating the images\n",
    "fixed_noise = torch.randn((batch_size, latent_vector_size, 1, 1), dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[[[-0.5059, -0.5529, -0.6000,  ..., -0.8510, -0.8039, -0.8118],\n",
      "          [-0.6078, -0.7333, -0.7490,  ..., -0.7098, -0.5294, -0.5216],\n",
      "          [-0.2471, -0.2627, -0.3490,  ..., -0.6471, -0.5451, -0.4588],\n",
      "          ...,\n",
      "          [-0.9608, -0.9608, -0.9765,  ..., -0.9765, -0.9608, -0.9451],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.2078, -0.2471, -0.2471,  ..., -0.2314, -0.2471, -0.2235]],\n",
      "\n",
      "         [[-0.0196, -0.0902, -0.1294,  ..., -0.2863, -0.2392, -0.2471],\n",
      "          [-0.0824, -0.2078, -0.2235,  ..., -0.1529, -0.0353, -0.0196],\n",
      "          [ 0.1529,  0.1294,  0.0431,  ..., -0.0667,  0.0275,  0.0588],\n",
      "          ...,\n",
      "          [-0.3098, -0.3333, -0.3647,  ..., -0.3882, -0.2941, -0.2941],\n",
      "          [-0.4980, -0.4118, -0.4039,  ..., -0.5059, -0.4667, -0.4745],\n",
      "          [ 0.0196, -0.0039,  0.0196,  ..., -0.0118, -0.0039,  0.0196]],\n",
      "\n",
      "         [[-0.7255, -0.7490, -0.7255,  ..., -0.8588, -0.7804, -0.7098],\n",
      "          [-0.7255, -0.7804, -0.7804,  ..., -0.9137, -0.8275, -0.6863],\n",
      "          [-0.5451, -0.6471, -0.7804,  ..., -0.8824, -0.8353, -0.6941],\n",
      "          ...,\n",
      "          [-0.6627, -0.7098, -0.7569,  ..., -0.6627, -0.5451, -0.5451],\n",
      "          [-0.8196, -0.7804, -0.7804,  ..., -0.7961, -0.7333, -0.7490],\n",
      "          [-0.1137, -0.1451, -0.1451,  ..., -0.1686, -0.1686, -0.1451]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1529,  0.1686,  0.2078,  ...,  0.1451,  0.1294,  0.1216],\n",
      "          [ 0.1608,  0.1765,  0.2078,  ...,  0.1529,  0.1373,  0.1294],\n",
      "          [ 0.1765,  0.1922,  0.2157,  ...,  0.1529,  0.1529,  0.1451],\n",
      "          ...,\n",
      "          [-0.1922, -0.1059, -0.1529,  ..., -0.2784, -0.3255, -0.3098],\n",
      "          [-0.2863, -0.2706, -0.2863,  ..., -0.2471, -0.2706, -0.3333],\n",
      "          [-0.3490, -0.3647, -0.3804,  ..., -0.3020, -0.3098, -0.3255]],\n",
      "\n",
      "         [[ 0.2392,  0.2549,  0.2941,  ...,  0.2157,  0.2000,  0.1922],\n",
      "          [ 0.2471,  0.2627,  0.2941,  ...,  0.2235,  0.2078,  0.2000],\n",
      "          [ 0.2627,  0.2784,  0.3020,  ...,  0.2235,  0.2235,  0.2157],\n",
      "          ...,\n",
      "          [-0.1529, -0.0510, -0.0980,  ..., -0.2941, -0.3412, -0.3255],\n",
      "          [-0.2706, -0.2392, -0.2627,  ..., -0.2863, -0.3098, -0.3725],\n",
      "          [-0.3490, -0.3569, -0.3804,  ..., -0.3725, -0.3804, -0.3961]],\n",
      "\n",
      "         [[ 0.2549,  0.2706,  0.3098,  ...,  0.1765,  0.1608,  0.1529],\n",
      "          [ 0.2627,  0.2784,  0.3098,  ...,  0.1843,  0.1686,  0.1608],\n",
      "          [ 0.2784,  0.2941,  0.3176,  ...,  0.1843,  0.1843,  0.1765],\n",
      "          ...,\n",
      "          [-0.4667, -0.5059, -0.5216,  ..., -0.3255, -0.3725, -0.3569],\n",
      "          [-0.4588, -0.5216, -0.4902,  ..., -0.3176, -0.3412, -0.4039],\n",
      "          [-0.4510, -0.4980, -0.4353,  ..., -0.4118, -0.4196, -0.4353]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7725,  0.8431,  0.6392,  ...,  0.2000,  0.1451, -0.1294],\n",
      "          [ 0.9765,  0.7725,  0.6000,  ...,  0.2078, -0.0667, -0.1686],\n",
      "          [ 0.8667,  0.8118,  0.6627,  ...,  0.1608, -0.1059, -0.4431],\n",
      "          ...,\n",
      "          [-0.3176, -0.7020, -0.7569,  ..., -0.8431, -0.5608, -0.5686],\n",
      "          [-0.1373, -0.4667, -0.6863,  ..., -0.6627, -0.5059, -0.4667],\n",
      "          [-0.0667, -0.3725, -0.7333,  ..., -0.3490, -0.2941, -0.3098]],\n",
      "\n",
      "         [[ 0.7882,  0.9216,  0.7882,  ...,  0.5529,  0.4824,  0.3255],\n",
      "          [ 0.9686,  0.8431,  0.7569,  ...,  0.4431,  0.3647,  0.3882],\n",
      "          [ 0.9294,  0.8745,  0.7255,  ...,  0.4039,  0.2863,  0.2078],\n",
      "          ...,\n",
      "          [-0.3725, -0.4980, -0.6078,  ..., -0.8745, -0.6706, -0.7412],\n",
      "          [-0.3255, -0.4196, -0.6157,  ..., -0.8196, -0.7412, -0.7412],\n",
      "          [-0.3490, -0.4667, -0.6863,  ..., -0.5608, -0.5608, -0.6000]],\n",
      "\n",
      "         [[ 0.6549,  0.7490,  0.5608,  ...,  0.4118,  0.4118,  0.3098],\n",
      "          [ 0.8902,  0.6549,  0.4510,  ...,  0.2235,  0.0275,  0.0353],\n",
      "          [ 0.7882,  0.6078,  0.2941,  ...,  0.3333,  0.3333,  0.2549],\n",
      "          ...,\n",
      "          [-0.9059, -0.7882, -0.7412,  ..., -0.9529, -0.8902, -0.9686],\n",
      "          [-0.8980, -0.8039, -0.7882,  ..., -0.9451, -0.9529, -0.9765],\n",
      "          [-0.9216, -0.9137, -0.8667,  ..., -0.9059, -0.9294, -0.9686]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.4118,  0.3020,  0.2235,  ...,  0.6863,  0.6549,  0.6784],\n",
      "          [ 0.5137,  0.4039,  0.3098,  ...,  0.7098,  0.6784,  0.7098],\n",
      "          [ 0.6000,  0.4902,  0.4039,  ...,  0.7020,  0.6863,  0.7255],\n",
      "          ...,\n",
      "          [-0.6549, -0.5843, -0.5451,  ..., -0.4118, -0.5608, -0.8353],\n",
      "          [-0.7569, -0.7882, -0.7804,  ..., -0.3647, -0.6000, -0.8196],\n",
      "          [-0.8588, -0.8275, -0.7882,  ..., -0.6941, -0.8510, -0.9059]],\n",
      "\n",
      "         [[ 0.4196,  0.3098,  0.2314,  ...,  0.6392,  0.6314,  0.6784],\n",
      "          [ 0.5216,  0.4118,  0.3176,  ...,  0.6627,  0.6549,  0.7098],\n",
      "          [ 0.6078,  0.4980,  0.4118,  ...,  0.6549,  0.6549,  0.7176],\n",
      "          ...,\n",
      "          [-0.7098, -0.6863, -0.7098,  ..., -0.5294, -0.6549, -0.9059],\n",
      "          [-0.8824, -0.9294, -0.9294,  ..., -0.5216, -0.7255, -0.8902],\n",
      "          [-0.9843, -0.9451, -0.8980,  ..., -0.8275, -0.9451, -0.9529]],\n",
      "\n",
      "         [[ 0.3804,  0.2706,  0.1922,  ...,  0.5373,  0.5294,  0.5922],\n",
      "          [ 0.4824,  0.3725,  0.2784,  ...,  0.5608,  0.5608,  0.6235],\n",
      "          [ 0.5686,  0.4588,  0.3725,  ...,  0.5529,  0.5608,  0.6392],\n",
      "          ...,\n",
      "          [-0.6392, -0.6000, -0.6000,  ..., -0.4275, -0.5529, -0.8118],\n",
      "          [-0.7490, -0.7961, -0.7961,  ..., -0.4039, -0.5765, -0.7490],\n",
      "          [-0.8588, -0.8196, -0.7804,  ..., -0.7255, -0.8275, -0.8196]]],\n",
      "\n",
      "\n",
      "        [[[-0.1294, -0.1294, -0.1137,  ..., -0.3412, -0.2627, -0.2235],\n",
      "          [-0.1373, -0.1137, -0.1137,  ..., -0.3176, -0.3098, -0.3255],\n",
      "          [-0.1216, -0.1373, -0.1451,  ..., -0.3490, -0.3255, -0.3882],\n",
      "          ...,\n",
      "          [ 0.0196,  0.0824,  0.0431,  ..., -0.2078, -0.2471, -0.2000],\n",
      "          [ 0.0588,  0.1451,  0.2078,  ..., -0.0980, -0.1137, -0.1294],\n",
      "          [ 0.0431,  0.0824,  0.1765,  ..., -0.0667, -0.0745, -0.0118]],\n",
      "\n",
      "         [[ 0.0039,  0.0039,  0.0196,  ..., -0.2000, -0.1922, -0.1843],\n",
      "          [-0.0039,  0.0196,  0.0196,  ..., -0.2078, -0.2627, -0.2941],\n",
      "          [ 0.0039, -0.0039, -0.0118,  ..., -0.2863, -0.3098, -0.3647],\n",
      "          ...,\n",
      "          [-0.0824, -0.0510, -0.0588,  ..., -0.1373, -0.1843, -0.1608],\n",
      "          [-0.0588,  0.0039,  0.0902,  ..., -0.0431, -0.0510, -0.0745],\n",
      "          [-0.0510, -0.0431,  0.0902,  ..., -0.0353, -0.0196,  0.0588]],\n",
      "\n",
      "         [[-0.4039, -0.4196, -0.4118,  ..., -0.4980, -0.4510, -0.4039],\n",
      "          [-0.4118, -0.3961, -0.4196,  ..., -0.4824, -0.4980, -0.4902],\n",
      "          [-0.3961, -0.4275, -0.4431,  ..., -0.5216, -0.5137, -0.5451],\n",
      "          ...,\n",
      "          [-0.3961, -0.4196, -0.4902,  ..., -0.4510, -0.4980, -0.4667],\n",
      "          [-0.3804, -0.3725, -0.3412,  ..., -0.4039, -0.4039, -0.4275],\n",
      "          [-0.3569, -0.3961, -0.3333,  ..., -0.3882, -0.3804, -0.3098]]],\n",
      "\n",
      "\n",
      "        [[[-0.1059,  0.0431,  0.1922,  ..., -0.5373, -0.5216, -0.8039],\n",
      "          [-0.1059,  0.0353,  0.1451,  ..., -0.4353, -0.5529, -0.7569],\n",
      "          [-0.1137,  0.0353,  0.0902,  ..., -0.3255, -0.4196, -0.5137],\n",
      "          ...,\n",
      "          [ 0.1373,  0.1294,  0.1059,  ..., -0.5137, -0.4980, -0.4902],\n",
      "          [ 0.0196,  0.0902,  0.1216,  ..., -0.4824, -0.6235, -0.6078],\n",
      "          [-0.0745, -0.0039,  0.0745,  ..., -0.5608, -0.6000, -0.6000]],\n",
      "\n",
      "         [[-0.1373, -0.0196,  0.1137,  ..., -0.5373, -0.5137, -0.7961],\n",
      "          [-0.1059, -0.0039,  0.0667,  ..., -0.4275, -0.5451, -0.7490],\n",
      "          [-0.0745,  0.0196,  0.0118,  ..., -0.3255, -0.4118, -0.5059],\n",
      "          ...,\n",
      "          [ 0.1137,  0.1059,  0.0745,  ..., -0.5922, -0.5608, -0.5529],\n",
      "          [-0.0039,  0.0667,  0.1059,  ..., -0.5529, -0.6863, -0.6706],\n",
      "          [-0.0980, -0.0275,  0.0510,  ..., -0.6235, -0.6627, -0.6627]],\n",
      "\n",
      "         [[-0.1843, -0.0510,  0.0902,  ..., -0.6000, -0.5608, -0.8431],\n",
      "          [-0.1529, -0.0510,  0.0196,  ..., -0.4980, -0.5922, -0.7961],\n",
      "          [-0.1137, -0.0353, -0.0510,  ..., -0.3882, -0.4588, -0.5529],\n",
      "          ...,\n",
      "          [ 0.0431,  0.0353,  0.0039,  ..., -0.6863, -0.6471, -0.6392],\n",
      "          [-0.0745, -0.0039,  0.0353,  ..., -0.6471, -0.7647, -0.7569],\n",
      "          [-0.1686, -0.0980, -0.0196,  ..., -0.7098, -0.7490, -0.7490]]]]) tensor([2, 2, 3, 8, 5, 9, 6, 3, 9, 8, 8, 1, 6, 8, 7, 0, 2, 7, 2, 3, 1, 4, 9, 1,\n",
      "        0, 2, 0, 4, 6, 0, 6, 5, 7, 0, 3, 7, 9, 2, 1, 4, 8, 0, 8, 2, 0, 0, 4, 8,\n",
      "        1, 9, 3, 8, 4, 0, 5, 3, 9, 8, 9, 2, 7, 0, 5, 5, 3, 3, 8, 7, 2, 3, 2, 9,\n",
      "        1, 8, 7, 6, 5, 2, 3, 2, 1, 9, 0, 5, 8, 5, 3, 2, 9, 3, 8, 0, 7, 5, 6, 5,\n",
      "        0, 0, 4, 3, 4, 7, 7, 6, 6, 4, 7, 5, 4, 1, 6, 9, 2, 4, 0, 8, 1, 9, 2, 7,\n",
      "        5, 6, 9, 5, 4, 6, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "for batch, (X, y) in enumerate(train_dataloader, 0):\n",
    "    print(batch, X, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "label.fill_(fake_label)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,\n",
       " tensor([[[[-0.0980, -0.0824, -0.0196,  ...,  0.0039, -0.0118, -0.0353],\n",
       "           [-0.0980, -0.0588, -0.0039,  ...,  0.0039, -0.0196, -0.0196],\n",
       "           [-0.1529, -0.0667, -0.0039,  ...,  0.0196,  0.0039, -0.0196],\n",
       "           ...,\n",
       "           [-0.1608, -0.1373, -0.1294,  ...,  0.0824,  0.0980, -0.0039],\n",
       "           [-0.1608, -0.1529, -0.0667,  ...,  0.0745,  0.1451, -0.0039],\n",
       "           [-0.1451, -0.1451, -0.1059,  ..., -0.0039,  0.1608,  0.0039]],\n",
       " \n",
       "          [[-0.0745, -0.0667, -0.0196,  ...,  0.0275,  0.0196,  0.0275],\n",
       "           [-0.0902, -0.0588, -0.0039,  ...,  0.0275,  0.0118,  0.0353],\n",
       "           [-0.1137, -0.0588, -0.0039,  ...,  0.0431,  0.0353,  0.0275],\n",
       "           ...,\n",
       "           [-0.0980, -0.0824, -0.0745,  ...,  0.0824,  0.0824,  0.0118],\n",
       "           [-0.0980, -0.0980, -0.0275,  ...,  0.0510,  0.0745,  0.0196],\n",
       "           [-0.0824, -0.0824, -0.0588,  ..., -0.0118,  0.0431,  0.0118]],\n",
       " \n",
       "          [[-0.0431, -0.0431, -0.0039,  ...,  0.0824,  0.0824,  0.0980],\n",
       "           [-0.0353, -0.0196,  0.0275,  ...,  0.0902,  0.0824,  0.1059],\n",
       "           [-0.0431, -0.0039,  0.0353,  ...,  0.1216,  0.1059,  0.0980],\n",
       "           ...,\n",
       "           [-0.0039,  0.0118,  0.0275,  ...,  0.0980,  0.0745,  0.0431],\n",
       "           [-0.0039,  0.0039,  0.0588,  ...,  0.0667,  0.0588,  0.0431],\n",
       "           [ 0.0118,  0.0275,  0.0510,  ...,  0.0353,  0.0667,  0.0431]]],\n",
       " \n",
       " \n",
       "         [[[-0.2078, -0.1686, -0.1451,  ...,  0.0980,  0.0980,  0.0745],\n",
       "           [-0.2157, -0.2078, -0.2000,  ...,  0.0510,  0.0667,  0.0588],\n",
       "           [-0.2392, -0.2235, -0.2000,  ...,  0.0275,  0.0667,  0.0824],\n",
       "           ...,\n",
       "           [-0.3333, -0.3176, -0.3098,  ...,  0.5686,  0.5451,  0.5137],\n",
       "           [-0.3255, -0.3098, -0.2941,  ...,  0.5608,  0.5529,  0.5059],\n",
       "           [-0.3255, -0.3176, -0.3098,  ...,  0.5373,  0.5529,  0.4980]],\n",
       " \n",
       "          [[-0.3333, -0.3098, -0.3020,  ...,  0.0667,  0.0667,  0.0431],\n",
       "           [-0.3412, -0.3255, -0.3255,  ...,  0.0039,  0.0196,  0.0196],\n",
       "           [-0.3569, -0.3333, -0.3176,  ..., -0.0431,  0.0039,  0.0196],\n",
       "           ...,\n",
       "           [-0.4902, -0.4667, -0.4353,  ...,  0.4275,  0.4118,  0.3961],\n",
       "           [-0.4745, -0.4510, -0.4196,  ...,  0.4196,  0.4118,  0.3804],\n",
       "           [-0.4588, -0.4353, -0.4196,  ...,  0.4196,  0.4196,  0.3647]],\n",
       " \n",
       "          [[-0.3725, -0.3569, -0.3490,  ..., -0.0745, -0.0745, -0.0980],\n",
       "           [-0.3647, -0.3725, -0.3725,  ..., -0.0980, -0.0980, -0.1216],\n",
       "           [-0.3804, -0.3725, -0.3569,  ..., -0.1294, -0.1059, -0.1059],\n",
       "           ...,\n",
       "           [-0.4824, -0.4588, -0.4431,  ...,  0.2471,  0.2314,  0.2078],\n",
       "           [-0.4667, -0.4510, -0.4275,  ...,  0.2392,  0.2392,  0.2000],\n",
       "           [-0.4588, -0.4431, -0.4353,  ...,  0.2235,  0.2314,  0.1765]]],\n",
       " \n",
       " \n",
       "         [[[ 0.4275,  0.7882,  0.5059,  ..., -0.5529, -0.7490, -0.7490],\n",
       "           [-0.2627,  0.0196, -0.1922,  ..., -0.1529, -0.5373, -0.7255],\n",
       "           [ 0.0588, -0.0275, -0.4588,  ...,  0.0667,  0.0118, -0.3725],\n",
       "           ...,\n",
       "           [ 0.1608,  0.2627,  0.2392,  ..., -0.1059, -0.0980, -0.2078],\n",
       "           [ 0.2157,  0.2627,  0.2941,  ..., -0.1765, -0.1059, -0.1922],\n",
       "           [ 0.2784,  0.3020,  0.2392,  ..., -0.1686, -0.1373, -0.2000]],\n",
       " \n",
       "          [[ 0.5843,  0.8824,  0.6549,  ..., -0.2235, -0.3961, -0.4667],\n",
       "           [ 0.0275,  0.2549,  0.0745,  ...,  0.4118, -0.0039, -0.3490],\n",
       "           [ 0.2157,  0.1373, -0.1922,  ...,  0.6392,  0.6392,  0.0667],\n",
       "           ...,\n",
       "           [ 0.3647,  0.4745,  0.4510,  ...,  0.2392,  0.2392,  0.1294],\n",
       "           [ 0.3961,  0.4510,  0.4824,  ...,  0.1765,  0.2471,  0.1686],\n",
       "           [ 0.4431,  0.4745,  0.4118,  ...,  0.2000,  0.2392,  0.1843]],\n",
       " \n",
       "          [[ 0.4353,  0.8431,  0.5843,  ..., -0.1529, -0.3804, -0.5294],\n",
       "           [-0.2471,  0.0510, -0.1294,  ...,  0.5216,  0.0353, -0.3804],\n",
       "           [ 0.1137, -0.0510, -0.4588,  ...,  0.7176,  0.7020,  0.0824],\n",
       "           ...,\n",
       "           [-0.0745,  0.0118, -0.0353,  ..., -0.1765, -0.1843, -0.2627],\n",
       "           [-0.0196,  0.0118,  0.0196,  ..., -0.2235, -0.1216, -0.2314],\n",
       "           [ 0.0588,  0.0667, -0.0275,  ..., -0.2078, -0.1451, -0.2235]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-0.3176, -0.2549, -0.2549,  ..., -0.1922, -0.1059, -0.2706],\n",
       "           [-0.3333, -0.2784, -0.2706,  ..., -0.1765, -0.0980, -0.2784],\n",
       "           [-0.3569, -0.3020, -0.3020,  ..., -0.1608, -0.0980, -0.2941],\n",
       "           ...,\n",
       "           [-0.6078, -0.6000, -0.6078,  ..., -0.6471, -0.6157, -0.7098],\n",
       "           [-0.5843, -0.5843, -0.5922,  ..., -0.6157, -0.5608, -0.6863],\n",
       "           [-0.5922, -0.5765, -0.5686,  ..., -0.4588, -0.4431, -0.6000]],\n",
       " \n",
       "          [[ 0.1373,  0.2314,  0.2235,  ...,  0.2784,  0.3490,  0.0824],\n",
       "           [ 0.1059,  0.2000,  0.2078,  ...,  0.3020,  0.3647,  0.0824],\n",
       "           [ 0.0745,  0.1608,  0.1686,  ...,  0.3020,  0.3647,  0.0745],\n",
       "           ...,\n",
       "           [-0.3569, -0.3412, -0.3490,  ..., -0.4431, -0.4510, -0.6000],\n",
       "           [-0.3333, -0.3176, -0.3255,  ..., -0.4588, -0.4431, -0.6000],\n",
       "           [-0.3647, -0.3255, -0.3176,  ..., -0.2784, -0.2863, -0.4745]],\n",
       " \n",
       "          [[ 0.6235,  0.7333,  0.7255,  ...,  0.7333,  0.8275,  0.4510],\n",
       "           [ 0.6235,  0.7255,  0.7255,  ...,  0.7333,  0.8431,  0.4510],\n",
       "           [ 0.6000,  0.6941,  0.7020,  ...,  0.7490,  0.8118,  0.4275],\n",
       "           ...,\n",
       "           [ 0.0353,  0.0667,  0.0588,  ..., -0.2235, -0.3882, -0.5922],\n",
       "           [-0.0118,  0.0118,  0.0039,  ..., -0.3569, -0.5137, -0.6784],\n",
       "           [-0.1137, -0.0667, -0.0510,  ..., -0.2706, -0.3569, -0.5451]]],\n",
       " \n",
       " \n",
       "         [[[-0.4196, -0.4196, -0.4118,  ..., -0.2627, -0.2627, -0.2784],\n",
       "           [-0.4039, -0.4118, -0.3961,  ..., -0.2549, -0.2549, -0.2627],\n",
       "           [-0.4039, -0.3961, -0.3882,  ..., -0.2627, -0.2627, -0.2627],\n",
       "           ...,\n",
       "           [-0.4824, -0.4588, -0.4275,  ..., -0.3569, -0.3647, -0.3647],\n",
       "           [-0.4275, -0.4118, -0.3961,  ..., -0.3490, -0.3490, -0.3569],\n",
       "           [-0.4039, -0.3961, -0.3961,  ..., -0.3647, -0.3647, -0.3569]],\n",
       " \n",
       "          [[-0.3961, -0.3961, -0.3882,  ..., -0.2627, -0.2706, -0.2863],\n",
       "           [-0.3804, -0.3882, -0.3725,  ..., -0.2549, -0.2549, -0.2627],\n",
       "           [-0.3804, -0.3725, -0.3647,  ..., -0.2627, -0.2627, -0.2627],\n",
       "           ...,\n",
       "           [-0.4588, -0.4431, -0.4039,  ..., -0.3255, -0.3255, -0.3255],\n",
       "           [-0.4118, -0.3961, -0.3725,  ..., -0.3098, -0.3098, -0.3176],\n",
       "           [-0.3882, -0.3725, -0.3725,  ..., -0.3255, -0.3255, -0.3176]],\n",
       " \n",
       "          [[-0.6392, -0.6392, -0.6314,  ..., -0.5373, -0.5216, -0.5059],\n",
       "           [-0.6314, -0.6314, -0.6157,  ..., -0.5373, -0.5294, -0.5137],\n",
       "           [-0.6235, -0.6157, -0.6078,  ..., -0.5294, -0.5373, -0.5294],\n",
       "           ...,\n",
       "           [-0.6392, -0.6392, -0.6314,  ..., -0.5686, -0.5765, -0.5765],\n",
       "           [-0.5922, -0.6000, -0.6078,  ..., -0.5529, -0.5608, -0.5686],\n",
       "           [-0.5686, -0.5765, -0.5843,  ..., -0.5765, -0.5765, -0.5686]]],\n",
       " \n",
       " \n",
       "         [[[ 0.3176,  0.3569,  0.3490,  ...,  0.2157,  0.1451,  0.0824],\n",
       "           [ 0.1373,  0.1451,  0.1765,  ...,  0.5686,  0.5373,  0.4745],\n",
       "           [ 0.0275,  0.0275,  0.0118,  ...,  0.5529,  0.5843,  0.6157],\n",
       "           ...,\n",
       "           [ 0.4824,  0.3255,  0.2392,  ...,  0.7176,  0.6549,  0.2000],\n",
       "           [ 0.5294,  0.5059,  0.4588,  ...,  0.3569,  0.1216, -0.1451],\n",
       "           [ 0.5216,  0.4745,  0.4980,  ...,  0.0196, -0.0667, -0.0118]],\n",
       " \n",
       "          [[ 0.1843,  0.2235,  0.2157,  ...,  0.2157,  0.1451,  0.0824],\n",
       "           [ 0.0039,  0.0118,  0.0431,  ...,  0.5686,  0.5373,  0.4745],\n",
       "           [-0.1137, -0.1137, -0.1294,  ...,  0.5529,  0.5843,  0.6157],\n",
       "           ...,\n",
       "           [ 0.4431,  0.2706,  0.1608,  ...,  0.7412,  0.6235,  0.0745],\n",
       "           [ 0.5216,  0.4980,  0.4510,  ...,  0.3098,  0.0118, -0.3490],\n",
       "           [ 0.5137,  0.4667,  0.4902,  ..., -0.2000, -0.3020, -0.2549]],\n",
       " \n",
       "          [[ 0.2471,  0.2863,  0.2784,  ...,  0.2863,  0.2078,  0.1451],\n",
       "           [ 0.0824,  0.0902,  0.1216,  ...,  0.6314,  0.6000,  0.5373],\n",
       "           [-0.0039, -0.0118, -0.0196,  ...,  0.6157,  0.6471,  0.6784],\n",
       "           ...,\n",
       "           [ 0.4431,  0.2549,  0.1373,  ...,  0.8196,  0.7020,  0.0980],\n",
       "           [ 0.5608,  0.5451,  0.4980,  ...,  0.3647,  0.0118, -0.4039],\n",
       "           [ 0.5608,  0.5137,  0.5373,  ..., -0.2471, -0.3804, -0.3882]]]]))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_cpu = data[0].to(device)\n",
    "b_size = real_cpu.size(0)\n",
    "b_size,  real_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.1950, 0.2300, 0.5285, 0.5896],\n",
      "          [0.5716, 0.6724, 0.7499, 0.5886],\n",
      "          [0.5728, 0.4234, 0.7107, 0.6535],\n",
      "          [0.3770, 0.5665, 0.7673, 0.5450]],\n",
      "\n",
      "         [[0.7160, 0.4667, 0.7805, 0.4853],\n",
      "          [0.7144, 0.6089, 0.8224, 0.3188],\n",
      "          [0.6232, 0.3933, 0.4965, 0.4608],\n",
      "          [0.4991, 0.8492, 0.7735, 0.8155]],\n",
      "\n",
      "         [[0.7492, 0.7031, 0.6195, 0.5004],\n",
      "          [0.1612, 0.2214, 0.1322, 0.3616],\n",
      "          [0.4305, 0.4333, 0.5556, 0.5596],\n",
      "          [0.7225, 0.6792, 0.5900, 0.5882]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.5153, 0.4766, 0.7324, 0.6432],\n",
      "          [0.4094, 0.5450, 0.7197, 0.6978],\n",
      "          [0.4342, 0.7949, 0.3640, 0.4233],\n",
      "          [0.2000, 0.7110, 0.1713, 0.3191]],\n",
      "\n",
      "         [[0.3699, 0.3325, 0.3186, 0.4511],\n",
      "          [0.6182, 0.4321, 0.7198, 0.0941],\n",
      "          [0.3208, 0.1823, 0.6420, 0.3315],\n",
      "          [0.5116, 0.7489, 0.3575, 0.2514]],\n",
      "\n",
      "         [[0.6837, 0.6520, 0.7078, 0.3880],\n",
      "          [0.3055, 0.8798, 0.6604, 0.4107],\n",
      "          [0.2375, 0.3348, 0.3797, 0.4073],\n",
      "          [0.5829, 0.5116, 0.4117, 0.4795]]],\n",
      "\n",
      "\n",
      "        [[[0.3343, 0.2502, 0.2570, 0.3831],\n",
      "          [0.5421, 0.3464, 0.6275, 0.5033],\n",
      "          [0.5253, 0.4148, 0.3452, 0.3033],\n",
      "          [0.3104, 0.4265, 0.3809, 0.4809]],\n",
      "\n",
      "         [[0.5513, 0.3414, 0.2908, 0.2957],\n",
      "          [0.3771, 0.1355, 0.0161, 0.1109],\n",
      "          [0.2340, 0.3373, 0.5950, 0.4762],\n",
      "          [0.4135, 0.1870, 0.3676, 0.2663]],\n",
      "\n",
      "         [[0.4806, 0.5806, 0.5609, 0.4030],\n",
      "          [0.6223, 0.8837, 0.7023, 0.4228],\n",
      "          [0.5802, 0.5751, 0.8284, 0.4559],\n",
      "          [0.7031, 0.7658, 0.4005, 0.4724]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.6409, 0.4502, 0.4846, 0.6160],\n",
      "          [0.1828, 0.1496, 0.2983, 0.3528],\n",
      "          [0.4026, 0.5453, 0.2832, 0.3237],\n",
      "          [0.1301, 0.1436, 0.1687, 0.8136]],\n",
      "\n",
      "         [[0.1817, 0.1917, 0.2393, 0.3471],\n",
      "          [0.6349, 0.5436, 0.5714, 0.4814],\n",
      "          [0.6918, 0.3273, 0.4826, 0.2215],\n",
      "          [0.4974, 0.6165, 0.6215, 0.7125]],\n",
      "\n",
      "         [[0.3271, 0.3971, 0.4084, 0.4634],\n",
      "          [0.4740, 0.5439, 0.7354, 0.4980],\n",
      "          [0.5350, 0.7823, 0.4274, 0.6065],\n",
      "          [0.7276, 0.5013, 0.2155, 0.6385]]],\n",
      "\n",
      "\n",
      "        [[[0.6025, 0.4590, 0.4605, 0.8752],\n",
      "          [0.6954, 0.7752, 0.8587, 0.8405],\n",
      "          [0.9102, 0.4660, 0.3399, 0.5071],\n",
      "          [0.4929, 0.6168, 0.5601, 0.8748]],\n",
      "\n",
      "         [[0.3236, 0.5604, 0.4069, 0.2438],\n",
      "          [0.3071, 0.5687, 0.6997, 0.1137],\n",
      "          [0.1180, 0.2059, 0.1638, 0.4008],\n",
      "          [0.5425, 0.7520, 0.6268, 0.4302]],\n",
      "\n",
      "         [[0.8255, 0.5011, 0.6477, 0.7955],\n",
      "          [0.8785, 0.3485, 0.6492, 0.6962],\n",
      "          [0.7928, 0.2373, 0.6214, 0.5751],\n",
      "          [0.5241, 0.4474, 0.4946, 0.7637]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.2476, 0.4224, 0.2614, 0.1952],\n",
      "          [0.5128, 0.0959, 0.0299, 0.3530],\n",
      "          [0.6446, 0.0615, 0.1188, 0.1609],\n",
      "          [0.6328, 0.1052, 0.2134, 0.2729]],\n",
      "\n",
      "         [[0.2033, 0.4344, 0.5407, 0.1719],\n",
      "          [0.2595, 0.6233, 0.2661, 0.0835],\n",
      "          [0.4378, 0.1987, 0.8442, 0.1680],\n",
      "          [0.5377, 0.7107, 0.6027, 0.4734]],\n",
      "\n",
      "         [[0.6404, 0.3596, 0.6629, 0.4893],\n",
      "          [0.6217, 0.5963, 0.8459, 0.3767],\n",
      "          [0.2553, 0.4000, 0.7065, 0.0936],\n",
      "          [0.4113, 0.6427, 0.3265, 0.3347]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.4982, 0.4395, 0.6445, 0.3627],\n",
      "          [0.2079, 0.4738, 0.9008, 0.5298],\n",
      "          [0.2763, 0.5158, 0.4519, 0.4180],\n",
      "          [0.3288, 0.4822, 0.3042, 0.5728]],\n",
      "\n",
      "         [[0.5728, 0.8167, 0.4116, 0.7239],\n",
      "          [0.5718, 0.1972, 0.0823, 0.6484],\n",
      "          [0.6352, 0.8525, 0.3031, 0.9119],\n",
      "          [0.6053, 0.7017, 0.4694, 0.7206]],\n",
      "\n",
      "         [[0.5798, 0.6568, 0.6195, 0.4096],\n",
      "          [0.8397, 0.5846, 0.3457, 0.5906],\n",
      "          [0.2570, 0.4844, 0.7062, 0.6000],\n",
      "          [0.6193, 0.5676, 0.5606, 0.6355]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.7717, 0.7477, 0.4753, 0.7392],\n",
      "          [0.5964, 0.1308, 0.7471, 0.6917],\n",
      "          [0.7980, 0.6847, 0.7515, 0.6869],\n",
      "          [0.4150, 0.4629, 0.5272, 0.4143]],\n",
      "\n",
      "         [[0.3518, 0.7152, 0.5262, 0.7032],\n",
      "          [0.3927, 0.5181, 0.1620, 0.5599],\n",
      "          [0.5331, 0.5311, 0.2371, 0.4940],\n",
      "          [0.3806, 0.6082, 0.4462, 0.5003]],\n",
      "\n",
      "         [[0.2319, 0.4869, 0.3852, 0.4908],\n",
      "          [0.3544, 0.6275, 0.2810, 0.2510],\n",
      "          [0.3434, 0.3173, 0.2415, 0.2473],\n",
      "          [0.3069, 0.5361, 0.5692, 0.6124]]],\n",
      "\n",
      "\n",
      "        [[[0.2139, 0.0783, 0.1898, 0.6733],\n",
      "          [0.6583, 0.5200, 0.5597, 0.7860],\n",
      "          [0.8138, 0.7915, 0.6900, 0.8886],\n",
      "          [0.8454, 0.7067, 0.7614, 0.8018]],\n",
      "\n",
      "         [[0.7476, 0.6443, 0.8543, 0.5838],\n",
      "          [0.3240, 0.5951, 0.3746, 0.4850],\n",
      "          [0.3579, 0.3934, 0.3595, 0.8282],\n",
      "          [0.7958, 0.8736, 0.6405, 0.7415]],\n",
      "\n",
      "         [[0.3211, 0.5454, 0.5182, 0.3641],\n",
      "          [0.1578, 0.3773, 0.0685, 0.4949],\n",
      "          [0.2886, 0.1994, 0.2121, 0.4322],\n",
      "          [0.3625, 0.3549, 0.1878, 0.5095]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.5176, 0.5172, 0.6768, 0.4054],\n",
      "          [0.3601, 0.5687, 0.7805, 0.8003],\n",
      "          [0.7630, 0.8336, 0.5964, 0.5679],\n",
      "          [0.5788, 0.4445, 0.3588, 0.5351]],\n",
      "\n",
      "         [[0.4454, 0.3162, 0.3924, 0.1949],\n",
      "          [0.2255, 0.8536, 0.6594, 0.2966],\n",
      "          [0.5022, 0.8378, 0.3132, 0.2674],\n",
      "          [0.4257, 0.6085, 0.3515, 0.1957]],\n",
      "\n",
      "         [[0.6782, 0.9269, 0.8870, 0.6834],\n",
      "          [0.5380, 0.4476, 0.6293, 0.3186],\n",
      "          [0.5322, 0.4299, 0.5411, 0.1915],\n",
      "          [0.1620, 0.4868, 0.1839, 0.0958]]],\n",
      "\n",
      "\n",
      "        [[[0.5175, 0.0320, 0.1677, 0.5693],\n",
      "          [0.2791, 0.1821, 0.1065, 0.6117],\n",
      "          [0.5411, 0.7429, 0.2911, 0.7637],\n",
      "          [0.6021, 0.4616, 0.4562, 0.4744]],\n",
      "\n",
      "         [[0.6358, 0.4412, 0.4956, 0.7383],\n",
      "          [0.6018, 0.9456, 0.8646, 0.7947],\n",
      "          [0.6219, 0.7803, 0.5399, 0.7190],\n",
      "          [0.2961, 0.1422, 0.2607, 0.4984]],\n",
      "\n",
      "         [[0.5592, 0.3961, 0.6027, 0.2546],\n",
      "          [0.1194, 0.0110, 0.0409, 0.0510],\n",
      "          [0.2604, 0.1081, 0.4667, 0.3831],\n",
      "          [0.6866, 0.6951, 0.5230, 0.2069]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.2725, 0.7417, 0.6947, 0.6384],\n",
      "          [0.6733, 0.5130, 0.5801, 0.6534],\n",
      "          [0.4362, 0.8237, 0.4286, 0.8519],\n",
      "          [0.5779, 0.1936, 0.5853, 0.3770]],\n",
      "\n",
      "         [[0.4380, 0.7324, 0.5089, 0.0936],\n",
      "          [0.7645, 0.8876, 0.9039, 0.8316],\n",
      "          [0.3224, 0.3700, 0.3040, 0.2638],\n",
      "          [0.5248, 0.8849, 0.7312, 0.6302]],\n",
      "\n",
      "         [[0.9443, 0.8909, 0.8804, 0.4368],\n",
      "          [0.7912, 0.6925, 0.6057, 0.2498],\n",
      "          [0.5401, 0.1226, 0.7453, 0.1137],\n",
      "          [0.7303, 0.2835, 0.6421, 0.6426]]]], grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1024, 4, 4])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[218], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_pred)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 2. Calculate  and accumulate loss\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m loss_real \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# loss_d.append(loss.item())\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 3. Optimizer zero grad\u001b[39;00m\n\u001b[1;32m     29\u001b[0m optimizerD\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.10/site-packages/torch/nn/modules/loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.10/site-packages/torch/nn/functional.py:3118\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3116\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[0;32m-> 3118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3121\u001b[0m     )\n\u001b[1;32m   3123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3124\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1024, 4, 4])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "#Training loop\n",
    "\n",
    "\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        #Train the discriminator (with real data)\n",
    "        \n",
    "        ############################\n",
    "        # (1) Update D network: maximize: log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        batch_size = X.shape[0]\n",
    "\n",
    "        real_data = torch.ones((batch_size,), device=device, dtype=torch.float32)\n",
    "        \n",
    "        # 1. Forward pass\n",
    "        y_pred = discriminator(X)\n",
    "        print(y_pred)\n",
    "        \n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss_real = loss_fn(y_pred, real_data)\n",
    "        # loss_d.append(loss.item())\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizerD.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss_real.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizerD.step()\n",
    "        \n",
    "        \n",
    "        #Train the discriminator (with fake data)\n",
    "        \n",
    "        noise = torch.randn((batch_size, latent_vector_size, 1, 1), device=device)\n",
    "        fake_data = torch.zeros((batch_size,), device=device, dtype=torch.float32)\n",
    "        noise_generated_by_generator = generator(noise)\n",
    "        \n",
    "        #1. Forward pass\n",
    "        y_pred = discriminator(noise_generated_by_generator)\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss_fake = loss_fn(y_pred, fake_data)\n",
    "        # loss_d.append(loss.item())\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizerD.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss_fake.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizerD.step()\n",
    "        \n",
    "        #Accumulating total discriminator loss\n",
    "        loss_d.append(loss_real + loss_fake)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "       \n",
    "        labels = torch.ones((batch_size,), device=device, dtype=torch.float32)\n",
    "        \n",
    "        #q. Forward pass\n",
    "        y_pred = discriminator(noise_generated_by_generator)\n",
    "        \n",
    "        #2. Calculate and accumulate loss\n",
    "        loss = loss_fn(y_pred,labels)\n",
    "        \n",
    "        \n",
    "        # 3. Optimizer zero grad\n",
    "        optimizerG.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizerG.step()\n",
    "        \n",
    "        loss_g.append(loss.item())\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch: \", epoch, \"Generator loss: \", loss_g, \"Discriminator loss: \", loss_d)\n",
    "            \n",
    "    if (epoch % 10 == 0) or ((epoch == epoch - 1) and (i == len(train_dataloader)-1)):\n",
    "        \n",
    "            generator.eval()\n",
    "            with torch.inference_mode():\n",
    "                fake = generator(fixed_noise).detach().cpu()\n",
    "            img_list.append(torchvision.utils.make_grid(fake, padding=2, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visulizing the losses\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(loss_g,label=\"G\")\n",
    "plt.plot(loss_d,label=\"D\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
