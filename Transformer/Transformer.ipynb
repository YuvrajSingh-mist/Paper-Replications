{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Pw7f2ghccuoK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer\n",
    "from huggingface_hub import PyTorchModelHubMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "adLpt7j7cuoL"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 21 05:28:24 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.42.03              Driver Version: 555.85         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4050 ...    On  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   42C    P3             11W /   70W |       0MiB /   6141MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LwR5_uvTcuoL"
   },
   "outputs": [],
   "source": [
    "#Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eDccPM5AcuoL",
    "outputId": "32131442-c002-4ea5-e829-de93c87066d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-19 13:21:04--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.007s  \n",
      "\n",
      "2024-06-19 13:21:04 (145 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Collab setup\n",
    "\n",
    "data_path = Path('data')\n",
    "data_path.mkdir(exist_ok=True)\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "!cp input.txt data/input.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0VBi6asbs4Vs"
   },
   "outputs": [],
   "source": [
    "#Datasets\n",
    "\n",
    "# Using tinyshakespeare\n",
    "\n",
    "with open('data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "####################################################################\n",
    "\n",
    "#Using BookCorpus\n",
    "# from datasets import load_dataset\n",
    "# data = load_dataset('bookcorpus/bookcorpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kxY7keFh0NLm",
    "outputId": "c74970c5-fb71-4afa-96ec-a4c60325c9f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 74004228\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "_D1tbt7L2c-D"
   },
   "outputs": [],
   "source": [
    "# Extracting the content of  the Dataset\n",
    "# Open a file for writing\n",
    "# with open('bookcorpus_text.txt', 'w', encoding='utf-8') as f:\n",
    "#     # Traverse the dataset and write text data to the file\n",
    "#     for record in data['train']['text']:\n",
    "#         f.write(record)\n",
    "\n",
    "# print(\"Writing to file complete.\")\n",
    "\n",
    "# Read the file contents into a single string\n",
    "with open('bookcorpus_text.txt', 'r', encoding='utf-8') as f:\n",
    "    concatenated_text = f.read()\n",
    "\n",
    "# print(\"Reading from file and concatenation complete.\")\n",
    "# print(concatenated_text[:225000000])  # Print the first 1000 characters\n",
    "# print(f\"Total characters: {len(concatenated_text)}\")\n",
    "# print(\"Total words: \", len(concatenated_text.split()))\n",
    "\n",
    "#Using only 1% of the total characters (225 million out of 4.2 billion ->Total words:  45756831 )\n",
    "concatenated_text = concatenated_text[:225000000]\n",
    "# print(\"Total words: \", len(concatenated_text.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IG5ZV9KEcuoL"
   },
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "\n",
    "#Subword level tokenization\n",
    "\n",
    "#Loading custom trained BPE\n",
    "# Load the tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"bpe_tokenizer_tinyshakespeare_1k.json\")\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "# Encode and decode functions\n",
    "encode = lambda s: tokenizer.encode(s).ids\n",
    "decode = lambda l: tokenizer.decode(l)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "#Character level tokenization\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "# chars = sorted(list(set(text)))\n",
    "# vocab_size = len(chars)\n",
    "\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "# stoi = { ch: i for i,ch in enumerate(chars) }\n",
    "# itos = { i:ch for i,ch in enumerate(chars) }\n",
    "# encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "# decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ndPfBp-Gb0KN"
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "\n",
    "block_size = 512\n",
    "batch_size = 64\n",
    "embeddings_dims = 768\n",
    "attn_dropout = 0.1\n",
    "no_of_heads = 12 #IMP needs to be thoroughly calculated\n",
    "dropout = 0.1\n",
    "epochs = 100\n",
    "max_lr = 2.5e-4\n",
    "no_of_decoder_layers = 12 #IMP needs to be thoroughly calculated\n",
    "weight_decay_optim = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "goaGJ8k1cuoM"
   },
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "383747"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) \n",
    "# steps_per_epoch = 383747 / 64 = 6000 \n",
    "#Tptal epochs = epcoh * steps_per_epoch = 1 * 6000 = 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "qAhkF6nmcuoN"
   },
   "outputs": [],
   "source": [
    "# Text embeddings\n",
    "class TextEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size = vocab_size,\n",
    "        embeddings_dims = embeddings_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embeddings_table = nn.Embedding(num_embeddings = vocab_size, embedding_dim=embeddings_dims, device=device) #Just a look up table to convert the toekns_ids to some numbers\n",
    "        # nn.init.normal_(self.embeddings_table.weight.data, mean=0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embeddings_table(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "XllUd3tqcuoN"
   },
   "outputs": [],
   "source": [
    "# #Position embeddings\n",
    "# class PositionEmbeddings(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embeddings_dims = embeddings_dims,\n",
    "#         block_size = block_size\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.position_embeddings = nn.Parameter(torch.randn(1, block_size, embeddings_dims, device=device), requires_grad=True) #To give positional embeddings to each token of the input text, hence num_embeddings=block_size\n",
    "#         # nn.init.normal_(self.position_embeddings.weight.data, mean=0, std=0.02)\n",
    "\n",
    "#     def forward(self):\n",
    "#         return self.position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "REUDHWrWcuoN"
   },
   "outputs": [],
   "source": [
    "#Layer Normalization\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims = embeddings_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embeddings_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "lEe02cH9cuoN"
   },
   "outputs": [],
   "source": [
    "#FeedForward Neural Network\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dropout = dropout,\n",
    "        embeddings_size = embeddings_dims,\n",
    "        # inner_dimensional_states: int = 3072\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(device=device, in_features=embeddings_size, out_features= 4 * embeddings_dims),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(device=device, in_features= 4 * embeddings_dims, out_features=embeddings_size),\n",
    "            nn.Dropout(p = dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # mlp_weights_init = self.mlp.apply(weights_init)\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "0Yv4qaTdcuoN"
   },
   "outputs": [],
   "source": [
    "# #Weights Initilization (for MLP Block)\n",
    "# def weights_init(m):\n",
    "#     classname = m.__class__.__name__\n",
    "#     if classname.find('Linear') != -1:\n",
    "#         nn.init.normal_(m.weight.data, 0.0, 0.02)  #mean = 0, std = 0.02\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "cf0Jf_7UcuoN"
   },
   "outputs": [],
   "source": [
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = attn_dropout,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.head_size = embeddings_dims // no_of_heads\n",
    "        self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device, bias=False)\n",
    "        self.keys = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,device=device, bias=False)\n",
    "        self.values = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device,bias=False)\n",
    "        self.dropout = nn.Dropout(p = attn_dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, block_size, embd_dims = x.shape\n",
    "        k = self.keys(x)\n",
    "        q = self.query(x)\n",
    "        v = self.values(x)\n",
    "        masked_table = torch.tril(torch.ones(block_size, block_size, device=device))\n",
    "        weights = q @ torch.transpose(k, dim0=-2, dim1=-1) * (k.shape[-1] ** -0.5)\n",
    "        masked_values = weights.masked_fill(masked_table[: block_size, : block_size] == 0, float('-inf'))\n",
    "        weights_normalized = nn.functional.softmax(masked_values, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
    "        # weights_normalized = self.dropout(weights_normalized)\n",
    "        out = weights_normalized @ v\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "# class AttentionHead(nn.Module):\n",
    "#     \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         attn_dropout = attn_dropout,\n",
    "#         embeddings_dims = embeddings_dims,\n",
    "#         no_of_heads = no_of_heads,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.head_size = embeddings_dims // no_of_heads\n",
    "#         self.key = nn.Linear(embeddings_dims, self.head_size, bias=False)\n",
    "#         self.query = nn.Linear(embeddings_dims, self.head_size, bias=False)\n",
    "#         self.value = nn.Linear(embeddings_dims, self.head_size, bias=False)\n",
    "#         self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "#         self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # input of size (batch, time-step, channels)\n",
    "#         # output of size (batch, time-step, head size)\n",
    "#         B,T,C = x.shape\n",
    "#         k = self.key(x)   # (B,T,hs)\n",
    "#         q = self.query(x) # (B,T,hs)\n",
    "#         # compute attention scores (\"affinities\")\n",
    "#         wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "#         wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "#         wei = nn.functional.softmax(wei, dim=-1) # (B, T, T)\n",
    "#         wei = self.dropout(wei)\n",
    "#         # perform the weighted aggregation of the values\n",
    "#         v = self.value(x) # (B,T,hs)\n",
    "#         out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "asiOs-sFcuoO"
   },
   "outputs": [],
   "source": [
    "# MHA\n",
    "\n",
    "# class MultiHeadAttention(nn.Module):\n",
    "#     \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "#     def __init__(self,\n",
    "#                   attn_dropout = attn_dropout,\n",
    "#                   embeddings_dims = embeddings_dims,\n",
    "#                   no_of_heads = no_of_heads,\n",
    "#                  ):\n",
    "#         super().__init__()\n",
    "#         self.heads = nn.ModuleList([AttentionHead(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads) for _ in range(num_heads)])\n",
    "#         self.proj = nn.Linear(embeddings_dims, embeddings_dims)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "#         out = self.dropout(self.proj(out))\n",
    "#         return out\n",
    "\n",
    "\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = attn_dropout,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads) for _ in range(no_of_heads)])\n",
    "        self.dropout = nn.Dropout(p = attn_dropout)\n",
    "        self.linear = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=device, bias=False) # 12 (no of heads) * (batch_size) 64 = 768 -> gives out the text embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        concat = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        linear_layer = self.linear(concat)\n",
    "        out = self.dropout(linear_layer)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9rJzO_XcuoO"
   },
   "outputs": [],
   "source": [
    "# Decoder Block\n",
    "\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = attn_dropout,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "        dropout = dropout,\n",
    "        vocab_size = vocab_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = MHA(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads)\n",
    "        self.layer_norm1 = LayerNormalization(embeddings_dims=embeddings_dims)\n",
    "        self.layer_norm2 = LayerNormalization(embeddings_dims=embeddings_dims)\n",
    "        self.mlp_block = MLPBlock(dropout=dropout, embeddings_size=embeddings_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.mha(x)\n",
    "        # x = x + self.layer_norm1(x)\n",
    "        # x = x + self.mlp_block(x)\n",
    "        # out = self.layer_norm2(x)\n",
    "        # x = x + self.mha(self.layer_norm1(x))  #Very important step -> Layer Norm on input and then passes it to the subsequent blocks\n",
    "        # x = x + self.mlp_block(self.layer_norm2(x)) #Very important step\n",
    "        x = self.layer_norm1(x + self.mha(x))\n",
    "        x = self.layer_norm1(x + self.mlp_block(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGh8ujQJcuoO"
   },
   "outputs": [],
   "source": [
    "# Decoder Block\n",
    "\n",
    "class DecoderModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = attn_dropout,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "        block_size = block_size,\n",
    "        dropout = dropout,\n",
    "        no_of_decoder_layers = no_of_decoder_layers,\n",
    "        vocab_size = vocab_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.positional_embeddings = nn.Parameter(torch.randn(1, block_size, embeddings_dims, device=device), requires_grad=True) #To give positional embeddings to each token of the input text, hence num_embeddings=block_size\n",
    "        torch.nn.init.normal_(self.positional_embeddings, mean=0.0, std=0.02)\n",
    "        self.text_embds = TextEmbeddings(vocab_size=vocab_size, embeddings_dims=embeddings_dims)\n",
    "        self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=vocab_size, device=device, bias=False) # Takes in logits of dimensions- embeds_dims and converts it into dimension of vocab_size (logits in range of vocab_size)\n",
    "        self.layer_norm = LayerNormalization(embeddings_dims=embeddings_dims)\n",
    "        self.decoder_layers = nn.Sequential(*[TransformerDecoderBlock(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads, dropout=dropout, vocab_size=vocab_size) for _ in range(no_of_decoder_layers)])\n",
    "        self.apply(self._init_weights)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "\n",
    "    def _init_weights(self, module):  #Weight Initialization\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.text_embds(x)\n",
    "        x = x + self.positional_embeddings\n",
    "        x = self.dropout(x)\n",
    "        x = self.decoder_layers(x)\n",
    "        x = self.layer_norm(x)\n",
    "        out = self.linear_layer(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "tpmbUwBEcuoO"
   },
   "outputs": [],
   "source": [
    "#Instantiating the model\n",
    "model = DecoderModel(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads, block_size=block_size, dropout=dropout, no_of_decoder_layers=no_of_decoder_layers, vocab_size=vocab_size)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yOXtmG-lcuoO",
    "outputId": "4624f4bf-915f-4777-8b00-5d1cf5dc55b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==================================================================================================================================\n",
       "Layer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n",
       "==================================================================================================================================\n",
       "DecoderModel (DecoderModel)                        [64, 512]            [64, 512, 1000]      393,216              True\n",
       "├─TextEmbeddings (text_embds)                      [64, 512]            [64, 512, 768]       --                   True\n",
       "│    └─Embedding (embeddings_table)                [64, 512]            [64, 512, 768]       768,000              True\n",
       "├─Dropout (dropout)                                [64, 512, 768]       [64, 512, 768]       --                   --\n",
       "├─Sequential (decoder_layers)                      [64, 512, 768]       [64, 512, 768]       --                   True\n",
       "│    └─TransformerDecoderBlock (0)                 [64, 512, 768]       [64, 512, 768]       --                   True\n",
       "│    │    └─LayerNormalization (layer_norm1)       [64, 512, 768]       [64, 512, 768]       1,536                True\n",
       "│    │    └─MHA (mha)                              [64, 512, 768]       [64, 512, 768]       2,359,296            True\n",
       "│    │    └─LayerNormalization (layer_norm2)       [64, 512, 768]       [64, 512, 768]       1,536                True\n",
       "│    │    └─MLPBlock (mlp_block)                   [64, 512, 768]       [64, 512, 768]       4,722,432            True\n",
       "│    └─TransformerDecoderBlock (1)                 [64, 512, 768]       [64, 512, 768]       --                   True\n",
       "│    │    └─LayerNormalization (layer_norm1)       [64, 512, 768]       [64, 512, 768]       1,536                True\n",
       "│    │    └─MHA (mha)                              [64, 512, 768]       [64, 512, 768]       2,359,296            True\n",
       "│    │    └─LayerNormalization (layer_norm2)       [64, 512, 768]       [64, 512, 768]       1,536                True\n",
       "│    │    └─MLPBlock (mlp_block)                   [64, 512, 768]       [64, 512, 768]       4,722,432            True\n",
       "│    └─TransformerDecoderBlock (2)                 [64, 512, 768]       [64, 512, 768]       --                   True\n",
       "│    │    └─LayerNormalization (layer_norm1)       [64, 512, 768]       [64, 512, 768]       1,536                True\n",
       "│    │    └─MHA (mha)                              [64, 512, 768]       [64, 512, 768]       2,359,296            True\n",
       "│    │    └─LayerNormalization (layer_norm2)       [64, 512, 768]       [64, 512, 768]       1,536                True\n",
       "│    │    └─MLPBlock (mlp_block)                   [64, 512, 768]       [64, 512, 768]       4,722,432            True\n",
       "│    └─TransformerDecoderBlock (3)                 [64, 512, 768]       [64, 512, 768]       --                   True\n",
       "│    │    └─LayerNormalization (layer_norm1)       [64, 512, 768]       [64, 512, 768]       1,536                True\n",
       "│    │    └─MHA (mha)                              [64, 512, 768]       [64, 512, 768]       2,359,296            True\n",
       "│    │    └─LayerNormalization (layer_norm2)       [64, 512, 768]       [64, 512, 768]       1,536                True\n",
       "│    │    └─MLPBlock (mlp_block)                   [64, 512, 768]       [64, 512, 768]       4,722,432            True\n",
       "│    └─TransformerDecoderBlock (4)                 [64, 512, 768]       [64, 512, 768]       --                   True\n",
       "│    │    └─LayerNormalization (layer_norm1)       [64, 512, 768]       [64, 512, 768]       1,536                True\n",
       "│    │    └─MHA (mha)                              [64, 512, 768]       [64, 512, 768]       2,359,296            True\n",
       "│    │    └─LayerNormalization (layer_norm2)       [64, 512, 768]       [64, 512, 768]       1,536                True\n",
       "│    │    └─MLPBlock (mlp_block)                   [64, 512, 768]       [64, 512, 768]       4,722,432            True\n",
       "│    └─TransformerDecoderBlock (5)                 [64, 512, 768]       [64, 512, 768]       --                   True\n",
       "│    │    └─LayerNormalization (layer_norm1)       [64, 512, 768]       [64, 512, 768]       1,536                True\n",
       "│    │    └─MHA (mha)                              [64, 512, 768]       [64, 512, 768]       2,359,296            True\n",
       "│    │    └─LayerNormalization (layer_norm2)       [64, 512, 768]       [64, 512, 768]       1,536                True\n",
       "│    │    └─MLPBlock (mlp_block)                   [64, 512, 768]       [64, 512, 768]       4,722,432            True\n",
       "│    └─TransformerDecoderBlock (6)                 [64, 512, 768]       [64, 512, 768]       --                   True\n",
       "│    │    └─LayerNormalization (layer_norm1)       [64, 512, 768]       [64, 512, 768]       1,536                True\n",
       "│    │    └─MHA (mha)                              [64, 512, 768]       [64, 512, 768]       2,359,296            True\n",
       "│    │    └─LayerNormalization (layer_norm2)       [64, 512, 768]       [64, 512, 768]       1,536                True\n",
       "│    │    └─MLPBlock (mlp_block)                   [64, 512, 768]       [64, 512, 768]       4,722,432            True\n",
       "│    └─TransformerDecoderBlock (7)                 [64, 512, 768]       [64, 512, 768]       --                   True\n",
       "│    │    └─LayerNormalization (layer_norm1)       [64, 512, 768]       [64, 512, 768]       1,536                True\n",
       "│    │    └─MHA (mha)                              [64, 512, 768]       [64, 512, 768]       2,359,296            True\n",
       "│    │    └─LayerNormalization (layer_norm2)       [64, 512, 768]       [64, 512, 768]       1,536                True\n",
       "│    │    └─MLPBlock (mlp_block)                   [64, 512, 768]       [64, 512, 768]       4,722,432            True\n",
       "│    └─TransformerDecoderBlock (8)                 [64, 512, 768]       [64, 512, 768]       --                   True\n",
       "│    │    └─LayerNormalization (layer_norm1)       [64, 512, 768]       [64, 512, 768]       1,536                True\n",
       "│    │    └─MHA (mha)                              [64, 512, 768]       [64, 512, 768]       2,359,296            True\n",
       "│    │    └─LayerNormalization (layer_norm2)       [64, 512, 768]       [64, 512, 768]       1,536                True\n",
       "│    │    └─MLPBlock (mlp_block)                   [64, 512, 768]       [64, 512, 768]       4,722,432            True\n",
       "│    └─TransformerDecoderBlock (9)                 [64, 512, 768]       [64, 512, 768]       --                   True\n",
       "│    │    └─LayerNormalization (layer_norm1)       [64, 512, 768]       [64, 512, 768]       1,536                True\n",
       "│    │    └─MHA (mha)                              [64, 512, 768]       [64, 512, 768]       2,359,296            True\n",
       "│    │    └─LayerNormalization (layer_norm2)       [64, 512, 768]       [64, 512, 768]       1,536                True\n",
       "│    │    └─MLPBlock (mlp_block)                   [64, 512, 768]       [64, 512, 768]       4,722,432            True\n",
       "│    └─TransformerDecoderBlock (10)                [64, 512, 768]       [64, 512, 768]       --                   True\n",
       "│    │    └─LayerNormalization (layer_norm1)       [64, 512, 768]       [64, 512, 768]       1,536                True\n",
       "│    │    └─MHA (mha)                              [64, 512, 768]       [64, 512, 768]       2,359,296            True\n",
       "│    │    └─LayerNormalization (layer_norm2)       [64, 512, 768]       [64, 512, 768]       1,536                True\n",
       "│    │    └─MLPBlock (mlp_block)                   [64, 512, 768]       [64, 512, 768]       4,722,432            True\n",
       "│    └─TransformerDecoderBlock (11)                [64, 512, 768]       [64, 512, 768]       --                   True\n",
       "│    │    └─LayerNormalization (layer_norm1)       [64, 512, 768]       [64, 512, 768]       1,536                True\n",
       "│    │    └─MHA (mha)                              [64, 512, 768]       [64, 512, 768]       2,359,296            True\n",
       "│    │    └─LayerNormalization (layer_norm2)       [64, 512, 768]       [64, 512, 768]       1,536                True\n",
       "│    │    └─MLPBlock (mlp_block)                   [64, 512, 768]       [64, 512, 768]       4,722,432            True\n",
       "├─LayerNormalization (layer_norm)                  [64, 512, 768]       [64, 512, 768]       --                   True\n",
       "│    └─LayerNorm (layer_norm)                      [64, 512, 768]       [64, 512, 768]       1,536                True\n",
       "├─Linear (linear_layer)                            [64, 512, 768]       [64, 512, 1000]      768,000              True\n",
       "==================================================================================================================================\n",
       "Total params: 86,948,352\n",
       "Trainable params: 86,948,352\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 5.54\n",
       "==================================================================================================================================\n",
       "Input size (MB): 0.26\n",
       "Forward/backward pass size (MB): 27239.91\n",
       "Params size (MB): 346.22\n",
       "Estimated Total Size (MB): 27586.39\n",
       "=================================================================================================================================="
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printing a summary of the architecture\n",
    "from torchinfo import summary\n",
    "idx, targets = get_batch('test')\n",
    "# idx = idx.to(device)\n",
    "summary(model=model,\n",
    "        input_data=idx,\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "LH95cJEvcuoO"
   },
   "outputs": [],
   "source": [
    "# Optimizer setup and scheduler steup\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=max_lr, weight_decay=weight_decay_optim)\n",
    "# initial_iters = 2000\n",
    "total_steps = 5000\n",
    "eval_iters = 100\n",
    "# warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period=2000)\n",
    "# lr_scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max= total_steps - initial_iters)\n",
    "# lr_scheduler_linear = torch.optim.lr_scheduler.LinearLR(optimizer=optimizer, total_iters=initial_iters)\n",
    "\n",
    "# @torch.inference_mode()\n",
    "# def estimate_loss():\n",
    "#     out = {}\n",
    "#     model.eval()\n",
    "#     for split in ['val']:\n",
    "#         # losses = torch.zeros(eval_iters)\n",
    "#         # for k in range(eval_iters):\n",
    "#         idx, targets = get_batch(split=split)\n",
    "#         logits = model(idx)\n",
    "#         batch_size, block_size, embeddings_dims = logits.shape\n",
    "#         logits = logits.view(batch_size*block_size, embeddings_dims) # Total tokens(words) => batch_size * block_size\n",
    "#         targets = targets.view(batch_size * block_size)\n",
    "#         loss = nn.functional.cross_entropy(logits, targets)\n",
    "#         # losses[k] = loss.item()\n",
    "#       # out[split] = losses.mean()\n",
    "#         out[split] = loss.item()\n",
    "#     model.train()\n",
    "#     return out\n",
    "@torch.inference_mode()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            idx, targets = get_batch(split=split)\n",
    "            logits = model(idx)\n",
    "            batch_size, block_size, embeddings_dims = logits.shape\n",
    "            logits = logits.view(batch_size*block_size, embeddings_dims) # Total tokens(words) => batch_size * block_size\n",
    "            targets = targets.view(batch_size * block_size)\n",
    "            loss = nn.functional.cross_entropy(logits, targets)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nPrSPPu8cuoO",
    "outputId": "ffd7b67d-f978-468b-a5cb-be17b7706e40"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 100/5000 [02:13<1:51:20,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100: train loss 5.5448, val loss 5.5904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 200/5000 [05:59<1:50:37,  1.38s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 200: train loss 4.7255, val loss 4.9167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 300/5000 [09:50<1:47:53,  1.38s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 300: train loss 4.3034, val loss 4.6327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 400/5000 [13:39<1:46:15,  1.39s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 400: train loss 4.0337, val loss 4.5531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 600/5000 [21:18<1:40:22,  1.37s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 600: train loss 3.0600, val loss 4.6718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 700/5000 [25:08<1:39:11,  1.38s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 700: train loss 2.0392, val loss 5.0161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 800/5000 [28:59<1:36:49,  1.38s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 800: train loss 0.9800, val loss 5.6312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 803/5000 [30:36<2:39:58,  2.29s/it] \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train the  model\n",
    "from tqdm import tqdm\n",
    "\n",
    "model.train()\n",
    "for step in tqdm(range(total_steps)):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if (step  % eval_iters == 0 and step != 0) or step == total_steps - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        torch.save(model.state_dict(), 'weights/gpt_86M_steps_%d.pth' % (step))\n",
    "\n",
    "    idx, targets = get_batch(split='train')\n",
    "    logits = model(idx)\n",
    "    batch_size, block_size, embeddings_dims = logits.shape\n",
    "    logits = logits.view(batch_size*block_size, embeddings_dims)\n",
    "    targets = targets.view(batch_size * block_size)\n",
    "    loss = nn.functional.cross_entropy(logits, targets)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # print(loss.item())\n",
    "    # break\n",
    "\n",
    "    # if step != 0 and (step % eval_iters == 0 or step == total_steps -1) :\n",
    "    #     loss_values = estimate_loss()\n",
    "    #     print(\"Train Loss at {} steps : {}\".format(step, loss.item()), \"Val Loss at {} steps : {}\".format(step, loss_values['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text embeddings\n",
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size = vocab_size,\n",
    "        embeddings_dims = embeddings_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embeddings_table = nn.Embedding(num_embeddings = vocab_size, embedding_dim=embeddings_dims, device=device) #Just a look up table to convert the toekns_ids to some numbers\n",
    " \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embeddings_table(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Position Embeddings\n",
    "\n",
    "class PositionEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block_size = block_size,\n",
    "        embeddings_dims = embeddings_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embeddings_table = nn.Embedding(num_embeddings = block_size, embedding_dim=embeddings_dims, device=device)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embeddings_table(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "_wm7EM3EdsPN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your token (input will not be visible):  ········\n",
      "Add token as git credential? (Y/n)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Pushing model to huggingface repo\n",
    "from huggingface_hub import interpreter_login\n",
    "interpreter_login() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'YuvrajSingh9886/GPT-86M\n",
    "model.push_to_hub(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6PgtNzhYP3I"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Specify the path to your custom model directory\n",
    "model_path = model_name\n",
    "\n",
    "# Load the tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Example usage\n",
    "text = \"This is a sample text.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Get model predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation\n",
    "!pip install evaluate\n",
    "from evaluate import load\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "results = perplexity.compute(predictions=predictions, model_id='gpt2')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
