{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:14.850460Z",
     "iopub.status.busy": "2024-12-30T23:41:14.849859Z",
     "iopub.status.idle": "2024-12-30T23:41:25.813585Z",
     "shell.execute_reply": "2024-12-30T23:41:25.812564Z",
     "shell.execute_reply.started": "2024-12-30T23:41:14.850416Z"
    },
    "id": "yZ8MZf2sjKI7",
    "outputId": "479153b2-3869-495e-f90b-6cb8b3aac3cf",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'de' are deprecated. Please use the\n",
      "full pipeline package name 'de_core_news_sm' instead.\u001b[0m\n",
      "Collecting de-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.7.0) (3.7.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (71.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (13.8.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -q torch==2.2.0 torchtext==0.17.0\n",
    "!python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:25.815754Z",
     "iopub.status.busy": "2024-12-30T23:41:25.815482Z",
     "iopub.status.idle": "2024-12-30T23:41:27.153981Z",
     "shell.execute_reply": "2024-12-30T23:41:27.153285Z",
     "shell.execute_reply.started": "2024-12-30T23:41:25.815732Z"
    },
    "id": "Pw7f2ghccuoK",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:27.155526Z",
     "iopub.status.busy": "2024-12-30T23:41:27.155145Z",
     "iopub.status.idle": "2024-12-30T23:41:27.161220Z",
     "shell.execute_reply": "2024-12-30T23:41:27.160393Z",
     "shell.execute_reply.started": "2024-12-30T23:41:27.155490Z"
    },
    "id": "aW5k-CsTj7ZP",
    "outputId": "e9449ec6-3e98-4c09-acb4-cf0cf887e703",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0+cu121'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:27.162394Z",
     "iopub.status.busy": "2024-12-30T23:41:27.162059Z",
     "iopub.status.idle": "2024-12-30T23:41:27.218628Z",
     "shell.execute_reply": "2024-12-30T23:41:27.217578Z",
     "shell.execute_reply.started": "2024-12-30T23:41:27.162363Z"
    },
    "id": "adLpt7j7cuoL",
    "outputId": "5d35ff16-bf18-4940-eed3-054cba8f547b",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:27.219915Z",
     "iopub.status.busy": "2024-12-30T23:41:27.219603Z",
     "iopub.status.idle": "2024-12-30T23:41:27.512559Z",
     "shell.execute_reply": "2024-12-30T23:41:27.511665Z",
     "shell.execute_reply.started": "2024-12-30T23:41:27.219890Z"
    },
    "id": "Yk3urEOwjHyM",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU Name: Tesla T4\n",
      "Tensor on GPU: tensor([1., 2., 3.], device='cuda:0')\n",
      "Result of operation on GPU: tensor([2., 4., 6.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "\n",
    "# If CUDA is available, print the GPU name and perform a test operation\n",
    "if cuda_available:\n",
    "    # Get the name of the GPU\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU Name: {gpu_name}\")\n",
    "\n",
    "    # Create a tensor and move it to the GPU\n",
    "    x = torch.tensor([1.0, 2.0, 3.0], device='cuda')\n",
    "    print(f\"Tensor on GPU: {x}\")\n",
    "\n",
    "    # Perform a simple operation\n",
    "    y = x * 2\n",
    "    print(f\"Result of operation on GPU: {y}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Please check your PyTorch installation and GPU drivers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:27.513712Z",
     "iopub.status.busy": "2024-12-30T23:41:27.513428Z",
     "iopub.status.idle": "2024-12-30T23:41:27.517228Z",
     "shell.execute_reply": "2024-12-30T23:41:27.516268Z",
     "shell.execute_reply.started": "2024-12-30T23:41:27.513688Z"
    },
    "id": "LwR5_uvTcuoL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:27.518612Z",
     "iopub.status.busy": "2024-12-30T23:41:27.518288Z",
     "iopub.status.idle": "2024-12-30T23:41:37.946170Z",
     "shell.execute_reply": "2024-12-30T23:41:37.945437Z",
     "shell.execute_reply.started": "2024-12-30T23:41:27.518583Z"
    },
    "id": "uEN3FPxEjHyP",
    "outputId": "7ac615fa-3a78-4936-ff9d-62771019ee80",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"de\" could not be loaded, trying \"de_core_news_sm\" instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "import io\n",
    "\n",
    "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
    "train_urls = ('train.de.gz', 'train.en.gz')\n",
    "val_urls = ('val.de.gz', 'val.en.gz')\n",
    "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
    "\n",
    "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
    "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
    "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n",
    "\n",
    "de_tokenizer = get_tokenizer('spacy', language='de')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en')\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def build_vocab(filepath, tokenizer):\n",
    "    counter = Counter()\n",
    "    with io.open(filepath, encoding=\"utf8\") as f:\n",
    "        for string_ in f:\n",
    "            counter.update(tokenizer(string_))\n",
    "    # Ensure '<pad>' is at index 0 by placing it first in the specials list\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        [counter.keys()],\n",
    "        specials=['<pad>', '<unk>', '<bos>', '<eos>']  # '<pad>' comes first\n",
    "    )\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "    return vocab\n",
    "\n",
    "\n",
    "de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
    "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)\n",
    "\n",
    "def data_process(filepaths):\n",
    "  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "  data = []\n",
    "  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
    "    de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)],\n",
    "                            dtype=torch.long)\n",
    "    en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)],\n",
    "                            dtype=torch.long)\n",
    "    data.append((de_tensor_, en_tensor_))\n",
    "  return data\n",
    "\n",
    "train_data = data_process(train_filepaths)\n",
    "val_data = data_process(val_filepaths)\n",
    "test_data = data_process(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:37.949239Z",
     "iopub.status.busy": "2024-12-30T23:41:37.948746Z",
     "iopub.status.idle": "2024-12-30T23:41:37.954281Z",
     "shell.execute_reply": "2024-12-30T23:41:37.953375Z",
     "shell.execute_reply.started": "2024-12-30T23:41:37.949170Z"
    },
    "id": "uJ5yZMaQMxb1",
    "outputId": "dbc051d0-2442-46d5-89d7-dae61c9a4c79",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE <pad> index: 0\n",
      "EN <pad> index: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"DE <pad> index: {de_vocab['<pad>']}\")\n",
    "print(f\"EN <pad> index: {en_vocab['<pad>']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:37.956230Z",
     "iopub.status.busy": "2024-12-30T23:41:37.955961Z",
     "iopub.status.idle": "2024-12-30T23:41:37.968761Z",
     "shell.execute_reply": "2024-12-30T23:41:37.967954Z",
     "shell.execute_reply.started": "2024-12-30T23:41:37.956179Z"
    },
    "id": "D7AP219KJzTs",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "\n",
    "block_size = 256\n",
    "batch_size = 64\n",
    "src_vocab_size = len(de_vocab)\n",
    "tgt_vocab_size = len(en_vocab)\n",
    "embeddings_dims = 384\n",
    "attn_dropout = 0.1\n",
    "no_of_heads = 6 #IMP needs to be thoroughly calculated\n",
    "dropout = 0.1\n",
    "epochs = 3\n",
    "max_lr = 2e-4\n",
    "no_of_decoder_layers = 6 #IMP needs to be thoroughly calculated\n",
    "attn_dropout = 0.1\n",
    "weight_decay_optim = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:37.969812Z",
     "iopub.status.busy": "2024-12-30T23:41:37.969525Z",
     "iopub.status.idle": "2024-12-30T23:41:37.985084Z",
     "shell.execute_reply": "2024-12-30T23:41:37.984149Z",
     "shell.execute_reply.started": "2024-12-30T23:41:37.969779Z"
    },
    "id": "LQ6lYiGJLTDq",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from textwrap import indent\n",
    "# # Custom collate function for padding\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     # Separate inputs and targets\n",
    "#     inputs, targets = zip(*batch)\n",
    "\n",
    "#     # Pad input sequences\n",
    "#     inputs_padded = pad_sequence([torch.tensor(seq) for seq in inputs], batch_first=True, padding_value=0)\n",
    "\n",
    "#     # Pad target sequences (if targets are sequences) or convert them to a tensor\n",
    "#     targets_padded = pad_sequence([torch.tensor(seq) for seq in targets], batch_first=True, padding_value=0)\n",
    "\n",
    "#     return inputs_padded, targets_padded\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = batch_size\n",
    "PAD_IDX = de_vocab['<pad>']\n",
    "BOS_IDX = de_vocab['<bos>']\n",
    "EOS_IDX = de_vocab['<eos>']\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Constants\n",
    "MAX_LENGTH = block_size  # Desired sequence length for padding\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    de_batch, en_batch, src_padding_masks, tgt_padding_masks= [], [], [], []\n",
    "    for (de_item, en_item) in data_batch:\n",
    "        # Ensure <BOS> is at the beginning and <EOS> at the end\n",
    "        de_item = torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0)\n",
    "        en_item = torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0)\n",
    "\n",
    "        # Manually pad sequences to the maximum length (256 in this case)\n",
    "        de_item_padded = F.pad(de_item, (0, MAX_LENGTH - de_item.size(0)), value=PAD_IDX)\n",
    "        en_item_padded = F.pad(en_item, (0, MAX_LENGTH - en_item.size(0)), value=PAD_IDX)\n",
    "\n",
    "         # Generate padding masks (1 for non-padding, 0 for padding)\n",
    "        src_mask = (de_item_padded != PAD_IDX).int()  # Source mask\n",
    "        tgt_mask = (en_item_padded != PAD_IDX).int()  # Target mask\n",
    "\n",
    "        # Append to the batch\n",
    "        de_batch.append(de_item_padded)\n",
    "        en_batch.append(en_item_padded)\n",
    "        src_padding_masks.append(src_mask)\n",
    "        tgt_padding_masks.append(tgt_mask)\n",
    "\n",
    "    # Stack batches together\n",
    "    de_batch = torch.stack(de_batch, dim=0)\n",
    "    en_batch = torch.stack(en_batch, dim=0)\n",
    "    src_padding_masks = torch.stack(src_padding_masks, dim=0)  # Shape: (batch_size, MAX_LENGTH)\n",
    "    tgt_padding_masks = torch.stack(tgt_padding_masks, dim=0)  # Shape: (batch_size, MAX_LENGTH)\n",
    "\n",
    "    return de_batch, en_batch, src_padding_masks, tgt_padding_masks\n",
    "\n",
    "# DataLoader with the custom collate_fn\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size,\n",
    "                               shuffle=True, collate_fn=generate_batch)\n",
    "val_dataloader = DataLoader(val_data, batch_size=batch_size,\n",
    "                             shuffle=False, collate_fn=generate_batch)\n",
    "# test_iter = DataLoader(test_data, batch_size=batch_size,\n",
    "#                        shuffle=True, collate_fn=generate_batch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:37.986292Z",
     "iopub.status.busy": "2024-12-30T23:41:37.985968Z",
     "iopub.status.idle": "2024-12-30T23:41:38.000621Z",
     "shell.execute_reply": "2024-12-30T23:41:37.999881Z",
     "shell.execute_reply.started": "2024-12-30T23:41:37.986261Z"
    },
    "id": "jXuim2eLI4gu",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# #Dataloaders\n",
    "# train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "# val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "# test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:38.001933Z",
     "iopub.status.busy": "2024-12-30T23:41:38.001642Z",
     "iopub.status.idle": "2024-12-30T23:41:38.013974Z",
     "shell.execute_reply": "2024-12-30T23:41:38.013055Z",
     "shell.execute_reply.started": "2024-12-30T23:41:38.001903Z"
    },
    "id": "Qi_iCJxbpbAS",
    "outputId": "05705705-7c1d-4524-abbc-198a8ccb8dc8",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German Vocabulary Size: 19215\n",
      "English Vocabulary Size: 10838\n"
     ]
    }
   ],
   "source": [
    "# Get the vocabulary size for German and English\n",
    "german_vocab_size = len(de_vocab)\n",
    "english_vocab_size = len(en_vocab)\n",
    "\n",
    "print(f\"German Vocabulary Size: {german_vocab_size}\")\n",
    "print(f\"English Vocabulary Size: {english_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:38.015271Z",
     "iopub.status.busy": "2024-12-30T23:41:38.014968Z",
     "iopub.status.idle": "2024-12-30T23:41:38.038436Z",
     "shell.execute_reply": "2024-12-30T23:41:38.037608Z",
     "shell.execute_reply.started": "2024-12-30T23:41:38.015246Z"
    },
    "id": "jkzEorgmo-JS",
    "outputId": "ca90003b-f309-4d3e-edcb-ad7a5a02a5fa",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    2,  2473, 15830,  ...,     0,     0,     0],\n",
       "         [    2,  2481,  6546,  ...,     0,     0,     0],\n",
       "         [    2,  2481,  4170,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [    2,  2473, 13509,  ...,     0,     0,     0],\n",
       "         [    2,  2473,  6546,  ...,     0,     0,     0],\n",
       "         [    2,  2473,  6546,  ...,     0,     0,     0]]),\n",
       " tensor([[    2,   102, 10819,  ...,     0,     0,     0],\n",
       "         [    2,   102,  6509,  ...,     0,     0,     0],\n",
       "         [    2,   102,  5371,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [    2,   102,  6365,  ...,     0,     0,     0],\n",
       "         [    2,   102,  6509,  ...,     0,     0,     0],\n",
       "         [    2,   102,  6509,  ...,     0,     0,     0]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:38.039608Z",
     "iopub.status.busy": "2024-12-30T23:41:38.039313Z",
     "iopub.status.idle": "2024-12-30T23:41:38.042776Z",
     "shell.execute_reply": "2024-12-30T23:41:38.041938Z",
     "shell.execute_reply.started": "2024-12-30T23:41:38.039577Z"
    },
    "id": "jhhR2b3dOjJY",
    "outputId": "e55b08b6-4a23-4d96-cf20-aeda8cc17655",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# for x, y in train_dataloader:\n",
    "#     print(x.shape)\n",
    "#     print(y.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:38.044041Z",
     "iopub.status.busy": "2024-12-30T23:41:38.043739Z",
     "iopub.status.idle": "2024-12-30T23:41:38.053157Z",
     "shell.execute_reply": "2024-12-30T23:41:38.052522Z",
     "shell.execute_reply.started": "2024-12-30T23:41:38.044011Z"
    },
    "id": "R9CSiuD2jHyT",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Text embeddings\n",
    "class TgtTextEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size = tgt_vocab_size,\n",
    "        embeddings_dims = embeddings_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embeddings_table = nn.Embedding(num_embeddings = tgt_vocab_size, embedding_dim=embeddings_dims, device=device) #Just a look up table to convert the toekns_ids to some numbers\n",
    "        # nn.init.normal_(self.embeddings_table.weight.data, mean=0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embeddings_table(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:38.054262Z",
     "iopub.status.busy": "2024-12-30T23:41:38.053981Z",
     "iopub.status.idle": "2024-12-30T23:41:38.068207Z",
     "shell.execute_reply": "2024-12-30T23:41:38.067509Z",
     "shell.execute_reply.started": "2024-12-30T23:41:38.054239Z"
    },
    "id": "qAhkF6nmcuoN",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Text embeddings\n",
    "class SrcTextEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size = src_vocab_size,\n",
    "        embeddings_dims = embeddings_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embeddings_table = nn.Embedding(num_embeddings = src_vocab_size, embedding_dim=embeddings_dims, device=device) #Just a look up table to convert the toekns_ids to some numbers\n",
    "        # nn.init.normal_(self.embeddings_table.weight.data, mean=0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embeddings_table(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:38.069221Z",
     "iopub.status.busy": "2024-12-30T23:41:38.068924Z",
     "iopub.status.idle": "2024-12-30T23:41:38.079344Z",
     "shell.execute_reply": "2024-12-30T23:41:38.078634Z",
     "shell.execute_reply.started": "2024-12-30T23:41:38.069166Z"
    },
    "id": "REUDHWrWcuoN",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Layer Normalization\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims = embeddings_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(normalized_shape=embeddings_dims)\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:38.080590Z",
     "iopub.status.busy": "2024-12-30T23:41:38.080282Z",
     "iopub.status.idle": "2024-12-30T23:41:38.093224Z",
     "shell.execute_reply": "2024-12-30T23:41:38.092377Z",
     "shell.execute_reply.started": "2024-12-30T23:41:38.080556Z"
    },
    "id": "lEe02cH9cuoN",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#FeedForward Neural Network\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dropout = dropout,\n",
    "        embeddings_size = embeddings_dims,\n",
    "        # inner_dimensional_states: int = 3072\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(device=device, in_features=embeddings_size, out_features= 4 * embeddings_dims),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(device=device, in_features= 4 * embeddings_dims, out_features=embeddings_size),\n",
    "            nn.Dropout(p = dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # mlp_weights_init = self.mlp.apply(weights_init)\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:38.094455Z",
     "iopub.status.busy": "2024-12-30T23:41:38.094129Z",
     "iopub.status.idle": "2024-12-30T23:41:38.107217Z",
     "shell.execute_reply": "2024-12-30T23:41:38.106611Z",
     "shell.execute_reply.started": "2024-12-30T23:41:38.094424Z"
    },
    "id": "cf0Jf_7UcuoN",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class MaskedAttentionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = attn_dropout,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.head_size = embeddings_dims // no_of_heads\n",
    "        self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device, bias=False)\n",
    "        self.keys = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,device=device, bias=False)\n",
    "        self.values = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device,bias=False)\n",
    "        self.dropout = nn.Dropout(p = attn_dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, block_size, embd_dims = x.shape\n",
    "        k = self.keys(x)\n",
    "        q = self.query(x)\n",
    "        v = self.values(x)\n",
    "        masked_table = torch.tril(torch.ones(block_size, block_size, device=device))\n",
    "        weights = q @ torch.transpose(k, dim0=-2, dim1=-1) * (k.shape[-1] ** -0.5)\n",
    "        masked_values = weights.masked_fill(masked_table[: block_size, : block_size] == 0, float('-inf'))\n",
    "        weights_normalized = nn.functional.softmax(masked_values, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
    "        weights_normalized = self.dropout(weights_normalized)\n",
    "        out = weights_normalized @ v\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:38.108386Z",
     "iopub.status.busy": "2024-12-30T23:41:38.108072Z",
     "iopub.status.idle": "2024-12-30T23:41:38.122997Z",
     "shell.execute_reply": "2024-12-30T23:41:38.122019Z",
     "shell.execute_reply.started": "2024-12-30T23:41:38.108365Z"
    },
    "id": "OUFERSL2u8LT",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class MaskedMHA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = attn_dropout,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([MaskedAttentionHead(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads) for _ in range(no_of_heads)])\n",
    "        self.dropout = nn.Dropout(p = attn_dropout)\n",
    "        self.linear = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=device, bias=False) # 12 (no of heads) * (batch_size) 64 = 768 -> gives out the text embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        concat = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        linear_layer = self.linear(concat)\n",
    "        out = self.dropout(linear_layer)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:38.124170Z",
     "iopub.status.busy": "2024-12-30T23:41:38.123871Z",
     "iopub.status.idle": "2024-12-30T23:41:38.136893Z",
     "shell.execute_reply": "2024-12-30T23:41:38.136282Z",
     "shell.execute_reply.started": "2024-12-30T23:41:38.124133Z"
    },
    "id": "oGGyyF4pjHyd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Single Attention Head\n",
    "\n",
    "class CrossAttentionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = attn_dropout,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.head_size = embeddings_dims // no_of_heads\n",
    "        self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device, bias=False)\n",
    "        self.keys = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,device=device, bias=False)\n",
    "        self.values = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device,bias=False)\n",
    "        self.dropout = nn.Dropout(p = attn_dropout)\n",
    "\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # batch, block_size, embd_dims = x.shape\n",
    "\n",
    "        # masked_table = torch.tril(torch.ones(block_size, block_size, device=device))\n",
    "        weights = query @ torch.transpose(key, dim0=-2, dim1=-1) * (key.shape[-1] ** -0.5)\n",
    "        if(mask != None):\n",
    "            mask = mask.unsqueeze(1)\n",
    "            masked_values = weights.masked_fill(mask == 0, float('-inf'))\n",
    "            weights_normalized = nn.functional.softmax(masked_values, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
    "            # weights_normalized = self.dropout(weights_normalized)\n",
    "            out = weights_normalized @ value\n",
    "            out = self.dropout(out)\n",
    "            return out\n",
    "        else:\n",
    "            weights_normalized = nn.functional.softmax(weights, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
    "            # weights_normalized = self.dropout(weights_normalized)\n",
    "            out = weights_normalized @ value\n",
    "            out = self.dropout(out)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:38.138084Z",
     "iopub.status.busy": "2024-12-30T23:41:38.137820Z",
     "iopub.status.idle": "2024-12-30T23:41:38.151910Z",
     "shell.execute_reply": "2024-12-30T23:41:38.150743Z",
     "shell.execute_reply.started": "2024-12-30T23:41:38.138064Z"
    },
    "id": "U5NmszzcjHyf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Single Attention Head\n",
    "\n",
    "class FullAttentionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = attn_dropout,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.head_size = embeddings_dims // no_of_heads\n",
    "        self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device, bias=False)\n",
    "        self.keys = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,device=device, bias=False)\n",
    "        self.values = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device,bias=False)\n",
    "        self.dropout = nn.Dropout(p = attn_dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # batch, block_size, embd_dims = x.shape\n",
    "        k = self.keys(x)\n",
    "        q = self.query(x)\n",
    "        v = self.values(x)\n",
    "        # masked_table = torch.tril(torch.ones(block_size, block_size, device=device))\n",
    "        weights = q @ torch.transpose(k, dim0=-2, dim1=-1) * (k.shape[-1] ** -0.5)\n",
    "        if(mask != None):\n",
    "            mask = mask.unsqueeze(1)\n",
    "            masked_values = weights.masked_fill(mask == 0, float('-inf'))\n",
    "            weights_normalized = nn.functional.softmax(masked_values, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
    "            # weights_normalized = self.dropout(weights_normalized)\n",
    "            out = weights_normalized @ v\n",
    "            out = self.dropout(out)\n",
    "            return out\n",
    "        else:\n",
    "            weights_normalized = nn.functional.softmax(weights, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
    "            # weights_normalized = self.dropout(weights_normalized)\n",
    "            out = weights_normalized @ v\n",
    "            out = self.dropout(out)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:38.153253Z",
     "iopub.status.busy": "2024-12-30T23:41:38.152929Z",
     "iopub.status.idle": "2024-12-30T23:41:38.167755Z",
     "shell.execute_reply": "2024-12-30T23:41:38.166944Z",
     "shell.execute_reply.started": "2024-12-30T23:41:38.153220Z"
    },
    "id": "v_BB7r7kqmOc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class FullMHA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = attn_dropout,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([FullAttentionHead(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads) for _ in range(no_of_heads)])\n",
    "        self.dropout = nn.Dropout(p = attn_dropout)\n",
    "        self.linear = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=device, bias=False) # 12 (no of heads) * (batch_size) 64 = 768 -> gives out the text embeddings\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        concat = torch.cat([head(x, mask) for head in self.heads], dim=-1)\n",
    "        linear_layer = self.linear(concat)\n",
    "        out = self.dropout(linear_layer)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:38.169064Z",
     "iopub.status.busy": "2024-12-30T23:41:38.168778Z",
     "iopub.status.idle": "2024-12-30T23:41:38.193706Z",
     "shell.execute_reply": "2024-12-30T23:41:38.192606Z",
     "shell.execute_reply.started": "2024-12-30T23:41:38.169035Z"
    },
    "id": "TTwRkBzcvE-_",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CrossMHA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = attn_dropout,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([CrossAttentionHead(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads) for _ in range(no_of_heads)])\n",
    "        self.dropout = nn.Dropout(p = attn_dropout)\n",
    "        self.linear = nn.Linear(in_features=no_of_decoder_layers * embeddings_dims, out_features=embeddings_dims, device=device, bias=False) # 12 (no of heads) * (batch_size) 64 = 768 -> gives out the text embeddings\n",
    "\n",
    "    def forward(self, query, key, x, mask=None):\n",
    "        concat = torch.cat([head(query, key, x,  mask) for head in self.heads], dim=-1)\n",
    "        linear_layer = self.linear(concat)\n",
    "        out = self.dropout(linear_layer)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:38.194950Z",
     "iopub.status.busy": "2024-12-30T23:41:38.194673Z",
     "iopub.status.idle": "2024-12-30T23:41:38.201695Z",
     "shell.execute_reply": "2024-12-30T23:41:38.200801Z",
     "shell.execute_reply.started": "2024-12-30T23:41:38.194926Z"
    },
    "id": "s9rJzO_XcuoO",
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Decoder Block\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTransformerDecoderBlock\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m      6\u001b[0m         attn_dropout \u001b[38;5;241m=\u001b[39m attn_dropout,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# vocab_size = vocab_size\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     ):\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# Decoder Block\n",
    "\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = attn_dropout,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "        dropout = dropout,\n",
    "        # vocab_size = vocab_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cross = CrossMHA(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads)\n",
    "        self.masked = MaskedMHA(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads)\n",
    "        self.layer_norm1 = LayerNormalization(embeddings_dims)\n",
    "        self.layer_norm2 = LayerNormalization(embeddings_dims)\n",
    "        # self.layer_norm3 = LayerNormalization(embeddings_dims=embeddings_dims)\n",
    "        self.layer_norm4 = LayerNormalization(embeddings_dims)\n",
    "        self.mlp_block = MLPBlock(dropout=dropout, embeddings_size=embeddings_dims)\n",
    "\n",
    "    def forward(self, key, value, x, mask=None):\n",
    "        x = self.layer_norm1(x + self.masked(x)) #Very important step -> Layer Norm on input and then passes it to the subsequent blocks\n",
    "        x = self.layer_norm2(x + self.cross(key, value, x, mask)) #Very important step\n",
    "        # x = x + self.mha(self.layer_norm1(x))  #Very important step -> Layer Norm on input and then passes it to the subsequent blocks\n",
    "        x = self.layer_norm4(x + self.mlp_block(x)) #Very important step\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:38.206345Z",
     "iopub.status.busy": "2024-12-30T23:41:38.206020Z",
     "iopub.status.idle": "2024-12-30T23:41:38.223220Z",
     "shell.execute_reply": "2024-12-30T23:41:38.222390Z",
     "shell.execute_reply.started": "2024-12-30T23:41:38.206310Z"
    },
    "id": "KGh8ujQJcuoO",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Decoder Block\n",
    "\n",
    "class DecoderModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = attn_dropout,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "        block_size = block_size,\n",
    "        dropout = dropout,\n",
    "        no_of_decoder_layers = no_of_decoder_layers,\n",
    "        # vocab_size = vocab_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        # self.positional_embeddings_tgt = nn.Parameter(torch.randn(1, block_size, embeddings_dims, device=device), requires_grad=True) #To give positional embeddings to each token of the input text, hence num_embeddings=block_size\n",
    "\n",
    "\n",
    "        # torch.nn.init.normal_(self.positional_embeddings_tgt, mean=0.0, std=0.02)\n",
    "\n",
    "        # self.text_embds = TextEmbeddings(vocab_size=vocab_size, embeddings_dims=embeddings_dims)\n",
    "\n",
    "\n",
    "        self.tgt_text_embds = TgtTextEmbeddings(vocab_size=tgt_vocab_size, embeddings_dims=embeddings_dims)\n",
    "        self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=tgt_vocab_size, device=device, bias=False) # Takes in logits of dimensions- embeds_dims and converts it into dimension of vocab_size (logits in range of vocab_size)\n",
    "        # self.layer_norm = LayerNormalization(embeddings_dims=embeddings_dims)\n",
    "        self.decoder_layers = nn.ModuleList([TransformerDecoderBlock(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads, dropout=dropout) for _ in range(no_of_decoder_layers)])\n",
    "        self.apply(self._init_weights)\n",
    "        self.positional_embeddings_tgt = nn.Parameter(torch.randn(1, block_size, embeddings_dims, device=device), requires_grad=True) #To give positional embeddings to each token of the input text, hence num_embeddings=block_size\n",
    "        torch.nn.init.normal_(self.positional_embeddings_tgt, mean=0.0, std=0.02)\n",
    "\n",
    "        # out = self.decoder_layers(query, key, x)\n",
    "        # Loop through each decoder layer\n",
    "    def _init_weights(self, module):  #Weight Initialization\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, key, value, x, mask):\n",
    "        x = self.tgt_text_embds(x)\n",
    "        x = x + self.positional_embeddings_tgt\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            x = decoder_layer(key, value, x, mask)\n",
    "        # x = self.layer_norm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tpmbUwBEcuoO",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:38.224795Z",
     "iopub.status.busy": "2024-12-30T23:41:38.224514Z",
     "iopub.status.idle": "2024-12-30T23:41:38.237595Z",
     "shell.execute_reply": "2024-12-30T23:41:38.236768Z",
     "shell.execute_reply.started": "2024-12-30T23:41:38.224767Z"
    },
    "id": "A3SgKrC-jHyd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:38.238697Z",
     "iopub.status.busy": "2024-12-30T23:41:38.238404Z",
     "iopub.status.idle": "2024-12-30T23:41:38.247055Z",
     "shell.execute_reply": "2024-12-30T23:41:38.246135Z",
     "shell.execute_reply.started": "2024-12-30T23:41:38.238669Z"
    },
    "id": "v6mbbO3yp-gh",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = attn_dropout,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "        dropout = dropout,\n",
    "        mask=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = FullMHA(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads)\n",
    "        self.layer_norm1 = LayerNormalization(embeddings_dims)\n",
    "        self.layer_norm2 = LayerNormalization(embeddings_dims)\n",
    "        self.mlp_block = MLPBlock(dropout=dropout, embeddings_size=embeddings_dims)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.layer_norm1(x + self.mha(x, mask))\n",
    "        x = self.layer_norm2(x + self.mlp_block(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:38.248257Z",
     "iopub.status.busy": "2024-12-30T23:41:38.247952Z",
     "iopub.status.idle": "2024-12-30T23:41:38.262347Z",
     "shell.execute_reply": "2024-12-30T23:41:38.261499Z",
     "shell.execute_reply.started": "2024-12-30T23:41:38.248229Z"
    },
    "id": "HxW0pvnV12Ms",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EncoderModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = attn_dropout,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "        block_size = block_size,\n",
    "        dropout = dropout,\n",
    "        no_of_decoder_layers = no_of_decoder_layers,\n",
    "        # vocab_size = vocab_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.positional_embeddings_src = nn.Parameter(torch.randn(1, block_size, embeddings_dims, device=device), requires_grad=True) #To give positional embeddings to each token of the input text, hence num_embeddings=block_size\n",
    "\n",
    "        # torch.nn.init.normal_(self.positional_embeddings_src, mean=0.0, std=0.02)\n",
    "\n",
    "        # self.text_embds = TextEmbeddings(vocab_size=vocab_size, embeddings_dims=embeddings_dims)\n",
    "\n",
    "        self.positional_embeddings_src = nn.Parameter(torch.randn(1, block_size, embeddings_dims, device=device), requires_grad=True) #To give positional embeddings to each token of the input text, hence num_embeddings=block_size\n",
    "        torch.nn.init.normal_(self.positional_embeddings_src, mean=0.0, std=0.02)\n",
    "\n",
    "        self.src_text_embeds = SrcTextEmbeddings(vocab_size=src_vocab_size, embeddings_dims=embeddings_dims)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([TransformerEncoderBlock(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads, dropout=dropout) for _ in range(no_of_decoder_layers)])\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):  #Weight Initialization\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        # print(x.shape)\n",
    "        x = self.src_text_embeds(x)\n",
    "        # print(self.positional_embeddings_src.shape)\n",
    "        # print(x.shape)\n",
    "        x = x + self.positional_embeddings_src\n",
    "\n",
    "        # print(x.shape)\n",
    "        # Loop through each encoder layer\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x, mask)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:38.263631Z",
     "iopub.status.busy": "2024-12-30T23:41:38.263313Z",
     "iopub.status.idle": "2024-12-30T23:41:38.278543Z",
     "shell.execute_reply": "2024-12-30T23:41:38.277881Z",
     "shell.execute_reply.started": "2024-12-30T23:41:38.263563Z"
    },
    "id": "2UWijIFl2Ykd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = EncoderModel()\n",
    "        self.decoder = DecoderModel()\n",
    "        self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=tgt_vocab_size, device=device, bias=False) # Takes in logits of dimensions- embeds_dims and converts it into dimension of vocab_size (logits in range of vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        x = self.encoder(src, src_mask)\n",
    "        x = self.decoder(x, x, tgt, None)\n",
    "        out = self.linear_layer(x)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:38.279371Z",
     "iopub.status.busy": "2024-12-30T23:41:38.279081Z",
     "iopub.status.idle": "2024-12-30T23:41:38.562125Z",
     "shell.execute_reply": "2024-12-30T23:41:38.561113Z",
     "shell.execute_reply.started": "2024-12-30T23:41:38.279345Z"
    },
    "id": "ntIaQj1U3pFX",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Instantiating the model\n",
    "model = Transformer()\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:38.563536Z",
     "iopub.status.busy": "2024-12-30T23:41:38.563144Z",
     "iopub.status.idle": "2024-12-30T23:41:38.570127Z",
     "shell.execute_reply": "2024-12-30T23:41:38.569260Z",
     "shell.execute_reply.started": "2024-12-30T23:41:38.563503Z"
    },
    "id": "FVsCsep95J9a",
    "outputId": "de4f6be8-e987-4cce-a10f-317f42f837b3",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[2][1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:38.571435Z",
     "iopub.status.busy": "2024-12-30T23:41:38.571114Z",
     "iopub.status.idle": "2024-12-30T23:41:42.449903Z",
     "shell.execute_reply": "2024-12-30T23:41:42.448944Z",
     "shell.execute_reply.started": "2024-12-30T23:41:38.571405Z"
    },
    "id": "yOXtmG-lcuoO",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=======================================================================================================================================\n",
       "Layer (type (var_name))                                 Input Shape          Output Shape         Param #              Trainable\n",
       "=======================================================================================================================================\n",
       "Transformer (Transformer)                               [64, 256]            [64, 256, 10838]     --                   True\n",
       "├─EncoderModel (encoder)                                [64, 256]            [64, 256, 384]       98,304               True\n",
       "│    └─SrcTextEmbeddings (src_text_embeds)              [64, 256]            [64, 256, 384]       --                   True\n",
       "│    │    └─Embedding (embeddings_table)                [64, 256]            [64, 256, 384]       7,378,560            True\n",
       "│    └─ModuleList (encoder_layers)                      --                   --                   --                   True\n",
       "│    │    └─TransformerEncoderBlock (0)                 [64, 256, 384]       [64, 256, 384]       1,772,928            True\n",
       "│    │    └─TransformerEncoderBlock (1)                 [64, 256, 384]       [64, 256, 384]       1,772,928            True\n",
       "│    │    └─TransformerEncoderBlock (2)                 [64, 256, 384]       [64, 256, 384]       1,772,928            True\n",
       "│    │    └─TransformerEncoderBlock (3)                 [64, 256, 384]       [64, 256, 384]       1,772,928            True\n",
       "│    │    └─TransformerEncoderBlock (4)                 [64, 256, 384]       [64, 256, 384]       1,772,928            True\n",
       "│    │    └─TransformerEncoderBlock (5)                 [64, 256, 384]       [64, 256, 384]       1,772,928            True\n",
       "├─DecoderModel (decoder)                                [64, 256, 384]       [64, 256, 384]       4,260,096            True\n",
       "│    └─TgtTextEmbeddings (tgt_text_embds)               [64, 256]            [64, 256, 384]       --                   True\n",
       "│    │    └─Embedding (embeddings_table)                [64, 256]            [64, 256, 384]       4,161,792            True\n",
       "│    └─ModuleList (decoder_layers)                      --                   --                   --                   True\n",
       "│    │    └─TransformerDecoderBlock (0)                 [64, 256, 384]       [64, 256, 384]       3,100,800            True\n",
       "│    │    └─TransformerDecoderBlock (1)                 [64, 256, 384]       [64, 256, 384]       3,100,800            True\n",
       "│    │    └─TransformerDecoderBlock (2)                 [64, 256, 384]       [64, 256, 384]       3,100,800            True\n",
       "│    │    └─TransformerDecoderBlock (3)                 [64, 256, 384]       [64, 256, 384]       3,100,800            True\n",
       "│    │    └─TransformerDecoderBlock (4)                 [64, 256, 384]       [64, 256, 384]       3,100,800            True\n",
       "│    │    └─TransformerDecoderBlock (5)                 [64, 256, 384]       [64, 256, 384]       3,100,800            True\n",
       "├─Linear (linear_layer)                                 [64, 256, 384]       [64, 256, 10838]     4,161,792            True\n",
       "=======================================================================================================================================\n",
       "Total params: 49,302,912\n",
       "Trainable params: 49,302,912\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 2.71\n",
       "=======================================================================================================================================\n",
       "Input size (MB): 0.52\n",
       "Forward/backward pass size (MB): 8768.98\n",
       "Params size (MB): 169.16\n",
       "Estimated Total Size (MB): 8938.66\n",
       "======================================================================================================================================="
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Printing a summary of the architecture\n",
    "!pip install torchinfo\n",
    "from torchinfo import summary\n",
    "# idx, targets = get_batch('test')\n",
    "# idx = idx.to(device)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (batch_size, block_size)).to(device)  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (batch_size, block_size)).to(device)  # (batch_size, seq_length)\n",
    "src_mask = torch.randint(1, src_vocab_size, (batch_size, block_size)).to(device)  # \n",
    "tgt_mask = torch.randint(1, tgt_vocab_size, (batch_size, block_size)).to(device)  #\n",
    "# print(src_data.shape)\n",
    "summary(model=model,\n",
    "        input_data=(src_data, tgt_data, src_mask, tgt_mask),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:42.451139Z",
     "iopub.status.busy": "2024-12-30T23:41:42.450897Z",
     "iopub.status.idle": "2024-12-30T23:41:43.142042Z",
     "shell.execute_reply": "2024-12-30T23:41:43.141341Z",
     "shell.execute_reply.started": "2024-12-30T23:41:42.451116Z"
    },
    "id": "LH95cJEvcuoO",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Optimizer setup and scheduler steup\n",
    "out = {\"Train\": None, \"val\": None}\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=max_lr, weight_decay=weight_decay_optim)\n",
    "# initial_iters = 2000\n",
    "# total_steps = 10000\n",
    "eval_iters = 5\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "count = 0\n",
    "\n",
    "@torch.inference_mode()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters * len(val_dataloader))\n",
    "    count = 0\n",
    "    for k in range(eval_iters):\n",
    "        for src_idx, tgt_idx, src_pad, tgt_pad in val_dataloader:\n",
    "            # idx, targets = get_batch(split=split)\n",
    "            src_idx, tgt_idx, src_pad, tgt_pad = src_idx.to(device), tgt_idx.to(device), src_pad.to(device), tgt_pad.to(device)\n",
    "          \n",
    "            logits = model(src_idx, tgt_idx, src_pad, tgt_pad)\n",
    "            batch_size, block_size, embeddings_dims = logits.shape\n",
    "            logits = logits.view(batch_size*block_size, embeddings_dims) # Total tokens(words) => batch_size * block_size\n",
    "            targets = tgt_idx.view(batch_size * block_size)\n",
    "            loss = nn.functional.cross_entropy(logits, targets)\n",
    "            losses[count] = loss.item()\n",
    "            count += 1\n",
    "\n",
    "    out['val'] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-30T23:41:43.143397Z",
     "iopub.status.busy": "2024-12-30T23:41:43.142879Z",
     "iopub.status.idle": "2024-12-31T00:20:53.976987Z",
     "shell.execute_reply": "2024-12-31T00:20:53.976269Z",
     "shell.execute_reply.started": "2024-12-30T23:41:43.143371Z"
    },
    "id": "nPrSPPu8cuoO",
    "outputId": "4c863fc2-765d-4fa1-8450-69e3358eedaa",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [12:34<25:08, 754.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: train loss 0.0248, val loss 0.0315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [25:51<12:59, 779.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: train loss 0.0147, val loss 0.0153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [39:10<00:00, 783.61s/it]\n"
     ]
    }
   ],
   "source": [
    "#Train the  model\n",
    "from tqdm import tqdm\n",
    "loss_ = torch.zeros(epochs * len(train_dataloader))\n",
    "model.train()\n",
    "count1 = 0\n",
    "# batch_counter = 0\n",
    "for epoch  in tqdm(range(epochs)):\n",
    "    eval = True\n",
    "    loss_ = torch.zeros(epochs * len(train_dataloader))\n",
    "    count1 = 0\n",
    "    for src_idx, tgt_idx, src_pad, tgt_pad in train_dataloader:\n",
    "\n",
    "      # Evaluate and print loss every epoch for a total of 5 val epochs (kinda like cross-val)\n",
    "      # is_eval_iter = (batch_counter % eval_iters == 0 and batch_counter > 0)\n",
    "      # is_last_batch = (epoch == epochs - 1 and batch_counter == epochs * len(train_dataloader))\n",
    "\n",
    "      # print(batch_counter)\n",
    "      if eval and epoch != 0:\n",
    "          # print(batch_counter)\n",
    "          # print(is_eval_iter)\n",
    "          # print(is_last_batch)\n",
    "          eval = False\n",
    "          losses = estimate_loss()\n",
    "          print(f\"epoch {epoch}: train loss {loss.item():.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "      else:\n",
    "        src_idx, tgt_idx, src_pad, tgt_pad = src_idx.to(device), tgt_idx.to(device), src_pad.to(device), tgt_pad.to(device)\n",
    "          \n",
    "        # idx, targets = get_batch(split='train')\n",
    "        logits = model(src_idx, tgt_idx, src_pad, tgt_pad)\n",
    "        batch_size, block_size, embeddings_dims = logits.shape\n",
    "        logits = logits.view(batch_size*block_size, embeddings_dims)\n",
    "        targets = tgt_idx.view(batch_size * block_size)\n",
    "        loss = nn.functional.cross_entropy(logits, targets)\n",
    "        # loss_[count1] = loss.item()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # count1 += 1\n",
    "        # batch_counter -= 1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
