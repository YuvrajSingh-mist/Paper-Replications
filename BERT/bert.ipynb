{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References\n",
    "# https://medium.com/data-and-beyond/complete-guide-to-building-bert-model-from-sratch-3e6562228891\n",
    "# https://ai.plainenglish.io/bert-pytorch-implementation-prepare-dataset-part-1-efd259113e5a\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "import transformers, datasets\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import itertools\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "n_warmup_steps = 1000 #4000\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.98\n",
    "epsilon = 1e-9\n",
    "n_segments = 3\n",
    "block_size = 64\n",
    "batch_size = 64\n",
    "embeddings_dims = 128\n",
    "attn_dropout = 0.1\n",
    "no_of_heads = 2 #IMP needs to be thoroughly calculated\n",
    "dropout = 0.1\n",
    "epochs = 20\n",
    "max_lr = 2.5e-5\n",
    "no_of_encoder_layers = 2 #IMP needs to be thoroughly calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "\n",
    "!wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
    "!unzip -qq cornell_movie_dialogs_corpus.zip\n",
    "!rm cornell_movie_dialogs_corpus.zip\n",
    "!mkdir datasets\n",
    "!mv cornell\\ movie-dialogs\\ corpus/movie_conversations.txt ./datasets\n",
    "!mv cornell\\ movie-dialogs\\ corpus/movie_lines.txt ./datasets\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### loading all data into memory\n",
    "corpus_movie_conv = './datasets/movie_conversations.txt'\n",
    "corpus_movie_lines = './datasets/movie_lines.txt'\n",
    "with open(corpus_movie_conv, 'r', encoding='iso-8859-1') as c:\n",
    "    conv = c.readlines()\n",
    "with open(corpus_movie_lines, 'r', encoding='iso-8859-1') as l:\n",
    "    lines = l.readlines()\n",
    "\n",
    "### splitting text using special lines\n",
    "lines_dic = {}\n",
    "for line in lines:\n",
    "    objects = line.split(\" +++$+++ \")\n",
    "    lines_dic[objects[0]] = objects[-1]\n",
    "\n",
    "### generate convo  pairs\n",
    "pairs = []\n",
    "for con in conv:\n",
    "    ids = eval(con.split(\" +++$+++ \")[-1]) #Evaluates the string as a list now\n",
    "    for i in range(len(ids)):\n",
    "        pair = []\n",
    "        \n",
    "        if i == len(ids) - 1:\n",
    "            break\n",
    "        # print(ids[i])\n",
    "        first = lines_dic[ids[i]].strip()  \n",
    "        second = lines_dic[ids[i+1]].strip() \n",
    "\n",
    "        pair.append(' '.join(first.split()[:block_size]))\n",
    "        pair.append(' '.join(second.split()[:block_size]))\n",
    "        pairs.append(pair)\n",
    "        # break\n",
    "    # break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pairs) #Total pairs-> 221K\n",
    "\n",
    "##########W Tokenization #################\n",
    "# WordPiece tokenizer\n",
    "\n",
    "### save data as txt file\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.encode('utf-8', 'ignore').decode('utf-8')\n",
    "\n",
    "for sample in tqdm([x[0] for x in pairs]):\n",
    "    # cleaned_sample = clean_text(sample)\n",
    "    text_data.append(sample)\n",
    "\n",
    "    # once we hit the 10K mark, save to file\n",
    "    # if len(text_data) == 10000:\n",
    "with open(f'./datasets/text.txt', 'w', encoding='utf-8') as fp:\n",
    "    fp.write('\\n'.join(text_data))\n",
    "        # text_data = []\n",
    "        # file_count += 1\n",
    "\n",
    "paths = 'datasets/text.txt'\n",
    "# print(paths)\n",
    "### training own tokenizer\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=False,\n",
    "    strip_accents=False,\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "tokenizer.train( \n",
    "    files=paths,\n",
    "    vocab_size=10000, \n",
    "    min_frequency=5,\n",
    "    # limit_alphabet=1000, \n",
    "    wordpieces_prefix='##',\n",
    "    special_tokens=['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']\n",
    "    )\n",
    "\n",
    "if not os.path.exists('./bert-it-1'): os.mkdir('./bert-it-1')\n",
    "tokenizer.save_model('./bert-it-1', 'bert-it')\n",
    "tokenizer = BertTokenizer.from_pretrained('./bert-it-1/bert-it-vocab.txt', local_files_only=True)\n",
    "\n",
    "#Setting vocab size\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, data_pair, tokenizer, seq_len=block_size):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.corpus_lines = len(data_pair)\n",
    "        self.lines = data_pair\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.corpus_lines\n",
    "\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        \n",
    "        #Getting NSP sentences\n",
    "        sent1, sent2, is_next = self.get_nsp(item)\n",
    "        \n",
    "        #Getting masked sentences\n",
    "        sent1_masked , label1 = self.get_masked_sentences(sent1)\n",
    "        sent2_masked , label2 = self.get_masked_sentences(sent2)\n",
    "        \n",
    "        #Adding CLS and SEP tokens\n",
    "        sent1_masked_cls_and_sep_aded = [self.tokenizer.vocab['[CLS]']]+ sent1_masked + [self.tokenizer.vocab['[SEP]']]\n",
    "        sent2_masked_cls_and_sep_aded = sent2_masked + [self.tokenizer.vocab['[SEP]']]\n",
    "        \n",
    "        label1_padding_added = [self.tokenizer.vocab['[PAD]']] + label1 + [self.tokenizer.vocab['[PAD]']] #because of [1:-1] thing (I removed CLS and SEP token before) and the middle one because of the added [SEP] token\n",
    "        label2_padding_added = label2 + [self.tokenizer.vocab['[PAD]']]\n",
    "        \n",
    "        #Segment ids\n",
    "        segment_ids = [1 for _ in range(len(sent1_masked_cls_and_sep_aded))] + [2 for _ in range(len(sent2_masked_cls_and_sep_aded))]\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        #Combine the sentences\n",
    "        combined_sentence = sent1_masked_cls_and_sep_aded + sent2_masked_cls_and_sep_aded\n",
    "        combined_labels = label1_padding_added + label2_padding_added\n",
    "        \n",
    "        #Padding and truncation\n",
    "        if(len(combined_sentence) > self.seq_len): \n",
    "            combined_sentence = combined_sentence[:self.seq_len]\n",
    "            combined_labels = combined_labels[:self.seq_len]\n",
    "            segment_ids = segment_ids[:self.seq_len]\n",
    "        elif (len(combined_sentence) < self.seq_len):\n",
    "            while(len(combined_sentence) < self.seq_len):\n",
    "                combined_sentence = [self.tokenizer.vocab['[PAD]']] + combined_sentence\n",
    "                segment_ids = [0] + segment_ids\n",
    "                combined_labels = [0] + combined_labels\n",
    "                \n",
    "        values = {\n",
    "            'bert_input_masked': combined_sentence,\n",
    "            'bert_input_labels': combined_labels,\n",
    "            'segment_ids': segment_ids,\n",
    "            'is_next': is_next\n",
    "        }\n",
    "\n",
    "\n",
    "        assert len(combined_labels) == len(combined_sentence)\n",
    "        return {key: torch.tensor(value) for key, value in values.items()} #Must be converted into tensor \n",
    "    \n",
    "    def get_nsp(self,index):\n",
    "            t1, t2 = self.lines[index][0], self.lines[index][1]\n",
    "            \n",
    "            prob = random.random()\n",
    "            if(prob < 0.5):\n",
    "                return t1, t2, 1\n",
    "            else:\n",
    "                return t1, self.lines[random.randrange(len(pairs))][1], 0\n",
    "        \n",
    "            \n",
    "    def get_masked_sentences(self, sentence):\n",
    "        tokens = self.tokenizer(sentence)['input_ids'][1:-1]\n",
    "        mask_label = []\n",
    "        output = []\n",
    "\n",
    "        for token in tokens:\n",
    "            prob = random.random()\n",
    "\n",
    "            if prob < 0.15:\n",
    "                prob /= 0.15\n",
    "\n",
    "                if prob < 0.8:\n",
    "                    output.append(self.tokenizer.vocab['[MASK]'])\n",
    "                elif prob < 0.9:\n",
    "                    output.append(random.randrange(len(self.tokenizer.vocab)))\n",
    "                else:\n",
    "                    output.append(token)\n",
    "                mask_label.append(token)\n",
    "            else:\n",
    "                output.append(token)\n",
    "                mask_label.append(0)\n",
    "\n",
    "        assert len(output) == len(mask_label)\n",
    "        return output, mask_label\n",
    "   \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an instance of the dataset class\n",
    "dataset = BERTDataset(data_pair=pairs, tokenizer=tokenizer, seq_len=block_size)\n",
    "\n",
    "# Assuming 'dataset' is already created\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "import os\n",
    "#Creating a dataloader\n",
    "# Create DataLoaders for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=False, num_workers=os.cpu_count())\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, pin_memory=False, num_workers=os.cpu_count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "#Loading a sample from the batch\n",
    "sample_data = next(iter(train_loader))\n",
    "# print('Batch Size', sample_data['bert_input_masked'].size())\n",
    "print(sample_data)\n",
    "\n",
    "# 3 is MASK\n",
    "# result = dataset[random.randrange(len(dataset))]\n",
    "# print(result)\n",
    "# print(tokenizer.convert_ids_to_tokens(result['bert_input_masked']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text embeddings\n",
    "class TextEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size = vocab_size,\n",
    "        embeddings_dims = embeddings_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embeddings_table = nn.Embedding(num_embeddings = vocab_size, embedding_dim=embeddings_dims, device=device, padding_idx=0) #Just a look up table to convert the toekns_ids to some numbers\n",
    "        # nn.init.normal_(self.embeddings_table.weight.data, mean=0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embeddings_table(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment embeddings\n",
    "class SegmentEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_segments = n_segments,\n",
    "        embeddings_dims = embeddings_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.seg_embds = nn.Embedding(num_embeddings = n_segments, embedding_dim=embeddings_dims, device=device, padding_idx=0)\n",
    "    def forward(self, x):\n",
    "        return self.seg_embds(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer Normalization\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims = embeddings_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embeddings_dims, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FeedForward Neural Network\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dropout = dropout,\n",
    "        embeddings_size = embeddings_dims,\n",
    "        # inner_dimensional_states: int = 3072\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(device=device, in_features=embeddings_size, out_features= 4 * embeddings_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(device=device, in_features= 4 * embeddings_size, out_features=embeddings_size),\n",
    "            nn.Dropout(p = dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # mlp_weights_init = self.mlp.apply(weights_init)\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single Attention Head\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = attn_dropout,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.head_size = embeddings_dims // no_of_heads\n",
    "        self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device, bias=False)\n",
    "        self.keys = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,device=device, bias=False)\n",
    "        self.values = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device,bias=False)\n",
    "        self.dropout = nn.Dropout(p = attn_dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # batch, block_size, embd_dims = x.shape\n",
    "        k = self.keys(x)\n",
    "        q = self.query(x)\n",
    "        v = self.values(x)\n",
    "        # masked_table = torch.tril(torch.ones(block_size, block_size, device=device))\n",
    "        weights = q @ torch.transpose(k, dim0=-2, dim1=-1) * (k.shape[-1] ** -0.5)\n",
    "        if(mask != None):\n",
    "            masked_values = weights.masked_fill(mask == 0, float('-inf'))\n",
    "            weights_normalized = nn.functional.softmax(masked_values, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
    "            # weights_normalized = self.dropout(weights_normalized)\n",
    "            out = weights_normalized @ v\n",
    "            out = self.dropout(out)\n",
    "            return out\n",
    "        else:\n",
    "            weights_normalized = nn.functional.softmax(weights, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
    "            # weights_normalized = self.dropout(weights_normalized)\n",
    "            out = weights_normalized @ v\n",
    "            out = self.dropout(out)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHA\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mask = None,\n",
    "        attn_dropout = attn_dropout,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads) for _ in range(no_of_heads)])\n",
    "        self.dropout = nn.Dropout(p = attn_dropout)\n",
    "        self.linear = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=device, bias=False) # 12 (no of heads) * (batch_size) 64 = 768 -> gives out the text embeddings\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        concat = torch.cat([head(x, mask) for head in self.heads], dim=-1)\n",
    "        linear_layer = self.linear(concat)\n",
    "        out = self.dropout(linear_layer)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "class PositionEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        block_size = block_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pos_embd = torch.ones((block_size, embeddings_dims), device=device, requires_grad=False)\n",
    "        for pos in range(block_size):\n",
    "            for i in range(embeddings_dims, 2):\n",
    "                self.pos_embd[pos, i] = math.sin(pos/(10000**((2*i)/embeddings_dims)))\n",
    "                self.pos_embd[pos, i + 1] = math.cos(pos/(10000**((2*(i + 1))/embeddings_dims)))\n",
    "                \n",
    "    def forward(self,x):\n",
    "        pos_embd = self.pos_embd\n",
    "        pos_embd = pos_embd.unsqueeze(0)\n",
    "        return pos_embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Block\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = attn_dropout,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "        dropout = dropout,\n",
    "        mask=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = MHA(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads)\n",
    "        self.layer_norm1 = LayerNormalization(embeddings_dims=embeddings_dims)\n",
    "        self.layer_norm2 = LayerNormalization(embeddings_dims=embeddings_dims)\n",
    "        self.mlp_block = MLPBlock(dropout=dropout, embeddings_size=embeddings_dims)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.layer_norm1(x + self.mha(x, mask))\n",
    "        x = self.layer_norm2(x + self.mlp_block(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Block\n",
    "\n",
    "class EncoderModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = attn_dropout,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "        block_size = block_size,\n",
    "        dropout = dropout,\n",
    "        no_of_encoder_layers = no_of_encoder_layers,\n",
    "        vocab_size = vocab_size,\n",
    "        n_segments = n_segments,\n",
    "        mask=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder_layer_stacked = []\n",
    "        self.positional_embeddings = PositionEmbeddings(block_size=block_size, embeddings_dims=embeddings_dims)\n",
    "        self.text_embds = TextEmbeddings(vocab_size=vocab_size, embeddings_dims=embeddings_dims)\n",
    "        # self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=vocab_size, device=device, bias=False) # Takes in logits of dimensions- embeds_dims and converts it into dimension of vocab_size (logits in range of vocab_size)\n",
    "        self.no_of_encoder_layers = no_of_encoder_layers\n",
    "        self.layer_norm = LayerNormalization(embeddings_dims=embeddings_dims)\n",
    "        # self.encoder_layers = nn.Sequential(*[TransformerEncoderBlock(embeddings_dims=embeddings_dims, attn_dropout=attn_dropout, no_of_heads=no_of_heads, dropout=dropout) for _ in range(no_of_encoder_layers)])\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        self.seg_embds = SegmentEmbeddings(n_segments=n_segments, embeddings_dims=embeddings_dims)\n",
    "        self.encoder_layer = TransformerEncoderBlock(embeddings_dims=embeddings_dims, attn_dropout=attn_dropout, no_of_heads=no_of_heads, dropout=dropout)\n",
    "        for _ in range(no_of_encoder_layers):\n",
    "            self.encoder_layer_stacked.append(self.encoder_layer)\n",
    "\n",
    "    def forward(self, x, segment_ids):\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1) #Create a boolean matrix of size (block_size * block_size) to mask all the padded tokens\n",
    "        x = (self.text_embds(x) + self.seg_embds(segment_ids) + self.positional_embeddings(x))*math.sqrt(embeddings_dims)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.encoder_layer_stacked:\n",
    "            x = layer(x, mask=mask)  \n",
    "        out = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NSP\n",
    "\n",
    "class NSP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # self.encoder_block = EncoderModel(no_of_encoder_layers=no_of_encoder_layers, attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads, block_size=block_size, dropout=dropout, vocab_size=vocab_size, n_segments=n_segments)\n",
    "        self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=2, device=device)\n",
    "    \n",
    "\n",
    "    def forward(self, x,  isnext):\n",
    "        logits = self.linear_layer(x[:,0]) #to get the CLS token embeddings across all batches\n",
    "        loss = nn.functional.cross_entropy(logits, isnext, ignore_index = 0)\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLM\n",
    "\n",
    "class MLM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        vocab_size = vocab_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # self.encoder_block = EncoderModel(no_of_encoder_layers=no_of_encoder_layers, attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads, block_size=block_size, dropout=dropout, vocab_size=vocab_size, n_segments=n_segments)\n",
    "        self.linear_layer1 = nn.Linear(in_features=embeddings_dims, out_features=vocab_size, device=device)\n",
    "        # self.linear_layer2 = nn.Linear(in_features=vocab_size, out_features=block_size, device=device)\n",
    "        \n",
    "    \n",
    "\n",
    "    def forward(self, x,  mask_labels):\n",
    "        # Get the logits from the linear layer\n",
    "        logits = self.linear_layer1(x)  # logits: (batch_size, seq_len, vocab_size)\n",
    "        # print(logits.shape)\n",
    "        # Reshape logits and mask_labels for cross_entropy\n",
    "        batch_size, seq_len, vocab_size = logits.shape\n",
    "        logits = logits.view(-1, vocab_size)        # logits: (batch_size * seq_len, vocab_size)\n",
    "        mask_labels = mask_labels.view(-1)          # mask_labels: (batch_size * seq_len)\n",
    "        \n",
    "        # Calculate the cross-entropy loss\n",
    "        loss = nn.functional.cross_entropy(logits, mask_labels, ignore_index = 0)\n",
    "        \n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = attn_dropout,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "        block_size = block_size,\n",
    "        dropout = dropout,\n",
    "        vocab_size = vocab_size,\n",
    "        n_segments = n_segments\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlm = MLM(embeddings_dims=embeddings_dims, vocab_size=vocab_size)\n",
    "        self.nsp = NSP(embeddings_dims=embeddings_dims)\n",
    "        self.encoder_layer = EncoderModel(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads, no_of_encoder_layers=no_of_encoder_layers, block_size=block_size,dropout=dropout,n_segments=n_segments)\n",
    "\n",
    "    def forward(self, x, segment_ids, labels, isnext):\n",
    "        x = self.encoder_layer(x, segment_ids)\n",
    "        mlm_loss, mlm_logits = self.mlm(x, labels)\n",
    "        nsp_loss, nsp_logits = self.nsp(x, isnext)\n",
    "        return mlm_loss, mlm_logits, nsp_loss , nsp_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT(embeddings_dims=embeddings_dims, vocab_size=vocab_size)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing a summary of the architecture\n",
    "from torchinfo import summary\n",
    "sample_data = {key: value.to(device) for key, value in sample_data.items()}\n",
    "summary(model=model,\n",
    "        input_data=(sample_data['bert_input_masked'], sample_data['segment_ids'], sample_data['bert_input_labels'], sample_data['is_next']),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])\n",
    "# model(result['bert_input_masked'],result['segment_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledOptim():\n",
    "    '''A simple wrapper class for learning rate scheduling'''\n",
    "\n",
    "    def __init__(self, optimizer, embeddings_dims, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(embeddings_dims, -0.5)\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients by the inner optimizer\"\n",
    "        self._optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_current_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up optimizer and lr scheduler\n",
    "\n",
    "#For use cases without a lr scheduler\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr = max_lr)\n",
    "\n",
    "#For use cases with a lr scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = max_lr, eps=epsilon, betas=(beta_1, beta_2))\n",
    "lr_scheduler = ScheduledOptim(optimizer=optimizer,embeddings_dims=embeddings_dims,n_warmup_steps=n_warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#For not showing the warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.inference_mode()\n",
    "def cal_val(val_loader):\n",
    "    mlm_accuracy = []\n",
    "    nsp_accuracy = []\n",
    "    tot_mlm_loss = []\n",
    "    tot_nsp_loss = []\n",
    "    loss = []\n",
    "    model.eval()\n",
    "    for epoch in range(1):\n",
    "        for data in val_loader:\n",
    "            result = {key: value.to(device) for key,value in data.items()}\n",
    "            mlm_loss, mlm_logits, loss_nsp, nsp_logits = model(result['bert_input_masked'], result['segment_ids'], result['bert_input_labels'], result['is_next'])\n",
    "            loss_tot = mlm_loss + loss_nsp\n",
    "            loss.append(loss_tot.item())\n",
    "            tot_mlm_loss.append(mlm_loss.item())\n",
    "            tot_nsp_loss.append(loss_nsp.item())\n",
    "    mean_loss = sum(loss) / len(loss)\n",
    "\n",
    "    mean_loss_mlm = sum(tot_mlm_loss) / len(tot_mlm_loss)\n",
    "    mean_loss_nsp = sum(tot_nsp_loss) / len(tot_nsp_loss)\n",
    "    model.train()\n",
    "    return mean_loss_mlm,  mean_loss_nsp, mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Loop\n",
    "# from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "model.train()\n",
    "loss = []\n",
    "items = []\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    loss = []\n",
    "    items = []\n",
    "    correct = 0\n",
    "    total_instances = 0\n",
    "    nsp_accuracy = []\n",
    "    mlm_accuracy = []\n",
    "    tot_mlm_loss = []\n",
    "    tot_nsp_loss = []\n",
    "    for i, data in enumerate(train_loader):\n",
    "        result = {key: value.to(device) for key,value in data.items()}\n",
    "        mlm_loss, mlm_logits, loss_nsp, nsp_logits = model(result['bert_input_masked'], result['segment_ids'], result['bert_input_labels'], result['is_next'])\n",
    "        loss_tot = mlm_loss + loss_nsp\n",
    "        \n",
    "        #Without lr scheduler\n",
    "        # optimizer.zero_grad(set_to_none=True)\n",
    "        # loss_tot.backward()\n",
    "        # optimizer.step()\n",
    "        \n",
    "        #With lr scheduler\n",
    "        lr_scheduler.zero_grad()\n",
    "        loss_tot.backward()\n",
    "        lr_scheduler.step_and_update_lr()\n",
    "        \n",
    "        loss.append(loss_tot.item())\n",
    "\n",
    "        tot_mlm_loss.append(mlm_loss.item())\n",
    "        tot_nsp_loss.append(loss_nsp.item())\n",
    "        if(i % 100 == 0 or i == len(train_loader) - 1):\n",
    "            mean_loss = sum(loss) / len(loss)\n",
    "            # mean_accuracy_mlm = sum(mlm_accuracy) / len(mlm_accuracy)\n",
    "            # mean_accuracy_nsp = sum(nsp_accuracy) / len(nsp_accuracy)\n",
    "            mean_mlm_loss = sum(tot_mlm_loss) / len(tot_mlm_loss)\n",
    "            mean_loss_nsp = sum(tot_nsp_loss) / len(tot_nsp_loss)\n",
    "            \n",
    "            #Validation losses\n",
    "            val_mean_loss_mlm,val_mean_loss_nsp, val_loss = cal_val(val_loader)\n",
    "            print(\"Steps: \",i, \"Epoch: \", epoch , \"Train loss: \", mean_loss, \"Train NSP Loss: \", mean_loss_nsp, \"Train MLM Loss: \", mean_mlm_loss, \"Val loss: \", val_loss, \"Val NSP Loss: \", val_mean_loss_nsp, \"Val MLM Loss: \", val_mean_loss_mlm)\n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
