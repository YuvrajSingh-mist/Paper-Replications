{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_warmup as warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-17 01:04:02--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
      "\n",
      "2024-06-17 01:04:02 (20.5 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "\n",
    "#Train BPE\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "\n",
    "block_size = 32\n",
    "batch_size = 16\n",
    "embeddings_dims = 64\n",
    "attn_dropout = 0.1,\n",
    "no_of_heads = 4,\n",
    "dropout = 0.1\n",
    "epochs = 30\n",
    "max_lr = 2.5e-4\n",
    "no_of_decoder_layers = 4\n",
    "attn_dropout = 0.1\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    idx = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    targets = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    idx, targets = idx.to(device), targets.to(device)\n",
    "    return idx, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx, targets = get_batch('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 32])"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21,  1, 51, 59, 57, 58,  1, 40, 43,  1, 45, 53, 52, 43,  8,  0,  0, 24,\n",
       "        33, 15, 17, 26, 32, 21, 27, 10,  0, 18, 39, 47, 58, 46],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 51, 59, 57, 58,  1, 40, 43,  1, 45, 53, 52, 43,  8,  0,  0, 24, 33,\n",
       "        15, 17, 26, 32, 21, 27, 10,  0, 18, 39, 47, 58, 46,  6],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15685.21875"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data) / 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text embeddings\n",
    "class TextEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = vocab_size,\n",
    "        embeddings_dims = embeddings_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embeddings_table = nn.Embedding(num_embeddings = vocab_size, embedding_dim=embeddings_dims, device=device) #Just a look up table to convert the toekns_ids to some numbers\n",
    "        # nn.init.normal_(self.embeddings_table.weight.data, mean=0, std=0.02)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embeddings_table(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Position embeddings\n",
    "class PositionEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        block_size = block_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.position_embeddings = nn.Embedding(num_embeddings = block_size, embedding_dim=embeddings_dims, device=device) #To give positional embeddings to each token of the input text, hence num_embeddings=block_size\n",
    "        # nn.init.normal_(self.position_embeddings.weight.data, mean=0, std=0.02)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.position_embeddings(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer Normalization\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims = embeddings_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embeddings_dims,device=device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FeedForward Neural Network\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dropout = 0.1,\n",
    "        embeddings_size = embeddings_dims,\n",
    "        # inner_dimensional_states: int = 3072\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(device=device, in_features=embeddings_size, out_features= 4 * embeddings_dims),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(device=device, in_features= 4 * embeddings_dims, out_features=embeddings_size), \n",
    "            nn.Dropout(p = dropout)     \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # mlp_weights_init = self.mlp.apply(weights_init)\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Weights Initilization (for MLP Block)\n",
    "# def weights_init(m):\n",
    "#     classname = m.__class__.__name__\n",
    "#     if classname.find('Linear') != -1:\n",
    "#         nn.init.normal_(m.weight.data, 0.0, 0.02)  #mean = 0, std = 0.02\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dims // no_of_heads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_of_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = 0.1,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.head_size = embeddings_dims // no_of_heads[0]\n",
    "        self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device, bias=False)\n",
    "        self.keys = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device, bias=False)\n",
    "        self.values = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device, bias=False)\n",
    "        self.dropout = nn.Dropout(p = attn_dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch, block_size, head_size = x.shape\n",
    "        masked_table = torch.tril(torch.ones(block_size, block_size, device=device))\n",
    "        weights = self.query(x) @ torch.transpose(self.keys(x), dim0=-2, dim1=-1)\n",
    "        masked_values = weights.masked_fill(masked_table[: block_size, : block_size] == 0, float('-inf'))\n",
    "        scaled_weights = masked_values * head_size ** -0.5\n",
    "        weights_normalized = nn.functional.softmax(scaled_weights, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
    "        weights_normalized = self.dropout(weights_normalized)\n",
    "        value_vector = weights_normalized @ self.values(x)\n",
    "        return value_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHA\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = 0.1,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads) for _ in range(no_of_heads[0])])\n",
    "        self.dropout = nn.Dropout(p = attn_dropout)\n",
    "        self.linear = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=device, bias=False) # 12 (no of heads) * (batch_size) 64 = 768 -> gives out the text embeddings \n",
    "        \n",
    "    def forward(self, x):\n",
    "        concat = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        linear_layer = self.linear(concat)\n",
    "        out = self.dropout(linear_layer)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Block\n",
    "\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = 0.1,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "        dropout = 0.1,\n",
    "        vocab_size = vocab_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mha = MHA(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads)\n",
    "        self.layer_norm1 = LayerNormalization(embeddings_dims=embeddings_dims)\n",
    "        self.layer_norm2 = LayerNormalization(embeddings_dims=embeddings_dims)\n",
    "        self.mlp_block = MLPBlock(dropout=dropout, embeddings_size=embeddings_dims)\n",
    "        self.text_embds = TextEmbeddings(vocab_size=vocab_size, embeddings_dims=embeddings_dims)\n",
    "        self.pos = PositionEmbeddings(embeddings_dims=embeddings_dims)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):  #Weight Initialization\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.mha(x)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = self.mlp_block(x) + x\n",
    "        out = self.layer_norm2(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Block\n",
    "\n",
    "class DecoderModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = 0.1,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "        block_size = block_size,\n",
    "        dropout = 0.1,\n",
    "        no_of_decoder_layers = no_of_decoder_layers,\n",
    "        vocab_size = vocab_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.positional_embeddings = nn.Parameter(torch.randn(1, block_size, embeddings_dims, device=device), requires_grad=True)\n",
    "        self.text_embds = TextEmbeddings(vocab_size=vocab_size, embeddings_dims=embeddings_dims)\n",
    "        # self.decoder = TransformerDecoderBlock(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads, block_size=block_size, dropout=dropout)\n",
    "        # self.pos = PositionEmbeddings(block_size=block_size, embeddings_dims=embeddings_dims)\n",
    "        self.decoder_layers = nn.Sequential(*[TransformerDecoderBlock(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads, dropout=dropout, vocab_size=vocab_size) for _ in range(no_of_decoder_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.text_embds(x)\n",
    "        x = x + self.positional_embeddings\n",
    "        # pos_emb = self.pos(torch.arange(block_size, device=device)) # (T,C)\n",
    "        # x = x + pos_emb # (B,T,C)\n",
    "        out = self.decoder_layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating the model\n",
    "model = DecoderModel(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads, block_size=block_size, dropout=dropout, no_of_decoder_layers=no_of_decoder_layers, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==================================================================================================================================\n",
       "Layer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n",
       "==================================================================================================================================\n",
       "DecoderModel (DecoderModel)                        [16, 32]             [16, 32, 64]         2,048                True\n",
       "├─TextEmbeddings (text_embds)                      [16, 32]             [16, 32, 64]         --                   True\n",
       "│    └─Embedding (embeddings_table)                [16, 32]             [16, 32, 64]         4,160                True\n",
       "├─Sequential (decoder_layers)                      [16, 32, 64]         [16, 32, 64]         --                   True\n",
       "│    └─TransformerDecoderBlock (0)                 [16, 32, 64]         [16, 32, 64]         6,208                True\n",
       "│    │    └─MHA (mha)                              [16, 32, 64]         [16, 32, 64]         16,384               True\n",
       "│    │    └─LayerNormalization (layer_norm1)       [16, 32, 64]         [16, 32, 64]         128                  True\n",
       "│    │    └─MLPBlock (mlp_block)                   [16, 32, 64]         [16, 32, 64]         33,088               True\n",
       "│    │    └─LayerNormalization (layer_norm2)       [16, 32, 64]         [16, 32, 64]         128                  True\n",
       "│    └─TransformerDecoderBlock (1)                 [16, 32, 64]         [16, 32, 64]         6,208                True\n",
       "│    │    └─MHA (mha)                              [16, 32, 64]         [16, 32, 64]         16,384               True\n",
       "│    │    └─LayerNormalization (layer_norm1)       [16, 32, 64]         [16, 32, 64]         128                  True\n",
       "│    │    └─MLPBlock (mlp_block)                   [16, 32, 64]         [16, 32, 64]         33,088               True\n",
       "│    │    └─LayerNormalization (layer_norm2)       [16, 32, 64]         [16, 32, 64]         128                  True\n",
       "│    └─TransformerDecoderBlock (2)                 [16, 32, 64]         [16, 32, 64]         6,208                True\n",
       "│    │    └─MHA (mha)                              [16, 32, 64]         [16, 32, 64]         16,384               True\n",
       "│    │    └─LayerNormalization (layer_norm1)       [16, 32, 64]         [16, 32, 64]         128                  True\n",
       "│    │    └─MLPBlock (mlp_block)                   [16, 32, 64]         [16, 32, 64]         33,088               True\n",
       "│    │    └─LayerNormalization (layer_norm2)       [16, 32, 64]         [16, 32, 64]         128                  True\n",
       "│    └─TransformerDecoderBlock (3)                 [16, 32, 64]         [16, 32, 64]         6,208                True\n",
       "│    │    └─MHA (mha)                              [16, 32, 64]         [16, 32, 64]         16,384               True\n",
       "│    │    └─LayerNormalization (layer_norm1)       [16, 32, 64]         [16, 32, 64]         128                  True\n",
       "│    │    └─MLPBlock (mlp_block)                   [16, 32, 64]         [16, 32, 64]         33,088               True\n",
       "│    │    └─LayerNormalization (layer_norm2)       [16, 32, 64]         [16, 32, 64]         128                  True\n",
       "==================================================================================================================================\n",
       "Total params: 229,952\n",
       "Trainable params: 229,952\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 3.25\n",
       "==================================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 11.80\n",
       "Params size (MB): 0.81\n",
       "Estimated Total Size (MB): 12.61\n",
       "=================================================================================================================================="
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printing a summary of the architecture\n",
    "from torchinfo import summary\n",
    "idx, targets = get_batch('test')\n",
    "idx = idx.to(device)\n",
    "summary(model=model,\n",
    "        input_data=idx,\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32, 64])\n"
     ]
    }
   ],
   "source": [
    "# Test the MHA module with a sample input tensor\n",
    "\n",
    "# temp_data = torch.randn(batch_size, block_size, embeddings_dims).to(device)\n",
    "# model = DecoderModel()\n",
    "# model = MHA()\n",
    "idx = idx.to(device)\n",
    "output = model(idx)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6024,  0.2741,  0.7171,  ..., -0.5354,  1.1934,  0.1516],\n",
       "        [-0.0102,  1.2337,  1.4975,  ..., -0.8420, -0.0202, -0.4261],\n",
       "        [-0.2834,  0.3554,  0.6282,  ..., -0.2938,  1.1183,  0.0055],\n",
       "        ...,\n",
       "        [-0.1144,  0.1136,  0.4371,  ..., -0.3045,  0.9004,  0.0537],\n",
       "        [ 0.0174,  0.2447,  0.4192,  ..., -0.1637,  0.9061,  0.1690],\n",
       "        [ 0.0998,  0.3597, -0.0945,  ..., -0.2256,  0.9286,  0.0997]],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer setup and scheduler steup\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n",
    "loss_ce = nn.CrossEntropyLoss()\n",
    "# initial_iters = 2000\n",
    "# total_steps = 10000 * epochs\n",
    "# warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period=2000)\n",
    "# lr_scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max= total_steps - initial_iters)\n",
    "# lr_scheduler_linear = torch.optim.lr_scheduler.LinearLR(optimizer=optimizer, total_iters=initial_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating the model\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    idx, targets = get_batch(split='train')\n",
    "    logits = model(idx)\n",
    "    loss = loss_ce(logits, targets)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvrajsingh/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the  model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 3, 1, 2, 3],\n",
      "         [3, 4, 5, 3, 4, 5],\n",
      "         [6, 8, 9, 6, 8, 9]]])\n",
      "torch.Size([1, 3, 3])\n",
      "torch.Size([1, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor1 = torch.tensor([[[1, 2, 3], [3,4,5], [6,8,9]]])\n",
    "tensor2 = torch.tensor([[[1, 2, 3], [3,4,5], [6,8,9]]])\n",
    "\n",
    "concatenated = torch.cat([tensor1, tensor2], dim=-1)\n",
    "print(concatenated)\n",
    "# Output: tensor([[1, 2, 3],\n",
    "#                 [4, 5, 6]])\n",
    "print(tensor1.shape)\n",
    "print(concatenated.shape)\n",
    "# Output: torch.Size([2, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
