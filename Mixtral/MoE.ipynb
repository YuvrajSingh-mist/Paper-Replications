{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T16:23:35.281536Z",
     "iopub.status.busy": "2025-02-08T16:23:35.281323Z"
    },
    "id": "Pw7f2ghccuoK",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adLpt7j7cuoL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwR5_uvTcuoL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eDccPM5AcuoL",
    "outputId": "314a00d7-c34d-471f-ab7f-3bfa29fa405e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Collab setup\n",
    "\n",
    "data_path = Path('/kaggle/working/data')\n",
    "data_path.mkdir(exist_ok=True)\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "!cp input.txt data/input.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-CsTcTonJuiW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Datasets\n",
    "\n",
    "# Using tinyshakespeare\n",
    "\n",
    "with open('data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "####################################################################\n",
    "\n",
    "#Using BookCorpus\n",
    "# from datasets import load_dataset\n",
    "# data = load_dataset('bookcorpus/bookcorpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VBi6asbs4Vs",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Datasets\n",
    "\n",
    "# Using tinyshakespeare\n",
    "\n",
    "with open('data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "####################################################################\n",
    "\n",
    "#Using BookCorpus\n",
    "# from datasets import load_dataset\n",
    "# data = load_dataset('bookcorpus/bookcorpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_D1tbt7L2c-D",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Extracting the content of  the Dataset\n",
    "# # Open a file for writing\n",
    "# with open('data/input.txt', 'w', encoding='utf-8') as f:\n",
    "#     # Traverse the dataset and write text data to the file\n",
    "#     for record in data['train']['text']:\n",
    "#         f.write(record)\n",
    "\n",
    "# print(\"Writing to file complete.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IG5ZV9KEcuoL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "#Character level tokenization\n",
    "\n",
    "# # here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch: i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndPfBp-Gb0KN",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "\n",
    "block_size = 128\n",
    "batch_size = 16\n",
    "embeddings_dims = 384\n",
    "attn_dropout = 0.1\n",
    "no_of_heads = 6 #IMP needs to be thoroughly calculated\n",
    "dropout = 0.1\n",
    "epochs = 100\n",
    "max_lr = 3e-4\n",
    "no_of_decoder_layers = 6 #IMP needs to be thoroughly calculated\n",
    "attn_dropout = 0.1\n",
    "weight_decay_optim = 0.01\n",
    "experts=8\n",
    "top_experts=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "goaGJ8k1cuoM",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAhkF6nmcuoN",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Text embeddings\n",
    "class TextEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size = vocab_size,\n",
    "        embeddings_dims = embeddings_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embeddings_table = nn.Embedding(num_embeddings = vocab_size, embedding_dim=embeddings_dims, device=device) #Just a look up table to convert the toekns_ids to some numbers\n",
    "        # nn.init.normal_(self.embeddings_table.weight.data, mean=0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embeddings_table(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "REUDHWrWcuoN",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Layer Normalization\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims = embeddings_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embeddings_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7EZKhq_OJuiY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block_size: int = block_size,\n",
    "        embeddings_dims: int = embeddings_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        swish = x * self.sig(x)\n",
    "\n",
    "        return swish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRQWhMhZJuiY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SWiGLUExpertMoE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block_size: int = block_size,\n",
    "        embeddings_dims: int = embeddings_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.swish = Swish(block_size=block_size, embeddings_dims=embeddings_dims)\n",
    "        self.linear_layer1 = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=device, bias=False, dtype=torch.float32)\n",
    "        self.linear_layer2 = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=device, bias=False, dtype=torch.float32)\n",
    "        self.linear_layer3 = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=device, bias=False, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        swish_res = self.swish(self.linear_layer1(x))\n",
    "        x_V = self.linear_layer2(x)\n",
    "        res = torch.mul(swish_res, x_V)\n",
    "        out = self.linear_layer3(res)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5N1dQuyBJuiY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#MoE Layer\n",
    "\n",
    "class MoeLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dropout = dropout,\n",
    "        embeddings_size = embeddings_dims,\n",
    "        # inner_dimensional_states: int = 3072\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([SWiGLUExpertMoE() for _ in range(experts)])\n",
    "        self.gate = nn.Linear(in_features=embeddings_dims, out_features=experts)\n",
    "        # self.outputs = torch.zeros((batch_size,block_size, embeddings_size), device=device) #batch size needs to be defined because we are accessing it explicitly\n",
    "\n",
    "    def forward(self, x):\n",
    "        # mlp_weights_init = self.mlp.apply(weights_init)\n",
    "        self.gate_out = self.gate(x)\n",
    "        top_k_values, top_k_indices = torch.topk(self.gate_out, k=top_experts)\n",
    "        probs = torch.nn.functional.softmax(top_k_values)\n",
    "        # print(top_k_indices[11])\n",
    "        # print(top_k_values[20])\n",
    "        # print(probs[20])\n",
    "        outputs = torch.zeros(x.size(), device=device)\n",
    "        out = 0\n",
    "        for batch in range(batch_size):\n",
    "            for i in range(block_size):\n",
    "                for j in range(top_experts):\n",
    "                    # print(i.shape)\n",
    "                    # print('X batched shape: ', x[batch].shape)\n",
    "                    # print('X shape: ', x.shape)\n",
    "                    current_head_idx = top_k_indices[batch, i][j]\n",
    "                    # print(top_k_indices[batch, i])\n",
    "                    # print(top_k_indices[batch, i][j])\n",
    "                    head_out = self.heads[current_head_idx](x[batch])\n",
    "                    # print('Head out shape: ', head_out.shape)\n",
    "\n",
    "                    # print('Softmax shape: ', torch.nn.functional.softmax(top_k_values[top_k_indices[i]]).shape)\n",
    "                    # print('Head out shape: ', head_out.shape)\n",
    "                    # print(\"Pro: \", probs.shape)\n",
    "                    # print(\"Top K indices: \", top_k_indices.shape)\n",
    "                    # print(probs[batch, top_k_indices[batch, i]])\n",
    "                    # print(probs[batch, top_k_indices[batch, i]].shape)\n",
    "                    # self.outputs[batch,i] = probs[batch, i]\n",
    "                    # print(probs[batch, i].shape)\n",
    "                    # print(probs[batch, i])\n",
    "                    # print(probs[batch, i][j])\n",
    "                    outputs[batch,i] = probs[batch, i][j]\n",
    "        # print(self.outputs.shape)\n",
    "        out += head_out * outputs\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf0Jf_7UcuoN",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = attn_dropout,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.head_size = embeddings_dims // no_of_heads\n",
    "        self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device, bias=False)\n",
    "        self.keys = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,device=device, bias=False)\n",
    "        self.values = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device,bias=False)\n",
    "        self.dropout = nn.Dropout(p = attn_dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, block_size, embd_dims = x.shape\n",
    "        k = self.keys(x)\n",
    "        q = self.query(x)\n",
    "        v = self.values(x)\n",
    "        masked_table = torch.tril(torch.ones(block_size, block_size, device=device))\n",
    "        weights = q @ torch.transpose(k, dim0=-2, dim1=-1) * (k.shape[-1] ** -0.5)\n",
    "        masked_values = weights.masked_fill(masked_table[: block_size, : block_size] == 0, float('-inf'))\n",
    "        weights_normalized = nn.functional.softmax(masked_values, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
    "        weights_normalized = self.dropout(weights_normalized)\n",
    "        out = weights_normalized @ v\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "asiOs-sFcuoO",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# MHA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = attn_dropout,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads) for _ in range(no_of_heads)])\n",
    "        self.dropout = nn.Dropout(p = attn_dropout)\n",
    "        self.linear = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=device, bias=False) # 12 (no of heads) * (batch_size) 64 = 768 -> gives out the text embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        concat = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        linear_layer = self.linear(concat)\n",
    "        out = self.dropout(linear_layer)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9rJzO_XcuoO",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Decoder Block\n",
    "\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = attn_dropout,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "        dropout = dropout,\n",
    "        vocab_size = vocab_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = MHA(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads)\n",
    "        self.layer_norm1 = LayerNormalization(embeddings_dims=embeddings_dims)\n",
    "        self.layer_norm2 = LayerNormalization(embeddings_dims=embeddings_dims)\n",
    "        self.moe_block = MoeLayer(dropout=dropout, embeddings_size=embeddings_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.mha(x)\n",
    "        # x = x + self.layer_norm1(x)\n",
    "        # x = x + self.mlp_block(x)\n",
    "        # out = self.layer_norm2(x)\n",
    "        x = x + self.mha(self.layer_norm1(x))  #Very important step -> Layer Norm on input and then passes it to the subsequent blocks\n",
    "        x = x + self.moe_block(self.layer_norm2(x)) #Very important step\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGh8ujQJcuoO",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Decoder Block\n",
    "\n",
    "class DecoderModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_dropout = attn_dropout,\n",
    "        embeddings_dims = embeddings_dims,\n",
    "        no_of_heads = no_of_heads,\n",
    "        block_size = block_size,\n",
    "        dropout = dropout,\n",
    "        no_of_decoder_layers = no_of_decoder_layers,\n",
    "        vocab_size = vocab_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.positional_embeddings = nn.Parameter(torch.randn(1, block_size, embeddings_dims, device=device), requires_grad=True) #To give positional embeddings to each token of the input text, hence num_embeddings=block_size\n",
    "        torch.nn.init.normal_(self.positional_embeddings, mean=0.0, std=0.02)\n",
    "        self.text_embds = TextEmbeddings(vocab_size=vocab_size, embeddings_dims=embeddings_dims)\n",
    "        self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=vocab_size, device=device, bias=False) # Takes in logits of dimensions- embeds_dims and converts it into dimension of vocab_size (logits in range of vocab_size)\n",
    "        # self.layer_norm = LayerNormalization(embeddings_dims=embeddings_dims)\n",
    "        self.decoder_layers = nn.Sequential(*[TransformerDecoderBlock(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads, dropout=dropout, vocab_size=vocab_size) for _ in range(no_of_decoder_layers)])\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):  #Weight Initialization\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.text_embds(x)\n",
    "        x = x + self.positional_embeddings\n",
    "        x = self.decoder_layers(x)\n",
    "        # x = self.layer_norm(x)\n",
    "        out = self.linear_layer(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tpmbUwBEcuoO",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Instantiating the model\n",
    "model = DecoderModel(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads, block_size=block_size, dropout=dropout, no_of_decoder_layers=no_of_decoder_layers, vocab_size=vocab_size)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yOXtmG-lcuoO",
    "outputId": "0b245137-5dea-4531-9c78-a7660973b3cf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Printing a summary of the architecture\n",
    "# !pip install torchinfo\n",
    "from torchinfo import summary\n",
    "idx, targets = get_batch('test')\n",
    "# idx = idx.to(device)\n",
    "summary(model=model,\n",
    "        input_data=idx,\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LH95cJEvcuoO",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Optimizer setup and scheduler steup\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=max_lr, weight_decay=weight_decay_optim)\n",
    "initial_iters = 2000\n",
    "total_steps = 1000\n",
    "eval_iters = 50\n",
    "\n",
    "@torch.inference_mode()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            idx, targets = get_batch(split=split)\n",
    "            logits = model(idx)\n",
    "            batch_size, block_size, embeddings_dims = logits.shape\n",
    "            logits = logits.view(batch_size*block_size, embeddings_dims) # Total tokens(words) => batch_size * block_size\n",
    "            targets = targets.view(batch_size * block_size)\n",
    "            loss = nn.functional.cross_entropy(logits, targets)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "id": "nPrSPPu8cuoO",
    "outputId": "6eee2020-99ee-4c4b-f08c-e36a9fb5f312",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Train the  model\n",
    "from tqdm import tqdm\n",
    "\n",
    "model.train()\n",
    "for step in tqdm(range(total_steps)):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if (step  % eval_iters == 0 and step != 0) or step == total_steps - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "\n",
    "    idx, targets = get_batch(split='train')\n",
    "    logits = model(idx)\n",
    "    batch_size, block_size, embeddings_dims = logits.shape\n",
    "    logits = logits.view(batch_size*block_size, embeddings_dims)\n",
    "    targets = targets.view(batch_size * block_size)\n",
    "    loss = nn.functional.cross_entropy(logits, targets)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    # print(loss.item())\n",
    "    # break\n",
    "\n",
    "    # if step != 0 and (step % eval_iters == 0 or step == total_steps -1) :\n",
    "    #     loss_values = estimate_loss()\n",
    "    #     print(\"Train Loss at {} steps : {}\".format(step, loss.item()), \"Val Loss at {} steps : {}\".format(step, loss_values['val']))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
