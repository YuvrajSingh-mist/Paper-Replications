{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTFGr5T2bXEE",
        "outputId": "4bd30576-7d23-4bed-caf6-720cc33493c6"
      },
      "outputs": [],
      "source": [
        "# !pip install torch==2.3.0 torchtext==0.18.0\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nUMPnXjmbXEF"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelArgs:\n",
        "    device = 'cuda'\n",
        "    no_of_neurons = 128\n",
        "    block_size = 32\n",
        "    batch_size = 32\n",
        "    en_vocab_size = None\n",
        "    de_vocab_size = None\n",
        "    dropout = 0.1\n",
        "    epoch = 10\n",
        "    max_lr = 1e-4\n",
        "    embedding_dims = 1000\n",
        "    num_layers = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "htu_YvmbdnMu"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    ModelArgs.device = 'cuda'\n",
        "    torch.set_default_device('cuda')\n",
        "else:\n",
        "\n",
        "    torch.set_default_device('cpu')\n",
        "    ModelArgs.device='cpu'\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  torch.set_default_device(ModelArgs.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peJSz_0DblE2",
        "outputId": "cc6b6723-72c6-4fc1-8e42-629116452442"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting de-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!python -m spacy download de_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLjqIzybbj82",
        "outputId": "7ce8d2b7-81b3-45b8-a009-85aca6d23e66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
            "Successfully installed en-core-web-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP5w_PyBbXEF",
        "outputId": "97dd9ff5-610c-42b2-c35a-7efaa0d81d04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrajceo2031\u001b[0m (\u001b[33mrentio\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5saagew8bXEF"
      },
      "outputs": [],
      "source": [
        "torch.set_default_device(ModelArgs.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMmCjvl9bXEF",
        "outputId": "6c5e3329-61b1-424d-8997-e21bb2ca0dc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "German batch shape: torch.Size([32, 32])\n",
            "English batch shape: torch.Size([32, 32])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "import io\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Download and extract data\n",
        "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
        "train_urls = ('train.de.gz', 'train.en.gz')\n",
        "val_urls = ('val.de.gz', 'val.en.gz')\n",
        "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
        "\n",
        "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
        "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
        "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n",
        "\n",
        "# Load SpaCy tokenizers\n",
        "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "# Build vocabulary\n",
        "def build_vocab(filepath, tokenizer):\n",
        "    counter = Counter()\n",
        "    with io.open(filepath, encoding=\"utf8\") as f:\n",
        "        for string_ in f:\n",
        "            counter.update(tokenizer(string_))\n",
        "    vocab = build_vocab_from_iterator(\n",
        "        [counter.keys()],\n",
        "        specials=['<unk>', '<bos>', '<eos>', '<pad>']\n",
        "    )\n",
        "    vocab.set_default_index(vocab['<unk>'])\n",
        "    return vocab\n",
        "\n",
        "de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
        "ModelArgs.de_vocab_size = len(de_vocab) + 1\n",
        "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)\n",
        "ModelArgs.en_vocab_size = len(en_vocab) + 1\n",
        "\n",
        "\n",
        "def data_process(filepaths):\n",
        "    raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "    raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
        "    data = []\n",
        "\n",
        "    # Get the indices for <bos> and <eos> tokens\n",
        "    de_bos_idx = de_vocab['<bos>']\n",
        "    de_eos_idx = de_vocab['<eos>']\n",
        "    en_bos_idx = en_vocab['<bos>']\n",
        "    en_eos_idx = en_vocab['<eos>']\n",
        "\n",
        "    for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
        "        # Tokenize and convert to indices\n",
        "        de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)], dtype=torch.long)\n",
        "        en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)], dtype=torch.long)\n",
        "\n",
        "        # Add <bos> and <eos> tokens\n",
        "        # de_tensor_ = torch.cat([torch.tensor([de_bos_idx]), de_tensor_, torch.tensor([de_eos_idx])])\n",
        "        en_tensor_ = torch.cat([torch.tensor([en_bos_idx]), en_tensor_, torch.tensor([en_eos_idx])])\n",
        "\n",
        "        # Flip the German tensor (if required)\n",
        "        # de_tensor_ = torch.flip(de_tensor_, dims=[0])\n",
        "\n",
        "        # Append to data\n",
        "        data.append((de_tensor_, en_tensor_))\n",
        "\n",
        "    return data\n",
        "\n",
        "def data_process_flip(filepaths):\n",
        "    raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "    raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
        "    data = []\n",
        "\n",
        "    # Get the indices for <bos> and <eos> tokens\n",
        "    de_bos_idx = de_vocab['<bos>']\n",
        "    de_eos_idx = de_vocab['<eos>']\n",
        "    en_bos_idx = en_vocab['<bos>']\n",
        "    en_eos_idx = en_vocab['<eos>']\n",
        "\n",
        "    for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
        "        # Tokenize and convert to indices\n",
        "        de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)], dtype=torch.long)\n",
        "        en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)], dtype=torch.long)\n",
        "\n",
        "        # Add <bos> and <eos> tokens\n",
        "        # de_tensor_ = torch.cat([torch.tensor([de_bos_idx]), de_tensor_, torch.tensor([de_eos_idx])])\n",
        "        en_tensor_ = torch.cat([torch.tensor([en_bos_idx]), en_tensor_, torch.tensor([en_eos_idx])])\n",
        "\n",
        "        # Flip the German tensor (if required)\n",
        "        de_tensor_ = torch.flip(de_tensor_, dims=[0])\n",
        "\n",
        "        # Append to data\n",
        "        data.append((de_tensor_, en_tensor_))\n",
        "\n",
        "    return data\n",
        "\n",
        "train_data = data_process(train_filepaths)\n",
        "val_data = data_process(val_filepaths)\n",
        "test_data = data_process(test_filepaths)\n",
        "\n",
        "# Create a custom Dataset class\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Create Dataset instances\n",
        "train_dataset = TranslationDataset(train_data)\n",
        "val_dataset = TranslationDataset(val_data)\n",
        "test_dataset = TranslationDataset(test_data)\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch, block_size=32):\n",
        "    \"\"\"\n",
        "    Collate function to pad or truncate sequences to a fixed block size.\n",
        "\n",
        "    Args:\n",
        "        batch: A list of tuples (de_tensor, en_tensor).\n",
        "        block_size: The fixed length to pad or truncate sequences to.\n",
        "\n",
        "    Returns:\n",
        "        de_batch: Padded/truncated German sequences (batch_size, block_size).\n",
        "        en_batch: Padded/truncated English sequences (batch_size, block_size).\n",
        "    \"\"\"\n",
        "    de_batch, en_batch = zip(*batch)\n",
        "\n",
        "    # Function to pad or truncate a sequence to the block size\n",
        "    def pad_or_truncate(sequence, block_size, pad_value):\n",
        "        if len(sequence) > block_size:\n",
        "            # Truncate the sequence if it's longer than block_size\n",
        "            return sequence[:block_size]\n",
        "        else:\n",
        "            # Pad the sequence if it's shorter than block_size\n",
        "            padding_length = block_size - len(sequence)\n",
        "            return torch.cat([sequence, torch.full((padding_length,), pad_value, dtype=sequence.dtype)])\n",
        "\n",
        "    # Pad or truncate each sequence in the batch\n",
        "    de_batch = [pad_or_truncate(seq, block_size, de_vocab['<pad>']) for seq in de_batch]\n",
        "    en_batch = [pad_or_truncate(seq, block_size, en_vocab['<pad>']) for seq in en_batch]\n",
        "\n",
        "    # Stack the sequences into a single tensor\n",
        "    de_batch = torch.stack(de_batch)\n",
        "    en_batch = torch.stack(en_batch)\n",
        "\n",
        "    return de_batch, en_batch\n",
        "\n",
        "generator = torch.Generator(device=ModelArgs.device)\n",
        "\n",
        "\n",
        "# Create DataLoader instances\n",
        "batch_size = ModelArgs.batch_size\n",
        "train_loader = DataLoader(train_dataset, generator=generator, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, generator=generator, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, drop_last=False)\n",
        "\n",
        "# Example usage\n",
        "for de_batch, en_batch in train_loader:\n",
        "    print(f\"German batch shape: {de_batch.shape}\")\n",
        "    print(f\"English batch shape: {en_batch.shape}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 303,
      "metadata": {
        "id": "5EU54spwbXEG"
      },
      "outputs": [],
      "source": [
        "class RNNCell(nn.Module):\n",
        "    def __init__(self, device, no_of_neurons, bi=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # if(bi):\n",
        "        #     print(\"eher\")\n",
        "        self.linear_layer_bi = nn.Linear(in_features=(2*ModelArgs.no_of_neurons) + ModelArgs.embedding_dims, out_features=no_of_neurons, device=ModelArgs.device)\n",
        "        # else:\n",
        "        self.linear_layer = nn.Linear(in_features=ModelArgs.no_of_neurons + ModelArgs.embedding_dims, out_features=no_of_neurons, device=ModelArgs.device)\n",
        "        self.bi = bi\n",
        "    def forward(self, x, ht_1):\n",
        "      # print(self.bi)\n",
        "      if(self.bi):\n",
        "        # print(x.shape)\n",
        "        # print(ht_1.shape)\n",
        "        x = self.linear_layer_bi(torch.cat([x, ht_1], dim=1))\n",
        "      else:\n",
        "        x =  self.linear_layer(torch.cat([x, ht_1], dim=1))\n",
        "\n",
        "      ht = torch.nn.functional.sigmoid(x)\n",
        "      return ht\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 320,
      "metadata": {
        "id": "rmpfUn33bXEG"
      },
      "outputs": [],
      "source": [
        "class RNNLayer(nn.Module):\n",
        "    def __init__(self, device, no_of_neurons, bi=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rnn_layer = RNNCell(bi=bi, device=device, no_of_neurons=no_of_neurons)\n",
        "        self.linear_layer = nn.Linear(in_features=ModelArgs.no_of_neurons, out_features=no_of_neurons, device=ModelArgs.device)\n",
        "\n",
        "        # self.birnn_layer = RNNCell(device=device, no_of_neurons=no_of_neurons, bi=True)\n",
        "    def forward(self, x, ht_1=None):\n",
        "\n",
        "        if(ht_1 is None):\n",
        "          ht_1 = torch.zeros((x.shape[0], ModelArgs.no_of_neurons), device=ModelArgs.device, requires_grad=True, dtype=torch.float32)\n",
        "\n",
        "        seq_len = x.shape[1]\n",
        "        outputs = []\n",
        "        for t in range(seq_len):\n",
        "            xt = x[:, t, :]\n",
        "            # xt = xt.unsqueeze(-1)\n",
        "            ht = self.rnn_layer(xt, ht_1)\n",
        "            ht_1 = ht\n",
        "            outputs.append(ht_1)\n",
        "\n",
        "        outputs = torch.stack(outputs, dim=1)\n",
        "        return ht_1, outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "metadata": {
        "id": "KIqZngodf7yi"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class EmbeddingTable_de(nn.Module):\n",
        "  def __init__(self, device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embed_de =  nn.Embedding(num_embeddings=ModelArgs.de_vocab_size, embedding_dim=ModelArgs.embedding_dims, device=device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # print('Indie: ', x)\n",
        "    return self.embed_de(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 322,
      "metadata": {
        "id": "tV6xhxRYf61L"
      },
      "outputs": [],
      "source": [
        "\n",
        "class EmbeddingTable_en(nn.Module):\n",
        "  def __init__(self, device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embed_en = nn.Embedding(num_embeddings=ModelArgs.en_vocab_size, embedding_dim=ModelArgs.embedding_dims, device=device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.embed_en(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 331,
      "metadata": {
        "id": "K-C3hiwYbXEG"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, device, no_of_neurons, out_features, bi=False):\n",
        "        super().__init__()\n",
        "        self.rnn = RNNLayer(device=device, no_of_neurons=no_of_neurons, bi=False)\n",
        "        self.embeds_table_en = EmbeddingTable_en(device=device)\n",
        "        self.output = nn.Linear(in_features=ModelArgs.no_of_neurons, out_features=ModelArgs.en_vocab_size, device=device, dtype=torch.float32)\n",
        "        self.dropout = nn.Dropout(p=ModelArgs.dropout)\n",
        "\n",
        "    def forward(self, x, ctx=None, inf=None, embeds=None, initial=None, outputs=None):\n",
        "\n",
        "      if(inf is not True and initial is True):\n",
        "        x = self.embeds_table_en(x)\n",
        "      if(inf is True):\n",
        "        # print(\"Before: \", x.shape)\n",
        "        x = embeds(x)\n",
        "\n",
        "      ht, outputs = self.rnn(x, ctx)\n",
        "      out = self.output(outputs)\n",
        "      out = self.dropout(out)\n",
        "      return out, outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 332,
      "metadata": {
        "id": "80ezApYwbXEG"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, device, no_of_neurons, out_features):\n",
        "        super().__init__()\n",
        "        self.rnn = RNNLayer(device=device, no_of_neurons=no_of_neurons, bi=False)\n",
        "        self.embeds_table_de = EmbeddingTable_de(device=device)\n",
        "        self.output = nn.Linear(in_features=2 * ModelArgs.no_of_neurons, out_features=ModelArgs.no_of_neurons, device=device, dtype=torch.float32)\n",
        "        # self.dropout = nn.Dropout(p=ModelArgs.dropout)\n",
        "\n",
        "    def forward(self, x, initial=None):\n",
        "        if(initial is not None and initial is True):\n",
        "          x = self.embeds_table_de(x)\n",
        "        ht_fd, outputs = self.rnn(x)\n",
        "        x_rev = torch.flip(x, dims=[1])\n",
        "        ht_bwd, outputs = self.rnn(x_rev)\n",
        "        ht = torch.cat([ht_fd, ht_bwd], dim=1)\n",
        "        out = self.output(ht)\n",
        "        # out = self.dropout(out)\n",
        "        return out, self.embeds_table_de\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 333,
      "metadata": {
        "id": "FL44a09rdr-G"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "\n",
        "    def __init__(self, device, no_of_neurons, out_features):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(device, no_of_neurons, out_features)\n",
        "        self.embeds_table_en = EmbeddingTable_en(device=device)\n",
        "        self.decoder = Decoder(device, no_of_neurons, out_features)\n",
        "        # self.encoders = nn.ModuleList(Encoder(device, no_of_neurons, out_features) for _  in range(ModelArgs.num_layers))\n",
        "        # self.decoders = nn.ModuleList(Decoder(device, no_of_neurons, out_features) for x in range(ModelArgs.num_layers))\n",
        "\n",
        "    def forward(self, x, y=None, inf=None):\n",
        "\n",
        "        # count = 0\n",
        "        # for i in self.encoders:\n",
        "        #   if(count == 0):\n",
        "        #     ht_encoder, ct_encoder,outputs_encoder, embeds_de = i(x, initial=True)\n",
        "        #     # x = ht_encoder\n",
        "        #   else:\n",
        "        ht_encoder , embeds_de = self.encoder(x, initial=True)\n",
        "        # print(\"encoder: \", ht_encoder.shape)\n",
        "            # x = ht_encoder\n",
        "          # count += 1\n",
        "\n",
        "        res = None\n",
        "        count = 0\n",
        "        if(y is not None and inf==False):\n",
        "          # for i in self.decoders:\n",
        "\n",
        "          #   # print(\"Hiii\")\n",
        "          #   if(count == 0):\n",
        "          y , outputs = self.decoder(y, ht_encoder, inf, embeds_de, True)\n",
        "              # res = x\n",
        "            # else:\n",
        "            #   y, outputs = i(y, ht_encoder, inf, embeds_de, outputs=outputs)\n",
        "              # res = x\n",
        "            # return res\n",
        "          # elif(y is not None and inf==False):\n",
        "            # print(\"Here\")\n",
        "            # res = self.decoder(y, ht_encoder)\n",
        "            # return res\n",
        "            # count += 1\n",
        "          return y\n",
        "\n",
        "\n",
        "        elif(inf==True and y is None):\n",
        "          # x_init = x\n",
        "          # count = 0\n",
        "          # for i in self.decoders:\n",
        "\n",
        "          #   if(count == 0):\n",
        "          x, outputs = self.decoder(x, ht_encoder, inf, embeds_de, True)\n",
        "            # res = x\n",
        "\n",
        "            # else:\n",
        "            #   x, outputs = i(x_init, ht_encoder, inf, embeds_de, outputs=outputs)\n",
        "\n",
        "            # count += 1\n",
        "          return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 334,
      "metadata": {
        "id": "1XjQJr5sbXEG"
      },
      "outputs": [],
      "source": [
        "\n",
        "# model = GRU(device=ModelArgs.device, no_of_neurons=ModelArgs.no_of_neurons, out_features=1)\n",
        "model = Seq2Seq(device=ModelArgs.device, no_of_neurons=ModelArgs.no_of_neurons, out_features=1)\n",
        "model = model.to(ModelArgs.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 335,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQMtQTYFbXEG",
        "outputId": "e94f8d04-12b5-4d6c-eb23-5b4019444da6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==================================================================================================================================\n",
              "Layer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n",
              "==================================================================================================================================\n",
              "Seq2Seq (Seq2Seq)                                  [32, 32]             [32, 32, 10839]      10,839,000           True\n",
              "├─Encoder (encoder)                                [32, 32]             [32, 128]            --                   True\n",
              "│    └─EmbeddingTable_de (embeds_table_de)         [32, 32]             [32, 32, 1000]       --                   True\n",
              "│    │    └─Embedding (embed_de)                   [32, 32]             [32, 32, 1000]       19,216,000           True\n",
              "│    └─RNNLayer (rnn)                              [32, 32, 1000]       [32, 128]            16,512               True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            305,408              True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    └─RNNLayer (rnn)                              [32, 32, 1000]       [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    └─Linear (output)                             [32, 256]            [32, 128]            32,896               True\n",
              "├─Decoder (decoder)                                [32, 32]             [32, 32, 10839]      --                   True\n",
              "│    └─EmbeddingTable_en (embeds_table_en)         [32, 32]             [32, 32, 1000]       --                   True\n",
              "│    │    └─Embedding (embed_en)                   [32, 32]             [32, 32, 1000]       10,839,000           True\n",
              "│    └─RNNLayer (rnn)                              [32, 32, 1000]       [32, 128]            16,512               True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            305,408              True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    │    └─RNNCell (rnn_layer)                    [32, 1000]           [32, 128]            (recursive)          True\n",
              "│    └─Linear (output)                             [32, 32, 128]        [32, 32, 10839]      1,398,231            True\n",
              "│    └─Dropout (dropout)                           [32, 32, 10839]      [32, 32, 10839]      --                   --\n",
              "==================================================================================================================================\n",
              "Total params: 42,968,967\n",
              "Trainable params: 42,968,967\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 1.45\n",
              "==================================================================================================================================\n",
              "Input size (MB): 0.02\n",
              "Forward/backward pass size (MB): 108.36\n",
              "Params size (MB): 127.10\n",
              "Estimated Total Size (MB): 235.47\n",
              "=================================================================================================================================="
            ]
          },
          "execution_count": 335,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# !pip install torchinfo\n",
        "\n",
        "from torchinfo import summary\n",
        "\n",
        "# x = torch.randint(0, 100, (ModelArgs.batch_size,ModelArgs.block_size))  # Random integer between 0 and 100\n",
        "# y = torch.randint(0, 100, (ModelArgs.batch_size,ModelArgs.block_size))\n",
        "# y = y.to(ModelArgs.device)\n",
        "# x = x.to(ModelArgs.device)\n",
        "\n",
        "\n",
        "# x = torch.randint(0, 100, (ModelArgs.batch_size,ModelArgs.block_size))  # Random integer between 0 and 100\n",
        "x,y = next(iter(train_loader))\n",
        "x = x.to(ModelArgs.device)\n",
        "y = y.to(ModelArgs.device)\n",
        "\n",
        "\n",
        "summary(model=model,\n",
        "        input_data=[x, y, False],\n",
        "        # input_size=(ModelArgs.batch_size, ModelArgs.block_size, ModelArgs.embeddings_dims),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 338,
      "metadata": {
        "id": "_xqaF7mUApTQ"
      },
      "outputs": [],
      "source": [
        "# from andrej karapathy github\n",
        "import torch.nn.functional as F\n",
        "def topk_sampling(model, prompt, tokenizer, device, max_length=50, top_k=50, temperature=1.0):\n",
        "\n",
        "    # input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "    input_ids = torch.tensor([de_vocab[token] for token in de_tokenizer(prompt)]).unsqueeze(0)\n",
        "    oov = []\n",
        "    generated_text = \"\"\n",
        "    for _ in range(max_length):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, None, True)\n",
        "            logits = outputs[:, -1, :]\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Top-k filtering\n",
        "            top_k_probs, top_k_indices = torch.topk(probs, top_k, dim=-1)\n",
        "#\n",
        "            # Apply temperature scaling\n",
        "            # probs = probs / temperature\n",
        "\n",
        "            # Sample from top-k\n",
        "            next_token = torch.multinomial(top_k_probs, num_samples=1)\n",
        "\n",
        "            # generated_tokens.append(next_token.item())\n",
        "\n",
        "            xcol = torch.gather(top_k_indices, -1, next_token)\n",
        "            # xcol = torch.argmax(probs, dim=-1)\n",
        "\n",
        "            # if(xcol == '<eos>'):\n",
        "            #   break\n",
        "            # print(xcol.shape)\n",
        "            # print(input_ids.shape)\n",
        "            # print(xcol.shape)\n",
        "            input_ids = torch.cat([input_ids, xcol], dim=-1) #1 because is it the dimension of the sequence\n",
        "    # print(input_ids)\n",
        "    count = 0\n",
        "    de_len = torch.tensor([de_vocab[token] for token in de_tokenizer(prompt)])\n",
        "    for i in input_ids[0]:\n",
        "      # print(de_len.shape)\n",
        "      if(count > de_len.shape[0]):\n",
        "      # print(i)\n",
        "      # try:\n",
        "        if(en_vocab.vocab.get_itos()[i] == '<eos>'):\n",
        "          print(\"Done\")\n",
        "          break\n",
        "        token = en_vocab.vocab.get_itos()[i]\n",
        "        generated_text += token\n",
        "\n",
        "        generated_text += ' '\n",
        "      # except:\n",
        "        # oov.append(i)\n",
        "      else:\n",
        "        count += 1\n",
        "\n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 339,
      "metadata": {
        "id": "48NmNadabXEH"
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=ModelArgs.max_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 340,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "MvMxXeHabXEH",
        "outputId": "b9566237-de9e-4951-aaaf-20f460c25174"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 32, 10839])\n",
            "counting coughing clown Whack Dutch bonding interior tough Scottish lifestyle anger formal dancers individuals budding examining viking examining Pedestrians arts pavement Yorkie entrances unfamiliar patio comforter pinata sunny gravity cutouts regular bonding entrances unfamiliar cacti desserts comforter mannequins enclosed screwdrivers gravity across noodles tough Dutch tricycle arts noodles comforter \n"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "train_losses =  torch.zeros(len(train_loader))\n",
        "val_losses = torch.zeros(len(val_loader))\n",
        "wandb.init(\n",
        "    project='Seq2Seq-From-Scratch'\n",
        ")\n",
        "for epoch in range(ModelArgs.epoch):\n",
        "\n",
        "    count = 0\n",
        "    for de, en in train_loader:\n",
        "        logits = model(de, en, False)\n",
        "        print(logits.shape)\n",
        "\n",
        "        batch_size, block_size, vocab = logits.shape\n",
        "        # print(\"Va: \", vocab)\n",
        "        logits = logits.view(batch_size*block_size, vocab)\n",
        "        targets = en.view(batch_size * block_size)\n",
        "        # print(\"HiiiL \", en.shape)\n",
        "        # print(\"HiiiT \", logits.shape)\n",
        "        loss = nn.functional.cross_entropy(logits, targets, ignore_index=en_vocab['<pad>'])\n",
        "        train_losses[count] = loss.item()\n",
        "        # print(\"Loss: \", loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        count += 1\n",
        "        # print(count)\n",
        "        break\n",
        "\n",
        "    # count = 0\n",
        "    model.eval()\n",
        "    count = 0\n",
        "    for de, en in val_loader:\n",
        "        logits = model(de, en, False)\n",
        "        # print(logits.shape)\n",
        "        batch_size, block_size, vocab = logits.shape\n",
        "\n",
        "        logits = logits.view(batch_size*block_size, vocab)\n",
        "        # print(\"Va: \", vocab)\n",
        "        targets = en.view(batch_size * block_size)\n",
        "        loss = nn.functional.cross_entropy(logits, targets, ignore_index=en_vocab['<pad>'])\n",
        "\n",
        "        # print(\"Loss: \", loss.item())\n",
        "        val_losses[count] = loss.item()\n",
        "\n",
        "        # optimizer.zero_grad()\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "        count += 1\n",
        "        break\n",
        "\n",
        "    # print(\"eval\")\n",
        "    generated_text = topk_sampling(model, 'Ich fahre heute mit dem Rad zur Schule', de_tokenizer, device=ModelArgs.device, max_length=50, top_k=50, temperature=1.0)\n",
        "\n",
        "    print(generated_text)\n",
        "\n",
        "    break\n",
        "    model.train()\n",
        "    wandb.log({\n",
        "      \"Train Loss\": train_losses.mean(),\n",
        "      \"Val Loss\": val_losses.mean(),\n",
        "      \"epoch\": epoch\n",
        "    })\n",
        "    print(\"Epoch: \", epoch, \"|\", \"Train Loss: \", train_losses.mean(),  \"|\", \"Val Loss: \", val_losses.mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 319,
      "metadata": {
        "id": "Y9YH9j-jbXEH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mU_rChPbXEH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sOiJMh5bXEH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH9H3fTmbXEH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcvyqvHRbXEH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQhuzjNhbXEH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nT6Y_z1bXEH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nh7jczubXEH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flTc0GM1bXEH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsIzD4KSbXEH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_Pj2Qs0bXEH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dx9u5AYEbXEH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_X39Cl_2bXEH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mt",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
