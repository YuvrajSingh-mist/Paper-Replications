{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T23:41:25.815754Z",
          "iopub.status.busy": "2024-12-30T23:41:25.815482Z",
          "iopub.status.idle": "2024-12-30T23:41:27.153981Z",
          "shell.execute_reply": "2024-12-30T23:41:27.153285Z",
          "shell.execute_reply.started": "2024-12-30T23:41:25.815732Z"
        },
        "id": "Pw7f2ghccuoK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "# from tokenizers import Tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-12-30T23:41:27.513712Z",
          "iopub.status.busy": "2024-12-30T23:41:27.513428Z",
          "iopub.status.idle": "2024-12-30T23:41:27.517228Z",
          "shell.execute_reply": "2024-12-30T23:41:27.516268Z",
          "shell.execute_reply.started": "2024-12-30T23:41:27.513688Z"
        },
        "id": "LwR5_uvTcuoL",
        "outputId": "492fa6c9-a295-4ebb-e3b7-8c007fbf9055",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Data\n",
        "# from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# load_dotenv()\n",
        "\n",
        "HF_TOKEN = '...'\n",
        "# Load model directly\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", token=HF_TOKEN)\n",
        "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
        "\n",
        "SOT = '<|startoftranscript|>'\n",
        "EOT = '<|endoftranscript|>'\n",
        "transcribe = '<|transcribe|>'\n",
        "prev = '<|prev|>'\n",
        "\n",
        "special_tokens_dict = {\n",
        "    'additional_special_tokens': [SOT, EOT, transcribe, prev]\n",
        "}\n",
        "\n",
        "# Update the tokenizer with the new special tokens\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
        "\n",
        "# tokenizer(\"hi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3vCVc6OlXe2",
        "outputId": "e0a26206-20aa-4bf4-98db-dd576e96b2d9"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install wandb\n",
        "import wandb\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T23:41:37.956230Z",
          "iopub.status.busy": "2024-12-30T23:41:37.955961Z",
          "iopub.status.idle": "2024-12-30T23:41:37.968761Z",
          "shell.execute_reply": "2024-12-30T23:41:37.967954Z",
          "shell.execute_reply.started": "2024-12-30T23:41:37.956179Z"
        },
        "id": "D7AP219KJzTs",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "epochs=10\n",
        "block_size = 64\n",
        "batch_size = 64\n",
        "# src_vocab_size = None\n",
        "tgt_vocab_size = len(tokenizer)\n",
        "embeddings_dims = 384\n",
        "attn_dropout = 0.1\n",
        "no_of_heads = 6 #IMP needs to be thoroughly calculated\n",
        "dropout = 0.1\n",
        "# epochs = 3\n",
        "max_lr = 2e-4\n",
        "no_of_decoder_layers = 6 #IMP needs to be thoroughly calculated\n",
        "attn_dropout = 0.1\n",
        "weight_decay_optim = 0.01\n",
        "log_mel_features = 80\n",
        "kernel_size = 3\n",
        "stride = (2,10)\n",
        "sr = 16000\n",
        "device= 'cuda:0'\n",
        "SAMPLING_RATE=16000\n",
        "N_MELS = 80  # 80-channel Mel spectrogram\n",
        "WINDOW_DURATION = 0.025  # 25 milliseconds\n",
        "STRIDE_DURATION = 0.010  # 10 milliseconds\n",
        "max_t = 500\n",
        "n_channels = N_MELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TmPkI_UEpvor"
      },
      "outputs": [],
      "source": [
        "torch.set_default_device(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IME1Ls95Y3gl",
        "outputId": "ab87d390-48f4-43be-b421-aad103deacca"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install datasets\n",
        "from tabnanny import verbose\n",
        "from datasets import load_dataset\n",
        "\n",
        "gs = load_dataset(\"speechcolab/gigaspeech\", \"xs\", token=HF_TOKEN, trust_remote_code=True) # Ensures only 'train' split of 'xs' is download)\n",
        "\n",
        "# see structure\n",
        "print(gs)\n",
        "\n",
        "# load audio sample on the fly\n",
        "audio_input = gs['train'][0][\"audio\"]  # first decoded audio sample\n",
        "transcription = gs[\"train\"][0][\"text\"]  # first transcription\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFlAHwhDlv-t",
        "outputId": "351a0b9e-f14e-41dd-e568-2c34f4e29ad7"
      },
      "outputs": [],
      "source": [
        "gs['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRV1EOlVY3gm",
        "outputId": "1052df52-a76c-4ee1-c4e7-cd495e3fb212"
      },
      "outputs": [],
      "source": [
        "MAX_DURATION_IN_SECONDS = 10\n",
        "\n",
        "import librosa\n",
        "from tqdm import tqdm\n",
        "def is_audio_length_in_range(input_length):\n",
        "    return input_length < MAX_DURATION_IN_SECONDS\n",
        "\n",
        "train_new_column = []\n",
        "# new_column = [librosa.get_duration(path=x) ]] #Because test data has more rows\n",
        "for x in tqdm(range(len(gs['test']))):\n",
        "    train_new_column.append(librosa.get_duration(path=gs['test'][x]['audio']['path']))\n",
        "\n",
        "gs_ = gs['test'].add_column(\"duration\", train_new_column)\n",
        "\n",
        "\n",
        "gs_ = gs_.filter(is_audio_length_in_range, input_columns=[\"duration\"])\n",
        "\n",
        "\n",
        "truncated_gs_train = gs_.remove_columns([\"duration\"])\n",
        "# truncated_gs\n",
        "\n",
        "\n",
        "\n",
        "val_new_column = []\n",
        "# new_column = [librosa.get_duration(path=x) ]]\n",
        "for x in tqdm(range(len(gs['validation']))):\n",
        "    val_new_column.append(librosa.get_duration(path=gs['validation'][x]['audio']['path']))\n",
        "\n",
        "gs_ = gs['validation'].add_column(\"duration\", val_new_column)\n",
        "\n",
        "\n",
        "gs_ = gs_.filter(is_audio_length_in_range, input_columns=[\"duration\"])\n",
        "\n",
        "\n",
        "truncated_gs_val = gs_.remove_columns([\"duration\"])\n",
        "# truncated_gs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NZ9Hbp5q1to",
        "outputId": "cf76473e-8044-4b1c-b1e3-a18861572149"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "n_fft = int(WINDOW_DURATION * MAX_DURATION_IN_SECONDS * SAMPLING_RATE)\n",
        "hop_length = int(STRIDE_DURATION * MAX_DURATION_IN_SECONDS * SAMPLING_RATE)\n",
        "\n",
        "train_outputs = []\n",
        "train_texts = []\n",
        "for i in tqdm(range(len(truncated_gs_train))):\n",
        "  S = librosa.feature.melspectrogram(\n",
        "      y=truncated_gs_train[i]['audio']['array'],\n",
        "      sr=SAMPLING_RATE,\n",
        "      n_mels=N_MELS,\n",
        "      n_fft=n_fft,\n",
        "      hop_length=hop_length,\n",
        "      win_length=n_fft,\n",
        "      fmax=SAMPLING_RATE // 2\n",
        "  )\n",
        "\n",
        "\n",
        "  S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "  train_outputs.append(S_dB)\n",
        "  train_texts.append(truncated_gs_train[i]['text'])\n",
        "\n",
        "val_outputs = []\n",
        "val_texts = []\n",
        "for i in tqdm(range(len(truncated_gs_val))):\n",
        "  S = librosa.feature.melspectrogram(\n",
        "      y=truncated_gs_val[i]['audio']['array'],\n",
        "      sr=SAMPLING_RATE,\n",
        "      n_mels=N_MELS,\n",
        "      n_fft=n_fft,\n",
        "      hop_length=hop_length,\n",
        "      win_length=n_fft,\n",
        "      fmax=SAMPLING_RATE // 2\n",
        "  )\n",
        "\n",
        "\n",
        "  S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "  val_outputs.append(S_dB)\n",
        "  val_texts.append(truncated_gs_val[i]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRJNMDQN7Ds8",
        "outputId": "7cab14a7-8044-45e5-8e3b-d87e5af66d2e"
      },
      "outputs": [],
      "source": [
        "train_outputs[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHXqMLR48Wpp",
        "outputId": "5d5e9f3e-b42e-42e0-955b-bfd33f65383f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Calculate the maximum t in the dataset\n",
        "max_t = max(spectrogram.shape[1] for spectrogram in train_outputs + val_outputs)\n",
        "print(f\"Maximum t in the dataset: {max_t}\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Calculate the average t for the training dataset\n",
        "train_t_lengths = [spectrogram.shape[1] for spectrogram in train_outputs + val_outputs]\n",
        "avg_t_train = np.mean(train_t_lengths)\n",
        "\n",
        "print(f\"Average t (training): {avg_t_train}\")\n",
        "# print(f\"Average t (validation): {avg_t_val}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apt0aGdiaR5e",
        "outputId": "2efa4ca4-450f-449d-9655-d6c79467a19c"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Example text\n",
        "text = \"AS THEY'RE LEAVING <COMMA> CAN KASH PULL ZAHRA ASIDE REALLY QUICKLY <QUESTIONMARK>\"\n",
        "\n",
        "# Use regex to remove anything between < and >\n",
        "cleaned_text = re.sub(r'<[^>]*>', '', text)\n",
        "\n",
        "print(cleaned_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "PURLJRNB9V_s",
        "outputId": "c12ac580-ed0d-4029-c017-43757b0ec1a2"
      },
      "outputs": [],
      "source": [
        "train_texts[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "z2aGf6_7xe9S"
      },
      "outputs": [],
      "source": [
        "# import math\n",
        "# print(round(random.random(), 1))\n",
        "class GigaSpeechDataset(Dataset):\n",
        "\n",
        "  def __init__(self, outputs, texts):\n",
        "\n",
        "    self.data = outputs\n",
        "    self.texts = texts\n",
        "    self.max_t = block_size\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "\n",
        "  def pad_to_max_t(self, spectrogram, max_t):\n",
        "\n",
        "    n_mels, t = spectrogram.shape\n",
        "    if t < max_t:\n",
        "        # Pad with zeros\n",
        "        pad_width = ((0, 0), (0, max_t - t))\n",
        "        spectrogram = np.pad(spectrogram, pad_width, mode='constant')\n",
        "    else:\n",
        "      spectrogram = spectrogram[:, :max_t]\n",
        "\n",
        "    return spectrogram\n",
        "\n",
        "  def clean(self, desc):\n",
        "    # Use regex to remove anything between < and >\n",
        "    cleaned_text = re.sub(r'<[^>]*>', '', desc)\n",
        "    return cleaned_text\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "      SOT = '<|startoftranscript|>'\n",
        "      EOT = '<|endoftranscript|>'\n",
        "      transcribe = '<|transcribe|>'\n",
        "      # prev = '<|prev|>'\n",
        "      spectrogram = self.pad_to_max_t(self.data[idx], self.max_t)\n",
        "      # probs = round(random.random(),1)\n",
        "      spectrogram = torch.tensor(spectrogram, dtype=torch.float32)\n",
        "\n",
        "      # if(probs == 0.5):\n",
        "        # Normalize the spectrogram between -1 and 1\n",
        "      spectrogram_min = spectrogram.min()\n",
        "      spectrogram_max = spectrogram.max()\n",
        "      # spectrogram = spectrogram.unsqueeze(0)  # Shape: (1, n_mels, max_t)\n",
        "      # prev_text =\n",
        "      text = self.clean(self.texts[idx])\n",
        "\n",
        "      text = text.lower()\n",
        "      text = SOT  + 'en' + transcribe +  text + EOT\n",
        "      tokenized_text = tokenizer(text, truncation=True, padding='max_length', max_length=block_size, return_tensors='pt')['input_ids']\n",
        "      # print(tokenized_text.shape)\n",
        "\n",
        "      epsilon = 1e-8  # To avoid division by zero\n",
        "      spectrogram = 2 * ((spectrogram - spectrogram_min) / (spectrogram_max - spectrogram_min + epsilon)) - 1\n",
        "\n",
        "      tokenized_text = tokenized_text.squeeze(0)\n",
        "      # print(tokenized_text.shape)\n",
        "      return spectrogram, tokenized_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2tvMuBSOynPy"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "shuffle = True\n",
        "\n",
        "train_dataset = GigaSpeechDataset(train_outputs, train_texts)\n",
        "val_dataset = GigaSpeechDataset(val_outputs, val_texts)\n",
        "\n",
        "generator = torch.Generator(device=device)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    generator=generator,\n",
        "    shuffle=shuffle,\n",
        "     drop_last=True,\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    generator=generator,\n",
        "    drop_last=True ,\n",
        "    shuffle=shuffle,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwZHkYst4Ink",
        "outputId": "45cc29ff-3af8-4a80-ba50-38772a4a2df4"
      },
      "outputs": [],
      "source": [
        "\n",
        "spec, texts = next(iter(train_dataloader))\n",
        "\n",
        "texts.shape\n",
        "# spec.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpOc27ctr8iM",
        "outputId": "8a1a1259-53ed-4c11-e217-f386520a9cc6"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "len(S_dB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCdlgZpeY3gm",
        "outputId": "ee223675-6fbf-4a97-e66b-10dc75604010"
      },
      "outputs": [],
      "source": [
        "len(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kzXk76ULY3gm"
      },
      "outputs": [],
      "source": [
        "from typing import Sequence\n",
        "\n",
        "\n",
        "class PositionEmbeddings(nn.Module):\n",
        "    def __init__(self):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.d_model = embeddings_dims\n",
        "        self.i = torch.arange(0, embeddings_dims, dtype=torch.float32, device=device)\n",
        "        # self.pos = torch.arange(0, block_size, dtype=torch.float32)\n",
        "        self.exp = ((2 * self.i)) / self.d_model\n",
        "        self.theta = 10000 ** self.exp\n",
        "        # print(self.theta.shape)\n",
        "        self.x_reshaped = torch.randn(batch_size, block_size, embeddings_dims, device=device, dtype=torch.float32)\n",
        "\n",
        "        self.cos = torch.cos((self.i / self.theta))\n",
        "        self.sin = torch.sin((self.i / self.theta))\n",
        "\n",
        "        self.even = self.sin[::2]\n",
        "        self.odd = self.cos[1::2]\n",
        "\n",
        "        # self.block = torch.empty((odd.size(0) + even.size(0),), dtype=self.even.dtype)\n",
        "        self.x_reshaped[..., : , ::2] = self.even\n",
        "        self.x_reshaped[..., : , 1::2] = self.odd\n",
        "\n",
        "\n",
        "\n",
        "    def pe_for_inference(self, x):\n",
        "\n",
        "            batch_size, seq_len, embeddings_dims = x.shape\n",
        "\n",
        "            self.d_model = embeddings_dims\n",
        "            self.i = torch.arange(0, embeddings_dims, dtype=torch.float32)\n",
        "            # self.pos = torch.arange(0, block_size, dtype=torch.float32)\n",
        "            self.exp = ((2 * self.i)) / self.d_model\n",
        "            self.theta = 10000 ** self.exp\n",
        "            # print(self.theta.shape)\n",
        "            x_reshaped = x.view(batch_size, seq_len, embeddings_dims)\n",
        "\n",
        "            self.cos = torch.cos((self.i / self.theta))\n",
        "            self.sin = torch.sin((self.i / self.theta))\n",
        "\n",
        "            self.even = self.sin[::2]\n",
        "            self.odd = self.cos[1::2]\n",
        "\n",
        "            # self.block = torch.empty((odd.size(0) + even.size(0),), dtype=self.even.dtype)\n",
        "            x_reshaped[..., : , ::2] = self.even\n",
        "            x_reshaped[..., : , 1::2] = self.odd\n",
        "\n",
        "    def forward(self, x, inference=False):\n",
        "\n",
        "        if(inference):\n",
        "            x = self.pe_for_inference(x)\n",
        "            return x\n",
        "        else:\n",
        "            out = self.x_reshaped\n",
        "            return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qth0bzoGY3gm"
      },
      "outputs": [],
      "source": [
        "# c = torch.arange(0, block_size)\n",
        "# odd = c[1::2]\n",
        "# even = c[::2]\n",
        "# res = torch.empty((odd.size(0) + even.size(0),), dtype=odd.dtype)\n",
        "# res[::2] = even\n",
        "# res[1::2] = odd\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T23:41:38.044041Z",
          "iopub.status.busy": "2024-12-30T23:41:38.043739Z",
          "iopub.status.idle": "2024-12-30T23:41:38.053157Z",
          "shell.execute_reply": "2024-12-30T23:41:38.052522Z",
          "shell.execute_reply.started": "2024-12-30T23:41:38.044011Z"
        },
        "id": "R9CSiuD2jHyT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Text embeddings\n",
        "class TgtTextEmbeddings(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size = tgt_vocab_size,\n",
        "        embeddings_dims = embeddings_dims\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embeddings_table = nn.Embedding(num_embeddings = tgt_vocab_size, embedding_dim=embeddings_dims, device=device) #Just a look up table to convert the toekns_ids to some numbers\n",
        "        # nn.init.normal_(self.embeddings_table.weight.data, mean=0, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embeddings_table(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T23:41:38.069221Z",
          "iopub.status.busy": "2024-12-30T23:41:38.068924Z",
          "iopub.status.idle": "2024-12-30T23:41:38.079344Z",
          "shell.execute_reply": "2024-12-30T23:41:38.078634Z",
          "shell.execute_reply.started": "2024-12-30T23:41:38.069166Z"
        },
        "id": "REUDHWrWcuoN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "#Layer Normalization\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embeddings_dims = embeddings_dims\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(normalized_shape=embeddings_dims)\n",
        "    def forward(self, x):\n",
        "\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T23:41:38.080590Z",
          "iopub.status.busy": "2024-12-30T23:41:38.080282Z",
          "iopub.status.idle": "2024-12-30T23:41:38.093224Z",
          "shell.execute_reply": "2024-12-30T23:41:38.092377Z",
          "shell.execute_reply.started": "2024-12-30T23:41:38.080556Z"
        },
        "id": "lEe02cH9cuoN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#FeedForward Neural Network\n",
        "\n",
        "class MLPBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dropout = dropout,\n",
        "        embeddings_size = embeddings_dims,\n",
        "        # inner_dimensional_states: int = 3072\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(device=device, in_features=embeddings_size, out_features= 4 * embeddings_dims),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(device=device, in_features= 4 * embeddings_dims, out_features=embeddings_size),\n",
        "            nn.Dropout(p = dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # mlp_weights_init = self.mlp.apply(weights_init)\n",
        "        return self.mlp(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T23:41:38.094455Z",
          "iopub.status.busy": "2024-12-30T23:41:38.094129Z",
          "iopub.status.idle": "2024-12-30T23:41:38.107217Z",
          "shell.execute_reply": "2024-12-30T23:41:38.106611Z",
          "shell.execute_reply.started": "2024-12-30T23:41:38.094424Z"
        },
        "id": "cf0Jf_7UcuoN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class MaskedAttentionHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.head_size = embeddings_dims // no_of_heads\n",
        "        self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device, bias=False)\n",
        "        self.keys = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,device=device, bias=False)\n",
        "        self.values = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device,bias=False)\n",
        "        self.dropout = nn.Dropout(p = attn_dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        batch, block_size, embd_dims = x.shape\n",
        "        k = self.keys(x)\n",
        "        q = self.query(x)\n",
        "        v = self.values(x)\n",
        "        masked_table = torch.tril(torch.ones(block_size, block_size, device=device))\n",
        "        weights = q @ torch.transpose(k, dim0=-2, dim1=-1) * (k.shape[-1] ** -0.5)\n",
        "        masked_values = weights.masked_fill(masked_table[: block_size, : block_size] == 0, float('-inf'))\n",
        "        weights_normalized = nn.functional.softmax(masked_values, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
        "        weights_normalized = self.dropout(weights_normalized)\n",
        "        out = weights_normalized @ v\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T23:41:38.108386Z",
          "iopub.status.busy": "2024-12-30T23:41:38.108072Z",
          "iopub.status.idle": "2024-12-30T23:41:38.122997Z",
          "shell.execute_reply": "2024-12-30T23:41:38.122019Z",
          "shell.execute_reply.started": "2024-12-30T23:41:38.108365Z"
        },
        "id": "OUFERSL2u8LT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class MaskedMHA(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([MaskedAttentionHead(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads) for _ in range(no_of_heads)])\n",
        "        self.dropout = nn.Dropout(p = attn_dropout)\n",
        "        self.linear = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=device, bias=False) # 12 (no of heads) * (batch_size) 64 = 768 -> gives out the text embeddings\n",
        "\n",
        "    def forward(self, x):\n",
        "        concat = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "        linear_layer = self.linear(concat)\n",
        "        out = self.dropout(linear_layer)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T23:41:38.124170Z",
          "iopub.status.busy": "2024-12-30T23:41:38.123871Z",
          "iopub.status.idle": "2024-12-30T23:41:38.136893Z",
          "shell.execute_reply": "2024-12-30T23:41:38.136282Z",
          "shell.execute_reply.started": "2024-12-30T23:41:38.124133Z"
        },
        "id": "oGGyyF4pjHyd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "#Single Attention Head\n",
        "\n",
        "class CrossAttentionHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.head_size = embeddings_dims // no_of_heads\n",
        "        self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device, bias=False)\n",
        "        self.keys = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,device=device, bias=False)\n",
        "        self.values = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device,bias=False)\n",
        "        self.dropout = nn.Dropout(p = attn_dropout)\n",
        "\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # batch, block_size, embd_dims = x.shape\n",
        "\n",
        "        # masked_table = torch.tril(torch.ones(block_size, block_size, device=device))\n",
        "        weights = query @ torch.transpose(key, dim0=-2, dim1=-1) * (key.shape[-1] ** -0.5)\n",
        "        if(mask != None):\n",
        "            mask = mask.unsqueeze(1)\n",
        "            masked_values = weights.masked_fill(mask == 0, float('-inf'))\n",
        "            weights_normalized = nn.functional.softmax(masked_values, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
        "            # weights_normalized = self.dropout(weights_normalized)\n",
        "            out = weights_normalized @ value\n",
        "            out = self.dropout(out)\n",
        "            return out\n",
        "        else:\n",
        "            weights_normalized = nn.functional.softmax(weights, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
        "            # weights_normalized = self.dropout(weights_normalized)\n",
        "            out = weights_normalized @ value\n",
        "            out = self.dropout(out)\n",
        "            return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T23:41:38.138084Z",
          "iopub.status.busy": "2024-12-30T23:41:38.137820Z",
          "iopub.status.idle": "2024-12-30T23:41:38.151910Z",
          "shell.execute_reply": "2024-12-30T23:41:38.150743Z",
          "shell.execute_reply.started": "2024-12-30T23:41:38.138064Z"
        },
        "id": "U5NmszzcjHyf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Single Attention Head\n",
        "\n",
        "class FullAttentionHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.head_size = embeddings_dims // no_of_heads\n",
        "        self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device, bias=False)\n",
        "        self.keys = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,device=device, bias=False)\n",
        "        self.values = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device,bias=False)\n",
        "        self.dropout = nn.Dropout(p = attn_dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # batch, block_size, embd_dims = x.shape\n",
        "        k = self.keys(x)\n",
        "        q = self.query(x)\n",
        "        v = self.values(x)\n",
        "        # masked_table = torch.tril(torch.ones(block_size, block_size, device=device))\n",
        "        weights = q @ torch.transpose(k, dim0=-2, dim1=-1) * (k.shape[-1] ** -0.5)\n",
        "        if(mask != None):\n",
        "            mask = mask.unsqueeze(1)\n",
        "            masked_values = weights.masked_fill(mask == 0, float('-inf'))\n",
        "            weights_normalized = nn.functional.softmax(masked_values, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
        "            # weights_normalized = self.dropout(weights_normalized)\n",
        "            out = weights_normalized @ v\n",
        "            out = self.dropout(out)\n",
        "            return out\n",
        "        else:\n",
        "            weights_normalized = nn.functional.softmax(weights, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
        "            # weights_normalized = self.dropout(weights_normalized)\n",
        "            out = weights_normalized @ v\n",
        "            out = self.dropout(out)\n",
        "            return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T23:41:38.153253Z",
          "iopub.status.busy": "2024-12-30T23:41:38.152929Z",
          "iopub.status.idle": "2024-12-30T23:41:38.167755Z",
          "shell.execute_reply": "2024-12-30T23:41:38.166944Z",
          "shell.execute_reply.started": "2024-12-30T23:41:38.153220Z"
        },
        "id": "v_BB7r7kqmOc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "class FullMHA(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([FullAttentionHead(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads) for _ in range(no_of_heads)])\n",
        "        self.dropout = nn.Dropout(p = attn_dropout)\n",
        "        self.linear = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=device, bias=False) # 12 (no of heads) * (batch_size) 64 = 768 -> gives out the text embeddings\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        concat = torch.cat([head(x, mask) for head in self.heads], dim=-1)\n",
        "        linear_layer = self.linear(concat)\n",
        "        out = self.dropout(linear_layer)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T23:41:38.169064Z",
          "iopub.status.busy": "2024-12-30T23:41:38.168778Z",
          "iopub.status.idle": "2024-12-30T23:41:38.193706Z",
          "shell.execute_reply": "2024-12-30T23:41:38.192606Z",
          "shell.execute_reply.started": "2024-12-30T23:41:38.169035Z"
        },
        "id": "TTwRkBzcvE-_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class CrossMHA(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([CrossAttentionHead(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads) for _ in range(no_of_heads)])\n",
        "        self.dropout = nn.Dropout(p = attn_dropout)\n",
        "        self.linear = nn.Linear(in_features=no_of_decoder_layers * embeddings_dims, out_features=embeddings_dims, device=device, bias=False) # 12 (no of heads) * (batch_size) 64 = 768 -> gives out the text embeddings\n",
        "\n",
        "    def forward(self, query, key, x, mask=None):\n",
        "        concat = torch.cat([head(query, key, x,  mask) for head in self.heads], dim=-1)\n",
        "        linear_layer = self.linear(concat)\n",
        "        out = self.dropout(linear_layer)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T23:41:38.194950Z",
          "iopub.status.busy": "2024-12-30T23:41:38.194673Z",
          "iopub.status.idle": "2024-12-30T23:41:38.201695Z",
          "shell.execute_reply": "2024-12-30T23:41:38.200801Z",
          "shell.execute_reply.started": "2024-12-30T23:41:38.194926Z"
        },
        "id": "s9rJzO_XcuoO",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Decoder Block\n",
        "\n",
        "class TransformerDecoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "        dropout = dropout,\n",
        "        # vocab_size = vocab_size\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cross = CrossMHA(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads)\n",
        "        self.masked = MaskedMHA(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads)\n",
        "        self.layer_norm1 = LayerNormalization(embeddings_dims)\n",
        "        self.layer_norm2 = LayerNormalization(embeddings_dims)\n",
        "        # self.layer_norm3 = LayerNormalization(embeddings_dims=embeddings_dims)\n",
        "        self.layer_norm4 = LayerNormalization(embeddings_dims)\n",
        "        self.mlp_block = MLPBlock(dropout=dropout, embeddings_size=embeddings_dims)\n",
        "\n",
        "    def forward(self, key, value, x, mask=None):\n",
        "        x = self.layer_norm1(x + self.masked(x)) #Very important step -> Layer Norm on input and then passes it to the subsequent blocks\n",
        "        x = self.layer_norm2(x + self.cross(value, key, x, mask)) #Very important step\n",
        "        # x = x + self.mha(self.layer_norm1(x))  #Very important step -> Layer Norm on input and then passes it to the subsequent blocks\n",
        "        x = self.layer_norm4(x + self.mlp_block(x)) #Very important step\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T23:41:38.206345Z",
          "iopub.status.busy": "2024-12-30T23:41:38.206020Z",
          "iopub.status.idle": "2024-12-30T23:41:38.223220Z",
          "shell.execute_reply": "2024-12-30T23:41:38.222390Z",
          "shell.execute_reply.started": "2024-12-30T23:41:38.206310Z"
        },
        "id": "KGh8ujQJcuoO",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Decoder Block\n",
        "\n",
        "class DecoderModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "        block_size = block_size,\n",
        "        dropout = dropout,\n",
        "        no_of_decoder_layers = no_of_decoder_layers,\n",
        "        # vocab_size = vocab_size\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.tgt_text_embds = TgtTextEmbeddings(vocab_size=tgt_vocab_size, embeddings_dims=embeddings_dims)\n",
        "        self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=tgt_vocab_size, device=device, bias=False) # Takes in logits of dimensions- embeds_dims and converts it into dimension of vocab_size (logits in range of vocab_size)\n",
        "        # self.layer_norm = LayerNormalization(embeddings_dims=embeddings_dims)\n",
        "        self.decoder_layers = nn.ModuleList([TransformerDecoderBlock(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads, dropout=dropout) for _ in range(no_of_decoder_layers)])\n",
        "        self.apply(self._init_weights)\n",
        "        # self.positional_embeddings_tgt = nn.Parameter(torch.randn(1, block_size, embeddings_dims, device=device), requires_grad=True) #To give positional embeddings to each token of the input text, hence num_embeddings=block_size\n",
        "        self.positional_embeddings_tgt = PositionEmbeddings()\n",
        "        # torch.nn.init.normal_(self.positional_embeddings_tgt, mean=0.0, std=0.02)\n",
        "\n",
        "        # out = self.decoder_layers(query, key, x)\n",
        "        # Loop through each decoder layer\n",
        "    def _init_weights(self, module):  #Weight Initialization\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "                if module.bias is not None:\n",
        "                    torch.nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, key, value, x, mask):\n",
        "        # x = self.tgt_text_embds(x)\n",
        "        x = x + self.positional_embeddings_tgt(x)\n",
        "        for decoder_layer in self.decoder_layers:\n",
        "            x = decoder_layer(key, value, x, mask)\n",
        "        # x = self.layer_norm(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T23:41:38.224795Z",
          "iopub.status.busy": "2024-12-30T23:41:38.224514Z",
          "iopub.status.idle": "2024-12-30T23:41:38.237595Z",
          "shell.execute_reply": "2024-12-30T23:41:38.236768Z",
          "shell.execute_reply.started": "2024-12-30T23:41:38.224767Z"
        },
        "id": "A3SgKrC-jHyd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "#Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T23:41:38.238697Z",
          "iopub.status.busy": "2024-12-30T23:41:38.238404Z",
          "iopub.status.idle": "2024-12-30T23:41:38.247055Z",
          "shell.execute_reply": "2024-12-30T23:41:38.246135Z",
          "shell.execute_reply.started": "2024-12-30T23:41:38.238669Z"
        },
        "id": "v6mbbO3yp-gh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "        dropout = dropout,\n",
        "        mask=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mha = FullMHA(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads)\n",
        "        self.layer_norm1 = LayerNormalization(embeddings_dims)\n",
        "        self.layer_norm2 = LayerNormalization(embeddings_dims)\n",
        "        self.mlp_block = MLPBlock(dropout=dropout, embeddings_size=embeddings_dims)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.layer_norm1(x + self.mha(x, mask))\n",
        "        x = self.layer_norm2(x + self.mlp_block(x))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T23:41:38.248257Z",
          "iopub.status.busy": "2024-12-30T23:41:38.247952Z",
          "iopub.status.idle": "2024-12-30T23:41:38.262347Z",
          "shell.execute_reply": "2024-12-30T23:41:38.261499Z",
          "shell.execute_reply.started": "2024-12-30T23:41:38.248229Z"
        },
        "id": "HxW0pvnV12Ms",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class EncoderModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "        block_size = block_size,\n",
        "        dropout = dropout,\n",
        "        no_of_decoder_layers = no_of_decoder_layers,\n",
        "        # vocab_size = vocab_size\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        # self.positional_embeddings_src = nn.Parameter(torch.randn(1, block_size, embeddings_dims, device=device), requires_grad=True) #To give positional embeddings to each token of the input text, hence num_embeddings=block_size\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels=n_channels, out_channels=embeddings_dims, kernel_size=kernel_size, device=device, padding=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=embeddings_dims, out_channels=embeddings_dims, kernel_size=kernel_size, device=device, padding=1)\n",
        "\n",
        "        self.positional_embeddings_src = PositionEmbeddings()\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([TransformerEncoderBlock(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads, dropout=dropout) for _ in range(no_of_decoder_layers)])\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):  #Weight Initialization\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "                if module.bias is not None:\n",
        "                    torch.nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = torch.nn.functional.gelu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = torch.nn.functional.gelu(x)\n",
        "        # print(x.shape)\n",
        "        # x = self.src_text_embeds(x)\n",
        "        # print(self.positional_embeddings_src.shape)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        # print(x.shape)\n",
        "        # print(self.positional_embeddings_src(x).shape)\n",
        "        x = x + self.positional_embeddings_src(x)\n",
        "\n",
        "        # print(x.shape)\n",
        "        # Loop through each encoder layer\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            x = encoder_layer(x, mask)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Qi3v6jczY3go"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T23:41:38.263631Z",
          "iopub.status.busy": "2024-12-30T23:41:38.263313Z",
          "iopub.status.idle": "2024-12-30T23:41:38.278543Z",
          "shell.execute_reply": "2024-12-30T23:41:38.277881Z",
          "shell.execute_reply.started": "2024-12-30T23:41:38.263563Z"
        },
        "id": "2UWijIFl2Ykd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = EncoderModel()\n",
        "        self.decoder = DecoderModel()\n",
        "        self.tgt_text_embds = TgtTextEmbeddings(vocab_size=tgt_vocab_size, embeddings_dims=embeddings_dims)\n",
        "        self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=tgt_vocab_size, device=device, bias=False) # Takes in logits of dimensions- embeds_dims and converts it into dimension of vocab_size (logits in range of vocab_size)\n",
        "        # self.src_text_embeds = SrcTextEmbeddings(vocab_size=src_vocab_size, embeddings_dims=embeddings_dims)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        # x = self.src_text_embeds(src)\n",
        "        x = self.encoder(src, src_mask)\n",
        "        y = self.tgt_text_embds(tgt)\n",
        "        y = self.decoder(x, x, y, None)\n",
        "        out = self.linear_layer(y)\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T23:41:38.279371Z",
          "iopub.status.busy": "2024-12-30T23:41:38.279081Z",
          "iopub.status.idle": "2024-12-30T23:41:38.562125Z",
          "shell.execute_reply": "2024-12-30T23:41:38.561113Z",
          "shell.execute_reply.started": "2024-12-30T23:41:38.279345Z"
        },
        "id": "ntIaQj1U3pFX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Instantiating the model\n",
        "model = Transformer()\n",
        "model = torch.compile(model)\n",
        "# model = model.to(device)\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-12-30T23:41:38.571435Z",
          "iopub.status.busy": "2024-12-30T23:41:38.571114Z",
          "iopub.status.idle": "2024-12-30T23:41:42.449903Z",
          "shell.execute_reply": "2024-12-30T23:41:42.448944Z",
          "shell.execute_reply.started": "2024-12-30T23:41:38.571405Z"
        },
        "id": "yOXtmG-lcuoO",
        "outputId": "00697eb7-4384-4bca-82bf-100f45e5216f",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "\n",
        "spec, text = next(iter(train_dataloader))\n",
        "spec = spec.to(device)\n",
        "texts = text.to(device)\n",
        "\n",
        "summary(model=model,\n",
        "        input_data=(spec, texts),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T23:41:42.451139Z",
          "iopub.status.busy": "2024-12-30T23:41:42.450897Z",
          "iopub.status.idle": "2024-12-30T23:41:43.142042Z",
          "shell.execute_reply": "2024-12-30T23:41:43.141341Z",
          "shell.execute_reply.started": "2024-12-30T23:41:42.451116Z"
        },
        "id": "LH95cJEvcuoO",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Optimizer setup and scheduler steup\n",
        "# out = {\"Train\": None, \"val\": None}\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "bbvONdUTWmvL"
      },
      "outputs": [],
      "source": [
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "scaler = torch.amp.GradScaler(enabled=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "execution": {
          "iopub.execute_input": "2024-12-30T23:41:43.143397Z",
          "iopub.status.busy": "2024-12-30T23:41:43.142879Z",
          "iopub.status.idle": "2024-12-31T00:20:53.976987Z",
          "shell.execute_reply": "2024-12-31T00:20:53.976269Z",
          "shell.execute_reply.started": "2024-12-30T23:41:43.143371Z"
        },
        "id": "nPrSPPu8cuoO",
        "outputId": "7ba86e12-999d-4b31-9a2b-53d645357d06",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.train()\n",
        "train_losses =  torch.zeros(len(train_dataloader))\n",
        "val_losses = torch.zeros(len(val_dataloader))\n",
        "wandb.init(\n",
        "    project='Whisper-From-Scratch'\n",
        ")\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    count = 0\n",
        "    print(\"Starting train...\")\n",
        "\n",
        "    for X, y in train_dataloader:\n",
        "      with torch.autocast(device_type=device, dtype=torch.float16):\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        logits = model(X, y)\n",
        "        # print(logits.shape)\n",
        "\n",
        "        batch_size, block_size, vocab = logits.shape\n",
        "        # print(\"Va: \", vocab)\n",
        "        logits = logits.view(batch_size*block_size, vocab)\n",
        "        targets = y.view(batch_size * block_size)\n",
        "        # print(\"HiiiL \", en.shape)\n",
        "        # print(\"HiiiT \", logits.shape)\n",
        "        loss = nn.functional.cross_entropy(logits, targets, ignore_index=tokenizer.pad_token_id)\n",
        "        train_losses[count] = loss.item()\n",
        "        # print(\"Loss: \", loss.item())\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      scaler.scale(loss).backward()\n",
        "      # loss.backward()\n",
        "      # optimizer.step()\n",
        "      scaler.step(optimizer)\n",
        "      scaler.update()\n",
        "      count += 1\n",
        "        # print()\n",
        "        # print(count)\n",
        "\n",
        "\n",
        "    # count = 0\n",
        "    model.eval()\n",
        "    count = 0\n",
        "    print(\"Starting val...\")\n",
        "    for X, y in val_dataloader:\n",
        "\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        logits = model(X,y)\n",
        "        # print(logits.shape)\n",
        "        batch_size, block_size, vocab = logits.shape\n",
        "\n",
        "        logits = logits.view(batch_size*block_size, vocab)\n",
        "        # print(\"Va: \", vocab)\n",
        "        targets = y.view(batch_size * block_size)\n",
        "        loss = nn.functional.cross_entropy(logits, targets, ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "        # print(\"Loss: \", loss.item())\n",
        "        val_losses[count] = loss.item()\n",
        "\n",
        "        # optimizer.zero_grad()\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "        count += 1\n",
        "\n",
        "\n",
        "    # print(\"eval\")\n",
        "    # print(\"Generating text...\")\n",
        "    # generated_text = topk_sampling(model, 'Ich fahre heute mit dem Rad zur Schule', de_tokenizer, device=ModelArgs.device, max_length=50, top_k=50, temperature=1.0)\n",
        "\n",
        "    # print(generated_text)\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    wandb.log({\n",
        "      \"Train Loss\": train_losses.mean(),\n",
        "      \"Val Loss\": val_losses.mean(),\n",
        "      \"epoch\": epoch\n",
        "    })\n",
        "    print(\"Epoch: \", epoch, \"|\", \"Train Loss: \", train_losses.mean(),  \"|\", \"Val Loss: \", val_losses.mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "_4oLJprzmeHl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30823,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
