{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass \n",
    "class ModelArgs:\n",
    "    device = 'cuda'\n",
    "    no_of_neurons = 16\n",
    "    block_size = 16\n",
    "    batch_size = 16\n",
    "    dropout = 0.1\n",
    "    epoch = 50\n",
    "    max_lr = 1e-4\n",
    "    embedding_dims: int = 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrajceo2031\u001b[0m (\u001b[33mrentio\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device(ModelArgs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "num_samples = 10000 \n",
    "seq_length = ModelArgs.block_size  \n",
    "device = ModelArgs.device  \n",
    "\n",
    "\n",
    "t = torch.linspace(0, 100, num_samples + seq_length, device=device)\n",
    "data = torch.sin(t) \n",
    "# data = t\n",
    "\n",
    "X_tensor = torch.stack([data[i:i+seq_length] for i in range(num_samples)])\n",
    "y_tensor = data[seq_length:]  # Next value prediction\n",
    "\n",
    "train_size = int(0.8 * num_samples)\n",
    "\n",
    "X_train, y_train = X_tensor[:train_size], y_tensor[:train_size]  \n",
    "X_val, y_val = X_tensor[train_size:], y_tensor[train_size:]  \n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "val_dataset = TimeSeriesDataset(X_val, y_val)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "generator = torch.Generator(device=device)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=ModelArgs.batch_size,\n",
    "    shuffle=True,  \n",
    "    generator=generator,  \n",
    "    # drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    generator=generator, \n",
    "    # drop_last=True,\n",
    "    batch_size=ModelArgs.batch_size,\n",
    "    shuffle=True, \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "    def __init__(self, device, no_of_neurons):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_layer = nn.Linear(in_features=ModelArgs.block_size + 1, out_features=no_of_neurons, device=ModelArgs.device)\n",
    "        \n",
    "    def forward(self, x, ht_1):\n",
    "        x = self.linear_layer(torch.cat([x, ht_1], dim=-1))\n",
    "        ht = torch.nn.functional.sigmoid(x)\n",
    "        return ht\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLayer(nn.Module):\n",
    "    def __init__(self, device, no_of_neurons):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn_layer = RNNCell(device=device, no_of_neurons=no_of_neurons)\n",
    "        self.linear_layer = nn.Linear(in_features=ModelArgs.block_size, out_features=no_of_neurons, device=ModelArgs.device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        ht_1 = torch.zeros((ModelArgs.batch_size, ModelArgs.block_size), device=ModelArgs.device, requires_grad=True, dtype=torch.float32)\n",
    "        \n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            xt = x[:, t]\n",
    "            xt = xt.unsqueeze(-1)\n",
    "            ht = self.rnn_layer(xt, ht_1)\n",
    "            ht_1 = ht\n",
    "            \n",
    "        return ht_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, device, no_of_neurons, out_features):\n",
    "        super().__init__()\n",
    "        self.rnn = RNNLayer(device=device, no_of_neurons=no_of_neurons)\n",
    "\n",
    "        self.output = nn.Linear(in_features=ModelArgs.no_of_neurons, out_features=out_features, device=device, dtype=torch.float32)\n",
    "        self.dropout = nn.Dropout(p=ModelArgs.dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        ht = self.rnn(x)\n",
    "        out = self.output(ht)\n",
    "        out = self.dropout(out)\n",
    "        return ht\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GRU(device=ModelArgs.device, no_of_neurons=ModelArgs.no_of_neurons, out_features=1)\n",
    "model = RNN(device=ModelArgs.device, no_of_neurons=ModelArgs.no_of_neurons, out_features=1)\n",
    "model = model.to(ModelArgs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /home/yuvrajsingh/anaconda3/envs/unsloth_env/lib/python3.11/site-packages (1.8.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "RNN (RNN)                                [16, 16]             [16, 16]             --                   True\n",
       "├─RNNLayer (rnn)                         [16, 16]             [16, 16]             272                  True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             --                   True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             288                  True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "├─Linear (output)                        [16, 16]             [16, 1]              17                   True\n",
       "├─Dropout (dropout)                      [16, 1]              [16, 1]              --                   --\n",
       "========================================================================================================================\n",
       "Total params: 577\n",
       "Trainable params: 577\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.07\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.03\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.04\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install torchinfo\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "x = torch.randint(0, 100, (ModelArgs.batch_size,ModelArgs.block_size))  # Random integer between 0 and 100\n",
    "x = x.to(ModelArgs.device)\n",
    "summary(model=model,\n",
    "        input_data=x,\n",
    "        # input_size=(ModelArgs.batch_size, ModelArgs.block_size, ModelArgs.embeddings_dims),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=ModelArgs.max_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrajceo2031\u001b[0m (\u001b[33mrentio\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac96aeb438fe4efcbe99469174006525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011443493633285269, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/yuvra/OneDrive/Desktop/Work/pytorch/Paper-Replications/RNNs/wandb/run-20250303_215325-b4gqd0jy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rentio/GRU-From-Scratch/runs/b4gqd0jy' target=\"_blank\">restful-forest-24</a></strong> to <a href='https://wandb.ai/rentio/GRU-From-Scratch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rentio/GRU-From-Scratch' target=\"_blank\">https://wandb.ai/rentio/GRU-From-Scratch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rentio/GRU-From-Scratch/runs/b4gqd0jy' target=\"_blank\">https://wandb.ai/rentio/GRU-From-Scratch/runs/b4gqd0jy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvrajsingh/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/utils/_device.py:106: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 16])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Step:  500 | Train Loss:  0.656163215637207\n",
      "Epoch:  0 | Step:  1 | Val Loss:  0.6380040049552917\n",
      "Epoch:  0 | Step:  2 | Val Loss:  0.6970006227493286\n",
      "Epoch:  0 | Step:  3 | Val Loss:  0.7294517159461975\n",
      "Epoch:  0 | Step:  4 | Val Loss:  0.9988241195678711\n",
      "Epoch:  0 | Step:  5 | Val Loss:  0.6275670528411865\n",
      "Epoch:  0 | Step:  6 | Val Loss:  0.4303639233112335\n",
      "Epoch:  0 | Step:  7 | Val Loss:  0.3101731240749359\n",
      "Epoch:  0 | Step:  8 | Val Loss:  0.8717292547225952\n",
      "Epoch:  0 | Step:  9 | Val Loss:  0.7538192272186279\n",
      "Epoch:  0 | Step:  10 | Val Loss:  0.766417920589447\n",
      "Epoch:  0 | Step:  11 | Val Loss:  0.7333325743675232\n",
      "Epoch:  0 | Step:  12 | Val Loss:  0.870871901512146\n",
      "Epoch:  0 | Step:  13 | Val Loss:  0.7366973161697388\n",
      "Epoch:  0 | Step:  14 | Val Loss:  0.3938369154930115\n",
      "Epoch:  0 | Step:  15 | Val Loss:  0.6606581211090088\n",
      "Epoch:  0 | Step:  16 | Val Loss:  0.8170782327651978\n",
      "Epoch:  0 | Step:  17 | Val Loss:  0.6676695346832275\n",
      "Epoch:  0 | Step:  18 | Val Loss:  0.7664165496826172\n",
      "Epoch:  0 | Step:  19 | Val Loss:  0.9173787236213684\n",
      "Epoch:  0 | Step:  20 | Val Loss:  0.635597825050354\n",
      "Epoch:  0 | Step:  21 | Val Loss:  0.8655332326889038\n",
      "Epoch:  0 | Step:  22 | Val Loss:  0.749855101108551\n",
      "Epoch:  0 | Step:  23 | Val Loss:  0.9031981229782104\n",
      "Epoch:  0 | Step:  24 | Val Loss:  1.1342976093292236\n",
      "Epoch:  0 | Step:  25 | Val Loss:  0.8630834817886353\n",
      "Epoch:  0 | Step:  26 | Val Loss:  0.7429576516151428\n",
      "Epoch:  0 | Step:  27 | Val Loss:  0.6375668048858643\n",
      "Epoch:  0 | Step:  28 | Val Loss:  0.771766185760498\n",
      "Epoch:  0 | Step:  29 | Val Loss:  0.6019240617752075\n",
      "Epoch:  0 | Step:  30 | Val Loss:  0.8756920099258423\n",
      "Epoch:  0 | Step:  31 | Val Loss:  0.561064600944519\n",
      "Epoch:  0 | Step:  32 | Val Loss:  0.7754737138748169\n",
      "Epoch:  0 | Step:  33 | Val Loss:  0.5967899560928345\n",
      "Epoch:  0 | Step:  34 | Val Loss:  1.0305947065353394\n",
      "Epoch:  0 | Step:  35 | Val Loss:  0.460729718208313\n",
      "Epoch:  0 | Step:  36 | Val Loss:  1.0294911861419678\n",
      "Epoch:  0 | Step:  37 | Val Loss:  0.9647067189216614\n",
      "Epoch:  0 | Step:  38 | Val Loss:  0.6893233060836792\n",
      "Epoch:  0 | Step:  39 | Val Loss:  0.8336751461029053\n",
      "Epoch:  0 | Step:  40 | Val Loss:  0.848983645439148\n",
      "Epoch:  0 | Step:  41 | Val Loss:  0.8474894762039185\n",
      "Epoch:  0 | Step:  42 | Val Loss:  0.5529791116714478\n",
      "Epoch:  0 | Step:  43 | Val Loss:  0.7073410749435425\n",
      "Epoch:  0 | Step:  44 | Val Loss:  0.8533482551574707\n",
      "Epoch:  0 | Step:  45 | Val Loss:  0.44815921783447266\n",
      "Epoch:  0 | Step:  46 | Val Loss:  0.6340157985687256\n",
      "Epoch:  0 | Step:  47 | Val Loss:  0.7860690355300903\n",
      "Epoch:  0 | Step:  48 | Val Loss:  0.6667389869689941\n",
      "Epoch:  0 | Step:  49 | Val Loss:  0.8650704622268677\n",
      "Epoch:  0 | Step:  50 | Val Loss:  0.7792069911956787\n",
      "Epoch:  0 | Step:  51 | Val Loss:  0.9089524149894714\n",
      "Epoch:  0 | Step:  52 | Val Loss:  0.816368579864502\n",
      "Epoch:  0 | Step:  53 | Val Loss:  0.7102339863777161\n",
      "Epoch:  0 | Step:  54 | Val Loss:  0.5626466870307922\n",
      "Epoch:  0 | Step:  55 | Val Loss:  0.7586139440536499\n",
      "Epoch:  0 | Step:  56 | Val Loss:  0.9098227024078369\n",
      "Epoch:  0 | Step:  57 | Val Loss:  0.44058966636657715\n",
      "Epoch:  0 | Step:  58 | Val Loss:  0.25696390867233276\n",
      "Epoch:  0 | Step:  59 | Val Loss:  0.5693632364273071\n",
      "Epoch:  0 | Step:  60 | Val Loss:  0.5500350594520569\n",
      "Epoch:  0 | Step:  61 | Val Loss:  0.6527854204177856\n",
      "Epoch:  0 | Step:  62 | Val Loss:  0.7153236865997314\n",
      "Epoch:  0 | Step:  63 | Val Loss:  0.5848033428192139\n",
      "Epoch:  0 | Step:  64 | Val Loss:  0.8350863456726074\n",
      "Epoch:  0 | Step:  65 | Val Loss:  0.5977410674095154\n",
      "Epoch:  0 | Step:  66 | Val Loss:  0.848732054233551\n",
      "Epoch:  0 | Step:  67 | Val Loss:  0.8492920994758606\n",
      "Epoch:  0 | Step:  68 | Val Loss:  0.8082090020179749\n",
      "Epoch:  0 | Step:  69 | Val Loss:  0.649474561214447\n",
      "Epoch:  0 | Step:  70 | Val Loss:  0.6217927932739258\n",
      "Epoch:  0 | Step:  71 | Val Loss:  0.7642439603805542\n",
      "Epoch:  0 | Step:  72 | Val Loss:  0.6793479919433594\n",
      "Epoch:  0 | Step:  73 | Val Loss:  0.9752345681190491\n",
      "Epoch:  0 | Step:  74 | Val Loss:  0.4601040482521057\n",
      "Epoch:  0 | Step:  75 | Val Loss:  0.6530185341835022\n",
      "Epoch:  0 | Step:  76 | Val Loss:  0.5967938303947449\n",
      "Epoch:  0 | Step:  77 | Val Loss:  0.7223306894302368\n",
      "Epoch:  0 | Step:  78 | Val Loss:  0.8536744117736816\n",
      "Epoch:  0 | Step:  79 | Val Loss:  0.712867021560669\n",
      "Epoch:  0 | Step:  80 | Val Loss:  0.8795564770698547\n",
      "Epoch:  0 | Step:  81 | Val Loss:  0.7547925710678101\n",
      "Epoch:  0 | Step:  82 | Val Loss:  0.6388753652572632\n",
      "Epoch:  0 | Step:  83 | Val Loss:  0.7540385723114014\n",
      "Epoch:  0 | Step:  84 | Val Loss:  0.7072365283966064\n",
      "Epoch:  0 | Step:  85 | Val Loss:  0.8460404872894287\n",
      "Epoch:  0 | Step:  86 | Val Loss:  0.719234824180603\n",
      "Epoch:  0 | Step:  87 | Val Loss:  0.8778978586196899\n",
      "Epoch:  0 | Step:  88 | Val Loss:  0.7109653353691101\n",
      "Epoch:  0 | Step:  89 | Val Loss:  0.5313617587089539\n",
      "Epoch:  0 | Step:  90 | Val Loss:  0.646870493888855\n",
      "Epoch:  0 | Step:  91 | Val Loss:  0.8152741193771362\n",
      "Epoch:  0 | Step:  92 | Val Loss:  0.780776858329773\n",
      "Epoch:  0 | Step:  93 | Val Loss:  0.6763038635253906\n",
      "Epoch:  0 | Step:  94 | Val Loss:  0.6513490676879883\n",
      "Epoch:  0 | Step:  95 | Val Loss:  0.7774693965911865\n",
      "Epoch:  0 | Step:  96 | Val Loss:  1.1034975051879883\n",
      "Epoch:  0 | Step:  97 | Val Loss:  0.6453271508216858\n",
      "Epoch:  0 | Step:  98 | Val Loss:  0.6251815557479858\n",
      "Epoch:  0 | Step:  99 | Val Loss:  0.5395767688751221\n",
      "Epoch:  0 | Step:  100 | Val Loss:  0.6770620346069336\n",
      "Epoch:  0 | Step:  101 | Val Loss:  0.7198775410652161\n",
      "Epoch:  0 | Step:  102 | Val Loss:  0.8005830645561218\n",
      "Epoch:  0 | Step:  103 | Val Loss:  0.559102475643158\n",
      "Epoch:  0 | Step:  104 | Val Loss:  0.6300143003463745\n",
      "Epoch:  0 | Step:  105 | Val Loss:  0.899992823600769\n",
      "Epoch:  0 | Step:  106 | Val Loss:  0.8372432589530945\n",
      "Epoch:  0 | Step:  107 | Val Loss:  0.7711073160171509\n",
      "Epoch:  0 | Step:  108 | Val Loss:  0.8226924538612366\n",
      "Epoch:  0 | Step:  109 | Val Loss:  0.3154466152191162\n",
      "Epoch:  0 | Step:  110 | Val Loss:  0.7551851868629456\n",
      "Epoch:  0 | Step:  111 | Val Loss:  0.5129020810127258\n",
      "Epoch:  0 | Step:  112 | Val Loss:  0.5468730926513672\n",
      "Epoch:  0 | Step:  113 | Val Loss:  0.4946310222148895\n",
      "Epoch:  0 | Step:  114 | Val Loss:  0.5955679416656494\n",
      "Epoch:  0 | Step:  115 | Val Loss:  1.1613926887512207\n",
      "Epoch:  0 | Step:  116 | Val Loss:  0.44992682337760925\n",
      "Epoch:  0 | Step:  117 | Val Loss:  0.8007798194885254\n",
      "Epoch:  0 | Step:  118 | Val Loss:  0.6711651086807251\n",
      "Epoch:  0 | Step:  119 | Val Loss:  0.7356034517288208\n",
      "Epoch:  0 | Step:  120 | Val Loss:  0.8425976037979126\n",
      "Epoch:  0 | Step:  121 | Val Loss:  0.9725484848022461\n",
      "Epoch:  0 | Step:  122 | Val Loss:  0.7873556613922119\n",
      "Epoch:  0 | Step:  123 | Val Loss:  0.9279757142066956\n",
      "Epoch:  0 | Step:  124 | Val Loss:  0.7165526151657104\n",
      "Epoch:  0 | Step:  125 | Val Loss:  0.7019851207733154\n",
      "Epoch:  0 | Train Loss:  tensor(0.6768, device='cuda:0') | Val Loss:  tensor(0.7244, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvrajsingh/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/utils/_device.py:106: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 16])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 | Step:  500 | Train Loss:  0.26001042127609253\n",
      "Epoch:  1 | Step:  1 | Val Loss:  0.8739071488380432\n",
      "Epoch:  1 | Step:  2 | Val Loss:  1.198178768157959\n",
      "Epoch:  1 | Step:  3 | Val Loss:  0.6544116735458374\n",
      "Epoch:  1 | Step:  4 | Val Loss:  0.5835412740707397\n",
      "Epoch:  1 | Step:  5 | Val Loss:  0.7006656527519226\n",
      "Epoch:  1 | Step:  6 | Val Loss:  0.8122824430465698\n",
      "Epoch:  1 | Step:  7 | Val Loss:  0.8781837821006775\n",
      "Epoch:  1 | Step:  8 | Val Loss:  0.9911659955978394\n",
      "Epoch:  1 | Step:  9 | Val Loss:  0.6607527732849121\n",
      "Epoch:  1 | Step:  10 | Val Loss:  0.9287918210029602\n",
      "Epoch:  1 | Step:  11 | Val Loss:  0.9807298183441162\n",
      "Epoch:  1 | Step:  12 | Val Loss:  0.5087670087814331\n",
      "Epoch:  1 | Step:  13 | Val Loss:  0.6317039728164673\n",
      "Epoch:  1 | Step:  14 | Val Loss:  0.6480879783630371\n",
      "Epoch:  1 | Step:  15 | Val Loss:  0.8496520519256592\n",
      "Epoch:  1 | Step:  16 | Val Loss:  1.100818157196045\n",
      "Epoch:  1 | Step:  17 | Val Loss:  0.6442431211471558\n",
      "Epoch:  1 | Step:  18 | Val Loss:  0.527889609336853\n",
      "Epoch:  1 | Step:  19 | Val Loss:  0.4295738935470581\n",
      "Epoch:  1 | Step:  20 | Val Loss:  0.5668150186538696\n",
      "Epoch:  1 | Step:  21 | Val Loss:  0.6551152467727661\n",
      "Epoch:  1 | Step:  22 | Val Loss:  0.8670835494995117\n",
      "Epoch:  1 | Step:  23 | Val Loss:  0.6077287793159485\n",
      "Epoch:  1 | Step:  24 | Val Loss:  0.6851209998130798\n",
      "Epoch:  1 | Step:  25 | Val Loss:  0.2987784147262573\n",
      "Epoch:  1 | Step:  26 | Val Loss:  0.8292942643165588\n",
      "Epoch:  1 | Step:  27 | Val Loss:  0.6943107843399048\n",
      "Epoch:  1 | Step:  28 | Val Loss:  0.46942612528800964\n",
      "Epoch:  1 | Step:  29 | Val Loss:  0.7762730121612549\n",
      "Epoch:  1 | Step:  30 | Val Loss:  0.46318453550338745\n",
      "Epoch:  1 | Step:  31 | Val Loss:  0.7431129217147827\n",
      "Epoch:  1 | Step:  32 | Val Loss:  0.8283737897872925\n",
      "Epoch:  1 | Step:  33 | Val Loss:  0.8093326687812805\n",
      "Epoch:  1 | Step:  34 | Val Loss:  0.7832667827606201\n",
      "Epoch:  1 | Step:  35 | Val Loss:  0.585905909538269\n",
      "Epoch:  1 | Step:  36 | Val Loss:  0.5005171298980713\n",
      "Epoch:  1 | Step:  37 | Val Loss:  0.9082852602005005\n",
      "Epoch:  1 | Step:  38 | Val Loss:  0.4724231958389282\n",
      "Epoch:  1 | Step:  39 | Val Loss:  0.7586219310760498\n",
      "Epoch:  1 | Step:  40 | Val Loss:  0.5053630471229553\n",
      "Epoch:  1 | Step:  41 | Val Loss:  0.7494902610778809\n",
      "Epoch:  1 | Step:  42 | Val Loss:  0.518008828163147\n",
      "Epoch:  1 | Step:  43 | Val Loss:  0.6097604632377625\n",
      "Epoch:  1 | Step:  44 | Val Loss:  0.6138279438018799\n",
      "Epoch:  1 | Step:  45 | Val Loss:  0.548721969127655\n",
      "Epoch:  1 | Step:  46 | Val Loss:  0.5812817811965942\n",
      "Epoch:  1 | Step:  47 | Val Loss:  0.4578937888145447\n",
      "Epoch:  1 | Step:  48 | Val Loss:  0.6056087017059326\n",
      "Epoch:  1 | Step:  49 | Val Loss:  0.6565384268760681\n",
      "Epoch:  1 | Step:  50 | Val Loss:  0.7514716982841492\n",
      "Epoch:  1 | Step:  51 | Val Loss:  0.794129490852356\n",
      "Epoch:  1 | Step:  52 | Val Loss:  0.5197311043739319\n",
      "Epoch:  1 | Step:  53 | Val Loss:  0.4207908511161804\n",
      "Epoch:  1 | Step:  54 | Val Loss:  0.813512921333313\n",
      "Epoch:  1 | Step:  55 | Val Loss:  0.9716134071350098\n",
      "Epoch:  1 | Step:  56 | Val Loss:  0.7216294407844543\n",
      "Epoch:  1 | Step:  57 | Val Loss:  0.4904431998729706\n",
      "Epoch:  1 | Step:  58 | Val Loss:  0.6033066511154175\n",
      "Epoch:  1 | Step:  59 | Val Loss:  0.8565082550048828\n",
      "Epoch:  1 | Step:  60 | Val Loss:  0.529769778251648\n",
      "Epoch:  1 | Step:  61 | Val Loss:  0.7842750549316406\n",
      "Epoch:  1 | Step:  62 | Val Loss:  0.5590744018554688\n",
      "Epoch:  1 | Step:  63 | Val Loss:  0.6495198607444763\n",
      "Epoch:  1 | Step:  64 | Val Loss:  0.8486791849136353\n",
      "Epoch:  1 | Step:  65 | Val Loss:  0.4737992286682129\n",
      "Epoch:  1 | Step:  66 | Val Loss:  0.47902002930641174\n",
      "Epoch:  1 | Step:  67 | Val Loss:  0.36533042788505554\n",
      "Epoch:  1 | Step:  68 | Val Loss:  0.5272877812385559\n",
      "Epoch:  1 | Step:  69 | Val Loss:  0.7777611613273621\n",
      "Epoch:  1 | Step:  70 | Val Loss:  0.8334071636199951\n",
      "Epoch:  1 | Step:  71 | Val Loss:  0.9696551561355591\n",
      "Epoch:  1 | Step:  72 | Val Loss:  0.9444524049758911\n",
      "Epoch:  1 | Step:  73 | Val Loss:  0.5621379017829895\n",
      "Epoch:  1 | Step:  74 | Val Loss:  0.7452300786972046\n",
      "Epoch:  1 | Step:  75 | Val Loss:  0.794743001461029\n",
      "Epoch:  1 | Step:  76 | Val Loss:  1.0060330629348755\n",
      "Epoch:  1 | Step:  77 | Val Loss:  0.7895478010177612\n",
      "Epoch:  1 | Step:  78 | Val Loss:  0.6006991863250732\n",
      "Epoch:  1 | Step:  79 | Val Loss:  0.8541220426559448\n",
      "Epoch:  1 | Step:  80 | Val Loss:  0.5180985927581787\n",
      "Epoch:  1 | Step:  81 | Val Loss:  0.48783794045448303\n",
      "Epoch:  1 | Step:  82 | Val Loss:  0.6995357275009155\n",
      "Epoch:  1 | Step:  83 | Val Loss:  0.7350360155105591\n",
      "Epoch:  1 | Step:  84 | Val Loss:  1.0043752193450928\n",
      "Epoch:  1 | Step:  85 | Val Loss:  0.5388918519020081\n",
      "Epoch:  1 | Step:  86 | Val Loss:  0.5073609352111816\n",
      "Epoch:  1 | Step:  87 | Val Loss:  0.7170588970184326\n",
      "Epoch:  1 | Step:  88 | Val Loss:  0.9110711812973022\n",
      "Epoch:  1 | Step:  89 | Val Loss:  0.8966516256332397\n",
      "Epoch:  1 | Step:  90 | Val Loss:  0.5287871360778809\n",
      "Epoch:  1 | Step:  91 | Val Loss:  0.8193597793579102\n",
      "Epoch:  1 | Step:  92 | Val Loss:  0.7915257215499878\n",
      "Epoch:  1 | Step:  93 | Val Loss:  0.7510925531387329\n",
      "Epoch:  1 | Step:  94 | Val Loss:  0.5885747075080872\n",
      "Epoch:  1 | Step:  95 | Val Loss:  1.020819902420044\n",
      "Epoch:  1 | Step:  96 | Val Loss:  0.931963324546814\n",
      "Epoch:  1 | Step:  97 | Val Loss:  0.6137748956680298\n",
      "Epoch:  1 | Step:  98 | Val Loss:  1.036301851272583\n",
      "Epoch:  1 | Step:  99 | Val Loss:  0.5905344486236572\n",
      "Epoch:  1 | Step:  100 | Val Loss:  0.9039855599403381\n",
      "Epoch:  1 | Step:  101 | Val Loss:  0.8673906922340393\n",
      "Epoch:  1 | Step:  102 | Val Loss:  0.6391440629959106\n",
      "Epoch:  1 | Step:  103 | Val Loss:  0.7025749683380127\n",
      "Epoch:  1 | Step:  104 | Val Loss:  0.6540371775627136\n",
      "Epoch:  1 | Step:  105 | Val Loss:  0.9811937808990479\n",
      "Epoch:  1 | Step:  106 | Val Loss:  0.6190875768661499\n",
      "Epoch:  1 | Step:  107 | Val Loss:  0.5384730696678162\n",
      "Epoch:  1 | Step:  108 | Val Loss:  0.6973167061805725\n",
      "Epoch:  1 | Step:  109 | Val Loss:  0.9018446207046509\n",
      "Epoch:  1 | Step:  110 | Val Loss:  0.7645286321640015\n",
      "Epoch:  1 | Step:  111 | Val Loss:  0.500949501991272\n",
      "Epoch:  1 | Step:  112 | Val Loss:  0.5026921033859253\n",
      "Epoch:  1 | Step:  113 | Val Loss:  0.7843056917190552\n",
      "Epoch:  1 | Step:  114 | Val Loss:  0.6746900081634521\n",
      "Epoch:  1 | Step:  115 | Val Loss:  0.357816219329834\n",
      "Epoch:  1 | Step:  116 | Val Loss:  0.6222130060195923\n",
      "Epoch:  1 | Step:  117 | Val Loss:  0.5996438264846802\n",
      "Epoch:  1 | Step:  118 | Val Loss:  0.6519830226898193\n",
      "Epoch:  1 | Step:  119 | Val Loss:  0.5687790513038635\n",
      "Epoch:  1 | Step:  120 | Val Loss:  1.0090268850326538\n",
      "Epoch:  1 | Step:  121 | Val Loss:  0.49951380491256714\n",
      "Epoch:  1 | Step:  122 | Val Loss:  0.667604386806488\n",
      "Epoch:  1 | Step:  123 | Val Loss:  0.8257513642311096\n",
      "Epoch:  1 | Step:  124 | Val Loss:  0.8206167221069336\n",
      "Epoch:  1 | Step:  125 | Val Loss:  0.9912877082824707\n",
      "Epoch:  1 | Train Loss:  tensor(0.6501, device='cuda:0') | Val Loss:  tensor(0.7026, device='cuda:0')\n",
      "Epoch:  2 | Step:  500 | Train Loss:  0.6360275745391846\n",
      "Epoch:  2 | Step:  1 | Val Loss:  0.6146892309188843\n",
      "Epoch:  2 | Step:  2 | Val Loss:  0.7597832679748535\n",
      "Epoch:  2 | Step:  3 | Val Loss:  0.9600443840026855\n",
      "Epoch:  2 | Step:  4 | Val Loss:  0.5425841808319092\n",
      "Epoch:  2 | Step:  5 | Val Loss:  0.5786073207855225\n",
      "Epoch:  2 | Step:  6 | Val Loss:  0.6109875440597534\n",
      "Epoch:  2 | Step:  7 | Val Loss:  0.49823516607284546\n",
      "Epoch:  2 | Step:  8 | Val Loss:  0.5584452152252197\n",
      "Epoch:  2 | Step:  9 | Val Loss:  0.6793356537818909\n",
      "Epoch:  2 | Step:  10 | Val Loss:  0.741950511932373\n",
      "Epoch:  2 | Step:  11 | Val Loss:  0.6169255375862122\n",
      "Epoch:  2 | Step:  12 | Val Loss:  0.5930550694465637\n",
      "Epoch:  2 | Step:  13 | Val Loss:  0.6181188821792603\n",
      "Epoch:  2 | Step:  14 | Val Loss:  0.8483544588088989\n",
      "Epoch:  2 | Step:  15 | Val Loss:  0.6060472726821899\n",
      "Epoch:  2 | Step:  16 | Val Loss:  0.7797094583511353\n",
      "Epoch:  2 | Step:  17 | Val Loss:  1.014134407043457\n",
      "Epoch:  2 | Step:  18 | Val Loss:  0.6232554316520691\n",
      "Epoch:  2 | Step:  19 | Val Loss:  0.860383152961731\n",
      "Epoch:  2 | Step:  20 | Val Loss:  0.7707081437110901\n",
      "Epoch:  2 | Step:  21 | Val Loss:  0.597008466720581\n",
      "Epoch:  2 | Step:  22 | Val Loss:  0.6551690697669983\n",
      "Epoch:  2 | Step:  23 | Val Loss:  0.46601492166519165\n",
      "Epoch:  2 | Step:  24 | Val Loss:  0.8282575011253357\n",
      "Epoch:  2 | Step:  25 | Val Loss:  0.8218398094177246\n",
      "Epoch:  2 | Step:  26 | Val Loss:  0.6282128095626831\n",
      "Epoch:  2 | Step:  27 | Val Loss:  0.6786715984344482\n",
      "Epoch:  2 | Step:  28 | Val Loss:  0.699097216129303\n",
      "Epoch:  2 | Step:  29 | Val Loss:  0.8710740804672241\n",
      "Epoch:  2 | Step:  30 | Val Loss:  0.5871091485023499\n",
      "Epoch:  2 | Step:  31 | Val Loss:  0.5137627124786377\n",
      "Epoch:  2 | Step:  32 | Val Loss:  0.6277570724487305\n",
      "Epoch:  2 | Step:  33 | Val Loss:  0.782687246799469\n",
      "Epoch:  2 | Step:  34 | Val Loss:  0.40692979097366333\n",
      "Epoch:  2 | Step:  35 | Val Loss:  0.7824670672416687\n",
      "Epoch:  2 | Step:  36 | Val Loss:  0.9292213916778564\n",
      "Epoch:  2 | Step:  37 | Val Loss:  0.7482848167419434\n",
      "Epoch:  2 | Step:  38 | Val Loss:  0.4293268918991089\n",
      "Epoch:  2 | Step:  39 | Val Loss:  0.7068788409233093\n",
      "Epoch:  2 | Step:  40 | Val Loss:  0.7138409614562988\n",
      "Epoch:  2 | Step:  41 | Val Loss:  0.6632323265075684\n",
      "Epoch:  2 | Step:  42 | Val Loss:  0.5225992798805237\n",
      "Epoch:  2 | Step:  43 | Val Loss:  0.4279942214488983\n",
      "Epoch:  2 | Step:  44 | Val Loss:  0.5222575664520264\n",
      "Epoch:  2 | Step:  45 | Val Loss:  0.6236570477485657\n",
      "Epoch:  2 | Step:  46 | Val Loss:  0.5944637060165405\n",
      "Epoch:  2 | Step:  47 | Val Loss:  0.7301366329193115\n",
      "Epoch:  2 | Step:  48 | Val Loss:  0.7781692743301392\n",
      "Epoch:  2 | Step:  49 | Val Loss:  0.835054874420166\n",
      "Epoch:  2 | Step:  50 | Val Loss:  0.8040897250175476\n",
      "Epoch:  2 | Step:  51 | Val Loss:  0.809702455997467\n",
      "Epoch:  2 | Step:  52 | Val Loss:  0.8102552890777588\n",
      "Epoch:  2 | Step:  53 | Val Loss:  0.5346311926841736\n",
      "Epoch:  2 | Step:  54 | Val Loss:  0.6167848706245422\n",
      "Epoch:  2 | Step:  55 | Val Loss:  0.8638075590133667\n",
      "Epoch:  2 | Step:  56 | Val Loss:  0.726455807685852\n",
      "Epoch:  2 | Step:  57 | Val Loss:  0.35654401779174805\n",
      "Epoch:  2 | Step:  58 | Val Loss:  0.3444805145263672\n",
      "Epoch:  2 | Step:  59 | Val Loss:  0.6363463997840881\n",
      "Epoch:  2 | Step:  60 | Val Loss:  0.6348139643669128\n",
      "Epoch:  2 | Step:  61 | Val Loss:  0.8218057751655579\n",
      "Epoch:  2 | Step:  62 | Val Loss:  0.6006553173065186\n",
      "Epoch:  2 | Step:  63 | Val Loss:  0.7357478141784668\n",
      "Epoch:  2 | Step:  64 | Val Loss:  0.5792028903961182\n",
      "Epoch:  2 | Step:  65 | Val Loss:  0.6375223398208618\n",
      "Epoch:  2 | Step:  66 | Val Loss:  0.8707634210586548\n",
      "Epoch:  2 | Step:  67 | Val Loss:  0.6868102550506592\n",
      "Epoch:  2 | Step:  68 | Val Loss:  0.6798640489578247\n",
      "Epoch:  2 | Step:  69 | Val Loss:  0.6120258569717407\n",
      "Epoch:  2 | Step:  70 | Val Loss:  0.44728997349739075\n",
      "Epoch:  2 | Step:  71 | Val Loss:  0.6821483373641968\n",
      "Epoch:  2 | Step:  72 | Val Loss:  0.6533077359199524\n",
      "Epoch:  2 | Step:  73 | Val Loss:  0.5774264931678772\n",
      "Epoch:  2 | Step:  74 | Val Loss:  0.7516242861747742\n",
      "Epoch:  2 | Step:  75 | Val Loss:  0.4223380386829376\n",
      "Epoch:  2 | Step:  76 | Val Loss:  0.5319570302963257\n",
      "Epoch:  2 | Step:  77 | Val Loss:  0.6636863350868225\n",
      "Epoch:  2 | Step:  78 | Val Loss:  0.6824349164962769\n",
      "Epoch:  2 | Step:  79 | Val Loss:  0.9146488308906555\n",
      "Epoch:  2 | Step:  80 | Val Loss:  0.6619572043418884\n",
      "Epoch:  2 | Step:  81 | Val Loss:  0.8127397894859314\n",
      "Epoch:  2 | Step:  82 | Val Loss:  0.8888963460922241\n",
      "Epoch:  2 | Step:  83 | Val Loss:  0.8127665519714355\n",
      "Epoch:  2 | Step:  84 | Val Loss:  0.6680175065994263\n",
      "Epoch:  2 | Step:  85 | Val Loss:  0.7838658690452576\n",
      "Epoch:  2 | Step:  86 | Val Loss:  0.6485954523086548\n",
      "Epoch:  2 | Step:  87 | Val Loss:  0.655167281627655\n",
      "Epoch:  2 | Step:  88 | Val Loss:  0.6672457456588745\n",
      "Epoch:  2 | Step:  89 | Val Loss:  0.8249835968017578\n",
      "Epoch:  2 | Step:  90 | Val Loss:  0.5826534628868103\n",
      "Epoch:  2 | Step:  91 | Val Loss:  0.8555446267127991\n",
      "Epoch:  2 | Step:  92 | Val Loss:  0.7923176288604736\n",
      "Epoch:  2 | Step:  93 | Val Loss:  1.0025582313537598\n",
      "Epoch:  2 | Step:  94 | Val Loss:  0.6237258911132812\n",
      "Epoch:  2 | Step:  95 | Val Loss:  0.7076493501663208\n",
      "Epoch:  2 | Step:  96 | Val Loss:  0.49475693702697754\n",
      "Epoch:  2 | Step:  97 | Val Loss:  0.6652994751930237\n",
      "Epoch:  2 | Step:  98 | Val Loss:  1.0070027112960815\n",
      "Epoch:  2 | Step:  99 | Val Loss:  0.6239240765571594\n",
      "Epoch:  2 | Step:  100 | Val Loss:  0.8246786594390869\n",
      "Epoch:  2 | Step:  101 | Val Loss:  0.6083498001098633\n",
      "Epoch:  2 | Step:  102 | Val Loss:  0.8014200925827026\n",
      "Epoch:  2 | Step:  103 | Val Loss:  0.6200485229492188\n",
      "Epoch:  2 | Step:  104 | Val Loss:  0.9664843678474426\n",
      "Epoch:  2 | Step:  105 | Val Loss:  0.8799558877944946\n",
      "Epoch:  2 | Step:  106 | Val Loss:  0.27212661504745483\n",
      "Epoch:  2 | Step:  107 | Val Loss:  0.753948450088501\n",
      "Epoch:  2 | Step:  108 | Val Loss:  0.6810041666030884\n",
      "Epoch:  2 | Step:  109 | Val Loss:  0.6922881603240967\n",
      "Epoch:  2 | Step:  110 | Val Loss:  0.7400016784667969\n",
      "Epoch:  2 | Step:  111 | Val Loss:  0.6546192169189453\n",
      "Epoch:  2 | Step:  112 | Val Loss:  0.8218395113945007\n",
      "Epoch:  2 | Step:  113 | Val Loss:  0.38153403997421265\n",
      "Epoch:  2 | Step:  114 | Val Loss:  0.3985136151313782\n",
      "Epoch:  2 | Step:  115 | Val Loss:  0.9133995771408081\n",
      "Epoch:  2 | Step:  116 | Val Loss:  0.6743682622909546\n",
      "Epoch:  2 | Step:  117 | Val Loss:  0.960033655166626\n",
      "Epoch:  2 | Step:  118 | Val Loss:  0.4846341609954834\n",
      "Epoch:  2 | Step:  119 | Val Loss:  0.6562893390655518\n",
      "Epoch:  2 | Step:  120 | Val Loss:  0.5088291168212891\n",
      "Epoch:  2 | Step:  121 | Val Loss:  0.625816822052002\n",
      "Epoch:  2 | Step:  122 | Val Loss:  0.7960546612739563\n",
      "Epoch:  2 | Step:  123 | Val Loss:  0.7299402952194214\n",
      "Epoch:  2 | Step:  124 | Val Loss:  0.710408091545105\n",
      "Epoch:  2 | Step:  125 | Val Loss:  0.36695852875709534\n",
      "Epoch:  2 | Train Loss:  tensor(0.6304, device='cuda:0') | Val Loss:  tensor(0.6803, device='cuda:0')\n",
      "Epoch:  3 | Step:  500 | Train Loss:  0.5869580507278442\n",
      "Epoch:  3 | Step:  1 | Val Loss:  0.6333967447280884\n",
      "Epoch:  3 | Step:  2 | Val Loss:  0.6930885314941406\n",
      "Epoch:  3 | Step:  3 | Val Loss:  0.8587231636047363\n",
      "Epoch:  3 | Step:  4 | Val Loss:  0.6610924005508423\n",
      "Epoch:  3 | Step:  5 | Val Loss:  0.647865891456604\n",
      "Epoch:  3 | Step:  6 | Val Loss:  0.5094012022018433\n",
      "Epoch:  3 | Step:  7 | Val Loss:  0.915630578994751\n",
      "Epoch:  3 | Step:  8 | Val Loss:  0.7292611598968506\n",
      "Epoch:  3 | Step:  9 | Val Loss:  0.7287960052490234\n",
      "Epoch:  3 | Step:  10 | Val Loss:  0.8039292693138123\n",
      "Epoch:  3 | Step:  11 | Val Loss:  0.5497035980224609\n",
      "Epoch:  3 | Step:  12 | Val Loss:  0.8768619298934937\n",
      "Epoch:  3 | Step:  13 | Val Loss:  0.634873628616333\n",
      "Epoch:  3 | Step:  14 | Val Loss:  0.5730322599411011\n",
      "Epoch:  3 | Step:  15 | Val Loss:  0.5869513154029846\n",
      "Epoch:  3 | Step:  16 | Val Loss:  0.5634922981262207\n",
      "Epoch:  3 | Step:  17 | Val Loss:  0.4663398861885071\n",
      "Epoch:  3 | Step:  18 | Val Loss:  0.4982531666755676\n",
      "Epoch:  3 | Step:  19 | Val Loss:  0.2815009355545044\n",
      "Epoch:  3 | Step:  20 | Val Loss:  0.5004462003707886\n",
      "Epoch:  3 | Step:  21 | Val Loss:  0.8273496031761169\n",
      "Epoch:  3 | Step:  22 | Val Loss:  0.6061711311340332\n",
      "Epoch:  3 | Step:  23 | Val Loss:  0.8073340654373169\n",
      "Epoch:  3 | Step:  24 | Val Loss:  0.3972586393356323\n",
      "Epoch:  3 | Step:  25 | Val Loss:  0.9272843599319458\n",
      "Epoch:  3 | Step:  26 | Val Loss:  0.7898555994033813\n",
      "Epoch:  3 | Step:  27 | Val Loss:  0.7089642286300659\n",
      "Epoch:  3 | Step:  28 | Val Loss:  0.7420345544815063\n",
      "Epoch:  3 | Step:  29 | Val Loss:  0.6466813087463379\n",
      "Epoch:  3 | Step:  30 | Val Loss:  0.57503342628479\n",
      "Epoch:  3 | Step:  31 | Val Loss:  0.8354353904724121\n",
      "Epoch:  3 | Step:  32 | Val Loss:  0.8962757587432861\n",
      "Epoch:  3 | Step:  33 | Val Loss:  0.4954832196235657\n",
      "Epoch:  3 | Step:  34 | Val Loss:  0.8977727890014648\n",
      "Epoch:  3 | Step:  35 | Val Loss:  0.8637402653694153\n",
      "Epoch:  3 | Step:  36 | Val Loss:  0.4040448069572449\n",
      "Epoch:  3 | Step:  37 | Val Loss:  0.7060835361480713\n",
      "Epoch:  3 | Step:  38 | Val Loss:  0.6475751996040344\n",
      "Epoch:  3 | Step:  39 | Val Loss:  0.4188082218170166\n",
      "Epoch:  3 | Step:  40 | Val Loss:  0.7068423628807068\n",
      "Epoch:  3 | Step:  41 | Val Loss:  0.40223219990730286\n",
      "Epoch:  3 | Step:  42 | Val Loss:  0.7946745157241821\n",
      "Epoch:  3 | Step:  43 | Val Loss:  0.7443324327468872\n",
      "Epoch:  3 | Step:  44 | Val Loss:  0.7339404821395874\n",
      "Epoch:  3 | Step:  45 | Val Loss:  0.5068178176879883\n",
      "Epoch:  3 | Step:  46 | Val Loss:  0.9379647970199585\n",
      "Epoch:  3 | Step:  47 | Val Loss:  0.5030317306518555\n",
      "Epoch:  3 | Step:  48 | Val Loss:  0.6744786500930786\n",
      "Epoch:  3 | Step:  49 | Val Loss:  0.6189868450164795\n",
      "Epoch:  3 | Step:  50 | Val Loss:  0.5581930875778198\n",
      "Epoch:  3 | Step:  51 | Val Loss:  0.7211812734603882\n",
      "Epoch:  3 | Step:  52 | Val Loss:  0.708181619644165\n",
      "Epoch:  3 | Step:  53 | Val Loss:  0.745011031627655\n",
      "Epoch:  3 | Step:  54 | Val Loss:  0.7739453911781311\n",
      "Epoch:  3 | Step:  55 | Val Loss:  0.640399158000946\n",
      "Epoch:  3 | Step:  56 | Val Loss:  0.7135205268859863\n",
      "Epoch:  3 | Step:  57 | Val Loss:  0.5855510234832764\n",
      "Epoch:  3 | Step:  58 | Val Loss:  0.6400247812271118\n",
      "Epoch:  3 | Step:  59 | Val Loss:  0.6701359748840332\n",
      "Epoch:  3 | Step:  60 | Val Loss:  0.8796308040618896\n",
      "Epoch:  3 | Step:  61 | Val Loss:  0.6355118751525879\n",
      "Epoch:  3 | Step:  62 | Val Loss:  0.9005499482154846\n",
      "Epoch:  3 | Step:  63 | Val Loss:  0.5362672209739685\n",
      "Epoch:  3 | Step:  64 | Val Loss:  0.5669394731521606\n",
      "Epoch:  3 | Step:  65 | Val Loss:  0.6802530884742737\n",
      "Epoch:  3 | Step:  66 | Val Loss:  0.7203205823898315\n",
      "Epoch:  3 | Step:  67 | Val Loss:  0.5869030356407166\n",
      "Epoch:  3 | Step:  68 | Val Loss:  0.6426982879638672\n",
      "Epoch:  3 | Step:  69 | Val Loss:  0.6783499717712402\n",
      "Epoch:  3 | Step:  70 | Val Loss:  0.4459109306335449\n",
      "Epoch:  3 | Step:  71 | Val Loss:  0.6344002485275269\n",
      "Epoch:  3 | Step:  72 | Val Loss:  0.4894850552082062\n",
      "Epoch:  3 | Step:  73 | Val Loss:  0.6506950855255127\n",
      "Epoch:  3 | Step:  74 | Val Loss:  0.4723302125930786\n",
      "Epoch:  3 | Step:  75 | Val Loss:  0.5366400480270386\n",
      "Epoch:  3 | Step:  76 | Val Loss:  0.6241397857666016\n",
      "Epoch:  3 | Step:  77 | Val Loss:  0.6435894966125488\n",
      "Epoch:  3 | Step:  78 | Val Loss:  0.6779344081878662\n",
      "Epoch:  3 | Step:  79 | Val Loss:  0.5871638059616089\n",
      "Epoch:  3 | Step:  80 | Val Loss:  0.6727081537246704\n",
      "Epoch:  3 | Step:  81 | Val Loss:  0.6505528688430786\n",
      "Epoch:  3 | Step:  82 | Val Loss:  0.6890138387680054\n",
      "Epoch:  3 | Step:  83 | Val Loss:  0.35466715693473816\n",
      "Epoch:  3 | Step:  84 | Val Loss:  0.6252867579460144\n",
      "Epoch:  3 | Step:  85 | Val Loss:  0.6936480402946472\n",
      "Epoch:  3 | Step:  86 | Val Loss:  0.46213144063949585\n",
      "Epoch:  3 | Step:  87 | Val Loss:  0.882828950881958\n",
      "Epoch:  3 | Step:  88 | Val Loss:  0.735941469669342\n",
      "Epoch:  3 | Step:  89 | Val Loss:  0.5725535154342651\n",
      "Epoch:  3 | Step:  90 | Val Loss:  0.6378311514854431\n",
      "Epoch:  3 | Step:  91 | Val Loss:  0.5337417721748352\n",
      "Epoch:  3 | Step:  92 | Val Loss:  0.9028114080429077\n",
      "Epoch:  3 | Step:  93 | Val Loss:  0.8039808869361877\n",
      "Epoch:  3 | Step:  94 | Val Loss:  0.6627117395401001\n",
      "Epoch:  3 | Step:  95 | Val Loss:  0.8147719502449036\n",
      "Epoch:  3 | Step:  96 | Val Loss:  0.7988818883895874\n",
      "Epoch:  3 | Step:  97 | Val Loss:  0.783591628074646\n",
      "Epoch:  3 | Step:  98 | Val Loss:  0.4865742325782776\n",
      "Epoch:  3 | Step:  99 | Val Loss:  0.6586768627166748\n",
      "Epoch:  3 | Step:  100 | Val Loss:  0.45883989334106445\n",
      "Epoch:  3 | Step:  101 | Val Loss:  0.7119395732879639\n",
      "Epoch:  3 | Step:  102 | Val Loss:  0.8533034324645996\n",
      "Epoch:  3 | Step:  103 | Val Loss:  0.8004730939865112\n",
      "Epoch:  3 | Step:  104 | Val Loss:  0.7849191427230835\n",
      "Epoch:  3 | Step:  105 | Val Loss:  0.5769357681274414\n",
      "Epoch:  3 | Step:  106 | Val Loss:  0.7082810997962952\n",
      "Epoch:  3 | Step:  107 | Val Loss:  0.7426426410675049\n",
      "Epoch:  3 | Step:  108 | Val Loss:  0.6656031608581543\n",
      "Epoch:  3 | Step:  109 | Val Loss:  0.6334167122840881\n",
      "Epoch:  3 | Step:  110 | Val Loss:  0.6476515531539917\n",
      "Epoch:  3 | Step:  111 | Val Loss:  0.937397301197052\n",
      "Epoch:  3 | Step:  112 | Val Loss:  0.7449367046356201\n",
      "Epoch:  3 | Step:  113 | Val Loss:  0.7490223050117493\n",
      "Epoch:  3 | Step:  114 | Val Loss:  0.7202173471450806\n",
      "Epoch:  3 | Step:  115 | Val Loss:  0.8636698722839355\n",
      "Epoch:  3 | Step:  116 | Val Loss:  0.48053109645843506\n",
      "Epoch:  3 | Step:  117 | Val Loss:  0.5706383585929871\n",
      "Epoch:  3 | Step:  118 | Val Loss:  0.7718023657798767\n",
      "Epoch:  3 | Step:  119 | Val Loss:  0.6805722713470459\n",
      "Epoch:  3 | Step:  120 | Val Loss:  0.5353307723999023\n",
      "Epoch:  3 | Step:  121 | Val Loss:  0.54791259765625\n",
      "Epoch:  3 | Step:  122 | Val Loss:  0.6652189493179321\n",
      "Epoch:  3 | Step:  123 | Val Loss:  0.5156217813491821\n",
      "Epoch:  3 | Step:  124 | Val Loss:  0.7185978889465332\n",
      "Epoch:  3 | Step:  125 | Val Loss:  0.6145536303520203\n",
      "Epoch:  3 | Train Loss:  tensor(0.6173, device='cuda:0') | Val Loss:  tensor(0.6640, device='cuda:0')\n",
      "Epoch:  4 | Step:  500 | Train Loss:  0.4651568830013275\n",
      "Epoch:  4 | Step:  1 | Val Loss:  0.6615675687789917\n",
      "Epoch:  4 | Step:  2 | Val Loss:  0.6378583908081055\n",
      "Epoch:  4 | Step:  3 | Val Loss:  0.7255129814147949\n",
      "Epoch:  4 | Step:  4 | Val Loss:  0.5346757769584656\n",
      "Epoch:  4 | Step:  5 | Val Loss:  0.6798615455627441\n",
      "Epoch:  4 | Step:  6 | Val Loss:  0.6062143445014954\n",
      "Epoch:  4 | Step:  7 | Val Loss:  0.5742904543876648\n",
      "Epoch:  4 | Step:  8 | Val Loss:  0.5445256233215332\n",
      "Epoch:  4 | Step:  9 | Val Loss:  0.7010403275489807\n",
      "Epoch:  4 | Step:  10 | Val Loss:  0.8457565307617188\n",
      "Epoch:  4 | Step:  11 | Val Loss:  0.4806874096393585\n",
      "Epoch:  4 | Step:  12 | Val Loss:  0.7191282510757446\n",
      "Epoch:  4 | Step:  13 | Val Loss:  0.7660866379737854\n",
      "Epoch:  4 | Step:  14 | Val Loss:  0.5779151916503906\n",
      "Epoch:  4 | Step:  15 | Val Loss:  0.6088569760322571\n",
      "Epoch:  4 | Step:  16 | Val Loss:  0.6033851504325867\n",
      "Epoch:  4 | Step:  17 | Val Loss:  0.5631470680236816\n",
      "Epoch:  4 | Step:  18 | Val Loss:  0.5669432878494263\n",
      "Epoch:  4 | Step:  19 | Val Loss:  0.6198805570602417\n",
      "Epoch:  4 | Step:  20 | Val Loss:  0.7052488327026367\n",
      "Epoch:  4 | Step:  21 | Val Loss:  0.8713222742080688\n",
      "Epoch:  4 | Step:  22 | Val Loss:  0.4804539680480957\n",
      "Epoch:  4 | Step:  23 | Val Loss:  0.8016241788864136\n",
      "Epoch:  4 | Step:  24 | Val Loss:  1.0951635837554932\n",
      "Epoch:  4 | Step:  25 | Val Loss:  0.5272525548934937\n",
      "Epoch:  4 | Step:  26 | Val Loss:  0.8289560079574585\n",
      "Epoch:  4 | Step:  27 | Val Loss:  0.4907299280166626\n",
      "Epoch:  4 | Step:  28 | Val Loss:  0.6785111427307129\n",
      "Epoch:  4 | Step:  29 | Val Loss:  0.800723135471344\n",
      "Epoch:  4 | Step:  30 | Val Loss:  0.45184263586997986\n",
      "Epoch:  4 | Step:  31 | Val Loss:  0.8179391026496887\n",
      "Epoch:  4 | Step:  32 | Val Loss:  0.5573363304138184\n",
      "Epoch:  4 | Step:  33 | Val Loss:  0.46362000703811646\n",
      "Epoch:  4 | Step:  34 | Val Loss:  0.6638563871383667\n",
      "Epoch:  4 | Step:  35 | Val Loss:  0.47075849771499634\n",
      "Epoch:  4 | Step:  36 | Val Loss:  0.8386833667755127\n",
      "Epoch:  4 | Step:  37 | Val Loss:  0.6274274587631226\n",
      "Epoch:  4 | Step:  38 | Val Loss:  0.7385239601135254\n",
      "Epoch:  4 | Step:  39 | Val Loss:  0.6842477321624756\n",
      "Epoch:  4 | Step:  40 | Val Loss:  0.7796975374221802\n",
      "Epoch:  4 | Step:  41 | Val Loss:  0.5753703117370605\n",
      "Epoch:  4 | Step:  42 | Val Loss:  0.5093443393707275\n",
      "Epoch:  4 | Step:  43 | Val Loss:  0.919856607913971\n",
      "Epoch:  4 | Step:  44 | Val Loss:  0.7509711980819702\n",
      "Epoch:  4 | Step:  45 | Val Loss:  0.772544264793396\n",
      "Epoch:  4 | Step:  46 | Val Loss:  0.6880492568016052\n",
      "Epoch:  4 | Step:  47 | Val Loss:  0.7473257780075073\n",
      "Epoch:  4 | Step:  48 | Val Loss:  0.6723186373710632\n",
      "Epoch:  4 | Step:  49 | Val Loss:  0.6838983297348022\n",
      "Epoch:  4 | Step:  50 | Val Loss:  0.7265819311141968\n",
      "Epoch:  4 | Step:  51 | Val Loss:  0.5918903350830078\n",
      "Epoch:  4 | Step:  52 | Val Loss:  0.9564467668533325\n",
      "Epoch:  4 | Step:  53 | Val Loss:  0.3490571975708008\n",
      "Epoch:  4 | Step:  54 | Val Loss:  0.5646510124206543\n",
      "Epoch:  4 | Step:  55 | Val Loss:  0.6212952136993408\n",
      "Epoch:  4 | Step:  56 | Val Loss:  0.5574649572372437\n",
      "Epoch:  4 | Step:  57 | Val Loss:  0.6600719094276428\n",
      "Epoch:  4 | Step:  58 | Val Loss:  0.7080888748168945\n",
      "Epoch:  4 | Step:  59 | Val Loss:  0.2757871150970459\n",
      "Epoch:  4 | Step:  60 | Val Loss:  0.5086886882781982\n",
      "Epoch:  4 | Step:  61 | Val Loss:  0.6610939502716064\n",
      "Epoch:  4 | Step:  62 | Val Loss:  0.5346581935882568\n",
      "Epoch:  4 | Step:  63 | Val Loss:  0.626233696937561\n",
      "Epoch:  4 | Step:  64 | Val Loss:  0.5196512341499329\n",
      "Epoch:  4 | Step:  65 | Val Loss:  0.7513253092765808\n",
      "Epoch:  4 | Step:  66 | Val Loss:  0.6851608753204346\n",
      "Epoch:  4 | Step:  67 | Val Loss:  0.6487292647361755\n",
      "Epoch:  4 | Step:  68 | Val Loss:  0.33371639251708984\n",
      "Epoch:  4 | Step:  69 | Val Loss:  0.5045425295829773\n",
      "Epoch:  4 | Step:  70 | Val Loss:  0.6702473163604736\n",
      "Epoch:  4 | Step:  71 | Val Loss:  0.6545044183731079\n",
      "Epoch:  4 | Step:  72 | Val Loss:  0.44296082854270935\n",
      "Epoch:  4 | Step:  73 | Val Loss:  0.513164758682251\n",
      "Epoch:  4 | Step:  74 | Val Loss:  0.46219396591186523\n",
      "Epoch:  4 | Step:  75 | Val Loss:  0.9148000478744507\n",
      "Epoch:  4 | Step:  76 | Val Loss:  0.7580665349960327\n",
      "Epoch:  4 | Step:  77 | Val Loss:  0.6941772699356079\n",
      "Epoch:  4 | Step:  78 | Val Loss:  0.6155319213867188\n",
      "Epoch:  4 | Step:  79 | Val Loss:  0.5051834583282471\n",
      "Epoch:  4 | Step:  80 | Val Loss:  0.5876795053482056\n",
      "Epoch:  4 | Step:  81 | Val Loss:  0.6299103498458862\n",
      "Epoch:  4 | Step:  82 | Val Loss:  0.6643584966659546\n",
      "Epoch:  4 | Step:  83 | Val Loss:  0.5937987565994263\n",
      "Epoch:  4 | Step:  84 | Val Loss:  0.5110327005386353\n",
      "Epoch:  4 | Step:  85 | Val Loss:  0.6984665393829346\n",
      "Epoch:  4 | Step:  86 | Val Loss:  0.5117406845092773\n",
      "Epoch:  4 | Step:  87 | Val Loss:  0.5554872751235962\n",
      "Epoch:  4 | Step:  88 | Val Loss:  0.47018712759017944\n",
      "Epoch:  4 | Step:  89 | Val Loss:  0.6200480461120605\n",
      "Epoch:  4 | Step:  90 | Val Loss:  0.7334144115447998\n",
      "Epoch:  4 | Step:  91 | Val Loss:  0.899668276309967\n",
      "Epoch:  4 | Step:  92 | Val Loss:  0.4422069191932678\n",
      "Epoch:  4 | Step:  93 | Val Loss:  0.7347885370254517\n",
      "Epoch:  4 | Step:  94 | Val Loss:  0.8179388642311096\n",
      "Epoch:  4 | Step:  95 | Val Loss:  0.5803385972976685\n",
      "Epoch:  4 | Step:  96 | Val Loss:  0.6716755032539368\n",
      "Epoch:  4 | Step:  97 | Val Loss:  1.0040768384933472\n",
      "Epoch:  4 | Step:  98 | Val Loss:  0.6609936952590942\n",
      "Epoch:  4 | Step:  99 | Val Loss:  0.7056898474693298\n",
      "Epoch:  4 | Step:  100 | Val Loss:  0.5690243244171143\n",
      "Epoch:  4 | Step:  101 | Val Loss:  0.5352252721786499\n",
      "Epoch:  4 | Step:  102 | Val Loss:  0.7061589360237122\n",
      "Epoch:  4 | Step:  103 | Val Loss:  0.667841911315918\n",
      "Epoch:  4 | Step:  104 | Val Loss:  0.4128641188144684\n",
      "Epoch:  4 | Step:  105 | Val Loss:  0.7641100287437439\n",
      "Epoch:  4 | Step:  106 | Val Loss:  0.9950414896011353\n",
      "Epoch:  4 | Step:  107 | Val Loss:  0.646325945854187\n",
      "Epoch:  4 | Step:  108 | Val Loss:  0.60283362865448\n",
      "Epoch:  4 | Step:  109 | Val Loss:  0.8582502007484436\n",
      "Epoch:  4 | Step:  110 | Val Loss:  0.6671351194381714\n",
      "Epoch:  4 | Step:  111 | Val Loss:  0.7938734292984009\n",
      "Epoch:  4 | Step:  112 | Val Loss:  0.8084216713905334\n",
      "Epoch:  4 | Step:  113 | Val Loss:  0.9241620302200317\n",
      "Epoch:  4 | Step:  114 | Val Loss:  0.6759500503540039\n",
      "Epoch:  4 | Step:  115 | Val Loss:  0.49633723497390747\n",
      "Epoch:  4 | Step:  116 | Val Loss:  0.5189231038093567\n",
      "Epoch:  4 | Step:  117 | Val Loss:  0.8495456576347351\n",
      "Epoch:  4 | Step:  118 | Val Loss:  0.8926709294319153\n",
      "Epoch:  4 | Step:  119 | Val Loss:  0.6728534698486328\n",
      "Epoch:  4 | Step:  120 | Val Loss:  0.6713278889656067\n",
      "Epoch:  4 | Step:  121 | Val Loss:  0.6996083855628967\n",
      "Epoch:  4 | Step:  122 | Val Loss:  0.5858253836631775\n",
      "Epoch:  4 | Step:  123 | Val Loss:  0.5276596546173096\n",
      "Epoch:  4 | Step:  124 | Val Loss:  0.8464970588684082\n",
      "Epoch:  4 | Step:  125 | Val Loss:  0.5580493211746216\n",
      "Epoch:  4 | Train Loss:  tensor(0.6063, device='cuda:0') | Val Loss:  tensor(0.6541, device='cuda:0')\n",
      "Epoch:  5 | Step:  500 | Train Loss:  0.40113314986228943\n",
      "Epoch:  5 | Step:  1 | Val Loss:  0.6063129901885986\n",
      "Epoch:  5 | Step:  2 | Val Loss:  0.5969772934913635\n",
      "Epoch:  5 | Step:  3 | Val Loss:  0.6536122560501099\n",
      "Epoch:  5 | Step:  4 | Val Loss:  0.5741540193557739\n",
      "Epoch:  5 | Step:  5 | Val Loss:  0.7662644982337952\n",
      "Epoch:  5 | Step:  6 | Val Loss:  0.5574182868003845\n",
      "Epoch:  5 | Step:  7 | Val Loss:  0.571287989616394\n",
      "Epoch:  5 | Step:  8 | Val Loss:  0.8699418306350708\n",
      "Epoch:  5 | Step:  9 | Val Loss:  0.40382760763168335\n",
      "Epoch:  5 | Step:  10 | Val Loss:  0.7799278497695923\n",
      "Epoch:  5 | Step:  11 | Val Loss:  0.4356552064418793\n",
      "Epoch:  5 | Step:  12 | Val Loss:  0.7566790580749512\n",
      "Epoch:  5 | Step:  13 | Val Loss:  0.40746334195137024\n",
      "Epoch:  5 | Step:  14 | Val Loss:  0.6157435178756714\n",
      "Epoch:  5 | Step:  15 | Val Loss:  0.4743838906288147\n",
      "Epoch:  5 | Step:  16 | Val Loss:  0.492351233959198\n",
      "Epoch:  5 | Step:  17 | Val Loss:  0.5304199457168579\n",
      "Epoch:  5 | Step:  18 | Val Loss:  0.3666061460971832\n",
      "Epoch:  5 | Step:  19 | Val Loss:  0.6564865112304688\n",
      "Epoch:  5 | Step:  20 | Val Loss:  0.6039580702781677\n",
      "Epoch:  5 | Step:  21 | Val Loss:  0.8492306470870972\n",
      "Epoch:  5 | Step:  22 | Val Loss:  0.8266386389732361\n",
      "Epoch:  5 | Step:  23 | Val Loss:  0.4203341603279114\n",
      "Epoch:  5 | Step:  24 | Val Loss:  0.622611403465271\n",
      "Epoch:  5 | Step:  25 | Val Loss:  0.7805852890014648\n",
      "Epoch:  5 | Step:  26 | Val Loss:  0.7628354430198669\n",
      "Epoch:  5 | Step:  27 | Val Loss:  0.6066159009933472\n",
      "Epoch:  5 | Step:  28 | Val Loss:  0.7299230098724365\n",
      "Epoch:  5 | Step:  29 | Val Loss:  0.5429378747940063\n",
      "Epoch:  5 | Step:  30 | Val Loss:  0.515451967716217\n",
      "Epoch:  5 | Step:  31 | Val Loss:  0.6418898105621338\n",
      "Epoch:  5 | Step:  32 | Val Loss:  0.38769054412841797\n",
      "Epoch:  5 | Step:  33 | Val Loss:  0.37395429611206055\n",
      "Epoch:  5 | Step:  34 | Val Loss:  0.6881110668182373\n",
      "Epoch:  5 | Step:  35 | Val Loss:  0.7649796605110168\n",
      "Epoch:  5 | Step:  36 | Val Loss:  0.6642220616340637\n",
      "Epoch:  5 | Step:  37 | Val Loss:  0.6146296262741089\n",
      "Epoch:  5 | Step:  38 | Val Loss:  0.8704838752746582\n",
      "Epoch:  5 | Step:  39 | Val Loss:  0.8145464658737183\n",
      "Epoch:  5 | Step:  40 | Val Loss:  0.6062887907028198\n",
      "Epoch:  5 | Step:  41 | Val Loss:  0.6814768314361572\n",
      "Epoch:  5 | Step:  42 | Val Loss:  0.5568605065345764\n",
      "Epoch:  5 | Step:  43 | Val Loss:  0.49501049518585205\n",
      "Epoch:  5 | Step:  44 | Val Loss:  0.8246047496795654\n",
      "Epoch:  5 | Step:  45 | Val Loss:  0.7633357644081116\n",
      "Epoch:  5 | Step:  46 | Val Loss:  0.6137884855270386\n",
      "Epoch:  5 | Step:  47 | Val Loss:  0.6685596704483032\n",
      "Epoch:  5 | Step:  48 | Val Loss:  0.9185901284217834\n",
      "Epoch:  5 | Step:  49 | Val Loss:  0.5715340971946716\n",
      "Epoch:  5 | Step:  50 | Val Loss:  0.6084760427474976\n",
      "Epoch:  5 | Step:  51 | Val Loss:  0.5078709125518799\n",
      "Epoch:  5 | Step:  52 | Val Loss:  0.8351518511772156\n",
      "Epoch:  5 | Step:  53 | Val Loss:  0.7007524967193604\n",
      "Epoch:  5 | Step:  54 | Val Loss:  0.411008358001709\n",
      "Epoch:  5 | Step:  55 | Val Loss:  0.6036313772201538\n",
      "Epoch:  5 | Step:  56 | Val Loss:  0.5903636813163757\n",
      "Epoch:  5 | Step:  57 | Val Loss:  0.6241270303726196\n",
      "Epoch:  5 | Step:  58 | Val Loss:  0.5263235569000244\n",
      "Epoch:  5 | Step:  59 | Val Loss:  0.8147588968276978\n",
      "Epoch:  5 | Step:  60 | Val Loss:  0.6235406398773193\n",
      "Epoch:  5 | Step:  61 | Val Loss:  0.7821729183197021\n",
      "Epoch:  5 | Step:  62 | Val Loss:  0.6792985200881958\n",
      "Epoch:  5 | Step:  63 | Val Loss:  0.6528265476226807\n",
      "Epoch:  5 | Step:  64 | Val Loss:  0.9951496720314026\n",
      "Epoch:  5 | Step:  65 | Val Loss:  0.5052495002746582\n",
      "Epoch:  5 | Step:  66 | Val Loss:  0.3227040767669678\n",
      "Epoch:  5 | Step:  67 | Val Loss:  0.6111606955528259\n",
      "Epoch:  5 | Step:  68 | Val Loss:  0.4807756841182709\n",
      "Epoch:  5 | Step:  69 | Val Loss:  0.6575239896774292\n",
      "Epoch:  5 | Step:  70 | Val Loss:  0.4088779091835022\n",
      "Epoch:  5 | Step:  71 | Val Loss:  0.6573023796081543\n",
      "Epoch:  5 | Step:  72 | Val Loss:  0.7952197194099426\n",
      "Epoch:  5 | Step:  73 | Val Loss:  0.631105899810791\n",
      "Epoch:  5 | Step:  74 | Val Loss:  0.5247126817703247\n",
      "Epoch:  5 | Step:  75 | Val Loss:  0.6180264949798584\n",
      "Epoch:  5 | Step:  76 | Val Loss:  1.0413331985473633\n",
      "Epoch:  5 | Step:  77 | Val Loss:  0.44813358783721924\n",
      "Epoch:  5 | Step:  78 | Val Loss:  0.5338612794876099\n",
      "Epoch:  5 | Step:  79 | Val Loss:  0.7816120386123657\n",
      "Epoch:  5 | Step:  80 | Val Loss:  0.9840730428695679\n",
      "Epoch:  5 | Step:  81 | Val Loss:  0.9011595845222473\n",
      "Epoch:  5 | Step:  82 | Val Loss:  0.5967819094657898\n",
      "Epoch:  5 | Step:  83 | Val Loss:  0.5871669054031372\n",
      "Epoch:  5 | Step:  84 | Val Loss:  0.7845054864883423\n",
      "Epoch:  5 | Step:  85 | Val Loss:  0.4103815257549286\n",
      "Epoch:  5 | Step:  86 | Val Loss:  0.8332105875015259\n",
      "Epoch:  5 | Step:  87 | Val Loss:  0.6661118268966675\n",
      "Epoch:  5 | Step:  88 | Val Loss:  0.609317421913147\n",
      "Epoch:  5 | Step:  89 | Val Loss:  0.6933423280715942\n",
      "Epoch:  5 | Step:  90 | Val Loss:  0.6182288527488708\n",
      "Epoch:  5 | Step:  91 | Val Loss:  0.6208015084266663\n",
      "Epoch:  5 | Step:  92 | Val Loss:  0.6043342351913452\n",
      "Epoch:  5 | Step:  93 | Val Loss:  0.5128823518753052\n",
      "Epoch:  5 | Step:  94 | Val Loss:  0.7201993465423584\n",
      "Epoch:  5 | Step:  95 | Val Loss:  0.6047932505607605\n",
      "Epoch:  5 | Step:  96 | Val Loss:  0.7051042318344116\n",
      "Epoch:  5 | Step:  97 | Val Loss:  0.7746769785881042\n",
      "Epoch:  5 | Step:  98 | Val Loss:  0.821799635887146\n",
      "Epoch:  5 | Step:  99 | Val Loss:  0.49951261281967163\n",
      "Epoch:  5 | Step:  100 | Val Loss:  0.8334035873413086\n",
      "Epoch:  5 | Step:  101 | Val Loss:  0.6116800308227539\n",
      "Epoch:  5 | Step:  102 | Val Loss:  0.6696258783340454\n",
      "Epoch:  5 | Step:  103 | Val Loss:  0.45194655656814575\n",
      "Epoch:  5 | Step:  104 | Val Loss:  0.8296335935592651\n",
      "Epoch:  5 | Step:  105 | Val Loss:  0.7147951126098633\n",
      "Epoch:  5 | Step:  106 | Val Loss:  0.7293193340301514\n",
      "Epoch:  5 | Step:  107 | Val Loss:  0.8465480804443359\n",
      "Epoch:  5 | Step:  108 | Val Loss:  0.5675181150436401\n",
      "Epoch:  5 | Step:  109 | Val Loss:  0.5439643263816833\n",
      "Epoch:  5 | Step:  110 | Val Loss:  0.6346502304077148\n",
      "Epoch:  5 | Step:  111 | Val Loss:  0.8430284261703491\n",
      "Epoch:  5 | Step:  112 | Val Loss:  0.46655112504959106\n",
      "Epoch:  5 | Step:  113 | Val Loss:  0.5592118501663208\n",
      "Epoch:  5 | Step:  114 | Val Loss:  0.6240711808204651\n",
      "Epoch:  5 | Step:  115 | Val Loss:  0.4043588936328888\n",
      "Epoch:  5 | Step:  116 | Val Loss:  0.6592013835906982\n",
      "Epoch:  5 | Step:  117 | Val Loss:  0.6578773856163025\n",
      "Epoch:  5 | Step:  118 | Val Loss:  0.7101437449455261\n",
      "Epoch:  5 | Step:  119 | Val Loss:  0.7920494675636292\n",
      "Epoch:  5 | Step:  120 | Val Loss:  0.5940068960189819\n",
      "Epoch:  5 | Step:  121 | Val Loss:  0.8040146231651306\n",
      "Epoch:  5 | Step:  122 | Val Loss:  0.7583235502243042\n",
      "Epoch:  5 | Step:  123 | Val Loss:  0.4799243211746216\n",
      "Epoch:  5 | Step:  124 | Val Loss:  0.5783216953277588\n",
      "Epoch:  5 | Step:  125 | Val Loss:  0.8405578136444092\n",
      "Epoch:  5 | Train Loss:  tensor(0.5974, device='cuda:0') | Val Loss:  tensor(0.6450, device='cuda:0')\n",
      "Epoch:  6 | Step:  500 | Train Loss:  0.6930729746818542\n",
      "Epoch:  6 | Step:  1 | Val Loss:  0.4880417585372925\n",
      "Epoch:  6 | Step:  2 | Val Loss:  0.6306495070457458\n",
      "Epoch:  6 | Step:  3 | Val Loss:  0.5035629868507385\n",
      "Epoch:  6 | Step:  4 | Val Loss:  0.5386772751808167\n",
      "Epoch:  6 | Step:  5 | Val Loss:  0.6781842708587646\n",
      "Epoch:  6 | Step:  6 | Val Loss:  0.6387348175048828\n",
      "Epoch:  6 | Step:  7 | Val Loss:  0.8028693795204163\n",
      "Epoch:  6 | Step:  8 | Val Loss:  0.6038089990615845\n",
      "Epoch:  6 | Step:  9 | Val Loss:  0.6229459047317505\n",
      "Epoch:  6 | Step:  10 | Val Loss:  0.8122466802597046\n",
      "Epoch:  6 | Step:  11 | Val Loss:  0.9457435011863708\n",
      "Epoch:  6 | Step:  12 | Val Loss:  0.4697313904762268\n",
      "Epoch:  6 | Step:  13 | Val Loss:  0.37451404333114624\n",
      "Epoch:  6 | Step:  14 | Val Loss:  0.7476606965065002\n",
      "Epoch:  6 | Step:  15 | Val Loss:  0.5256450772285461\n",
      "Epoch:  6 | Step:  16 | Val Loss:  0.5529767274856567\n",
      "Epoch:  6 | Step:  17 | Val Loss:  0.7879581451416016\n",
      "Epoch:  6 | Step:  18 | Val Loss:  0.5500392913818359\n",
      "Epoch:  6 | Step:  19 | Val Loss:  0.5710195899009705\n",
      "Epoch:  6 | Step:  20 | Val Loss:  0.7132382392883301\n",
      "Epoch:  6 | Step:  21 | Val Loss:  0.5250171422958374\n",
      "Epoch:  6 | Step:  22 | Val Loss:  0.6717170476913452\n",
      "Epoch:  6 | Step:  23 | Val Loss:  0.30057504773139954\n",
      "Epoch:  6 | Step:  24 | Val Loss:  0.8409658670425415\n",
      "Epoch:  6 | Step:  25 | Val Loss:  0.6429364085197449\n",
      "Epoch:  6 | Step:  26 | Val Loss:  0.5810820460319519\n",
      "Epoch:  6 | Step:  27 | Val Loss:  0.5368218421936035\n",
      "Epoch:  6 | Step:  28 | Val Loss:  0.7167502641677856\n",
      "Epoch:  6 | Step:  29 | Val Loss:  0.5376777648925781\n",
      "Epoch:  6 | Step:  30 | Val Loss:  0.6109004020690918\n",
      "Epoch:  6 | Step:  31 | Val Loss:  0.5669153928756714\n",
      "Epoch:  6 | Step:  32 | Val Loss:  0.5748686790466309\n",
      "Epoch:  6 | Step:  33 | Val Loss:  0.5569921731948853\n",
      "Epoch:  6 | Step:  34 | Val Loss:  0.4407162070274353\n",
      "Epoch:  6 | Step:  35 | Val Loss:  0.6506637334823608\n",
      "Epoch:  6 | Step:  36 | Val Loss:  0.7744773626327515\n",
      "Epoch:  6 | Step:  37 | Val Loss:  0.6204094290733337\n",
      "Epoch:  6 | Step:  38 | Val Loss:  0.501185417175293\n",
      "Epoch:  6 | Step:  39 | Val Loss:  0.7048479318618774\n",
      "Epoch:  6 | Step:  40 | Val Loss:  0.6813600063323975\n",
      "Epoch:  6 | Step:  41 | Val Loss:  0.5441886186599731\n",
      "Epoch:  6 | Step:  42 | Val Loss:  0.7200558185577393\n",
      "Epoch:  6 | Step:  43 | Val Loss:  0.5109123587608337\n",
      "Epoch:  6 | Step:  44 | Val Loss:  0.8505985736846924\n",
      "Epoch:  6 | Step:  45 | Val Loss:  0.615150511264801\n",
      "Epoch:  6 | Step:  46 | Val Loss:  0.7336344122886658\n",
      "Epoch:  6 | Step:  47 | Val Loss:  0.7081103324890137\n",
      "Epoch:  6 | Step:  48 | Val Loss:  0.7321816682815552\n",
      "Epoch:  6 | Step:  49 | Val Loss:  0.5393890142440796\n",
      "Epoch:  6 | Step:  50 | Val Loss:  0.8005845546722412\n",
      "Epoch:  6 | Step:  51 | Val Loss:  0.5999473929405212\n",
      "Epoch:  6 | Step:  52 | Val Loss:  0.9451055526733398\n",
      "Epoch:  6 | Step:  53 | Val Loss:  0.4501798748970032\n",
      "Epoch:  6 | Step:  54 | Val Loss:  0.55440354347229\n",
      "Epoch:  6 | Step:  55 | Val Loss:  0.7837666869163513\n",
      "Epoch:  6 | Step:  56 | Val Loss:  0.5405084490776062\n",
      "Epoch:  6 | Step:  57 | Val Loss:  0.5486589670181274\n",
      "Epoch:  6 | Step:  58 | Val Loss:  0.5500359535217285\n",
      "Epoch:  6 | Step:  59 | Val Loss:  0.8558229207992554\n",
      "Epoch:  6 | Step:  60 | Val Loss:  0.8633491396903992\n",
      "Epoch:  6 | Step:  61 | Val Loss:  0.9259533882141113\n",
      "Epoch:  6 | Step:  62 | Val Loss:  0.6961695551872253\n",
      "Epoch:  6 | Step:  63 | Val Loss:  0.6738963723182678\n",
      "Epoch:  6 | Step:  64 | Val Loss:  0.6549586057662964\n",
      "Epoch:  6 | Step:  65 | Val Loss:  0.5931926369667053\n",
      "Epoch:  6 | Step:  66 | Val Loss:  0.6151415109634399\n",
      "Epoch:  6 | Step:  67 | Val Loss:  0.5891530513763428\n",
      "Epoch:  6 | Step:  68 | Val Loss:  0.6108629703521729\n",
      "Epoch:  6 | Step:  69 | Val Loss:  0.7858631610870361\n",
      "Epoch:  6 | Step:  70 | Val Loss:  0.6127071380615234\n",
      "Epoch:  6 | Step:  71 | Val Loss:  0.7637573480606079\n",
      "Epoch:  6 | Step:  72 | Val Loss:  0.302604615688324\n",
      "Epoch:  6 | Step:  73 | Val Loss:  0.6022319793701172\n",
      "Epoch:  6 | Step:  74 | Val Loss:  0.8426793813705444\n",
      "Epoch:  6 | Step:  75 | Val Loss:  0.5929142832756042\n",
      "Epoch:  6 | Step:  76 | Val Loss:  0.4745720624923706\n",
      "Epoch:  6 | Step:  77 | Val Loss:  0.6416000127792358\n",
      "Epoch:  6 | Step:  78 | Val Loss:  0.7769842743873596\n",
      "Epoch:  6 | Step:  79 | Val Loss:  0.4481556713581085\n",
      "Epoch:  6 | Step:  80 | Val Loss:  0.5878464579582214\n",
      "Epoch:  6 | Step:  81 | Val Loss:  0.5056561231613159\n",
      "Epoch:  6 | Step:  82 | Val Loss:  0.8861669301986694\n",
      "Epoch:  6 | Step:  83 | Val Loss:  0.6600264310836792\n",
      "Epoch:  6 | Step:  84 | Val Loss:  0.6521117091178894\n",
      "Epoch:  6 | Step:  85 | Val Loss:  0.7809168100357056\n",
      "Epoch:  6 | Step:  86 | Val Loss:  0.5931504964828491\n",
      "Epoch:  6 | Step:  87 | Val Loss:  0.7206671237945557\n",
      "Epoch:  6 | Step:  88 | Val Loss:  0.8110761642456055\n",
      "Epoch:  6 | Step:  89 | Val Loss:  0.5926598310470581\n",
      "Epoch:  6 | Step:  90 | Val Loss:  0.5640811324119568\n",
      "Epoch:  6 | Step:  91 | Val Loss:  0.9173625707626343\n",
      "Epoch:  6 | Step:  92 | Val Loss:  0.683704137802124\n",
      "Epoch:  6 | Step:  93 | Val Loss:  0.4434591829776764\n",
      "Epoch:  6 | Step:  94 | Val Loss:  0.6819782853126526\n",
      "Epoch:  6 | Step:  95 | Val Loss:  0.7341485619544983\n",
      "Epoch:  6 | Step:  96 | Val Loss:  0.5405234694480896\n",
      "Epoch:  6 | Step:  97 | Val Loss:  0.6476202011108398\n",
      "Epoch:  6 | Step:  98 | Val Loss:  0.7745362520217896\n",
      "Epoch:  6 | Step:  99 | Val Loss:  0.6175656318664551\n",
      "Epoch:  6 | Step:  100 | Val Loss:  0.5868310928344727\n",
      "Epoch:  6 | Step:  101 | Val Loss:  0.5357773900032043\n",
      "Epoch:  6 | Step:  102 | Val Loss:  0.7245981693267822\n",
      "Epoch:  6 | Step:  103 | Val Loss:  0.5235779285430908\n",
      "Epoch:  6 | Step:  104 | Val Loss:  0.8141298890113831\n",
      "Epoch:  6 | Step:  105 | Val Loss:  0.5749304294586182\n",
      "Epoch:  6 | Step:  106 | Val Loss:  0.5927711725234985\n",
      "Epoch:  6 | Step:  107 | Val Loss:  0.6613836288452148\n",
      "Epoch:  6 | Step:  108 | Val Loss:  0.5097764730453491\n",
      "Epoch:  6 | Step:  109 | Val Loss:  0.4384436011314392\n",
      "Epoch:  6 | Step:  110 | Val Loss:  0.5614969730377197\n",
      "Epoch:  6 | Step:  111 | Val Loss:  0.7645072340965271\n",
      "Epoch:  6 | Step:  112 | Val Loss:  0.46142834424972534\n",
      "Epoch:  6 | Step:  113 | Val Loss:  0.6078000068664551\n",
      "Epoch:  6 | Step:  114 | Val Loss:  0.760619044303894\n",
      "Epoch:  6 | Step:  115 | Val Loss:  0.5743073225021362\n",
      "Epoch:  6 | Step:  116 | Val Loss:  0.5341185331344604\n",
      "Epoch:  6 | Step:  117 | Val Loss:  0.648461103439331\n",
      "Epoch:  6 | Step:  118 | Val Loss:  0.5929878950119019\n",
      "Epoch:  6 | Step:  119 | Val Loss:  0.5636395812034607\n",
      "Epoch:  6 | Step:  120 | Val Loss:  0.4305580258369446\n",
      "Epoch:  6 | Step:  121 | Val Loss:  0.5000983476638794\n",
      "Epoch:  6 | Step:  122 | Val Loss:  0.7939976453781128\n",
      "Epoch:  6 | Step:  123 | Val Loss:  0.8821589946746826\n",
      "Epoch:  6 | Step:  124 | Val Loss:  0.6288546323776245\n",
      "Epoch:  6 | Step:  125 | Val Loss:  0.7795303463935852\n",
      "Epoch:  6 | Train Loss:  tensor(0.5900, device='cuda:0') | Val Loss:  tensor(0.6375, device='cuda:0')\n",
      "Epoch:  7 | Step:  500 | Train Loss:  0.7399448752403259\n",
      "Epoch:  7 | Step:  1 | Val Loss:  0.7652917504310608\n",
      "Epoch:  7 | Step:  2 | Val Loss:  0.6317139863967896\n",
      "Epoch:  7 | Step:  3 | Val Loss:  0.6427892446517944\n",
      "Epoch:  7 | Step:  4 | Val Loss:  0.5087735652923584\n",
      "Epoch:  7 | Step:  5 | Val Loss:  0.6890367269515991\n",
      "Epoch:  7 | Step:  6 | Val Loss:  0.524105429649353\n",
      "Epoch:  7 | Step:  7 | Val Loss:  0.5865560173988342\n",
      "Epoch:  7 | Step:  8 | Val Loss:  0.869861364364624\n",
      "Epoch:  7 | Step:  9 | Val Loss:  0.7633750438690186\n",
      "Epoch:  7 | Step:  10 | Val Loss:  0.48238682746887207\n",
      "Epoch:  7 | Step:  11 | Val Loss:  0.8084465265274048\n",
      "Epoch:  7 | Step:  12 | Val Loss:  0.7102820873260498\n",
      "Epoch:  7 | Step:  13 | Val Loss:  0.7510148286819458\n",
      "Epoch:  7 | Step:  14 | Val Loss:  0.4772163927555084\n",
      "Epoch:  7 | Step:  15 | Val Loss:  0.6278706789016724\n",
      "Epoch:  7 | Step:  16 | Val Loss:  0.564906895160675\n",
      "Epoch:  7 | Step:  17 | Val Loss:  0.8031395077705383\n",
      "Epoch:  7 | Step:  18 | Val Loss:  0.6887816786766052\n",
      "Epoch:  7 | Step:  19 | Val Loss:  0.8665322661399841\n",
      "Epoch:  7 | Step:  20 | Val Loss:  0.6662863492965698\n",
      "Epoch:  7 | Step:  21 | Val Loss:  0.7314454317092896\n",
      "Epoch:  7 | Step:  22 | Val Loss:  0.7929645776748657\n",
      "Epoch:  7 | Step:  23 | Val Loss:  0.6513694524765015\n",
      "Epoch:  7 | Step:  24 | Val Loss:  0.6790341734886169\n",
      "Epoch:  7 | Step:  25 | Val Loss:  0.9226969480514526\n",
      "Epoch:  7 | Step:  26 | Val Loss:  0.7660847306251526\n",
      "Epoch:  7 | Step:  27 | Val Loss:  0.6276980638504028\n",
      "Epoch:  7 | Step:  28 | Val Loss:  0.49450579285621643\n",
      "Epoch:  7 | Step:  29 | Val Loss:  0.5071648359298706\n",
      "Epoch:  7 | Step:  30 | Val Loss:  0.5563291907310486\n",
      "Epoch:  7 | Step:  31 | Val Loss:  0.9093767404556274\n",
      "Epoch:  7 | Step:  32 | Val Loss:  0.4137462377548218\n",
      "Epoch:  7 | Step:  33 | Val Loss:  0.5800514221191406\n",
      "Epoch:  7 | Step:  34 | Val Loss:  0.7020376920700073\n",
      "Epoch:  7 | Step:  35 | Val Loss:  0.47214505076408386\n",
      "Epoch:  7 | Step:  36 | Val Loss:  0.7687712907791138\n",
      "Epoch:  7 | Step:  37 | Val Loss:  0.7816752195358276\n",
      "Epoch:  7 | Step:  38 | Val Loss:  0.8196332454681396\n",
      "Epoch:  7 | Step:  39 | Val Loss:  0.3416799306869507\n",
      "Epoch:  7 | Step:  40 | Val Loss:  0.651030957698822\n",
      "Epoch:  7 | Step:  41 | Val Loss:  0.42876654863357544\n",
      "Epoch:  7 | Step:  42 | Val Loss:  0.5734918117523193\n",
      "Epoch:  7 | Step:  43 | Val Loss:  0.5404094457626343\n",
      "Epoch:  7 | Step:  44 | Val Loss:  0.42653101682662964\n",
      "Epoch:  7 | Step:  45 | Val Loss:  0.46662914752960205\n",
      "Epoch:  7 | Step:  46 | Val Loss:  0.4460088312625885\n",
      "Epoch:  7 | Step:  47 | Val Loss:  0.6356984376907349\n",
      "Epoch:  7 | Step:  48 | Val Loss:  0.7149892449378967\n",
      "Epoch:  7 | Step:  49 | Val Loss:  0.5223696231842041\n",
      "Epoch:  7 | Step:  50 | Val Loss:  0.5499342679977417\n",
      "Epoch:  7 | Step:  51 | Val Loss:  0.5885621309280396\n",
      "Epoch:  7 | Step:  52 | Val Loss:  0.3348172605037689\n",
      "Epoch:  7 | Step:  53 | Val Loss:  0.7024500370025635\n",
      "Epoch:  7 | Step:  54 | Val Loss:  0.7512644529342651\n",
      "Epoch:  7 | Step:  55 | Val Loss:  0.346269816160202\n",
      "Epoch:  7 | Step:  56 | Val Loss:  0.47069042921066284\n",
      "Epoch:  7 | Step:  57 | Val Loss:  0.4092722237110138\n",
      "Epoch:  7 | Step:  58 | Val Loss:  0.5640354156494141\n",
      "Epoch:  7 | Step:  59 | Val Loss:  0.587414562702179\n",
      "Epoch:  7 | Step:  60 | Val Loss:  0.7252318263053894\n",
      "Epoch:  7 | Step:  61 | Val Loss:  0.6009920835494995\n",
      "Epoch:  7 | Step:  62 | Val Loss:  0.8030003905296326\n",
      "Epoch:  7 | Step:  63 | Val Loss:  0.5401039123535156\n",
      "Epoch:  7 | Step:  64 | Val Loss:  0.7232964634895325\n",
      "Epoch:  7 | Step:  65 | Val Loss:  0.6862547397613525\n",
      "Epoch:  7 | Step:  66 | Val Loss:  0.49547988176345825\n",
      "Epoch:  7 | Step:  67 | Val Loss:  0.515548825263977\n",
      "Epoch:  7 | Step:  68 | Val Loss:  0.6367940902709961\n",
      "Epoch:  7 | Step:  69 | Val Loss:  0.6239111423492432\n",
      "Epoch:  7 | Step:  70 | Val Loss:  0.4756293296813965\n",
      "Epoch:  7 | Step:  71 | Val Loss:  0.8394205570220947\n",
      "Epoch:  7 | Step:  72 | Val Loss:  0.7028294205665588\n",
      "Epoch:  7 | Step:  73 | Val Loss:  0.7545397281646729\n",
      "Epoch:  7 | Step:  74 | Val Loss:  0.7776476144790649\n",
      "Epoch:  7 | Step:  75 | Val Loss:  0.37691712379455566\n",
      "Epoch:  7 | Step:  76 | Val Loss:  0.8049542903900146\n",
      "Epoch:  7 | Step:  77 | Val Loss:  0.8697638511657715\n",
      "Epoch:  7 | Step:  78 | Val Loss:  0.2820292115211487\n",
      "Epoch:  7 | Step:  79 | Val Loss:  0.8371652364730835\n",
      "Epoch:  7 | Step:  80 | Val Loss:  0.5287807583808899\n",
      "Epoch:  7 | Step:  81 | Val Loss:  0.4949764013290405\n",
      "Epoch:  7 | Step:  82 | Val Loss:  0.5698920488357544\n",
      "Epoch:  7 | Step:  83 | Val Loss:  0.6623146533966064\n",
      "Epoch:  7 | Step:  84 | Val Loss:  0.5079625844955444\n",
      "Epoch:  7 | Step:  85 | Val Loss:  0.5152426958084106\n",
      "Epoch:  7 | Step:  86 | Val Loss:  0.6668964624404907\n",
      "Epoch:  7 | Step:  87 | Val Loss:  0.4929365813732147\n",
      "Epoch:  7 | Step:  88 | Val Loss:  0.6305098533630371\n",
      "Epoch:  7 | Step:  89 | Val Loss:  0.5987412929534912\n",
      "Epoch:  7 | Step:  90 | Val Loss:  0.8161195516586304\n",
      "Epoch:  7 | Step:  91 | Val Loss:  0.5891880989074707\n",
      "Epoch:  7 | Step:  92 | Val Loss:  0.564757227897644\n",
      "Epoch:  7 | Step:  93 | Val Loss:  0.7994207143783569\n",
      "Epoch:  7 | Step:  94 | Val Loss:  0.6752772331237793\n",
      "Epoch:  7 | Step:  95 | Val Loss:  0.8910685181617737\n",
      "Epoch:  7 | Step:  96 | Val Loss:  0.6321630477905273\n",
      "Epoch:  7 | Step:  97 | Val Loss:  0.464540958404541\n",
      "Epoch:  7 | Step:  98 | Val Loss:  0.8272358775138855\n",
      "Epoch:  7 | Step:  99 | Val Loss:  0.43119481205940247\n",
      "Epoch:  7 | Step:  100 | Val Loss:  0.7935717701911926\n",
      "Epoch:  7 | Step:  101 | Val Loss:  0.432670533657074\n",
      "Epoch:  7 | Step:  102 | Val Loss:  0.7729141712188721\n",
      "Epoch:  7 | Step:  103 | Val Loss:  0.6941918134689331\n",
      "Epoch:  7 | Step:  104 | Val Loss:  0.5132652521133423\n",
      "Epoch:  7 | Step:  105 | Val Loss:  0.8338900804519653\n",
      "Epoch:  7 | Step:  106 | Val Loss:  0.41058260202407837\n",
      "Epoch:  7 | Step:  107 | Val Loss:  0.5184289216995239\n",
      "Epoch:  7 | Step:  108 | Val Loss:  0.8120404481887817\n",
      "Epoch:  7 | Step:  109 | Val Loss:  0.5787018537521362\n",
      "Epoch:  7 | Step:  110 | Val Loss:  0.8102741241455078\n",
      "Epoch:  7 | Step:  111 | Val Loss:  0.7961457371711731\n",
      "Epoch:  7 | Step:  112 | Val Loss:  0.5304092764854431\n",
      "Epoch:  7 | Step:  113 | Val Loss:  0.38141024112701416\n",
      "Epoch:  7 | Step:  114 | Val Loss:  0.5081306099891663\n",
      "Epoch:  7 | Step:  115 | Val Loss:  0.41388872265815735\n",
      "Epoch:  7 | Step:  116 | Val Loss:  0.6562000513076782\n",
      "Epoch:  7 | Step:  117 | Val Loss:  0.41428831219673157\n",
      "Epoch:  7 | Step:  118 | Val Loss:  0.7504581212997437\n",
      "Epoch:  7 | Step:  119 | Val Loss:  0.8363739252090454\n",
      "Epoch:  7 | Step:  120 | Val Loss:  0.5972434878349304\n",
      "Epoch:  7 | Step:  121 | Val Loss:  0.6838501691818237\n",
      "Epoch:  7 | Step:  122 | Val Loss:  0.6377217769622803\n",
      "Epoch:  7 | Step:  123 | Val Loss:  0.6667281985282898\n",
      "Epoch:  7 | Step:  124 | Val Loss:  0.8177837133407593\n",
      "Epoch:  7 | Step:  125 | Val Loss:  0.5880756378173828\n",
      "Epoch:  7 | Train Loss:  tensor(0.5835, device='cuda:0') | Val Loss:  tensor(0.6283, device='cuda:0')\n",
      "Epoch:  8 | Step:  500 | Train Loss:  0.4505160450935364\n",
      "Epoch:  8 | Step:  1 | Val Loss:  0.4813874363899231\n",
      "Epoch:  8 | Step:  2 | Val Loss:  0.5228844881057739\n",
      "Epoch:  8 | Step:  3 | Val Loss:  0.6463807821273804\n",
      "Epoch:  8 | Step:  4 | Val Loss:  0.3476364016532898\n",
      "Epoch:  8 | Step:  5 | Val Loss:  0.761002779006958\n",
      "Epoch:  8 | Step:  6 | Val Loss:  0.4815225601196289\n",
      "Epoch:  8 | Step:  7 | Val Loss:  0.42455196380615234\n",
      "Epoch:  8 | Step:  8 | Val Loss:  0.5117160081863403\n",
      "Epoch:  8 | Step:  9 | Val Loss:  0.8703207969665527\n",
      "Epoch:  8 | Step:  10 | Val Loss:  0.44328731298446655\n",
      "Epoch:  8 | Step:  11 | Val Loss:  0.5761723518371582\n",
      "Epoch:  8 | Step:  12 | Val Loss:  0.651350200176239\n",
      "Epoch:  8 | Step:  13 | Val Loss:  0.6176663041114807\n",
      "Epoch:  8 | Step:  14 | Val Loss:  0.5097838640213013\n",
      "Epoch:  8 | Step:  15 | Val Loss:  0.6771988272666931\n",
      "Epoch:  8 | Step:  16 | Val Loss:  0.8056423664093018\n",
      "Epoch:  8 | Step:  17 | Val Loss:  0.7220787405967712\n",
      "Epoch:  8 | Step:  18 | Val Loss:  0.6710938215255737\n",
      "Epoch:  8 | Step:  19 | Val Loss:  0.3607913851737976\n",
      "Epoch:  8 | Step:  20 | Val Loss:  0.5452965497970581\n",
      "Epoch:  8 | Step:  21 | Val Loss:  0.6842451095581055\n",
      "Epoch:  8 | Step:  22 | Val Loss:  0.4651913046836853\n",
      "Epoch:  8 | Step:  23 | Val Loss:  0.801180362701416\n",
      "Epoch:  8 | Step:  24 | Val Loss:  0.5340853929519653\n",
      "Epoch:  8 | Step:  25 | Val Loss:  0.7044563293457031\n",
      "Epoch:  8 | Step:  26 | Val Loss:  0.8425309062004089\n",
      "Epoch:  8 | Step:  27 | Val Loss:  0.7769484519958496\n",
      "Epoch:  8 | Step:  28 | Val Loss:  0.6380935907363892\n",
      "Epoch:  8 | Step:  29 | Val Loss:  0.5675075650215149\n",
      "Epoch:  8 | Step:  30 | Val Loss:  0.618126392364502\n",
      "Epoch:  8 | Step:  31 | Val Loss:  0.4553123116493225\n",
      "Epoch:  8 | Step:  32 | Val Loss:  0.598721981048584\n",
      "Epoch:  8 | Step:  33 | Val Loss:  0.7397691607475281\n",
      "Epoch:  8 | Step:  34 | Val Loss:  0.6239099502563477\n",
      "Epoch:  8 | Step:  35 | Val Loss:  0.7109625339508057\n",
      "Epoch:  8 | Step:  36 | Val Loss:  0.44946998357772827\n",
      "Epoch:  8 | Step:  37 | Val Loss:  0.7214725613594055\n",
      "Epoch:  8 | Step:  38 | Val Loss:  0.9230501651763916\n",
      "Epoch:  8 | Step:  39 | Val Loss:  0.6765035390853882\n",
      "Epoch:  8 | Step:  40 | Val Loss:  0.6230083703994751\n",
      "Epoch:  8 | Step:  41 | Val Loss:  0.553729236125946\n",
      "Epoch:  8 | Step:  42 | Val Loss:  0.45068755745887756\n",
      "Epoch:  8 | Step:  43 | Val Loss:  0.49450406432151794\n",
      "Epoch:  8 | Step:  44 | Val Loss:  0.8259567618370056\n",
      "Epoch:  8 | Step:  45 | Val Loss:  0.6965658664703369\n",
      "Epoch:  8 | Step:  46 | Val Loss:  0.5659164786338806\n",
      "Epoch:  8 | Step:  47 | Val Loss:  0.7278540730476379\n",
      "Epoch:  8 | Step:  48 | Val Loss:  0.45325395464897156\n",
      "Epoch:  8 | Step:  49 | Val Loss:  0.6138637065887451\n",
      "Epoch:  8 | Step:  50 | Val Loss:  0.5245126485824585\n",
      "Epoch:  8 | Step:  51 | Val Loss:  0.6022261381149292\n",
      "Epoch:  8 | Step:  52 | Val Loss:  0.438997358083725\n",
      "Epoch:  8 | Step:  53 | Val Loss:  0.5882000923156738\n",
      "Epoch:  8 | Step:  54 | Val Loss:  0.6816153526306152\n",
      "Epoch:  8 | Step:  55 | Val Loss:  0.47659212350845337\n",
      "Epoch:  8 | Step:  56 | Val Loss:  0.686766505241394\n",
      "Epoch:  8 | Step:  57 | Val Loss:  0.6200893521308899\n",
      "Epoch:  8 | Step:  58 | Val Loss:  0.8504319190979004\n",
      "Epoch:  8 | Step:  59 | Val Loss:  0.6607739925384521\n",
      "Epoch:  8 | Step:  60 | Val Loss:  0.775209903717041\n",
      "Epoch:  8 | Step:  61 | Val Loss:  0.5395666360855103\n",
      "Epoch:  8 | Step:  62 | Val Loss:  0.6897155046463013\n",
      "Epoch:  8 | Step:  63 | Val Loss:  0.3989979028701782\n",
      "Epoch:  8 | Step:  64 | Val Loss:  0.7923122048377991\n",
      "Epoch:  8 | Step:  65 | Val Loss:  0.6645706295967102\n",
      "Epoch:  8 | Step:  66 | Val Loss:  0.5027209520339966\n",
      "Epoch:  8 | Step:  67 | Val Loss:  0.4368223547935486\n",
      "Epoch:  8 | Step:  68 | Val Loss:  0.8413257598876953\n",
      "Epoch:  8 | Step:  69 | Val Loss:  0.5950248837471008\n",
      "Epoch:  8 | Step:  70 | Val Loss:  0.707421600818634\n",
      "Epoch:  8 | Step:  71 | Val Loss:  0.5756332874298096\n",
      "Epoch:  8 | Step:  72 | Val Loss:  0.792435884475708\n",
      "Epoch:  8 | Step:  73 | Val Loss:  0.6464331746101379\n",
      "Epoch:  8 | Step:  74 | Val Loss:  0.5636090040206909\n",
      "Epoch:  8 | Step:  75 | Val Loss:  0.5795823931694031\n",
      "Epoch:  8 | Step:  76 | Val Loss:  0.8011658191680908\n",
      "Epoch:  8 | Step:  77 | Val Loss:  0.4471924901008606\n",
      "Epoch:  8 | Step:  78 | Val Loss:  0.5709336996078491\n",
      "Epoch:  8 | Step:  79 | Val Loss:  0.7153390645980835\n",
      "Epoch:  8 | Step:  80 | Val Loss:  0.7132470607757568\n",
      "Epoch:  8 | Step:  81 | Val Loss:  0.5415900945663452\n",
      "Epoch:  8 | Step:  82 | Val Loss:  0.7829117774963379\n",
      "Epoch:  8 | Step:  83 | Val Loss:  0.5749146938323975\n",
      "Epoch:  8 | Step:  84 | Val Loss:  0.6289039850234985\n",
      "Epoch:  8 | Step:  85 | Val Loss:  0.676082193851471\n",
      "Epoch:  8 | Step:  86 | Val Loss:  0.7056276798248291\n",
      "Epoch:  8 | Step:  87 | Val Loss:  0.5087336301803589\n",
      "Epoch:  8 | Step:  88 | Val Loss:  0.3573518991470337\n",
      "Epoch:  8 | Step:  89 | Val Loss:  0.896526575088501\n",
      "Epoch:  8 | Step:  90 | Val Loss:  0.6202249526977539\n",
      "Epoch:  8 | Step:  91 | Val Loss:  0.43442463874816895\n",
      "Epoch:  8 | Step:  92 | Val Loss:  0.48972272872924805\n",
      "Epoch:  8 | Step:  93 | Val Loss:  0.717941403388977\n",
      "Epoch:  8 | Step:  94 | Val Loss:  0.5355852246284485\n",
      "Epoch:  8 | Step:  95 | Val Loss:  0.4937622547149658\n",
      "Epoch:  8 | Step:  96 | Val Loss:  0.6666337251663208\n",
      "Epoch:  8 | Step:  97 | Val Loss:  0.5603477954864502\n",
      "Epoch:  8 | Step:  98 | Val Loss:  0.5664150714874268\n",
      "Epoch:  8 | Step:  99 | Val Loss:  0.6070181727409363\n",
      "Epoch:  8 | Step:  100 | Val Loss:  0.6979823112487793\n",
      "Epoch:  8 | Step:  101 | Val Loss:  0.546723484992981\n",
      "Epoch:  8 | Step:  102 | Val Loss:  0.6709473729133606\n",
      "Epoch:  8 | Step:  103 | Val Loss:  0.6199151277542114\n",
      "Epoch:  8 | Step:  104 | Val Loss:  0.538415789604187\n",
      "Epoch:  8 | Step:  105 | Val Loss:  0.7821081876754761\n",
      "Epoch:  8 | Step:  106 | Val Loss:  0.573104739189148\n",
      "Epoch:  8 | Step:  107 | Val Loss:  0.709707498550415\n",
      "Epoch:  8 | Step:  108 | Val Loss:  0.5321271419525146\n",
      "Epoch:  8 | Step:  109 | Val Loss:  0.4162675142288208\n",
      "Epoch:  8 | Step:  110 | Val Loss:  0.47815805673599243\n",
      "Epoch:  8 | Step:  111 | Val Loss:  0.5262957215309143\n",
      "Epoch:  8 | Step:  112 | Val Loss:  0.7081973552703857\n",
      "Epoch:  8 | Step:  113 | Val Loss:  0.77000892162323\n",
      "Epoch:  8 | Step:  114 | Val Loss:  0.6438297033309937\n",
      "Epoch:  8 | Step:  115 | Val Loss:  0.6997269988059998\n",
      "Epoch:  8 | Step:  116 | Val Loss:  0.7255662679672241\n",
      "Epoch:  8 | Step:  117 | Val Loss:  0.7250581979751587\n",
      "Epoch:  8 | Step:  118 | Val Loss:  0.39188897609710693\n",
      "Epoch:  8 | Step:  119 | Val Loss:  0.5572457909584045\n",
      "Epoch:  8 | Step:  120 | Val Loss:  0.8417392373085022\n",
      "Epoch:  8 | Step:  121 | Val Loss:  0.307567834854126\n",
      "Epoch:  8 | Step:  122 | Val Loss:  0.8399642705917358\n",
      "Epoch:  8 | Step:  123 | Val Loss:  0.642092764377594\n",
      "Epoch:  8 | Step:  124 | Val Loss:  0.8544276356697083\n",
      "Epoch:  8 | Step:  125 | Val Loss:  0.837088942527771\n",
      "Epoch:  8 | Train Loss:  tensor(0.5777, device='cuda:0') | Val Loss:  tensor(0.6212, device='cuda:0')\n",
      "Epoch:  9 | Step:  500 | Train Loss:  0.5244203805923462\n",
      "Epoch:  9 | Step:  1 | Val Loss:  0.6113584637641907\n",
      "Epoch:  9 | Step:  2 | Val Loss:  0.46860915422439575\n",
      "Epoch:  9 | Step:  3 | Val Loss:  0.6475542187690735\n",
      "Epoch:  9 | Step:  4 | Val Loss:  0.8646246194839478\n",
      "Epoch:  9 | Step:  5 | Val Loss:  0.6939104795455933\n",
      "Epoch:  9 | Step:  6 | Val Loss:  0.6627503633499146\n",
      "Epoch:  9 | Step:  7 | Val Loss:  0.6316732168197632\n",
      "Epoch:  9 | Step:  8 | Val Loss:  0.6242592930793762\n",
      "Epoch:  9 | Step:  9 | Val Loss:  0.5803807377815247\n",
      "Epoch:  9 | Step:  10 | Val Loss:  0.610339343547821\n",
      "Epoch:  9 | Step:  11 | Val Loss:  0.612488865852356\n",
      "Epoch:  9 | Step:  12 | Val Loss:  0.35469067096710205\n",
      "Epoch:  9 | Step:  13 | Val Loss:  0.5950773358345032\n",
      "Epoch:  9 | Step:  14 | Val Loss:  0.7253236174583435\n",
      "Epoch:  9 | Step:  15 | Val Loss:  0.43441951274871826\n",
      "Epoch:  9 | Step:  16 | Val Loss:  0.4994407892227173\n",
      "Epoch:  9 | Step:  17 | Val Loss:  0.49128690361976624\n",
      "Epoch:  9 | Step:  18 | Val Loss:  0.6715953350067139\n",
      "Epoch:  9 | Step:  19 | Val Loss:  0.8611262440681458\n",
      "Epoch:  9 | Step:  20 | Val Loss:  0.6642059683799744\n",
      "Epoch:  9 | Step:  21 | Val Loss:  0.3937975764274597\n",
      "Epoch:  9 | Step:  22 | Val Loss:  0.546073853969574\n",
      "Epoch:  9 | Step:  23 | Val Loss:  0.6086797714233398\n",
      "Epoch:  9 | Step:  24 | Val Loss:  0.5789468884468079\n",
      "Epoch:  9 | Step:  25 | Val Loss:  0.5639784932136536\n",
      "Epoch:  9 | Step:  26 | Val Loss:  0.5667076706886292\n",
      "Epoch:  9 | Step:  27 | Val Loss:  0.6421589255332947\n",
      "Epoch:  9 | Step:  28 | Val Loss:  0.5560381412506104\n",
      "Epoch:  9 | Step:  29 | Val Loss:  0.5634574890136719\n",
      "Epoch:  9 | Step:  30 | Val Loss:  0.5074707269668579\n",
      "Epoch:  9 | Step:  31 | Val Loss:  0.6087328195571899\n",
      "Epoch:  9 | Step:  32 | Val Loss:  0.679085910320282\n",
      "Epoch:  9 | Step:  33 | Val Loss:  0.6297297477722168\n",
      "Epoch:  9 | Step:  34 | Val Loss:  0.568587601184845\n",
      "Epoch:  9 | Step:  35 | Val Loss:  0.6075044870376587\n",
      "Epoch:  9 | Step:  36 | Val Loss:  0.29561516642570496\n",
      "Epoch:  9 | Step:  37 | Val Loss:  0.6063346862792969\n",
      "Epoch:  9 | Step:  38 | Val Loss:  0.6859956979751587\n",
      "Epoch:  9 | Step:  39 | Val Loss:  0.4043562412261963\n",
      "Epoch:  9 | Step:  40 | Val Loss:  0.9926880598068237\n",
      "Epoch:  9 | Step:  41 | Val Loss:  0.6341723799705505\n",
      "Epoch:  9 | Step:  42 | Val Loss:  0.5591426491737366\n",
      "Epoch:  9 | Step:  43 | Val Loss:  0.6933168172836304\n",
      "Epoch:  9 | Step:  44 | Val Loss:  0.5709405541419983\n",
      "Epoch:  9 | Step:  45 | Val Loss:  0.5563936233520508\n",
      "Epoch:  9 | Step:  46 | Val Loss:  0.708685040473938\n",
      "Epoch:  9 | Step:  47 | Val Loss:  0.6494191288948059\n",
      "Epoch:  9 | Step:  48 | Val Loss:  0.7830604314804077\n",
      "Epoch:  9 | Step:  49 | Val Loss:  0.86365807056427\n",
      "Epoch:  9 | Step:  50 | Val Loss:  0.5530880689620972\n",
      "Epoch:  9 | Step:  51 | Val Loss:  0.3927098512649536\n",
      "Epoch:  9 | Step:  52 | Val Loss:  0.754502534866333\n",
      "Epoch:  9 | Step:  53 | Val Loss:  0.6442149877548218\n",
      "Epoch:  9 | Step:  54 | Val Loss:  0.6523429155349731\n",
      "Epoch:  9 | Step:  55 | Val Loss:  0.5774078369140625\n",
      "Epoch:  9 | Step:  56 | Val Loss:  0.6218541264533997\n",
      "Epoch:  9 | Step:  57 | Val Loss:  0.547184944152832\n",
      "Epoch:  9 | Step:  58 | Val Loss:  0.49761199951171875\n",
      "Epoch:  9 | Step:  59 | Val Loss:  0.8075014352798462\n",
      "Epoch:  9 | Step:  60 | Val Loss:  0.4161868691444397\n",
      "Epoch:  9 | Step:  61 | Val Loss:  0.6517783403396606\n",
      "Epoch:  9 | Step:  62 | Val Loss:  0.6674965023994446\n",
      "Epoch:  9 | Step:  63 | Val Loss:  0.5256075263023376\n",
      "Epoch:  9 | Step:  64 | Val Loss:  0.6862735152244568\n",
      "Epoch:  9 | Step:  65 | Val Loss:  0.5867787599563599\n",
      "Epoch:  9 | Step:  66 | Val Loss:  0.4809700846672058\n",
      "Epoch:  9 | Step:  67 | Val Loss:  0.40134644508361816\n",
      "Epoch:  9 | Step:  68 | Val Loss:  0.5935450792312622\n",
      "Epoch:  9 | Step:  69 | Val Loss:  0.6295678615570068\n",
      "Epoch:  9 | Step:  70 | Val Loss:  0.39646875858306885\n",
      "Epoch:  9 | Step:  71 | Val Loss:  0.8181987404823303\n",
      "Epoch:  9 | Step:  72 | Val Loss:  0.5991058349609375\n",
      "Epoch:  9 | Step:  73 | Val Loss:  0.658795952796936\n",
      "Epoch:  9 | Step:  74 | Val Loss:  0.6928623914718628\n",
      "Epoch:  9 | Step:  75 | Val Loss:  0.40197205543518066\n",
      "Epoch:  9 | Step:  76 | Val Loss:  0.7159532308578491\n",
      "Epoch:  9 | Step:  77 | Val Loss:  0.9161760807037354\n",
      "Epoch:  9 | Step:  78 | Val Loss:  0.4482671022415161\n",
      "Epoch:  9 | Step:  79 | Val Loss:  0.5247784852981567\n",
      "Epoch:  9 | Step:  80 | Val Loss:  0.5172946453094482\n",
      "Epoch:  9 | Step:  81 | Val Loss:  0.5899317264556885\n",
      "Epoch:  9 | Step:  82 | Val Loss:  0.5881232023239136\n",
      "Epoch:  9 | Step:  83 | Val Loss:  0.658614993095398\n",
      "Epoch:  9 | Step:  84 | Val Loss:  0.6423409581184387\n",
      "Epoch:  9 | Step:  85 | Val Loss:  0.40854209661483765\n",
      "Epoch:  9 | Step:  86 | Val Loss:  0.6881756782531738\n",
      "Epoch:  9 | Step:  87 | Val Loss:  0.551199197769165\n",
      "Epoch:  9 | Step:  88 | Val Loss:  0.6654139757156372\n",
      "Epoch:  9 | Step:  89 | Val Loss:  0.5533157587051392\n",
      "Epoch:  9 | Step:  90 | Val Loss:  0.5892304182052612\n",
      "Epoch:  9 | Step:  91 | Val Loss:  0.8921748995780945\n",
      "Epoch:  9 | Step:  92 | Val Loss:  0.7647165060043335\n",
      "Epoch:  9 | Step:  93 | Val Loss:  0.6008408069610596\n",
      "Epoch:  9 | Step:  94 | Val Loss:  0.7645841836929321\n",
      "Epoch:  9 | Step:  95 | Val Loss:  0.2997448742389679\n",
      "Epoch:  9 | Step:  96 | Val Loss:  0.666509211063385\n",
      "Epoch:  9 | Step:  97 | Val Loss:  0.5130404233932495\n",
      "Epoch:  9 | Step:  98 | Val Loss:  0.5835639834403992\n",
      "Epoch:  9 | Step:  99 | Val Loss:  0.5713974237442017\n",
      "Epoch:  9 | Step:  100 | Val Loss:  0.639182448387146\n",
      "Epoch:  9 | Step:  101 | Val Loss:  0.6085538864135742\n",
      "Epoch:  9 | Step:  102 | Val Loss:  0.5712472200393677\n",
      "Epoch:  9 | Step:  103 | Val Loss:  0.7086153030395508\n",
      "Epoch:  9 | Step:  104 | Val Loss:  0.6388004422187805\n",
      "Epoch:  9 | Step:  105 | Val Loss:  0.6232627630233765\n",
      "Epoch:  9 | Step:  106 | Val Loss:  0.5767005085945129\n",
      "Epoch:  9 | Step:  107 | Val Loss:  0.5122441053390503\n",
      "Epoch:  9 | Step:  108 | Val Loss:  0.5373764634132385\n",
      "Epoch:  9 | Step:  109 | Val Loss:  0.8645598292350769\n",
      "Epoch:  9 | Step:  110 | Val Loss:  0.7520416975021362\n",
      "Epoch:  9 | Step:  111 | Val Loss:  0.8698758482933044\n",
      "Epoch:  9 | Step:  112 | Val Loss:  0.5913393497467041\n",
      "Epoch:  9 | Step:  113 | Val Loss:  0.782768964767456\n",
      "Epoch:  9 | Step:  114 | Val Loss:  0.7568502426147461\n",
      "Epoch:  9 | Step:  115 | Val Loss:  0.616473913192749\n",
      "Epoch:  9 | Step:  116 | Val Loss:  0.7945283651351929\n",
      "Epoch:  9 | Step:  117 | Val Loss:  0.6820341348648071\n",
      "Epoch:  9 | Step:  118 | Val Loss:  0.6972575783729553\n",
      "Epoch:  9 | Step:  119 | Val Loss:  0.47477269172668457\n",
      "Epoch:  9 | Step:  120 | Val Loss:  0.8301100730895996\n",
      "Epoch:  9 | Step:  121 | Val Loss:  0.6801641583442688\n",
      "Epoch:  9 | Step:  122 | Val Loss:  0.6764221787452698\n",
      "Epoch:  9 | Step:  123 | Val Loss:  0.49054428935050964\n",
      "Epoch:  9 | Step:  124 | Val Loss:  0.7067652940750122\n",
      "Epoch:  9 | Step:  125 | Val Loss:  0.5139249563217163\n",
      "Epoch:  9 | Train Loss:  tensor(0.5731, device='cuda:0') | Val Loss:  tensor(0.6168, device='cuda:0')\n",
      "Epoch:  10 | Step:  500 | Train Loss:  0.5894757509231567\n",
      "Epoch:  10 | Step:  1 | Val Loss:  0.5343786478042603\n",
      "Epoch:  10 | Step:  2 | Val Loss:  0.47982460260391235\n",
      "Epoch:  10 | Step:  3 | Val Loss:  0.6257716417312622\n",
      "Epoch:  10 | Step:  4 | Val Loss:  0.5947473049163818\n",
      "Epoch:  10 | Step:  5 | Val Loss:  0.6112427115440369\n",
      "Epoch:  10 | Step:  6 | Val Loss:  0.612099289894104\n",
      "Epoch:  10 | Step:  7 | Val Loss:  0.6571671366691589\n",
      "Epoch:  10 | Step:  8 | Val Loss:  0.6977895498275757\n",
      "Epoch:  10 | Step:  9 | Val Loss:  0.6871815323829651\n",
      "Epoch:  10 | Step:  10 | Val Loss:  0.9617451429367065\n",
      "Epoch:  10 | Step:  11 | Val Loss:  0.6641979217529297\n",
      "Epoch:  10 | Step:  12 | Val Loss:  0.6310265064239502\n",
      "Epoch:  10 | Step:  13 | Val Loss:  0.5760462284088135\n",
      "Epoch:  10 | Step:  14 | Val Loss:  0.48313722014427185\n",
      "Epoch:  10 | Step:  15 | Val Loss:  0.7118101716041565\n",
      "Epoch:  10 | Step:  16 | Val Loss:  0.6985506415367126\n",
      "Epoch:  10 | Step:  17 | Val Loss:  0.7735822200775146\n",
      "Epoch:  10 | Step:  18 | Val Loss:  0.8572169542312622\n",
      "Epoch:  10 | Step:  19 | Val Loss:  0.837746262550354\n",
      "Epoch:  10 | Step:  20 | Val Loss:  0.7468011975288391\n",
      "Epoch:  10 | Step:  21 | Val Loss:  0.5000629425048828\n",
      "Epoch:  10 | Step:  22 | Val Loss:  0.7026443481445312\n",
      "Epoch:  10 | Step:  23 | Val Loss:  0.4938804507255554\n",
      "Epoch:  10 | Step:  24 | Val Loss:  0.49789297580718994\n",
      "Epoch:  10 | Step:  25 | Val Loss:  0.8266580104827881\n",
      "Epoch:  10 | Step:  26 | Val Loss:  0.5515570640563965\n",
      "Epoch:  10 | Step:  27 | Val Loss:  0.562064528465271\n",
      "Epoch:  10 | Step:  28 | Val Loss:  0.4317101836204529\n",
      "Epoch:  10 | Step:  29 | Val Loss:  0.4008897840976715\n",
      "Epoch:  10 | Step:  30 | Val Loss:  0.5934303402900696\n",
      "Epoch:  10 | Step:  31 | Val Loss:  0.5635896325111389\n",
      "Epoch:  10 | Step:  32 | Val Loss:  0.6068953275680542\n",
      "Epoch:  10 | Step:  33 | Val Loss:  0.6239412426948547\n",
      "Epoch:  10 | Step:  34 | Val Loss:  0.7342008352279663\n",
      "Epoch:  10 | Step:  35 | Val Loss:  0.4092645049095154\n",
      "Epoch:  10 | Step:  36 | Val Loss:  0.6199640035629272\n",
      "Epoch:  10 | Step:  37 | Val Loss:  0.46417665481567383\n",
      "Epoch:  10 | Step:  38 | Val Loss:  0.5763179659843445\n",
      "Epoch:  10 | Step:  39 | Val Loss:  0.5658057928085327\n",
      "Epoch:  10 | Step:  40 | Val Loss:  0.5399808883666992\n",
      "Epoch:  10 | Step:  41 | Val Loss:  0.5828045606613159\n",
      "Epoch:  10 | Step:  42 | Val Loss:  0.9646346569061279\n",
      "Epoch:  10 | Step:  43 | Val Loss:  0.5795819759368896\n",
      "Epoch:  10 | Step:  44 | Val Loss:  0.7748397588729858\n",
      "Epoch:  10 | Step:  45 | Val Loss:  0.7829618453979492\n",
      "Epoch:  10 | Step:  46 | Val Loss:  0.512252926826477\n",
      "Epoch:  10 | Step:  47 | Val Loss:  0.48613834381103516\n",
      "Epoch:  10 | Step:  48 | Val Loss:  0.5779361724853516\n",
      "Epoch:  10 | Step:  49 | Val Loss:  0.5471614599227905\n",
      "Epoch:  10 | Step:  50 | Val Loss:  0.8711552619934082\n",
      "Epoch:  10 | Step:  51 | Val Loss:  0.48931923508644104\n",
      "Epoch:  10 | Step:  52 | Val Loss:  0.6365566253662109\n",
      "Epoch:  10 | Step:  53 | Val Loss:  0.6238608956336975\n",
      "Epoch:  10 | Step:  54 | Val Loss:  0.6272252202033997\n",
      "Epoch:  10 | Step:  55 | Val Loss:  0.7013881206512451\n",
      "Epoch:  10 | Step:  56 | Val Loss:  0.7268351316452026\n",
      "Epoch:  10 | Step:  57 | Val Loss:  0.7047572731971741\n",
      "Epoch:  10 | Step:  58 | Val Loss:  0.7557355165481567\n",
      "Epoch:  10 | Step:  59 | Val Loss:  0.5535144209861755\n",
      "Epoch:  10 | Step:  60 | Val Loss:  0.6685454845428467\n",
      "Epoch:  10 | Step:  61 | Val Loss:  0.5618163347244263\n",
      "Epoch:  10 | Step:  62 | Val Loss:  0.6117109060287476\n",
      "Epoch:  10 | Step:  63 | Val Loss:  0.6810814142227173\n",
      "Epoch:  10 | Step:  64 | Val Loss:  0.6052348613739014\n",
      "Epoch:  10 | Step:  65 | Val Loss:  0.6578631401062012\n",
      "Epoch:  10 | Step:  66 | Val Loss:  0.4574376046657562\n",
      "Epoch:  10 | Step:  67 | Val Loss:  0.504101037979126\n",
      "Epoch:  10 | Step:  68 | Val Loss:  0.5913779735565186\n",
      "Epoch:  10 | Step:  69 | Val Loss:  0.38043415546417236\n",
      "Epoch:  10 | Step:  70 | Val Loss:  0.5576305389404297\n",
      "Epoch:  10 | Step:  71 | Val Loss:  0.3649026155471802\n",
      "Epoch:  10 | Step:  72 | Val Loss:  0.63889479637146\n",
      "Epoch:  10 | Step:  73 | Val Loss:  0.6388693451881409\n",
      "Epoch:  10 | Step:  74 | Val Loss:  0.3545878827571869\n",
      "Epoch:  10 | Step:  75 | Val Loss:  0.44933396577835083\n",
      "Epoch:  10 | Step:  76 | Val Loss:  0.7274601459503174\n",
      "Epoch:  10 | Step:  77 | Val Loss:  0.5162742137908936\n",
      "Epoch:  10 | Step:  78 | Val Loss:  0.5418499708175659\n",
      "Epoch:  10 | Step:  79 | Val Loss:  0.5933026075363159\n",
      "Epoch:  10 | Step:  80 | Val Loss:  0.6203123331069946\n",
      "Epoch:  10 | Step:  81 | Val Loss:  0.5453636050224304\n",
      "Epoch:  10 | Step:  82 | Val Loss:  0.4864954650402069\n",
      "Epoch:  10 | Step:  83 | Val Loss:  0.5860990285873413\n",
      "Epoch:  10 | Step:  84 | Val Loss:  0.720455527305603\n",
      "Epoch:  10 | Step:  85 | Val Loss:  0.5365129709243774\n",
      "Epoch:  10 | Step:  86 | Val Loss:  0.3861822187900543\n",
      "Epoch:  10 | Step:  87 | Val Loss:  0.6642808318138123\n",
      "Epoch:  10 | Step:  88 | Val Loss:  0.5240582823753357\n",
      "Epoch:  10 | Step:  89 | Val Loss:  0.7610740661621094\n",
      "Epoch:  10 | Step:  90 | Val Loss:  0.6220799684524536\n",
      "Epoch:  10 | Step:  91 | Val Loss:  0.5191417932510376\n",
      "Epoch:  10 | Step:  92 | Val Loss:  0.4941539168357849\n",
      "Epoch:  10 | Step:  93 | Val Loss:  0.6688294410705566\n",
      "Epoch:  10 | Step:  94 | Val Loss:  0.6255769729614258\n",
      "Epoch:  10 | Step:  95 | Val Loss:  0.6481713056564331\n",
      "Epoch:  10 | Step:  96 | Val Loss:  0.6170903444290161\n",
      "Epoch:  10 | Step:  97 | Val Loss:  0.643317699432373\n",
      "Epoch:  10 | Step:  98 | Val Loss:  0.7006477117538452\n",
      "Epoch:  10 | Step:  99 | Val Loss:  0.5881757736206055\n",
      "Epoch:  10 | Step:  100 | Val Loss:  0.3189540505409241\n",
      "Epoch:  10 | Step:  101 | Val Loss:  0.5266350507736206\n",
      "Epoch:  10 | Step:  102 | Val Loss:  0.5912909507751465\n",
      "Epoch:  10 | Step:  103 | Val Loss:  0.5295453667640686\n",
      "Epoch:  10 | Step:  104 | Val Loss:  0.8401893377304077\n",
      "Epoch:  10 | Step:  105 | Val Loss:  0.7532931566238403\n",
      "Epoch:  10 | Step:  106 | Val Loss:  0.5507076978683472\n",
      "Epoch:  10 | Step:  107 | Val Loss:  0.6513323783874512\n",
      "Epoch:  10 | Step:  108 | Val Loss:  0.7781468629837036\n",
      "Epoch:  10 | Step:  109 | Val Loss:  0.8022350072860718\n",
      "Epoch:  10 | Step:  110 | Val Loss:  0.4064571261405945\n",
      "Epoch:  10 | Step:  111 | Val Loss:  0.5697447657585144\n",
      "Epoch:  10 | Step:  112 | Val Loss:  0.531470000743866\n",
      "Epoch:  10 | Step:  113 | Val Loss:  0.6540694832801819\n",
      "Epoch:  10 | Step:  114 | Val Loss:  0.7376039028167725\n",
      "Epoch:  10 | Step:  115 | Val Loss:  0.3611987233161926\n",
      "Epoch:  10 | Step:  116 | Val Loss:  0.6133611798286438\n",
      "Epoch:  10 | Step:  117 | Val Loss:  0.9244089722633362\n",
      "Epoch:  10 | Step:  118 | Val Loss:  0.6876400113105774\n",
      "Epoch:  10 | Step:  119 | Val Loss:  0.7390683889389038\n",
      "Epoch:  10 | Step:  120 | Val Loss:  0.5003114342689514\n",
      "Epoch:  10 | Step:  121 | Val Loss:  0.5703045129776001\n",
      "Epoch:  10 | Step:  122 | Val Loss:  0.6978576183319092\n",
      "Epoch:  10 | Step:  123 | Val Loss:  0.834791898727417\n",
      "Epoch:  10 | Step:  124 | Val Loss:  0.48666539788246155\n",
      "Epoch:  10 | Step:  125 | Val Loss:  0.615768313407898\n",
      "Epoch:  10 | Train Loss:  tensor(0.5685, device='cuda:0') | Val Loss:  tensor(0.6123, device='cuda:0')\n",
      "Epoch:  11 | Step:  500 | Train Loss:  0.5322076678276062\n",
      "Epoch:  11 | Step:  1 | Val Loss:  0.7755832672119141\n",
      "Epoch:  11 | Step:  2 | Val Loss:  0.952620267868042\n",
      "Epoch:  11 | Step:  3 | Val Loss:  0.4738185703754425\n",
      "Epoch:  11 | Step:  4 | Val Loss:  0.42543718218803406\n",
      "Epoch:  11 | Step:  5 | Val Loss:  0.6776860356330872\n",
      "Epoch:  11 | Step:  6 | Val Loss:  0.5975438356399536\n",
      "Epoch:  11 | Step:  7 | Val Loss:  0.7841740846633911\n",
      "Epoch:  11 | Step:  8 | Val Loss:  0.5140222311019897\n",
      "Epoch:  11 | Step:  9 | Val Loss:  0.5570581555366516\n",
      "Epoch:  11 | Step:  10 | Val Loss:  0.7665790319442749\n",
      "Epoch:  11 | Step:  11 | Val Loss:  0.5532667636871338\n",
      "Epoch:  11 | Step:  12 | Val Loss:  0.7022331953048706\n",
      "Epoch:  11 | Step:  13 | Val Loss:  0.433409720659256\n",
      "Epoch:  11 | Step:  14 | Val Loss:  0.6480348706245422\n",
      "Epoch:  11 | Step:  15 | Val Loss:  0.7067969441413879\n",
      "Epoch:  11 | Step:  16 | Val Loss:  0.6516531109809875\n",
      "Epoch:  11 | Step:  17 | Val Loss:  0.68520587682724\n",
      "Epoch:  11 | Step:  18 | Val Loss:  0.7424847483634949\n",
      "Epoch:  11 | Step:  19 | Val Loss:  0.640046238899231\n",
      "Epoch:  11 | Step:  20 | Val Loss:  0.7084364891052246\n",
      "Epoch:  11 | Step:  21 | Val Loss:  0.6019027233123779\n",
      "Epoch:  11 | Step:  22 | Val Loss:  0.5186071991920471\n",
      "Epoch:  11 | Step:  23 | Val Loss:  0.5424357056617737\n",
      "Epoch:  11 | Step:  24 | Val Loss:  0.8873340487480164\n",
      "Epoch:  11 | Step:  25 | Val Loss:  0.48775017261505127\n",
      "Epoch:  11 | Step:  26 | Val Loss:  0.5593264698982239\n",
      "Epoch:  11 | Step:  27 | Val Loss:  0.7842644453048706\n",
      "Epoch:  11 | Step:  28 | Val Loss:  0.5058865547180176\n",
      "Epoch:  11 | Step:  29 | Val Loss:  0.7764492034912109\n",
      "Epoch:  11 | Step:  30 | Val Loss:  0.45886436104774475\n",
      "Epoch:  11 | Step:  31 | Val Loss:  0.5858699083328247\n",
      "Epoch:  11 | Step:  32 | Val Loss:  0.6188995838165283\n",
      "Epoch:  11 | Step:  33 | Val Loss:  0.695264458656311\n",
      "Epoch:  11 | Step:  34 | Val Loss:  0.7752836346626282\n",
      "Epoch:  11 | Step:  35 | Val Loss:  0.47359198331832886\n",
      "Epoch:  11 | Step:  36 | Val Loss:  0.5676206350326538\n",
      "Epoch:  11 | Step:  37 | Val Loss:  0.5551312565803528\n",
      "Epoch:  11 | Step:  38 | Val Loss:  0.5849583148956299\n",
      "Epoch:  11 | Step:  39 | Val Loss:  0.6344448328018188\n",
      "Epoch:  11 | Step:  40 | Val Loss:  0.46254605054855347\n",
      "Epoch:  11 | Step:  41 | Val Loss:  0.5722344517707825\n",
      "Epoch:  11 | Step:  42 | Val Loss:  0.38550710678100586\n",
      "Epoch:  11 | Step:  43 | Val Loss:  0.38164377212524414\n",
      "Epoch:  11 | Step:  44 | Val Loss:  0.4039451777935028\n",
      "Epoch:  11 | Step:  45 | Val Loss:  0.6384222507476807\n",
      "Epoch:  11 | Step:  46 | Val Loss:  0.47188514471054077\n",
      "Epoch:  11 | Step:  47 | Val Loss:  0.42561742663383484\n",
      "Epoch:  11 | Step:  48 | Val Loss:  0.6492955088615417\n",
      "Epoch:  11 | Step:  49 | Val Loss:  0.4630495309829712\n",
      "Epoch:  11 | Step:  50 | Val Loss:  0.7237679362297058\n",
      "Epoch:  11 | Step:  51 | Val Loss:  0.5267928242683411\n",
      "Epoch:  11 | Step:  52 | Val Loss:  0.6555725336074829\n",
      "Epoch:  11 | Step:  53 | Val Loss:  0.4438611567020416\n",
      "Epoch:  11 | Step:  54 | Val Loss:  0.6554688215255737\n",
      "Epoch:  11 | Step:  55 | Val Loss:  0.565075159072876\n",
      "Epoch:  11 | Step:  56 | Val Loss:  0.6035581827163696\n",
      "Epoch:  11 | Step:  57 | Val Loss:  0.6010441184043884\n",
      "Epoch:  11 | Step:  58 | Val Loss:  0.5941135287284851\n",
      "Epoch:  11 | Step:  59 | Val Loss:  0.4921470880508423\n",
      "Epoch:  11 | Step:  60 | Val Loss:  0.6925028562545776\n",
      "Epoch:  11 | Step:  61 | Val Loss:  0.659471869468689\n",
      "Epoch:  11 | Step:  62 | Val Loss:  0.5976594686508179\n",
      "Epoch:  11 | Step:  63 | Val Loss:  0.6152418851852417\n",
      "Epoch:  11 | Step:  64 | Val Loss:  0.7228482961654663\n",
      "Epoch:  11 | Step:  65 | Val Loss:  0.6555945873260498\n",
      "Epoch:  11 | Step:  66 | Val Loss:  0.4421018064022064\n",
      "Epoch:  11 | Step:  67 | Val Loss:  0.6359544396400452\n",
      "Epoch:  11 | Step:  68 | Val Loss:  0.6023310422897339\n",
      "Epoch:  11 | Step:  69 | Val Loss:  0.6448253393173218\n",
      "Epoch:  11 | Step:  70 | Val Loss:  0.7633564472198486\n",
      "Epoch:  11 | Step:  71 | Val Loss:  0.6497211456298828\n",
      "Epoch:  11 | Step:  72 | Val Loss:  0.6285903453826904\n",
      "Epoch:  11 | Step:  73 | Val Loss:  0.5772330164909363\n",
      "Epoch:  11 | Step:  74 | Val Loss:  0.5262782573699951\n",
      "Epoch:  11 | Step:  75 | Val Loss:  0.4722643792629242\n",
      "Epoch:  11 | Step:  76 | Val Loss:  0.6087193489074707\n",
      "Epoch:  11 | Step:  77 | Val Loss:  0.6215199828147888\n",
      "Epoch:  11 | Step:  78 | Val Loss:  0.4377579689025879\n",
      "Epoch:  11 | Step:  79 | Val Loss:  0.6079269647598267\n",
      "Epoch:  11 | Step:  80 | Val Loss:  0.6027100086212158\n",
      "Epoch:  11 | Step:  81 | Val Loss:  0.5625066161155701\n",
      "Epoch:  11 | Step:  82 | Val Loss:  0.5880351066589355\n",
      "Epoch:  11 | Step:  83 | Val Loss:  0.6631696820259094\n",
      "Epoch:  11 | Step:  84 | Val Loss:  0.5551401376724243\n",
      "Epoch:  11 | Step:  85 | Val Loss:  0.7607842087745667\n",
      "Epoch:  11 | Step:  86 | Val Loss:  0.7013508677482605\n",
      "Epoch:  11 | Step:  87 | Val Loss:  0.30271032452583313\n",
      "Epoch:  11 | Step:  88 | Val Loss:  0.6116921901702881\n",
      "Epoch:  11 | Step:  89 | Val Loss:  0.6929827928543091\n",
      "Epoch:  11 | Step:  90 | Val Loss:  0.641486644744873\n",
      "Epoch:  11 | Step:  91 | Val Loss:  0.4415368437767029\n",
      "Epoch:  11 | Step:  92 | Val Loss:  0.5826084613800049\n",
      "Epoch:  11 | Step:  93 | Val Loss:  0.674042284488678\n",
      "Epoch:  11 | Step:  94 | Val Loss:  0.7819312810897827\n",
      "Epoch:  11 | Step:  95 | Val Loss:  0.26405322551727295\n",
      "Epoch:  11 | Step:  96 | Val Loss:  0.882172703742981\n",
      "Epoch:  11 | Step:  97 | Val Loss:  0.6491550803184509\n",
      "Epoch:  11 | Step:  98 | Val Loss:  0.396493136882782\n",
      "Epoch:  11 | Step:  99 | Val Loss:  0.8167562484741211\n",
      "Epoch:  11 | Step:  100 | Val Loss:  0.5906437635421753\n",
      "Epoch:  11 | Step:  101 | Val Loss:  0.6929439306259155\n",
      "Epoch:  11 | Step:  102 | Val Loss:  0.47312939167022705\n",
      "Epoch:  11 | Step:  103 | Val Loss:  0.579034686088562\n",
      "Epoch:  11 | Step:  104 | Val Loss:  0.2888784408569336\n",
      "Epoch:  11 | Step:  105 | Val Loss:  0.5415133833885193\n",
      "Epoch:  11 | Step:  106 | Val Loss:  0.49027347564697266\n",
      "Epoch:  11 | Step:  107 | Val Loss:  0.6194593906402588\n",
      "Epoch:  11 | Step:  108 | Val Loss:  0.49751904606819153\n",
      "Epoch:  11 | Step:  109 | Val Loss:  0.48682838678359985\n",
      "Epoch:  11 | Step:  110 | Val Loss:  0.664422869682312\n",
      "Epoch:  11 | Step:  111 | Val Loss:  0.6604361534118652\n",
      "Epoch:  11 | Step:  112 | Val Loss:  0.883703351020813\n",
      "Epoch:  11 | Step:  113 | Val Loss:  0.6867703199386597\n",
      "Epoch:  11 | Step:  114 | Val Loss:  0.808664858341217\n",
      "Epoch:  11 | Step:  115 | Val Loss:  0.7328907251358032\n",
      "Epoch:  11 | Step:  116 | Val Loss:  0.40761613845825195\n",
      "Epoch:  11 | Step:  117 | Val Loss:  0.6301311254501343\n",
      "Epoch:  11 | Step:  118 | Val Loss:  0.6974065899848938\n",
      "Epoch:  11 | Step:  119 | Val Loss:  0.7213531732559204\n",
      "Epoch:  11 | Step:  120 | Val Loss:  0.7630431652069092\n",
      "Epoch:  11 | Step:  121 | Val Loss:  0.5178080797195435\n",
      "Epoch:  11 | Step:  122 | Val Loss:  0.7612721920013428\n",
      "Epoch:  11 | Step:  123 | Val Loss:  0.414897620677948\n",
      "Epoch:  11 | Step:  124 | Val Loss:  0.854129433631897\n",
      "Epoch:  11 | Step:  125 | Val Loss:  0.6936827898025513\n",
      "Epoch:  11 | Train Loss:  tensor(0.5651, device='cuda:0') | Val Loss:  tensor(0.6065, device='cuda:0')\n",
      "Epoch:  12 | Step:  500 | Train Loss:  0.5310754179954529\n",
      "Epoch:  12 | Step:  1 | Val Loss:  0.6055254936218262\n",
      "Epoch:  12 | Step:  2 | Val Loss:  0.6726424694061279\n",
      "Epoch:  12 | Step:  3 | Val Loss:  0.5515896081924438\n",
      "Epoch:  12 | Step:  4 | Val Loss:  0.6655498743057251\n",
      "Epoch:  12 | Step:  5 | Val Loss:  0.5474564433097839\n",
      "Epoch:  12 | Step:  6 | Val Loss:  0.6714295148849487\n",
      "Epoch:  12 | Step:  7 | Val Loss:  0.682689905166626\n",
      "Epoch:  12 | Step:  8 | Val Loss:  0.4301782250404358\n",
      "Epoch:  12 | Step:  9 | Val Loss:  0.6017163991928101\n",
      "Epoch:  12 | Step:  10 | Val Loss:  0.521345317363739\n",
      "Epoch:  12 | Step:  11 | Val Loss:  0.5778599381446838\n",
      "Epoch:  12 | Step:  12 | Val Loss:  0.49358198046684265\n",
      "Epoch:  12 | Step:  13 | Val Loss:  0.74488365650177\n",
      "Epoch:  12 | Step:  14 | Val Loss:  0.44693952798843384\n",
      "Epoch:  12 | Step:  15 | Val Loss:  0.6652622222900391\n",
      "Epoch:  12 | Step:  16 | Val Loss:  0.5098041892051697\n",
      "Epoch:  12 | Step:  17 | Val Loss:  0.36434808373451233\n",
      "Epoch:  12 | Step:  18 | Val Loss:  0.4770631492137909\n",
      "Epoch:  12 | Step:  19 | Val Loss:  0.6149204969406128\n",
      "Epoch:  12 | Step:  20 | Val Loss:  0.5962740182876587\n",
      "Epoch:  12 | Step:  21 | Val Loss:  0.7109737396240234\n",
      "Epoch:  12 | Step:  22 | Val Loss:  0.8692386150360107\n",
      "Epoch:  12 | Step:  23 | Val Loss:  0.5356091856956482\n",
      "Epoch:  12 | Step:  24 | Val Loss:  0.5698589086532593\n",
      "Epoch:  12 | Step:  25 | Val Loss:  0.4038695693016052\n",
      "Epoch:  12 | Step:  26 | Val Loss:  0.5910449028015137\n",
      "Epoch:  12 | Step:  27 | Val Loss:  0.5436551570892334\n",
      "Epoch:  12 | Step:  28 | Val Loss:  0.7386497259140015\n",
      "Epoch:  12 | Step:  29 | Val Loss:  0.6834375262260437\n",
      "Epoch:  12 | Step:  30 | Val Loss:  0.45945650339126587\n",
      "Epoch:  12 | Step:  31 | Val Loss:  0.6264667510986328\n",
      "Epoch:  12 | Step:  32 | Val Loss:  0.3792095184326172\n",
      "Epoch:  12 | Step:  33 | Val Loss:  0.42768609523773193\n",
      "Epoch:  12 | Step:  34 | Val Loss:  0.5245527625083923\n",
      "Epoch:  12 | Step:  35 | Val Loss:  0.49371397495269775\n",
      "Epoch:  12 | Step:  36 | Val Loss:  0.6676573157310486\n",
      "Epoch:  12 | Step:  37 | Val Loss:  0.7264906764030457\n",
      "Epoch:  12 | Step:  38 | Val Loss:  0.7107823491096497\n",
      "Epoch:  12 | Step:  39 | Val Loss:  0.6840965747833252\n",
      "Epoch:  12 | Step:  40 | Val Loss:  0.7155618667602539\n",
      "Epoch:  12 | Step:  41 | Val Loss:  0.4860081672668457\n",
      "Epoch:  12 | Step:  42 | Val Loss:  0.45343703031539917\n",
      "Epoch:  12 | Step:  43 | Val Loss:  0.2371538281440735\n",
      "Epoch:  12 | Step:  44 | Val Loss:  1.0176544189453125\n",
      "Epoch:  12 | Step:  45 | Val Loss:  0.8069190382957458\n",
      "Epoch:  12 | Step:  46 | Val Loss:  0.7915330529212952\n",
      "Epoch:  12 | Step:  47 | Val Loss:  0.7675531506538391\n",
      "Epoch:  12 | Step:  48 | Val Loss:  0.6807029843330383\n",
      "Epoch:  12 | Step:  49 | Val Loss:  0.5773372650146484\n",
      "Epoch:  12 | Step:  50 | Val Loss:  0.4808681011199951\n",
      "Epoch:  12 | Step:  51 | Val Loss:  0.6117517948150635\n",
      "Epoch:  12 | Step:  52 | Val Loss:  0.6577585935592651\n",
      "Epoch:  12 | Step:  53 | Val Loss:  0.7594228982925415\n",
      "Epoch:  12 | Step:  54 | Val Loss:  0.6916332244873047\n",
      "Epoch:  12 | Step:  55 | Val Loss:  0.8201169371604919\n",
      "Epoch:  12 | Step:  56 | Val Loss:  0.7755297422409058\n",
      "Epoch:  12 | Step:  57 | Val Loss:  0.5092023611068726\n",
      "Epoch:  12 | Step:  58 | Val Loss:  0.6148453950881958\n",
      "Epoch:  12 | Step:  59 | Val Loss:  0.6160863637924194\n",
      "Epoch:  12 | Step:  60 | Val Loss:  0.5747728943824768\n",
      "Epoch:  12 | Step:  61 | Val Loss:  0.5099987983703613\n",
      "Epoch:  12 | Step:  62 | Val Loss:  0.594457745552063\n",
      "Epoch:  12 | Step:  63 | Val Loss:  0.5805165767669678\n",
      "Epoch:  12 | Step:  64 | Val Loss:  0.7559695243835449\n",
      "Epoch:  12 | Step:  65 | Val Loss:  0.580069363117218\n",
      "Epoch:  12 | Step:  66 | Val Loss:  0.5734362602233887\n",
      "Epoch:  12 | Step:  67 | Val Loss:  0.6732046604156494\n",
      "Epoch:  12 | Step:  68 | Val Loss:  0.5811079144477844\n",
      "Epoch:  12 | Step:  69 | Val Loss:  0.5167949199676514\n",
      "Epoch:  12 | Step:  70 | Val Loss:  0.6982003450393677\n",
      "Epoch:  12 | Step:  71 | Val Loss:  0.5856742262840271\n",
      "Epoch:  12 | Step:  72 | Val Loss:  0.48986053466796875\n",
      "Epoch:  12 | Step:  73 | Val Loss:  0.6877835988998413\n",
      "Epoch:  12 | Step:  74 | Val Loss:  0.47143709659576416\n",
      "Epoch:  12 | Step:  75 | Val Loss:  0.6941943168640137\n",
      "Epoch:  12 | Step:  76 | Val Loss:  0.6968971490859985\n",
      "Epoch:  12 | Step:  77 | Val Loss:  0.7270568609237671\n",
      "Epoch:  12 | Step:  78 | Val Loss:  0.5155705213546753\n",
      "Epoch:  12 | Step:  79 | Val Loss:  0.37063586711883545\n",
      "Epoch:  12 | Step:  80 | Val Loss:  0.5838578343391418\n",
      "Epoch:  12 | Step:  81 | Val Loss:  0.7538138628005981\n",
      "Epoch:  12 | Step:  82 | Val Loss:  0.6976243853569031\n",
      "Epoch:  12 | Step:  83 | Val Loss:  0.553191602230072\n",
      "Epoch:  12 | Step:  84 | Val Loss:  0.3666735291481018\n",
      "Epoch:  12 | Step:  85 | Val Loss:  0.5113040208816528\n",
      "Epoch:  12 | Step:  86 | Val Loss:  0.6724980473518372\n",
      "Epoch:  12 | Step:  87 | Val Loss:  0.5488027334213257\n",
      "Epoch:  12 | Step:  88 | Val Loss:  0.5978259444236755\n",
      "Epoch:  12 | Step:  89 | Val Loss:  0.5776329040527344\n",
      "Epoch:  12 | Step:  90 | Val Loss:  0.6517130732536316\n",
      "Epoch:  12 | Step:  91 | Val Loss:  0.543826162815094\n",
      "Epoch:  12 | Step:  92 | Val Loss:  0.7791669368743896\n",
      "Epoch:  12 | Step:  93 | Val Loss:  0.4383794069290161\n",
      "Epoch:  12 | Step:  94 | Val Loss:  0.8736519813537598\n",
      "Epoch:  12 | Step:  95 | Val Loss:  0.5828340649604797\n",
      "Epoch:  12 | Step:  96 | Val Loss:  0.6914998292922974\n",
      "Epoch:  12 | Step:  97 | Val Loss:  0.699569821357727\n",
      "Epoch:  12 | Step:  98 | Val Loss:  0.22935578227043152\n",
      "Epoch:  12 | Step:  99 | Val Loss:  0.7677401304244995\n",
      "Epoch:  12 | Step:  100 | Val Loss:  0.5618163347244263\n",
      "Epoch:  12 | Step:  101 | Val Loss:  0.5711386203765869\n",
      "Epoch:  12 | Step:  102 | Val Loss:  0.6675273180007935\n",
      "Epoch:  12 | Step:  103 | Val Loss:  0.47285428643226624\n",
      "Epoch:  12 | Step:  104 | Val Loss:  0.28096574544906616\n",
      "Epoch:  12 | Step:  105 | Val Loss:  0.5516347289085388\n",
      "Epoch:  12 | Step:  106 | Val Loss:  0.6177718639373779\n",
      "Epoch:  12 | Step:  107 | Val Loss:  0.4952567219734192\n",
      "Epoch:  12 | Step:  108 | Val Loss:  0.6700742244720459\n",
      "Epoch:  12 | Step:  109 | Val Loss:  0.44961878657341003\n",
      "Epoch:  12 | Step:  110 | Val Loss:  0.4727059006690979\n",
      "Epoch:  12 | Step:  111 | Val Loss:  0.8164286017417908\n",
      "Epoch:  12 | Step:  112 | Val Loss:  0.8377813100814819\n",
      "Epoch:  12 | Step:  113 | Val Loss:  0.6002848744392395\n",
      "Epoch:  12 | Step:  114 | Val Loss:  0.7182576656341553\n",
      "Epoch:  12 | Step:  115 | Val Loss:  0.762814998626709\n",
      "Epoch:  12 | Step:  116 | Val Loss:  0.4771287441253662\n",
      "Epoch:  12 | Step:  117 | Val Loss:  0.6500250697135925\n",
      "Epoch:  12 | Step:  118 | Val Loss:  0.5620461106300354\n",
      "Epoch:  12 | Step:  119 | Val Loss:  0.6371166706085205\n",
      "Epoch:  12 | Step:  120 | Val Loss:  0.6568162441253662\n",
      "Epoch:  12 | Step:  121 | Val Loss:  0.6920245885848999\n",
      "Epoch:  12 | Step:  122 | Val Loss:  0.6061563491821289\n",
      "Epoch:  12 | Step:  123 | Val Loss:  0.7016844749450684\n",
      "Epoch:  12 | Step:  124 | Val Loss:  0.6810188293457031\n",
      "Epoch:  12 | Step:  125 | Val Loss:  0.3435285687446594\n",
      "Epoch:  12 | Train Loss:  tensor(0.5614, device='cuda:0') | Val Loss:  tensor(0.6033, device='cuda:0')\n",
      "Epoch:  13 | Step:  500 | Train Loss:  0.7976566553115845\n",
      "Epoch:  13 | Step:  1 | Val Loss:  0.747643768787384\n",
      "Epoch:  13 | Step:  2 | Val Loss:  0.5279050469398499\n",
      "Epoch:  13 | Step:  3 | Val Loss:  0.5483501553535461\n",
      "Epoch:  13 | Step:  4 | Val Loss:  0.5895929336547852\n",
      "Epoch:  13 | Step:  5 | Val Loss:  0.5938928127288818\n",
      "Epoch:  13 | Step:  6 | Val Loss:  0.6455216407775879\n",
      "Epoch:  13 | Step:  7 | Val Loss:  0.7783606052398682\n",
      "Epoch:  13 | Step:  8 | Val Loss:  0.3560984432697296\n",
      "Epoch:  13 | Step:  9 | Val Loss:  0.4379661977291107\n",
      "Epoch:  13 | Step:  10 | Val Loss:  0.6788508296012878\n",
      "Epoch:  13 | Step:  11 | Val Loss:  0.7257660031318665\n",
      "Epoch:  13 | Step:  12 | Val Loss:  0.48433032631874084\n",
      "Epoch:  13 | Step:  13 | Val Loss:  0.6868398785591125\n",
      "Epoch:  13 | Step:  14 | Val Loss:  0.5005265474319458\n",
      "Epoch:  13 | Step:  15 | Val Loss:  0.5235710740089417\n",
      "Epoch:  13 | Step:  16 | Val Loss:  0.35807791352272034\n",
      "Epoch:  13 | Step:  17 | Val Loss:  0.5932979583740234\n",
      "Epoch:  13 | Step:  18 | Val Loss:  0.5970662832260132\n",
      "Epoch:  13 | Step:  19 | Val Loss:  0.42432719469070435\n",
      "Epoch:  13 | Step:  20 | Val Loss:  0.7620479464530945\n",
      "Epoch:  13 | Step:  21 | Val Loss:  0.5208185911178589\n",
      "Epoch:  13 | Step:  22 | Val Loss:  0.7444469928741455\n",
      "Epoch:  13 | Step:  23 | Val Loss:  0.7335706949234009\n",
      "Epoch:  13 | Step:  24 | Val Loss:  0.5730026960372925\n",
      "Epoch:  13 | Step:  25 | Val Loss:  0.7238991260528564\n",
      "Epoch:  13 | Step:  26 | Val Loss:  0.6264164447784424\n",
      "Epoch:  13 | Step:  27 | Val Loss:  0.8080703020095825\n",
      "Epoch:  13 | Step:  28 | Val Loss:  0.4938141703605652\n",
      "Epoch:  13 | Step:  29 | Val Loss:  0.6850801706314087\n",
      "Epoch:  13 | Step:  30 | Val Loss:  0.5948739051818848\n",
      "Epoch:  13 | Step:  31 | Val Loss:  0.5425446033477783\n",
      "Epoch:  13 | Step:  32 | Val Loss:  0.49282169342041016\n",
      "Epoch:  13 | Step:  33 | Val Loss:  0.4838797450065613\n",
      "Epoch:  13 | Step:  34 | Val Loss:  0.6030288934707642\n",
      "Epoch:  13 | Step:  35 | Val Loss:  0.5999306440353394\n",
      "Epoch:  13 | Step:  36 | Val Loss:  0.7808539867401123\n",
      "Epoch:  13 | Step:  37 | Val Loss:  0.5418176054954529\n",
      "Epoch:  13 | Step:  38 | Val Loss:  0.6076077818870544\n",
      "Epoch:  13 | Step:  39 | Val Loss:  0.5579982399940491\n",
      "Epoch:  13 | Step:  40 | Val Loss:  0.8333433866500854\n",
      "Epoch:  13 | Step:  41 | Val Loss:  0.6664318442344666\n",
      "Epoch:  13 | Step:  42 | Val Loss:  0.5966516733169556\n",
      "Epoch:  13 | Step:  43 | Val Loss:  0.6399868130683899\n",
      "Epoch:  13 | Step:  44 | Val Loss:  0.5803521275520325\n",
      "Epoch:  13 | Step:  45 | Val Loss:  0.4687839150428772\n",
      "Epoch:  13 | Step:  46 | Val Loss:  0.5079340934753418\n",
      "Epoch:  13 | Step:  47 | Val Loss:  0.6332347393035889\n",
      "Epoch:  13 | Step:  48 | Val Loss:  0.6665526628494263\n",
      "Epoch:  13 | Step:  49 | Val Loss:  0.45108312368392944\n",
      "Epoch:  13 | Step:  50 | Val Loss:  0.6591206789016724\n",
      "Epoch:  13 | Step:  51 | Val Loss:  0.616240918636322\n",
      "Epoch:  13 | Step:  52 | Val Loss:  0.5685251355171204\n",
      "Epoch:  13 | Step:  53 | Val Loss:  0.7196756601333618\n",
      "Epoch:  13 | Step:  54 | Val Loss:  0.5833542943000793\n",
      "Epoch:  13 | Step:  55 | Val Loss:  0.5848590135574341\n",
      "Epoch:  13 | Step:  56 | Val Loss:  0.7144293785095215\n",
      "Epoch:  13 | Step:  57 | Val Loss:  0.35149282217025757\n",
      "Epoch:  13 | Step:  58 | Val Loss:  0.5912079215049744\n",
      "Epoch:  13 | Step:  59 | Val Loss:  0.6399853229522705\n",
      "Epoch:  13 | Step:  60 | Val Loss:  0.586256742477417\n",
      "Epoch:  13 | Step:  61 | Val Loss:  0.3201426863670349\n",
      "Epoch:  13 | Step:  62 | Val Loss:  0.48394519090652466\n",
      "Epoch:  13 | Step:  63 | Val Loss:  0.5334003567695618\n",
      "Epoch:  13 | Step:  64 | Val Loss:  0.3681863844394684\n",
      "Epoch:  13 | Step:  65 | Val Loss:  0.7572847604751587\n",
      "Epoch:  13 | Step:  66 | Val Loss:  0.5167486667633057\n",
      "Epoch:  13 | Step:  67 | Val Loss:  0.6617860794067383\n",
      "Epoch:  13 | Step:  68 | Val Loss:  0.37987250089645386\n",
      "Epoch:  13 | Step:  69 | Val Loss:  0.5612471103668213\n",
      "Epoch:  13 | Step:  70 | Val Loss:  0.526679277420044\n",
      "Epoch:  13 | Step:  71 | Val Loss:  0.731703519821167\n",
      "Epoch:  13 | Step:  72 | Val Loss:  0.412708580493927\n",
      "Epoch:  13 | Step:  73 | Val Loss:  0.6502615809440613\n",
      "Epoch:  13 | Step:  74 | Val Loss:  0.7432462573051453\n",
      "Epoch:  13 | Step:  75 | Val Loss:  0.5584275722503662\n",
      "Epoch:  13 | Step:  76 | Val Loss:  0.7933703064918518\n",
      "Epoch:  13 | Step:  77 | Val Loss:  0.7287994027137756\n",
      "Epoch:  13 | Step:  78 | Val Loss:  0.6859298944473267\n",
      "Epoch:  13 | Step:  79 | Val Loss:  0.5689549446105957\n",
      "Epoch:  13 | Step:  80 | Val Loss:  0.7732216119766235\n",
      "Epoch:  13 | Step:  81 | Val Loss:  0.6230239868164062\n",
      "Epoch:  13 | Step:  82 | Val Loss:  0.46063292026519775\n",
      "Epoch:  13 | Step:  83 | Val Loss:  0.5577062964439392\n",
      "Epoch:  13 | Step:  84 | Val Loss:  0.5654070377349854\n",
      "Epoch:  13 | Step:  85 | Val Loss:  0.6383016109466553\n",
      "Epoch:  13 | Step:  86 | Val Loss:  0.652772068977356\n",
      "Epoch:  13 | Step:  87 | Val Loss:  0.5732651948928833\n",
      "Epoch:  13 | Step:  88 | Val Loss:  0.764458954334259\n",
      "Epoch:  13 | Step:  89 | Val Loss:  0.7753713130950928\n",
      "Epoch:  13 | Step:  90 | Val Loss:  0.43689030408859253\n",
      "Epoch:  13 | Step:  91 | Val Loss:  0.7350571155548096\n",
      "Epoch:  13 | Step:  92 | Val Loss:  1.0658206939697266\n",
      "Epoch:  13 | Step:  93 | Val Loss:  0.4111291170120239\n",
      "Epoch:  13 | Step:  94 | Val Loss:  0.456770658493042\n",
      "Epoch:  13 | Step:  95 | Val Loss:  0.47137248516082764\n",
      "Epoch:  13 | Step:  96 | Val Loss:  0.23796868324279785\n",
      "Epoch:  13 | Step:  97 | Val Loss:  0.5983915328979492\n",
      "Epoch:  13 | Step:  98 | Val Loss:  0.5243706107139587\n",
      "Epoch:  13 | Step:  99 | Val Loss:  0.5445891618728638\n",
      "Epoch:  13 | Step:  100 | Val Loss:  0.3376837968826294\n",
      "Epoch:  13 | Step:  101 | Val Loss:  0.4336497187614441\n",
      "Epoch:  13 | Step:  102 | Val Loss:  0.6559739112854004\n",
      "Epoch:  13 | Step:  103 | Val Loss:  0.6597571969032288\n",
      "Epoch:  13 | Step:  104 | Val Loss:  0.7116498947143555\n",
      "Epoch:  13 | Step:  105 | Val Loss:  0.6065305471420288\n",
      "Epoch:  13 | Step:  106 | Val Loss:  0.5806819200515747\n",
      "Epoch:  13 | Step:  107 | Val Loss:  0.6160789728164673\n",
      "Epoch:  13 | Step:  108 | Val Loss:  0.5539149045944214\n",
      "Epoch:  13 | Step:  109 | Val Loss:  0.6547163724899292\n",
      "Epoch:  13 | Step:  110 | Val Loss:  0.7212668061256409\n",
      "Epoch:  13 | Step:  111 | Val Loss:  0.6510971784591675\n",
      "Epoch:  13 | Step:  112 | Val Loss:  0.7500196695327759\n",
      "Epoch:  13 | Step:  113 | Val Loss:  0.728691577911377\n",
      "Epoch:  13 | Step:  114 | Val Loss:  0.7320736646652222\n",
      "Epoch:  13 | Step:  115 | Val Loss:  0.4673328995704651\n",
      "Epoch:  13 | Step:  116 | Val Loss:  0.6363487839698792\n",
      "Epoch:  13 | Step:  117 | Val Loss:  0.6966933012008667\n",
      "Epoch:  13 | Step:  118 | Val Loss:  0.5988204479217529\n",
      "Epoch:  13 | Step:  119 | Val Loss:  0.6236890554428101\n",
      "Epoch:  13 | Step:  120 | Val Loss:  0.7165389657020569\n",
      "Epoch:  13 | Step:  121 | Val Loss:  0.6385996341705322\n",
      "Epoch:  13 | Step:  122 | Val Loss:  0.38705572485923767\n",
      "Epoch:  13 | Step:  123 | Val Loss:  0.6740285754203796\n",
      "Epoch:  13 | Step:  124 | Val Loss:  0.7207162380218506\n",
      "Epoch:  13 | Step:  125 | Val Loss:  0.5347151160240173\n",
      "Epoch:  13 | Train Loss:  tensor(0.5589, device='cuda:0') | Val Loss:  tensor(0.5990, device='cuda:0')\n",
      "Epoch:  14 | Step:  500 | Train Loss:  0.5369832515716553\n",
      "Epoch:  14 | Step:  1 | Val Loss:  0.6955820322036743\n",
      "Epoch:  14 | Step:  2 | Val Loss:  0.5743143558502197\n",
      "Epoch:  14 | Step:  3 | Val Loss:  0.37664610147476196\n",
      "Epoch:  14 | Step:  4 | Val Loss:  0.6398403644561768\n",
      "Epoch:  14 | Step:  5 | Val Loss:  0.5541781187057495\n",
      "Epoch:  14 | Step:  6 | Val Loss:  0.6162605285644531\n",
      "Epoch:  14 | Step:  7 | Val Loss:  0.7494771480560303\n",
      "Epoch:  14 | Step:  8 | Val Loss:  0.609514057636261\n",
      "Epoch:  14 | Step:  9 | Val Loss:  0.5811841487884521\n",
      "Epoch:  14 | Step:  10 | Val Loss:  0.5453778505325317\n",
      "Epoch:  14 | Step:  11 | Val Loss:  0.45925232768058777\n",
      "Epoch:  14 | Step:  12 | Val Loss:  0.47239184379577637\n",
      "Epoch:  14 | Step:  13 | Val Loss:  0.5967317223548889\n",
      "Epoch:  14 | Step:  14 | Val Loss:  0.392645001411438\n",
      "Epoch:  14 | Step:  15 | Val Loss:  0.41781875491142273\n",
      "Epoch:  14 | Step:  16 | Val Loss:  0.7310518026351929\n",
      "Epoch:  14 | Step:  17 | Val Loss:  0.7215684652328491\n",
      "Epoch:  14 | Step:  18 | Val Loss:  0.5937551856040955\n",
      "Epoch:  14 | Step:  19 | Val Loss:  0.7211239337921143\n",
      "Epoch:  14 | Step:  20 | Val Loss:  0.4011877179145813\n",
      "Epoch:  14 | Step:  21 | Val Loss:  0.5796924829483032\n",
      "Epoch:  14 | Step:  22 | Val Loss:  0.7622233629226685\n",
      "Epoch:  14 | Step:  23 | Val Loss:  0.6498395800590515\n",
      "Epoch:  14 | Step:  24 | Val Loss:  0.47264227271080017\n",
      "Epoch:  14 | Step:  25 | Val Loss:  0.7524644732475281\n",
      "Epoch:  14 | Step:  26 | Val Loss:  0.7474544048309326\n",
      "Epoch:  14 | Step:  27 | Val Loss:  0.533798336982727\n",
      "Epoch:  14 | Step:  28 | Val Loss:  0.5728296637535095\n",
      "Epoch:  14 | Step:  29 | Val Loss:  0.5288631319999695\n",
      "Epoch:  14 | Step:  30 | Val Loss:  0.5728877782821655\n",
      "Epoch:  14 | Step:  31 | Val Loss:  0.5017718076705933\n",
      "Epoch:  14 | Step:  32 | Val Loss:  0.5605145692825317\n",
      "Epoch:  14 | Step:  33 | Val Loss:  0.38422131538391113\n",
      "Epoch:  14 | Step:  34 | Val Loss:  0.5870569944381714\n",
      "Epoch:  14 | Step:  35 | Val Loss:  0.6089737415313721\n",
      "Epoch:  14 | Step:  36 | Val Loss:  0.83445143699646\n",
      "Epoch:  14 | Step:  37 | Val Loss:  0.6549796462059021\n",
      "Epoch:  14 | Step:  38 | Val Loss:  0.6201042532920837\n",
      "Epoch:  14 | Step:  39 | Val Loss:  0.48359519243240356\n",
      "Epoch:  14 | Step:  40 | Val Loss:  0.5798105001449585\n",
      "Epoch:  14 | Step:  41 | Val Loss:  0.5941085815429688\n",
      "Epoch:  14 | Step:  42 | Val Loss:  0.6562705039978027\n",
      "Epoch:  14 | Step:  43 | Val Loss:  0.6547992825508118\n",
      "Epoch:  14 | Step:  44 | Val Loss:  0.7234213352203369\n",
      "Epoch:  14 | Step:  45 | Val Loss:  0.3432164788246155\n",
      "Epoch:  14 | Step:  46 | Val Loss:  0.6282923221588135\n",
      "Epoch:  14 | Step:  47 | Val Loss:  0.6812499761581421\n",
      "Epoch:  14 | Step:  48 | Val Loss:  0.7097669839859009\n",
      "Epoch:  14 | Step:  49 | Val Loss:  0.8325610160827637\n",
      "Epoch:  14 | Step:  50 | Val Loss:  0.6250213384628296\n",
      "Epoch:  14 | Step:  51 | Val Loss:  0.3972489535808563\n",
      "Epoch:  14 | Step:  52 | Val Loss:  0.8408994078636169\n",
      "Epoch:  14 | Step:  53 | Val Loss:  0.5900903344154358\n",
      "Epoch:  14 | Step:  54 | Val Loss:  0.3909611999988556\n",
      "Epoch:  14 | Step:  55 | Val Loss:  0.6956818103790283\n",
      "Epoch:  14 | Step:  56 | Val Loss:  0.5806666612625122\n",
      "Epoch:  14 | Step:  57 | Val Loss:  0.6177188158035278\n",
      "Epoch:  14 | Step:  58 | Val Loss:  0.5020635724067688\n",
      "Epoch:  14 | Step:  59 | Val Loss:  0.6401879191398621\n",
      "Epoch:  14 | Step:  60 | Val Loss:  0.7456072568893433\n",
      "Epoch:  14 | Step:  61 | Val Loss:  0.8032511472702026\n",
      "Epoch:  14 | Step:  62 | Val Loss:  0.6463176012039185\n",
      "Epoch:  14 | Step:  63 | Val Loss:  0.4471673369407654\n",
      "Epoch:  14 | Step:  64 | Val Loss:  0.3717184066772461\n",
      "Epoch:  14 | Step:  65 | Val Loss:  0.601252019405365\n",
      "Epoch:  14 | Step:  66 | Val Loss:  0.4336296319961548\n",
      "Epoch:  14 | Step:  67 | Val Loss:  0.5981770753860474\n",
      "Epoch:  14 | Step:  68 | Val Loss:  0.5624037384986877\n",
      "Epoch:  14 | Step:  69 | Val Loss:  0.7126052379608154\n",
      "Epoch:  14 | Step:  70 | Val Loss:  0.7743171453475952\n",
      "Epoch:  14 | Step:  71 | Val Loss:  0.5852441787719727\n",
      "Epoch:  14 | Step:  72 | Val Loss:  0.5202120542526245\n",
      "Epoch:  14 | Step:  73 | Val Loss:  0.5313766002655029\n",
      "Epoch:  14 | Step:  74 | Val Loss:  0.6259214878082275\n",
      "Epoch:  14 | Step:  75 | Val Loss:  0.5892003774642944\n",
      "Epoch:  14 | Step:  76 | Val Loss:  0.6007436513900757\n",
      "Epoch:  14 | Step:  77 | Val Loss:  0.5728162527084351\n",
      "Epoch:  14 | Step:  78 | Val Loss:  0.5058746337890625\n",
      "Epoch:  14 | Step:  79 | Val Loss:  0.6234888434410095\n",
      "Epoch:  14 | Step:  80 | Val Loss:  0.683448076248169\n",
      "Epoch:  14 | Step:  81 | Val Loss:  0.5638324022293091\n",
      "Epoch:  14 | Step:  82 | Val Loss:  0.6053282022476196\n",
      "Epoch:  14 | Step:  83 | Val Loss:  0.6086061000823975\n",
      "Epoch:  14 | Step:  84 | Val Loss:  0.7714613676071167\n",
      "Epoch:  14 | Step:  85 | Val Loss:  0.5514366626739502\n",
      "Epoch:  14 | Step:  86 | Val Loss:  0.5117812156677246\n",
      "Epoch:  14 | Step:  87 | Val Loss:  0.6213574409484863\n",
      "Epoch:  14 | Step:  88 | Val Loss:  0.8130208253860474\n",
      "Epoch:  14 | Step:  89 | Val Loss:  0.4427882134914398\n",
      "Epoch:  14 | Step:  90 | Val Loss:  0.7494206428527832\n",
      "Epoch:  14 | Step:  91 | Val Loss:  0.5950479507446289\n",
      "Epoch:  14 | Step:  92 | Val Loss:  0.63849937915802\n",
      "Epoch:  14 | Step:  93 | Val Loss:  0.45982885360717773\n",
      "Epoch:  14 | Step:  94 | Val Loss:  0.5589664578437805\n",
      "Epoch:  14 | Step:  95 | Val Loss:  0.6534404158592224\n",
      "Epoch:  14 | Step:  96 | Val Loss:  0.5870282649993896\n",
      "Epoch:  14 | Step:  97 | Val Loss:  0.5912404656410217\n",
      "Epoch:  14 | Step:  98 | Val Loss:  0.5349574089050293\n",
      "Epoch:  14 | Step:  99 | Val Loss:  0.6915030479431152\n",
      "Epoch:  14 | Step:  100 | Val Loss:  0.5104793310165405\n",
      "Epoch:  14 | Step:  101 | Val Loss:  0.772786021232605\n",
      "Epoch:  14 | Step:  102 | Val Loss:  0.4490690529346466\n",
      "Epoch:  14 | Step:  103 | Val Loss:  0.5073817372322083\n",
      "Epoch:  14 | Step:  104 | Val Loss:  0.43253177404403687\n",
      "Epoch:  14 | Step:  105 | Val Loss:  0.531899094581604\n",
      "Epoch:  14 | Step:  106 | Val Loss:  0.4664517343044281\n",
      "Epoch:  14 | Step:  107 | Val Loss:  0.5598070621490479\n",
      "Epoch:  14 | Step:  108 | Val Loss:  0.5119941234588623\n",
      "Epoch:  14 | Step:  109 | Val Loss:  0.5369898080825806\n",
      "Epoch:  14 | Step:  110 | Val Loss:  0.6741838455200195\n",
      "Epoch:  14 | Step:  111 | Val Loss:  0.6500808000564575\n",
      "Epoch:  14 | Step:  112 | Val Loss:  0.5499323606491089\n",
      "Epoch:  14 | Step:  113 | Val Loss:  0.6979392766952515\n",
      "Epoch:  14 | Step:  114 | Val Loss:  0.672704815864563\n",
      "Epoch:  14 | Step:  115 | Val Loss:  0.7813260555267334\n",
      "Epoch:  14 | Step:  116 | Val Loss:  0.8663094639778137\n",
      "Epoch:  14 | Step:  117 | Val Loss:  0.42681199312210083\n",
      "Epoch:  14 | Step:  118 | Val Loss:  0.4145900011062622\n",
      "Epoch:  14 | Step:  119 | Val Loss:  0.4322453737258911\n",
      "Epoch:  14 | Step:  120 | Val Loss:  0.6199057102203369\n",
      "Epoch:  14 | Step:  121 | Val Loss:  0.801090657711029\n",
      "Epoch:  14 | Step:  122 | Val Loss:  0.5001614093780518\n",
      "Epoch:  14 | Step:  123 | Val Loss:  0.5614104270935059\n",
      "Epoch:  14 | Step:  124 | Val Loss:  0.9003034830093384\n",
      "Epoch:  14 | Step:  125 | Val Loss:  0.616741418838501\n",
      "Epoch:  14 | Train Loss:  tensor(0.5564, device='cuda:0') | Val Loss:  tensor(0.5974, device='cuda:0')\n",
      "Epoch:  15 | Step:  500 | Train Loss:  0.48673713207244873\n",
      "Epoch:  15 | Step:  1 | Val Loss:  0.9110596776008606\n",
      "Epoch:  15 | Step:  2 | Val Loss:  0.681264340877533\n",
      "Epoch:  15 | Step:  3 | Val Loss:  0.6457957029342651\n",
      "Epoch:  15 | Step:  4 | Val Loss:  0.4925908148288727\n",
      "Epoch:  15 | Step:  5 | Val Loss:  0.6990441083908081\n",
      "Epoch:  15 | Step:  6 | Val Loss:  0.7444255352020264\n",
      "Epoch:  15 | Step:  7 | Val Loss:  0.2539044916629791\n",
      "Epoch:  15 | Step:  8 | Val Loss:  0.5747918486595154\n",
      "Epoch:  15 | Step:  9 | Val Loss:  0.6971725821495056\n",
      "Epoch:  15 | Step:  10 | Val Loss:  0.7927692532539368\n",
      "Epoch:  15 | Step:  11 | Val Loss:  0.5348501801490784\n",
      "Epoch:  15 | Step:  12 | Val Loss:  0.646501898765564\n",
      "Epoch:  15 | Step:  13 | Val Loss:  0.7745649814605713\n",
      "Epoch:  15 | Step:  14 | Val Loss:  0.655777096748352\n",
      "Epoch:  15 | Step:  15 | Val Loss:  0.5710725784301758\n",
      "Epoch:  15 | Step:  16 | Val Loss:  0.6497336030006409\n",
      "Epoch:  15 | Step:  17 | Val Loss:  0.617896318435669\n",
      "Epoch:  15 | Step:  18 | Val Loss:  0.6473516821861267\n",
      "Epoch:  15 | Step:  19 | Val Loss:  0.7043669819831848\n",
      "Epoch:  15 | Step:  20 | Val Loss:  0.7154641151428223\n",
      "Epoch:  15 | Step:  21 | Val Loss:  0.7272469997406006\n",
      "Epoch:  15 | Step:  22 | Val Loss:  0.591132640838623\n",
      "Epoch:  15 | Step:  23 | Val Loss:  0.32425224781036377\n",
      "Epoch:  15 | Step:  24 | Val Loss:  0.7398557066917419\n",
      "Epoch:  15 | Step:  25 | Val Loss:  0.89557945728302\n",
      "Epoch:  15 | Step:  26 | Val Loss:  0.38603460788726807\n",
      "Epoch:  15 | Step:  27 | Val Loss:  0.7720136642456055\n",
      "Epoch:  15 | Step:  28 | Val Loss:  0.8234948515892029\n",
      "Epoch:  15 | Step:  29 | Val Loss:  0.6441551446914673\n",
      "Epoch:  15 | Step:  30 | Val Loss:  0.8680154085159302\n",
      "Epoch:  15 | Step:  31 | Val Loss:  0.5591367483139038\n",
      "Epoch:  15 | Step:  32 | Val Loss:  0.6617887020111084\n",
      "Epoch:  15 | Step:  33 | Val Loss:  0.6194275617599487\n",
      "Epoch:  15 | Step:  34 | Val Loss:  0.35614532232284546\n",
      "Epoch:  15 | Step:  35 | Val Loss:  0.5652940273284912\n",
      "Epoch:  15 | Step:  36 | Val Loss:  0.5445126295089722\n",
      "Epoch:  15 | Step:  37 | Val Loss:  0.5515919923782349\n",
      "Epoch:  15 | Step:  38 | Val Loss:  0.6258957386016846\n",
      "Epoch:  15 | Step:  39 | Val Loss:  0.7825722694396973\n",
      "Epoch:  15 | Step:  40 | Val Loss:  0.4230548143386841\n",
      "Epoch:  15 | Step:  41 | Val Loss:  0.8529328107833862\n",
      "Epoch:  15 | Step:  42 | Val Loss:  0.4620167911052704\n",
      "Epoch:  15 | Step:  43 | Val Loss:  0.759717583656311\n",
      "Epoch:  15 | Step:  44 | Val Loss:  0.48911887407302856\n",
      "Epoch:  15 | Step:  45 | Val Loss:  0.45295724272727966\n",
      "Epoch:  15 | Step:  46 | Val Loss:  0.4666282534599304\n",
      "Epoch:  15 | Step:  47 | Val Loss:  0.46583929657936096\n",
      "Epoch:  15 | Step:  48 | Val Loss:  0.6566424369812012\n",
      "Epoch:  15 | Step:  49 | Val Loss:  0.4098607301712036\n",
      "Epoch:  15 | Step:  50 | Val Loss:  0.6323839426040649\n",
      "Epoch:  15 | Step:  51 | Val Loss:  0.5600311756134033\n",
      "Epoch:  15 | Step:  52 | Val Loss:  0.510274350643158\n",
      "Epoch:  15 | Step:  53 | Val Loss:  0.7028821706771851\n",
      "Epoch:  15 | Step:  54 | Val Loss:  0.9316420555114746\n",
      "Epoch:  15 | Step:  55 | Val Loss:  0.3852367401123047\n",
      "Epoch:  15 | Step:  56 | Val Loss:  0.461070716381073\n",
      "Epoch:  15 | Step:  57 | Val Loss:  0.738129734992981\n",
      "Epoch:  15 | Step:  58 | Val Loss:  0.5565216541290283\n",
      "Epoch:  15 | Step:  59 | Val Loss:  0.9066795110702515\n",
      "Epoch:  15 | Step:  60 | Val Loss:  0.6722142696380615\n",
      "Epoch:  15 | Step:  61 | Val Loss:  0.3586469292640686\n",
      "Epoch:  15 | Step:  62 | Val Loss:  0.3375365436077118\n",
      "Epoch:  15 | Step:  63 | Val Loss:  0.553375244140625\n",
      "Epoch:  15 | Step:  64 | Val Loss:  0.4579886198043823\n",
      "Epoch:  15 | Step:  65 | Val Loss:  0.527984082698822\n",
      "Epoch:  15 | Step:  66 | Val Loss:  0.7982912659645081\n",
      "Epoch:  15 | Step:  67 | Val Loss:  0.5550492405891418\n",
      "Epoch:  15 | Step:  68 | Val Loss:  0.4871358871459961\n",
      "Epoch:  15 | Step:  69 | Val Loss:  0.26858633756637573\n",
      "Epoch:  15 | Step:  70 | Val Loss:  0.5755408406257629\n",
      "Epoch:  15 | Step:  71 | Val Loss:  0.5028278231620789\n",
      "Epoch:  15 | Step:  72 | Val Loss:  0.5691491365432739\n",
      "Epoch:  15 | Step:  73 | Val Loss:  0.7419252991676331\n",
      "Epoch:  15 | Step:  74 | Val Loss:  0.3848010301589966\n",
      "Epoch:  15 | Step:  75 | Val Loss:  0.5417495965957642\n",
      "Epoch:  15 | Step:  76 | Val Loss:  0.471994549036026\n",
      "Epoch:  15 | Step:  77 | Val Loss:  0.6321763396263123\n",
      "Epoch:  15 | Step:  78 | Val Loss:  0.8766167163848877\n",
      "Epoch:  15 | Step:  79 | Val Loss:  0.6328927874565125\n",
      "Epoch:  15 | Step:  80 | Val Loss:  0.365755558013916\n",
      "Epoch:  15 | Step:  81 | Val Loss:  0.5647188425064087\n",
      "Epoch:  15 | Step:  82 | Val Loss:  0.5448071360588074\n",
      "Epoch:  15 | Step:  83 | Val Loss:  0.7059258222579956\n",
      "Epoch:  15 | Step:  84 | Val Loss:  0.6233656406402588\n",
      "Epoch:  15 | Step:  85 | Val Loss:  0.7370468378067017\n",
      "Epoch:  15 | Step:  86 | Val Loss:  0.41101282835006714\n",
      "Epoch:  15 | Step:  87 | Val Loss:  0.4532417058944702\n",
      "Epoch:  15 | Step:  88 | Val Loss:  0.44158169627189636\n",
      "Epoch:  15 | Step:  89 | Val Loss:  0.2735205888748169\n",
      "Epoch:  15 | Step:  90 | Val Loss:  0.620474636554718\n",
      "Epoch:  15 | Step:  91 | Val Loss:  0.7911864519119263\n",
      "Epoch:  15 | Step:  92 | Val Loss:  0.5430335402488708\n",
      "Epoch:  15 | Step:  93 | Val Loss:  0.614870548248291\n",
      "Epoch:  15 | Step:  94 | Val Loss:  0.7453675270080566\n",
      "Epoch:  15 | Step:  95 | Val Loss:  0.8000332713127136\n",
      "Epoch:  15 | Step:  96 | Val Loss:  0.4392562508583069\n",
      "Epoch:  15 | Step:  97 | Val Loss:  0.4989975392818451\n",
      "Epoch:  15 | Step:  98 | Val Loss:  0.3752685785293579\n",
      "Epoch:  15 | Step:  99 | Val Loss:  0.7887474298477173\n",
      "Epoch:  15 | Step:  100 | Val Loss:  0.5660092830657959\n",
      "Epoch:  15 | Step:  101 | Val Loss:  0.5163903832435608\n",
      "Epoch:  15 | Step:  102 | Val Loss:  0.8132109642028809\n",
      "Epoch:  15 | Step:  103 | Val Loss:  0.4423452615737915\n",
      "Epoch:  15 | Step:  104 | Val Loss:  0.5149857997894287\n",
      "Epoch:  15 | Step:  105 | Val Loss:  0.6951636672019958\n",
      "Epoch:  15 | Step:  106 | Val Loss:  0.5075245499610901\n",
      "Epoch:  15 | Step:  107 | Val Loss:  0.4630935788154602\n",
      "Epoch:  15 | Step:  108 | Val Loss:  0.5239228010177612\n",
      "Epoch:  15 | Step:  109 | Val Loss:  0.659011960029602\n",
      "Epoch:  15 | Step:  110 | Val Loss:  0.6771642565727234\n",
      "Epoch:  15 | Step:  111 | Val Loss:  0.6207236647605896\n",
      "Epoch:  15 | Step:  112 | Val Loss:  0.6839331984519958\n",
      "Epoch:  15 | Step:  113 | Val Loss:  0.46359729766845703\n",
      "Epoch:  15 | Step:  114 | Val Loss:  0.503312349319458\n",
      "Epoch:  15 | Step:  115 | Val Loss:  0.7044377326965332\n",
      "Epoch:  15 | Step:  116 | Val Loss:  0.5303019285202026\n",
      "Epoch:  15 | Step:  117 | Val Loss:  0.6798563003540039\n",
      "Epoch:  15 | Step:  118 | Val Loss:  0.4513128995895386\n",
      "Epoch:  15 | Step:  119 | Val Loss:  0.4465371370315552\n",
      "Epoch:  15 | Step:  120 | Val Loss:  0.43940699100494385\n",
      "Epoch:  15 | Step:  121 | Val Loss:  0.6320691704750061\n",
      "Epoch:  15 | Step:  122 | Val Loss:  0.5468137860298157\n",
      "Epoch:  15 | Step:  123 | Val Loss:  0.7184390425682068\n",
      "Epoch:  15 | Step:  124 | Val Loss:  0.6799597144126892\n",
      "Epoch:  15 | Step:  125 | Val Loss:  0.54494309425354\n",
      "Epoch:  15 | Train Loss:  tensor(0.5538, device='cuda:0') | Val Loss:  tensor(0.5935, device='cuda:0')\n",
      "Epoch:  16 | Step:  500 | Train Loss:  0.36915329098701477\n",
      "Epoch:  16 | Step:  1 | Val Loss:  0.5618019104003906\n",
      "Epoch:  16 | Step:  2 | Val Loss:  0.5928980708122253\n",
      "Epoch:  16 | Step:  3 | Val Loss:  0.5916000604629517\n",
      "Epoch:  16 | Step:  4 | Val Loss:  0.4168034791946411\n",
      "Epoch:  16 | Step:  5 | Val Loss:  0.45888620615005493\n",
      "Epoch:  16 | Step:  6 | Val Loss:  0.4228670299053192\n",
      "Epoch:  16 | Step:  7 | Val Loss:  0.5531457662582397\n",
      "Epoch:  16 | Step:  8 | Val Loss:  0.5679011344909668\n",
      "Epoch:  16 | Step:  9 | Val Loss:  0.44781017303466797\n",
      "Epoch:  16 | Step:  10 | Val Loss:  0.4131684899330139\n",
      "Epoch:  16 | Step:  11 | Val Loss:  0.6993774175643921\n",
      "Epoch:  16 | Step:  12 | Val Loss:  0.5731521844863892\n",
      "Epoch:  16 | Step:  13 | Val Loss:  0.7634321451187134\n",
      "Epoch:  16 | Step:  14 | Val Loss:  0.643478512763977\n",
      "Epoch:  16 | Step:  15 | Val Loss:  0.6329327821731567\n",
      "Epoch:  16 | Step:  16 | Val Loss:  0.6070478558540344\n",
      "Epoch:  16 | Step:  17 | Val Loss:  0.5339678525924683\n",
      "Epoch:  16 | Step:  18 | Val Loss:  0.5770366191864014\n",
      "Epoch:  16 | Step:  19 | Val Loss:  0.5105818510055542\n",
      "Epoch:  16 | Step:  20 | Val Loss:  0.6988149881362915\n",
      "Epoch:  16 | Step:  21 | Val Loss:  0.5631691813468933\n",
      "Epoch:  16 | Step:  22 | Val Loss:  0.6767693758010864\n",
      "Epoch:  16 | Step:  23 | Val Loss:  0.5079135894775391\n",
      "Epoch:  16 | Step:  24 | Val Loss:  0.5864328145980835\n",
      "Epoch:  16 | Step:  25 | Val Loss:  0.5971395969390869\n",
      "Epoch:  16 | Step:  26 | Val Loss:  0.6749788522720337\n",
      "Epoch:  16 | Step:  27 | Val Loss:  0.5923584699630737\n",
      "Epoch:  16 | Step:  28 | Val Loss:  0.5091969966888428\n",
      "Epoch:  16 | Step:  29 | Val Loss:  0.8927435874938965\n",
      "Epoch:  16 | Step:  30 | Val Loss:  0.7628490328788757\n",
      "Epoch:  16 | Step:  31 | Val Loss:  0.672948956489563\n",
      "Epoch:  16 | Step:  32 | Val Loss:  0.678299069404602\n",
      "Epoch:  16 | Step:  33 | Val Loss:  0.5036318898200989\n",
      "Epoch:  16 | Step:  34 | Val Loss:  0.4758372902870178\n",
      "Epoch:  16 | Step:  35 | Val Loss:  0.607132077217102\n",
      "Epoch:  16 | Step:  36 | Val Loss:  0.5555134415626526\n",
      "Epoch:  16 | Step:  37 | Val Loss:  0.629277229309082\n",
      "Epoch:  16 | Step:  38 | Val Loss:  0.8144104480743408\n",
      "Epoch:  16 | Step:  39 | Val Loss:  0.6598639488220215\n",
      "Epoch:  16 | Step:  40 | Val Loss:  0.6138661503791809\n",
      "Epoch:  16 | Step:  41 | Val Loss:  0.5246109366416931\n",
      "Epoch:  16 | Step:  42 | Val Loss:  0.5345816612243652\n",
      "Epoch:  16 | Step:  43 | Val Loss:  0.5613279938697815\n",
      "Epoch:  16 | Step:  44 | Val Loss:  0.6009000539779663\n",
      "Epoch:  16 | Step:  45 | Val Loss:  0.5725352168083191\n",
      "Epoch:  16 | Step:  46 | Val Loss:  0.5021936893463135\n",
      "Epoch:  16 | Step:  47 | Val Loss:  0.6052354574203491\n",
      "Epoch:  16 | Step:  48 | Val Loss:  0.6816055774688721\n",
      "Epoch:  16 | Step:  49 | Val Loss:  0.52419513463974\n",
      "Epoch:  16 | Step:  50 | Val Loss:  0.8829509019851685\n",
      "Epoch:  16 | Step:  51 | Val Loss:  0.65234375\n",
      "Epoch:  16 | Step:  52 | Val Loss:  0.40770429372787476\n",
      "Epoch:  16 | Step:  53 | Val Loss:  0.8757336735725403\n",
      "Epoch:  16 | Step:  54 | Val Loss:  0.5348856449127197\n",
      "Epoch:  16 | Step:  55 | Val Loss:  0.6648257970809937\n",
      "Epoch:  16 | Step:  56 | Val Loss:  0.6471019983291626\n",
      "Epoch:  16 | Step:  57 | Val Loss:  0.6023432016372681\n",
      "Epoch:  16 | Step:  58 | Val Loss:  0.6955050230026245\n",
      "Epoch:  16 | Step:  59 | Val Loss:  0.4262896180152893\n",
      "Epoch:  16 | Step:  60 | Val Loss:  0.3887079954147339\n",
      "Epoch:  16 | Step:  61 | Val Loss:  0.501173734664917\n",
      "Epoch:  16 | Step:  62 | Val Loss:  0.5154772400856018\n",
      "Epoch:  16 | Step:  63 | Val Loss:  0.585290789604187\n",
      "Epoch:  16 | Step:  64 | Val Loss:  0.7214607000350952\n",
      "Epoch:  16 | Step:  65 | Val Loss:  0.7792003154754639\n",
      "Epoch:  16 | Step:  66 | Val Loss:  0.6507930755615234\n",
      "Epoch:  16 | Step:  67 | Val Loss:  0.4559650719165802\n",
      "Epoch:  16 | Step:  68 | Val Loss:  0.409670889377594\n",
      "Epoch:  16 | Step:  69 | Val Loss:  0.4925271272659302\n",
      "Epoch:  16 | Step:  70 | Val Loss:  0.5135267972946167\n",
      "Epoch:  16 | Step:  71 | Val Loss:  0.6608676314353943\n",
      "Epoch:  16 | Step:  72 | Val Loss:  0.696650505065918\n",
      "Epoch:  16 | Step:  73 | Val Loss:  0.4545392394065857\n",
      "Epoch:  16 | Step:  74 | Val Loss:  0.6781808733940125\n",
      "Epoch:  16 | Step:  75 | Val Loss:  0.5134667754173279\n",
      "Epoch:  16 | Step:  76 | Val Loss:  0.548140823841095\n",
      "Epoch:  16 | Step:  77 | Val Loss:  0.4922529458999634\n",
      "Epoch:  16 | Step:  78 | Val Loss:  0.411399245262146\n",
      "Epoch:  16 | Step:  79 | Val Loss:  0.6383525133132935\n",
      "Epoch:  16 | Step:  80 | Val Loss:  0.6919223666191101\n",
      "Epoch:  16 | Step:  81 | Val Loss:  0.4497237801551819\n",
      "Epoch:  16 | Step:  82 | Val Loss:  0.7784508466720581\n",
      "Epoch:  16 | Step:  83 | Val Loss:  0.5300838947296143\n",
      "Epoch:  16 | Step:  84 | Val Loss:  0.8118121027946472\n",
      "Epoch:  16 | Step:  85 | Val Loss:  0.5058649182319641\n",
      "Epoch:  16 | Step:  86 | Val Loss:  0.5365546941757202\n",
      "Epoch:  16 | Step:  87 | Val Loss:  0.6683575510978699\n",
      "Epoch:  16 | Step:  88 | Val Loss:  0.6452829241752625\n",
      "Epoch:  16 | Step:  89 | Val Loss:  0.7459721565246582\n",
      "Epoch:  16 | Step:  90 | Val Loss:  0.5627421140670776\n",
      "Epoch:  16 | Step:  91 | Val Loss:  0.743094265460968\n",
      "Epoch:  16 | Step:  92 | Val Loss:  0.7147667407989502\n",
      "Epoch:  16 | Step:  93 | Val Loss:  0.5864425897598267\n",
      "Epoch:  16 | Step:  94 | Val Loss:  0.5841519832611084\n",
      "Epoch:  16 | Step:  95 | Val Loss:  0.3993437886238098\n",
      "Epoch:  16 | Step:  96 | Val Loss:  0.4515378475189209\n",
      "Epoch:  16 | Step:  97 | Val Loss:  0.44014599919319153\n",
      "Epoch:  16 | Step:  98 | Val Loss:  0.7085791230201721\n",
      "Epoch:  16 | Step:  99 | Val Loss:  0.4420742988586426\n",
      "Epoch:  16 | Step:  100 | Val Loss:  0.7092874050140381\n",
      "Epoch:  16 | Step:  101 | Val Loss:  0.5179954171180725\n",
      "Epoch:  16 | Step:  102 | Val Loss:  0.6133387088775635\n",
      "Epoch:  16 | Step:  103 | Val Loss:  0.5653793811798096\n",
      "Epoch:  16 | Step:  104 | Val Loss:  0.5196325778961182\n",
      "Epoch:  16 | Step:  105 | Val Loss:  0.6769653558731079\n",
      "Epoch:  16 | Step:  106 | Val Loss:  0.7196917533874512\n",
      "Epoch:  16 | Step:  107 | Val Loss:  0.6062155365943909\n",
      "Epoch:  16 | Step:  108 | Val Loss:  0.5583038330078125\n",
      "Epoch:  16 | Step:  109 | Val Loss:  0.48012179136276245\n",
      "Epoch:  16 | Step:  110 | Val Loss:  0.48959672451019287\n",
      "Epoch:  16 | Step:  111 | Val Loss:  0.5138364434242249\n",
      "Epoch:  16 | Step:  112 | Val Loss:  0.711381733417511\n",
      "Epoch:  16 | Step:  113 | Val Loss:  0.5432379245758057\n",
      "Epoch:  16 | Step:  114 | Val Loss:  0.4658554494380951\n",
      "Epoch:  16 | Step:  115 | Val Loss:  0.6861742734909058\n",
      "Epoch:  16 | Step:  116 | Val Loss:  0.6005331873893738\n",
      "Epoch:  16 | Step:  117 | Val Loss:  0.7750957012176514\n",
      "Epoch:  16 | Step:  118 | Val Loss:  0.6018396019935608\n",
      "Epoch:  16 | Step:  119 | Val Loss:  0.6637755632400513\n",
      "Epoch:  16 | Step:  120 | Val Loss:  0.3844295144081116\n",
      "Epoch:  16 | Step:  121 | Val Loss:  0.6452367305755615\n",
      "Epoch:  16 | Step:  122 | Val Loss:  0.5972616672515869\n",
      "Epoch:  16 | Step:  123 | Val Loss:  0.5670119524002075\n",
      "Epoch:  16 | Step:  124 | Val Loss:  0.6238963603973389\n",
      "Epoch:  16 | Step:  125 | Val Loss:  0.7015834450721741\n",
      "Epoch:  16 | Train Loss:  tensor(0.5512, device='cuda:0') | Val Loss:  tensor(0.5918, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "train_losses =  torch.zeros(len(train_loader))\n",
    "val_losses = torch.zeros(len(val_loader))\n",
    "wandb.init(\n",
    "    project='GRU-From-Scratch'\n",
    ")\n",
    "for epoch in range(ModelArgs.epoch):\n",
    "    \n",
    "    count = 0\n",
    "    for X, y in train_loader:\n",
    "        y_pred = model(X)\n",
    "        # print(y_pred.shape)\n",
    "        loss = criterion(y_pred, y)\n",
    "        train_losses[count] = loss.item()\n",
    "        # print(\"Loss: \", loss.item())\n",
    "        \n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        # if(count != 0):\n",
    "        #     total_norm_before = torch.norm(\n",
    "        #             torch.stack([torch.norm(p.grad.detach(), 2) for p in model.parameters()]), 2\n",
    "        #         )\n",
    "\n",
    "        #     torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        #         # Compute gradient norms after clipping\n",
    "        #     total_norm_after = torch.norm(\n",
    "        #             torch.stack([torch.norm(p.grad.detach(), 2) for p in model.parameters()]), 2\n",
    "        #         )\n",
    "                \n",
    "        #     # if(device  == 0 and step !=0):\n",
    "        #     print(f\"Gradient Norm Before Clipping: {total_norm_before.item():.4f}\")\n",
    "        #     print(f\"Gradient Norm After Clipping: {total_norm_after.item():.4f}\")\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "        \n",
    "    #     wandb.log({\n",
    "    #   \"Train Loss\": loss.item(),\n",
    "    #   \"Val Loss\": loss.item(),\n",
    "    #   \"step\": count  \n",
    "    # })\n",
    "    # count = 0\n",
    "    print(\"Epoch: \", epoch, \"|\", \"Step: \", count, \"|\", \"Train Loss: \", loss.item())\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    \n",
    "    for X, y in val_loader:\n",
    "        y_pred = model(X)\n",
    "        # print(y_pred.shape)\n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        # print(\"Loss: \", loss.item())\n",
    "        val_losses[count] = loss.item()\n",
    "        \n",
    "        # optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        count += 1\n",
    "    #     wandb.log({\n",
    "    #   \"Train Loss\": loss.item(),\n",
    "    #   \"Val Loss\": loss.item(),\n",
    "    #   \"step\": count  \n",
    "    # })\n",
    "        print(\"Epoch: \", epoch, \"|\", \"Step: \", count, \"|\", \"Val Loss: \", loss.item())\n",
    "    model.train()\n",
    "    wandb.log({\n",
    "      \"Train Loss\": train_losses.mean(),\n",
    "      \"Val Loss\": val_losses.mean(),\n",
    "      \"epoch\": epoch  \n",
    "    })\n",
    "    print(\"Epoch: \", epoch, \"|\", \"Train Loss: \", train_losses.mean(),  \"|\", \"Val Loss: \", val_losses.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
