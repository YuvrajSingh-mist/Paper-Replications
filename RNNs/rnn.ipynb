{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass \n",
    "class ModelArgs:\n",
    "    device = 'cuda'\n",
    "    no_of_neurons = 16\n",
    "    block_size = 16\n",
    "    batch_size = 16\n",
    "    dropout = 0.1\n",
    "    epoch = 50\n",
    "    max_lr = 1e-4\n",
    "    embedding_dims: int = 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrajceo2031\u001b[0m (\u001b[33mrentio\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device(ModelArgs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "num_samples = 10000 \n",
    "seq_length = ModelArgs.block_size  \n",
    "device = ModelArgs.device  \n",
    "\n",
    "\n",
    "t = torch.linspace(0, 100, num_samples + seq_length, device=device)\n",
    "data = torch.sin(t) \n",
    "# data = t\n",
    "\n",
    "X_tensor = torch.stack([data[i:i+seq_length] for i in range(num_samples)])\n",
    "y_tensor = data[seq_length:]  # Next value prediction\n",
    "\n",
    "train_size = int(0.8 * num_samples)\n",
    "\n",
    "X_train, y_train = X_tensor[:train_size], y_tensor[:train_size]  \n",
    "X_val, y_val = X_tensor[train_size:], y_tensor[train_size:]  \n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "val_dataset = TimeSeriesDataset(X_val, y_val)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "generator = torch.Generator(device=device)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=ModelArgs.batch_size,\n",
    "    shuffle=True,  \n",
    "    generator=generator,  \n",
    "    # drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    generator=generator, \n",
    "    # drop_last=True,\n",
    "    batch_size=ModelArgs.batch_size,\n",
    "    shuffle=True, \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "    def __init__(self, device, no_of_neurons):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_layer = nn.Linear(in_features=ModelArgs.block_size + 1, out_features=no_of_neurons, device=ModelArgs.device)\n",
    "        \n",
    "    def forward(self, x, ht_1):\n",
    "        x = self.linear_layer(torch.cat([x, ht_1], dim=-1))\n",
    "        ht = torch.nn.functional.sigmoid(x)\n",
    "        return ht\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLayer(nn.Module):\n",
    "    def __init__(self, device, no_of_neurons):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn_layer = RNNCell(device=device, no_of_neurons=no_of_neurons)\n",
    "        self.linear_layer = nn.Linear(in_features=ModelArgs.block_size, out_features=no_of_neurons, device=ModelArgs.device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        ht_1 = torch.zeros((ModelArgs.batch_size, ModelArgs.block_size), device=ModelArgs.device, requires_grad=True, dtype=torch.float32)\n",
    "        \n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            xt = x[:, t]\n",
    "            xt = xt.unsqueeze(-1)\n",
    "            ht = self.rnn_layer(xt, ht_1)\n",
    "            ht_1 = ht\n",
    "            \n",
    "        return ht_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, device, no_of_neurons, out_features):\n",
    "        super().__init__()\n",
    "        self.rnn = RNNLayer(device=device, no_of_neurons=no_of_neurons)\n",
    "\n",
    "        self.output = nn.Linear(in_features=ModelArgs.no_of_neurons, out_features=out_features, device=device, dtype=torch.float32)\n",
    "        self.dropout = nn.Dropout(p=ModelArgs.dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        ht = self.rnn(x)\n",
    "        out = self.output(ht)\n",
    "        out = self.dropout(out)\n",
    "        return ht\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GRU(device=ModelArgs.device, no_of_neurons=ModelArgs.no_of_neurons, out_features=1)\n",
    "model = RNN(device=ModelArgs.device, no_of_neurons=ModelArgs.no_of_neurons, out_features=1)\n",
    "model = model.to(ModelArgs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /home/yuvrajsingh/anaconda3/envs/unsloth_env/lib/python3.11/site-packages (1.8.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "RNN (RNN)                                [16, 16]             [16, 16]             --                   True\n",
       "├─RNNLayer (rnn)                         [16, 16]             [16, 16]             272                  True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             --                   True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             288                  True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "│    └─RNNCell (rnn_layer)               [16, 1]              [16, 16]             (recursive)          True\n",
       "│    │    └─Linear (linear_layer)        [16, 17]             [16, 16]             (recursive)          True\n",
       "├─Linear (output)                        [16, 16]             [16, 1]              17                   True\n",
       "├─Dropout (dropout)                      [16, 1]              [16, 1]              --                   --\n",
       "========================================================================================================================\n",
       "Total params: 577\n",
       "Trainable params: 577\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.07\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.03\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.04\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install torchinfo\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "x = torch.randint(0, 100, (ModelArgs.batch_size,ModelArgs.block_size))  # Random integer between 0 and 100\n",
    "x = x.to(ModelArgs.device)\n",
    "summary(model=model,\n",
    "        input_data=x,\n",
    "        # input_size=(ModelArgs.batch_size, ModelArgs.block_size, ModelArgs.embeddings_dims),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=ModelArgs.max_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrajceo2031\u001b[0m (\u001b[33mrentio\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac96aeb438fe4efcbe99469174006525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011443493633285269, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/yuvra/OneDrive/Desktop/Work/pytorch/Paper-Replications/RNNs/wandb/run-20250303_215325-b4gqd0jy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rentio/GRU-From-Scratch/runs/b4gqd0jy' target=\"_blank\">restful-forest-24</a></strong> to <a href='https://wandb.ai/rentio/GRU-From-Scratch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rentio/GRU-From-Scratch' target=\"_blank\">https://wandb.ai/rentio/GRU-From-Scratch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rentio/GRU-From-Scratch/runs/b4gqd0jy' target=\"_blank\">https://wandb.ai/rentio/GRU-From-Scratch/runs/b4gqd0jy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvrajsingh/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/utils/_device.py:106: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 16])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Step:  500 | Train Loss:  0.656163215637207\n",
      "Epoch:  0 | Step:  1 | Val Loss:  0.6380040049552917\n",
      "Epoch:  0 | Step:  2 | Val Loss:  0.6970006227493286\n",
      "Epoch:  0 | Step:  3 | Val Loss:  0.7294517159461975\n",
      "Epoch:  0 | Step:  4 | Val Loss:  0.9988241195678711\n",
      "Epoch:  0 | Step:  5 | Val Loss:  0.6275670528411865\n",
      "Epoch:  0 | Step:  6 | Val Loss:  0.4303639233112335\n",
      "Epoch:  0 | Step:  7 | Val Loss:  0.3101731240749359\n",
      "Epoch:  0 | Step:  8 | Val Loss:  0.8717292547225952\n",
      "Epoch:  0 | Step:  9 | Val Loss:  0.7538192272186279\n",
      "Epoch:  0 | Step:  10 | Val Loss:  0.766417920589447\n",
      "Epoch:  0 | Step:  11 | Val Loss:  0.7333325743675232\n",
      "Epoch:  0 | Step:  12 | Val Loss:  0.870871901512146\n",
      "Epoch:  0 | Step:  13 | Val Loss:  0.7366973161697388\n",
      "Epoch:  0 | Step:  14 | Val Loss:  0.3938369154930115\n",
      "Epoch:  0 | Step:  15 | Val Loss:  0.6606581211090088\n",
      "Epoch:  0 | Step:  16 | Val Loss:  0.8170782327651978\n",
      "Epoch:  0 | Step:  17 | Val Loss:  0.6676695346832275\n",
      "Epoch:  0 | Step:  18 | Val Loss:  0.7664165496826172\n",
      "Epoch:  0 | Step:  19 | Val Loss:  0.9173787236213684\n",
      "Epoch:  0 | Step:  20 | Val Loss:  0.635597825050354\n",
      "Epoch:  0 | Step:  21 | Val Loss:  0.8655332326889038\n",
      "Epoch:  0 | Step:  22 | Val Loss:  0.749855101108551\n",
      "Epoch:  0 | Step:  23 | Val Loss:  0.9031981229782104\n",
      "Epoch:  0 | Step:  24 | Val Loss:  1.1342976093292236\n",
      "Epoch:  0 | Step:  25 | Val Loss:  0.8630834817886353\n",
      "Epoch:  0 | Step:  26 | Val Loss:  0.7429576516151428\n",
      "Epoch:  0 | Step:  27 | Val Loss:  0.6375668048858643\n",
      "Epoch:  0 | Step:  28 | Val Loss:  0.771766185760498\n",
      "Epoch:  0 | Step:  29 | Val Loss:  0.6019240617752075\n",
      "Epoch:  0 | Step:  30 | Val Loss:  0.8756920099258423\n",
      "Epoch:  0 | Step:  31 | Val Loss:  0.561064600944519\n",
      "Epoch:  0 | Step:  32 | Val Loss:  0.7754737138748169\n",
      "Epoch:  0 | Step:  33 | Val Loss:  0.5967899560928345\n",
      "Epoch:  0 | Step:  34 | Val Loss:  1.0305947065353394\n",
      "Epoch:  0 | Step:  35 | Val Loss:  0.460729718208313\n",
      "Epoch:  0 | Step:  36 | Val Loss:  1.0294911861419678\n",
      "Epoch:  0 | Step:  37 | Val Loss:  0.9647067189216614\n",
      "Epoch:  0 | Step:  38 | Val Loss:  0.6893233060836792\n",
      "Epoch:  0 | Step:  39 | Val Loss:  0.8336751461029053\n",
      "Epoch:  0 | Step:  40 | Val Loss:  0.848983645439148\n",
      "Epoch:  0 | Step:  41 | Val Loss:  0.8474894762039185\n",
      "Epoch:  0 | Step:  42 | Val Loss:  0.5529791116714478\n",
      "Epoch:  0 | Step:  43 | Val Loss:  0.7073410749435425\n",
      "Epoch:  0 | Step:  44 | Val Loss:  0.8533482551574707\n",
      "Epoch:  0 | Step:  45 | Val Loss:  0.44815921783447266\n",
      "Epoch:  0 | Step:  46 | Val Loss:  0.6340157985687256\n",
      "Epoch:  0 | Step:  47 | Val Loss:  0.7860690355300903\n",
      "Epoch:  0 | Step:  48 | Val Loss:  0.6667389869689941\n",
      "Epoch:  0 | Step:  49 | Val Loss:  0.8650704622268677\n",
      "Epoch:  0 | Step:  50 | Val Loss:  0.7792069911956787\n",
      "Epoch:  0 | Step:  51 | Val Loss:  0.9089524149894714\n",
      "Epoch:  0 | Step:  52 | Val Loss:  0.816368579864502\n",
      "Epoch:  0 | Step:  53 | Val Loss:  0.7102339863777161\n",
      "Epoch:  0 | Step:  54 | Val Loss:  0.5626466870307922\n",
      "Epoch:  0 | Step:  55 | Val Loss:  0.7586139440536499\n",
      "Epoch:  0 | Step:  56 | Val Loss:  0.9098227024078369\n",
      "Epoch:  0 | Step:  57 | Val Loss:  0.44058966636657715\n",
      "Epoch:  0 | Step:  58 | Val Loss:  0.25696390867233276\n",
      "Epoch:  0 | Step:  59 | Val Loss:  0.5693632364273071\n",
      "Epoch:  0 | Step:  60 | Val Loss:  0.5500350594520569\n",
      "Epoch:  0 | Step:  61 | Val Loss:  0.6527854204177856\n",
      "Epoch:  0 | Step:  62 | Val Loss:  0.7153236865997314\n",
      "Epoch:  0 | Step:  63 | Val Loss:  0.5848033428192139\n",
      "Epoch:  0 | Step:  64 | Val Loss:  0.8350863456726074\n",
      "Epoch:  0 | Step:  65 | Val Loss:  0.5977410674095154\n",
      "Epoch:  0 | Step:  66 | Val Loss:  0.848732054233551\n",
      "Epoch:  0 | Step:  67 | Val Loss:  0.8492920994758606\n",
      "Epoch:  0 | Step:  68 | Val Loss:  0.8082090020179749\n",
      "Epoch:  0 | Step:  69 | Val Loss:  0.649474561214447\n",
      "Epoch:  0 | Step:  70 | Val Loss:  0.6217927932739258\n",
      "Epoch:  0 | Step:  71 | Val Loss:  0.7642439603805542\n",
      "Epoch:  0 | Step:  72 | Val Loss:  0.6793479919433594\n",
      "Epoch:  0 | Step:  73 | Val Loss:  0.9752345681190491\n",
      "Epoch:  0 | Step:  74 | Val Loss:  0.4601040482521057\n",
      "Epoch:  0 | Step:  75 | Val Loss:  0.6530185341835022\n",
      "Epoch:  0 | Step:  76 | Val Loss:  0.5967938303947449\n",
      "Epoch:  0 | Step:  77 | Val Loss:  0.7223306894302368\n",
      "Epoch:  0 | Step:  78 | Val Loss:  0.8536744117736816\n",
      "Epoch:  0 | Step:  79 | Val Loss:  0.712867021560669\n",
      "Epoch:  0 | Step:  80 | Val Loss:  0.8795564770698547\n",
      "Epoch:  0 | Step:  81 | Val Loss:  0.7547925710678101\n",
      "Epoch:  0 | Step:  82 | Val Loss:  0.6388753652572632\n",
      "Epoch:  0 | Step:  83 | Val Loss:  0.7540385723114014\n",
      "Epoch:  0 | Step:  84 | Val Loss:  0.7072365283966064\n",
      "Epoch:  0 | Step:  85 | Val Loss:  0.8460404872894287\n",
      "Epoch:  0 | Step:  86 | Val Loss:  0.719234824180603\n",
      "Epoch:  0 | Step:  87 | Val Loss:  0.8778978586196899\n",
      "Epoch:  0 | Step:  88 | Val Loss:  0.7109653353691101\n",
      "Epoch:  0 | Step:  89 | Val Loss:  0.5313617587089539\n",
      "Epoch:  0 | Step:  90 | Val Loss:  0.646870493888855\n",
      "Epoch:  0 | Step:  91 | Val Loss:  0.8152741193771362\n",
      "Epoch:  0 | Step:  92 | Val Loss:  0.780776858329773\n",
      "Epoch:  0 | Step:  93 | Val Loss:  0.6763038635253906\n",
      "Epoch:  0 | Step:  94 | Val Loss:  0.6513490676879883\n",
      "Epoch:  0 | Step:  95 | Val Loss:  0.7774693965911865\n",
      "Epoch:  0 | Step:  96 | Val Loss:  1.1034975051879883\n",
      "Epoch:  0 | Step:  97 | Val Loss:  0.6453271508216858\n",
      "Epoch:  0 | Step:  98 | Val Loss:  0.6251815557479858\n",
      "Epoch:  0 | Step:  99 | Val Loss:  0.5395767688751221\n",
      "Epoch:  0 | Step:  100 | Val Loss:  0.6770620346069336\n",
      "Epoch:  0 | Step:  101 | Val Loss:  0.7198775410652161\n",
      "Epoch:  0 | Step:  102 | Val Loss:  0.8005830645561218\n",
      "Epoch:  0 | Step:  103 | Val Loss:  0.559102475643158\n",
      "Epoch:  0 | Step:  104 | Val Loss:  0.6300143003463745\n",
      "Epoch:  0 | Step:  105 | Val Loss:  0.899992823600769\n",
      "Epoch:  0 | Step:  106 | Val Loss:  0.8372432589530945\n",
      "Epoch:  0 | Step:  107 | Val Loss:  0.7711073160171509\n",
      "Epoch:  0 | Step:  108 | Val Loss:  0.8226924538612366\n",
      "Epoch:  0 | Step:  109 | Val Loss:  0.3154466152191162\n",
      "Epoch:  0 | Step:  110 | Val Loss:  0.7551851868629456\n",
      "Epoch:  0 | Step:  111 | Val Loss:  0.5129020810127258\n",
      "Epoch:  0 | Step:  112 | Val Loss:  0.5468730926513672\n",
      "Epoch:  0 | Step:  113 | Val Loss:  0.4946310222148895\n",
      "Epoch:  0 | Step:  114 | Val Loss:  0.5955679416656494\n",
      "Epoch:  0 | Step:  115 | Val Loss:  1.1613926887512207\n",
      "Epoch:  0 | Step:  116 | Val Loss:  0.44992682337760925\n",
      "Epoch:  0 | Step:  117 | Val Loss:  0.8007798194885254\n",
      "Epoch:  0 | Step:  118 | Val Loss:  0.6711651086807251\n",
      "Epoch:  0 | Step:  119 | Val Loss:  0.7356034517288208\n",
      "Epoch:  0 | Step:  120 | Val Loss:  0.8425976037979126\n",
      "Epoch:  0 | Step:  121 | Val Loss:  0.9725484848022461\n",
      "Epoch:  0 | Step:  122 | Val Loss:  0.7873556613922119\n",
      "Epoch:  0 | Step:  123 | Val Loss:  0.9279757142066956\n",
      "Epoch:  0 | Step:  124 | Val Loss:  0.7165526151657104\n",
      "Epoch:  0 | Step:  125 | Val Loss:  0.7019851207733154\n",
      "Epoch:  0 | Train Loss:  tensor(0.6768, device='cuda:0') | Val Loss:  tensor(0.7244, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvrajsingh/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/utils/_device.py:106: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 16])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 | Step:  500 | Train Loss:  0.26001042127609253\n",
      "Epoch:  1 | Step:  1 | Val Loss:  0.8739071488380432\n",
      "Epoch:  1 | Step:  2 | Val Loss:  1.198178768157959\n",
      "Epoch:  1 | Step:  3 | Val Loss:  0.6544116735458374\n",
      "Epoch:  1 | Step:  4 | Val Loss:  0.5835412740707397\n",
      "Epoch:  1 | Step:  5 | Val Loss:  0.7006656527519226\n",
      "Epoch:  1 | Step:  6 | Val Loss:  0.8122824430465698\n",
      "Epoch:  1 | Step:  7 | Val Loss:  0.8781837821006775\n",
      "Epoch:  1 | Step:  8 | Val Loss:  0.9911659955978394\n",
      "Epoch:  1 | Step:  9 | Val Loss:  0.6607527732849121\n",
      "Epoch:  1 | Step:  10 | Val Loss:  0.9287918210029602\n",
      "Epoch:  1 | Step:  11 | Val Loss:  0.9807298183441162\n",
      "Epoch:  1 | Step:  12 | Val Loss:  0.5087670087814331\n",
      "Epoch:  1 | Step:  13 | Val Loss:  0.6317039728164673\n",
      "Epoch:  1 | Step:  14 | Val Loss:  0.6480879783630371\n",
      "Epoch:  1 | Step:  15 | Val Loss:  0.8496520519256592\n",
      "Epoch:  1 | Step:  16 | Val Loss:  1.100818157196045\n",
      "Epoch:  1 | Step:  17 | Val Loss:  0.6442431211471558\n",
      "Epoch:  1 | Step:  18 | Val Loss:  0.527889609336853\n",
      "Epoch:  1 | Step:  19 | Val Loss:  0.4295738935470581\n",
      "Epoch:  1 | Step:  20 | Val Loss:  0.5668150186538696\n",
      "Epoch:  1 | Step:  21 | Val Loss:  0.6551152467727661\n",
      "Epoch:  1 | Step:  22 | Val Loss:  0.8670835494995117\n",
      "Epoch:  1 | Step:  23 | Val Loss:  0.6077287793159485\n",
      "Epoch:  1 | Step:  24 | Val Loss:  0.6851209998130798\n",
      "Epoch:  1 | Step:  25 | Val Loss:  0.2987784147262573\n",
      "Epoch:  1 | Step:  26 | Val Loss:  0.8292942643165588\n",
      "Epoch:  1 | Step:  27 | Val Loss:  0.6943107843399048\n",
      "Epoch:  1 | Step:  28 | Val Loss:  0.46942612528800964\n",
      "Epoch:  1 | Step:  29 | Val Loss:  0.7762730121612549\n",
      "Epoch:  1 | Step:  30 | Val Loss:  0.46318453550338745\n",
      "Epoch:  1 | Step:  31 | Val Loss:  0.7431129217147827\n",
      "Epoch:  1 | Step:  32 | Val Loss:  0.8283737897872925\n",
      "Epoch:  1 | Step:  33 | Val Loss:  0.8093326687812805\n",
      "Epoch:  1 | Step:  34 | Val Loss:  0.7832667827606201\n",
      "Epoch:  1 | Step:  35 | Val Loss:  0.585905909538269\n",
      "Epoch:  1 | Step:  36 | Val Loss:  0.5005171298980713\n",
      "Epoch:  1 | Step:  37 | Val Loss:  0.9082852602005005\n",
      "Epoch:  1 | Step:  38 | Val Loss:  0.4724231958389282\n",
      "Epoch:  1 | Step:  39 | Val Loss:  0.7586219310760498\n",
      "Epoch:  1 | Step:  40 | Val Loss:  0.5053630471229553\n",
      "Epoch:  1 | Step:  41 | Val Loss:  0.7494902610778809\n",
      "Epoch:  1 | Step:  42 | Val Loss:  0.518008828163147\n",
      "Epoch:  1 | Step:  43 | Val Loss:  0.6097604632377625\n",
      "Epoch:  1 | Step:  44 | Val Loss:  0.6138279438018799\n",
      "Epoch:  1 | Step:  45 | Val Loss:  0.548721969127655\n",
      "Epoch:  1 | Step:  46 | Val Loss:  0.5812817811965942\n",
      "Epoch:  1 | Step:  47 | Val Loss:  0.4578937888145447\n",
      "Epoch:  1 | Step:  48 | Val Loss:  0.6056087017059326\n",
      "Epoch:  1 | Step:  49 | Val Loss:  0.6565384268760681\n",
      "Epoch:  1 | Step:  50 | Val Loss:  0.7514716982841492\n",
      "Epoch:  1 | Step:  51 | Val Loss:  0.794129490852356\n",
      "Epoch:  1 | Step:  52 | Val Loss:  0.5197311043739319\n",
      "Epoch:  1 | Step:  53 | Val Loss:  0.4207908511161804\n",
      "Epoch:  1 | Step:  54 | Val Loss:  0.813512921333313\n",
      "Epoch:  1 | Step:  55 | Val Loss:  0.9716134071350098\n",
      "Epoch:  1 | Step:  56 | Val Loss:  0.7216294407844543\n",
      "Epoch:  1 | Step:  57 | Val Loss:  0.4904431998729706\n",
      "Epoch:  1 | Step:  58 | Val Loss:  0.6033066511154175\n",
      "Epoch:  1 | Step:  59 | Val Loss:  0.8565082550048828\n",
      "Epoch:  1 | Step:  60 | Val Loss:  0.529769778251648\n",
      "Epoch:  1 | Step:  61 | Val Loss:  0.7842750549316406\n",
      "Epoch:  1 | Step:  62 | Val Loss:  0.5590744018554688\n",
      "Epoch:  1 | Step:  63 | Val Loss:  0.6495198607444763\n",
      "Epoch:  1 | Step:  64 | Val Loss:  0.8486791849136353\n",
      "Epoch:  1 | Step:  65 | Val Loss:  0.4737992286682129\n",
      "Epoch:  1 | Step:  66 | Val Loss:  0.47902002930641174\n",
      "Epoch:  1 | Step:  67 | Val Loss:  0.36533042788505554\n",
      "Epoch:  1 | Step:  68 | Val Loss:  0.5272877812385559\n",
      "Epoch:  1 | Step:  69 | Val Loss:  0.7777611613273621\n",
      "Epoch:  1 | Step:  70 | Val Loss:  0.8334071636199951\n",
      "Epoch:  1 | Step:  71 | Val Loss:  0.9696551561355591\n",
      "Epoch:  1 | Step:  72 | Val Loss:  0.9444524049758911\n",
      "Epoch:  1 | Step:  73 | Val Loss:  0.5621379017829895\n",
      "Epoch:  1 | Step:  74 | Val Loss:  0.7452300786972046\n",
      "Epoch:  1 | Step:  75 | Val Loss:  0.794743001461029\n",
      "Epoch:  1 | Step:  76 | Val Loss:  1.0060330629348755\n",
      "Epoch:  1 | Step:  77 | Val Loss:  0.7895478010177612\n",
      "Epoch:  1 | Step:  78 | Val Loss:  0.6006991863250732\n",
      "Epoch:  1 | Step:  79 | Val Loss:  0.8541220426559448\n",
      "Epoch:  1 | Step:  80 | Val Loss:  0.5180985927581787\n",
      "Epoch:  1 | Step:  81 | Val Loss:  0.48783794045448303\n",
      "Epoch:  1 | Step:  82 | Val Loss:  0.6995357275009155\n",
      "Epoch:  1 | Step:  83 | Val Loss:  0.7350360155105591\n",
      "Epoch:  1 | Step:  84 | Val Loss:  1.0043752193450928\n",
      "Epoch:  1 | Step:  85 | Val Loss:  0.5388918519020081\n",
      "Epoch:  1 | Step:  86 | Val Loss:  0.5073609352111816\n",
      "Epoch:  1 | Step:  87 | Val Loss:  0.7170588970184326\n",
      "Epoch:  1 | Step:  88 | Val Loss:  0.9110711812973022\n",
      "Epoch:  1 | Step:  89 | Val Loss:  0.8966516256332397\n",
      "Epoch:  1 | Step:  90 | Val Loss:  0.5287871360778809\n",
      "Epoch:  1 | Step:  91 | Val Loss:  0.8193597793579102\n",
      "Epoch:  1 | Step:  92 | Val Loss:  0.7915257215499878\n",
      "Epoch:  1 | Step:  93 | Val Loss:  0.7510925531387329\n",
      "Epoch:  1 | Step:  94 | Val Loss:  0.5885747075080872\n",
      "Epoch:  1 | Step:  95 | Val Loss:  1.020819902420044\n",
      "Epoch:  1 | Step:  96 | Val Loss:  0.931963324546814\n",
      "Epoch:  1 | Step:  97 | Val Loss:  0.6137748956680298\n",
      "Epoch:  1 | Step:  98 | Val Loss:  1.036301851272583\n",
      "Epoch:  1 | Step:  99 | Val Loss:  0.5905344486236572\n",
      "Epoch:  1 | Step:  100 | Val Loss:  0.9039855599403381\n",
      "Epoch:  1 | Step:  101 | Val Loss:  0.8673906922340393\n",
      "Epoch:  1 | Step:  102 | Val Loss:  0.6391440629959106\n",
      "Epoch:  1 | Step:  103 | Val Loss:  0.7025749683380127\n",
      "Epoch:  1 | Step:  104 | Val Loss:  0.6540371775627136\n",
      "Epoch:  1 | Step:  105 | Val Loss:  0.9811937808990479\n",
      "Epoch:  1 | Step:  106 | Val Loss:  0.6190875768661499\n",
      "Epoch:  1 | Step:  107 | Val Loss:  0.5384730696678162\n",
      "Epoch:  1 | Step:  108 | Val Loss:  0.6973167061805725\n",
      "Epoch:  1 | Step:  109 | Val Loss:  0.9018446207046509\n",
      "Epoch:  1 | Step:  110 | Val Loss:  0.7645286321640015\n",
      "Epoch:  1 | Step:  111 | Val Loss:  0.500949501991272\n",
      "Epoch:  1 | Step:  112 | Val Loss:  0.5026921033859253\n",
      "Epoch:  1 | Step:  113 | Val Loss:  0.7843056917190552\n",
      "Epoch:  1 | Step:  114 | Val Loss:  0.6746900081634521\n",
      "Epoch:  1 | Step:  115 | Val Loss:  0.357816219329834\n",
      "Epoch:  1 | Step:  116 | Val Loss:  0.6222130060195923\n",
      "Epoch:  1 | Step:  117 | Val Loss:  0.5996438264846802\n",
      "Epoch:  1 | Step:  118 | Val Loss:  0.6519830226898193\n",
      "Epoch:  1 | Step:  119 | Val Loss:  0.5687790513038635\n",
      "Epoch:  1 | Step:  120 | Val Loss:  1.0090268850326538\n",
      "Epoch:  1 | Step:  121 | Val Loss:  0.49951380491256714\n",
      "Epoch:  1 | Step:  122 | Val Loss:  0.667604386806488\n",
      "Epoch:  1 | Step:  123 | Val Loss:  0.8257513642311096\n",
      "Epoch:  1 | Step:  124 | Val Loss:  0.8206167221069336\n",
      "Epoch:  1 | Step:  125 | Val Loss:  0.9912877082824707\n",
      "Epoch:  1 | Train Loss:  tensor(0.6501, device='cuda:0') | Val Loss:  tensor(0.7026, device='cuda:0')\n",
      "Epoch:  2 | Step:  500 | Train Loss:  0.6360275745391846\n",
      "Epoch:  2 | Step:  1 | Val Loss:  0.6146892309188843\n",
      "Epoch:  2 | Step:  2 | Val Loss:  0.7597832679748535\n",
      "Epoch:  2 | Step:  3 | Val Loss:  0.9600443840026855\n",
      "Epoch:  2 | Step:  4 | Val Loss:  0.5425841808319092\n",
      "Epoch:  2 | Step:  5 | Val Loss:  0.5786073207855225\n",
      "Epoch:  2 | Step:  6 | Val Loss:  0.6109875440597534\n",
      "Epoch:  2 | Step:  7 | Val Loss:  0.49823516607284546\n",
      "Epoch:  2 | Step:  8 | Val Loss:  0.5584452152252197\n",
      "Epoch:  2 | Step:  9 | Val Loss:  0.6793356537818909\n",
      "Epoch:  2 | Step:  10 | Val Loss:  0.741950511932373\n",
      "Epoch:  2 | Step:  11 | Val Loss:  0.6169255375862122\n",
      "Epoch:  2 | Step:  12 | Val Loss:  0.5930550694465637\n",
      "Epoch:  2 | Step:  13 | Val Loss:  0.6181188821792603\n",
      "Epoch:  2 | Step:  14 | Val Loss:  0.8483544588088989\n",
      "Epoch:  2 | Step:  15 | Val Loss:  0.6060472726821899\n",
      "Epoch:  2 | Step:  16 | Val Loss:  0.7797094583511353\n",
      "Epoch:  2 | Step:  17 | Val Loss:  1.014134407043457\n",
      "Epoch:  2 | Step:  18 | Val Loss:  0.6232554316520691\n",
      "Epoch:  2 | Step:  19 | Val Loss:  0.860383152961731\n",
      "Epoch:  2 | Step:  20 | Val Loss:  0.7707081437110901\n",
      "Epoch:  2 | Step:  21 | Val Loss:  0.597008466720581\n",
      "Epoch:  2 | Step:  22 | Val Loss:  0.6551690697669983\n",
      "Epoch:  2 | Step:  23 | Val Loss:  0.46601492166519165\n",
      "Epoch:  2 | Step:  24 | Val Loss:  0.8282575011253357\n",
      "Epoch:  2 | Step:  25 | Val Loss:  0.8218398094177246\n",
      "Epoch:  2 | Step:  26 | Val Loss:  0.6282128095626831\n",
      "Epoch:  2 | Step:  27 | Val Loss:  0.6786715984344482\n",
      "Epoch:  2 | Step:  28 | Val Loss:  0.699097216129303\n",
      "Epoch:  2 | Step:  29 | Val Loss:  0.8710740804672241\n",
      "Epoch:  2 | Step:  30 | Val Loss:  0.5871091485023499\n",
      "Epoch:  2 | Step:  31 | Val Loss:  0.5137627124786377\n",
      "Epoch:  2 | Step:  32 | Val Loss:  0.6277570724487305\n",
      "Epoch:  2 | Step:  33 | Val Loss:  0.782687246799469\n",
      "Epoch:  2 | Step:  34 | Val Loss:  0.40692979097366333\n",
      "Epoch:  2 | Step:  35 | Val Loss:  0.7824670672416687\n",
      "Epoch:  2 | Step:  36 | Val Loss:  0.9292213916778564\n",
      "Epoch:  2 | Step:  37 | Val Loss:  0.7482848167419434\n",
      "Epoch:  2 | Step:  38 | Val Loss:  0.4293268918991089\n",
      "Epoch:  2 | Step:  39 | Val Loss:  0.7068788409233093\n",
      "Epoch:  2 | Step:  40 | Val Loss:  0.7138409614562988\n",
      "Epoch:  2 | Step:  41 | Val Loss:  0.6632323265075684\n",
      "Epoch:  2 | Step:  42 | Val Loss:  0.5225992798805237\n",
      "Epoch:  2 | Step:  43 | Val Loss:  0.4279942214488983\n",
      "Epoch:  2 | Step:  44 | Val Loss:  0.5222575664520264\n",
      "Epoch:  2 | Step:  45 | Val Loss:  0.6236570477485657\n",
      "Epoch:  2 | Step:  46 | Val Loss:  0.5944637060165405\n",
      "Epoch:  2 | Step:  47 | Val Loss:  0.7301366329193115\n",
      "Epoch:  2 | Step:  48 | Val Loss:  0.7781692743301392\n",
      "Epoch:  2 | Step:  49 | Val Loss:  0.835054874420166\n",
      "Epoch:  2 | Step:  50 | Val Loss:  0.8040897250175476\n",
      "Epoch:  2 | Step:  51 | Val Loss:  0.809702455997467\n",
      "Epoch:  2 | Step:  52 | Val Loss:  0.8102552890777588\n",
      "Epoch:  2 | Step:  53 | Val Loss:  0.5346311926841736\n",
      "Epoch:  2 | Step:  54 | Val Loss:  0.6167848706245422\n",
      "Epoch:  2 | Step:  55 | Val Loss:  0.8638075590133667\n",
      "Epoch:  2 | Step:  56 | Val Loss:  0.726455807685852\n",
      "Epoch:  2 | Step:  57 | Val Loss:  0.35654401779174805\n",
      "Epoch:  2 | Step:  58 | Val Loss:  0.3444805145263672\n",
      "Epoch:  2 | Step:  59 | Val Loss:  0.6363463997840881\n",
      "Epoch:  2 | Step:  60 | Val Loss:  0.6348139643669128\n",
      "Epoch:  2 | Step:  61 | Val Loss:  0.8218057751655579\n",
      "Epoch:  2 | Step:  62 | Val Loss:  0.6006553173065186\n",
      "Epoch:  2 | Step:  63 | Val Loss:  0.7357478141784668\n",
      "Epoch:  2 | Step:  64 | Val Loss:  0.5792028903961182\n",
      "Epoch:  2 | Step:  65 | Val Loss:  0.6375223398208618\n",
      "Epoch:  2 | Step:  66 | Val Loss:  0.8707634210586548\n",
      "Epoch:  2 | Step:  67 | Val Loss:  0.6868102550506592\n",
      "Epoch:  2 | Step:  68 | Val Loss:  0.6798640489578247\n",
      "Epoch:  2 | Step:  69 | Val Loss:  0.6120258569717407\n",
      "Epoch:  2 | Step:  70 | Val Loss:  0.44728997349739075\n",
      "Epoch:  2 | Step:  71 | Val Loss:  0.6821483373641968\n",
      "Epoch:  2 | Step:  72 | Val Loss:  0.6533077359199524\n",
      "Epoch:  2 | Step:  73 | Val Loss:  0.5774264931678772\n",
      "Epoch:  2 | Step:  74 | Val Loss:  0.7516242861747742\n",
      "Epoch:  2 | Step:  75 | Val Loss:  0.4223380386829376\n",
      "Epoch:  2 | Step:  76 | Val Loss:  0.5319570302963257\n",
      "Epoch:  2 | Step:  77 | Val Loss:  0.6636863350868225\n",
      "Epoch:  2 | Step:  78 | Val Loss:  0.6824349164962769\n",
      "Epoch:  2 | Step:  79 | Val Loss:  0.9146488308906555\n",
      "Epoch:  2 | Step:  80 | Val Loss:  0.6619572043418884\n",
      "Epoch:  2 | Step:  81 | Val Loss:  0.8127397894859314\n",
      "Epoch:  2 | Step:  82 | Val Loss:  0.8888963460922241\n",
      "Epoch:  2 | Step:  83 | Val Loss:  0.8127665519714355\n",
      "Epoch:  2 | Step:  84 | Val Loss:  0.6680175065994263\n",
      "Epoch:  2 | Step:  85 | Val Loss:  0.7838658690452576\n",
      "Epoch:  2 | Step:  86 | Val Loss:  0.6485954523086548\n",
      "Epoch:  2 | Step:  87 | Val Loss:  0.655167281627655\n",
      "Epoch:  2 | Step:  88 | Val Loss:  0.6672457456588745\n",
      "Epoch:  2 | Step:  89 | Val Loss:  0.8249835968017578\n",
      "Epoch:  2 | Step:  90 | Val Loss:  0.5826534628868103\n",
      "Epoch:  2 | Step:  91 | Val Loss:  0.8555446267127991\n",
      "Epoch:  2 | Step:  92 | Val Loss:  0.7923176288604736\n",
      "Epoch:  2 | Step:  93 | Val Loss:  1.0025582313537598\n",
      "Epoch:  2 | Step:  94 | Val Loss:  0.6237258911132812\n",
      "Epoch:  2 | Step:  95 | Val Loss:  0.7076493501663208\n",
      "Epoch:  2 | Step:  96 | Val Loss:  0.49475693702697754\n",
      "Epoch:  2 | Step:  97 | Val Loss:  0.6652994751930237\n",
      "Epoch:  2 | Step:  98 | Val Loss:  1.0070027112960815\n",
      "Epoch:  2 | Step:  99 | Val Loss:  0.6239240765571594\n",
      "Epoch:  2 | Step:  100 | Val Loss:  0.8246786594390869\n",
      "Epoch:  2 | Step:  101 | Val Loss:  0.6083498001098633\n",
      "Epoch:  2 | Step:  102 | Val Loss:  0.8014200925827026\n",
      "Epoch:  2 | Step:  103 | Val Loss:  0.6200485229492188\n",
      "Epoch:  2 | Step:  104 | Val Loss:  0.9664843678474426\n",
      "Epoch:  2 | Step:  105 | Val Loss:  0.8799558877944946\n",
      "Epoch:  2 | Step:  106 | Val Loss:  0.27212661504745483\n",
      "Epoch:  2 | Step:  107 | Val Loss:  0.753948450088501\n",
      "Epoch:  2 | Step:  108 | Val Loss:  0.6810041666030884\n",
      "Epoch:  2 | Step:  109 | Val Loss:  0.6922881603240967\n",
      "Epoch:  2 | Step:  110 | Val Loss:  0.7400016784667969\n",
      "Epoch:  2 | Step:  111 | Val Loss:  0.6546192169189453\n",
      "Epoch:  2 | Step:  112 | Val Loss:  0.8218395113945007\n",
      "Epoch:  2 | Step:  113 | Val Loss:  0.38153403997421265\n",
      "Epoch:  2 | Step:  114 | Val Loss:  0.3985136151313782\n",
      "Epoch:  2 | Step:  115 | Val Loss:  0.9133995771408081\n",
      "Epoch:  2 | Step:  116 | Val Loss:  0.6743682622909546\n",
      "Epoch:  2 | Step:  117 | Val Loss:  0.960033655166626\n",
      "Epoch:  2 | Step:  118 | Val Loss:  0.4846341609954834\n",
      "Epoch:  2 | Step:  119 | Val Loss:  0.6562893390655518\n",
      "Epoch:  2 | Step:  120 | Val Loss:  0.5088291168212891\n",
      "Epoch:  2 | Step:  121 | Val Loss:  0.625816822052002\n",
      "Epoch:  2 | Step:  122 | Val Loss:  0.7960546612739563\n",
      "Epoch:  2 | Step:  123 | Val Loss:  0.7299402952194214\n",
      "Epoch:  2 | Step:  124 | Val Loss:  0.710408091545105\n",
      "Epoch:  2 | Step:  125 | Val Loss:  0.36695852875709534\n",
      "Epoch:  2 | Train Loss:  tensor(0.6304, device='cuda:0') | Val Loss:  tensor(0.6803, device='cuda:0')\n",
      "Epoch:  3 | Step:  500 | Train Loss:  0.5869580507278442\n",
      "Epoch:  3 | Step:  1 | Val Loss:  0.6333967447280884\n",
      "Epoch:  3 | Step:  2 | Val Loss:  0.6930885314941406\n",
      "Epoch:  3 | Step:  3 | Val Loss:  0.8587231636047363\n",
      "Epoch:  3 | Step:  4 | Val Loss:  0.6610924005508423\n",
      "Epoch:  3 | Step:  5 | Val Loss:  0.647865891456604\n",
      "Epoch:  3 | Step:  6 | Val Loss:  0.5094012022018433\n",
      "Epoch:  3 | Step:  7 | Val Loss:  0.915630578994751\n",
      "Epoch:  3 | Step:  8 | Val Loss:  0.7292611598968506\n",
      "Epoch:  3 | Step:  9 | Val Loss:  0.7287960052490234\n",
      "Epoch:  3 | Step:  10 | Val Loss:  0.8039292693138123\n",
      "Epoch:  3 | Step:  11 | Val Loss:  0.5497035980224609\n",
      "Epoch:  3 | Step:  12 | Val Loss:  0.8768619298934937\n",
      "Epoch:  3 | Step:  13 | Val Loss:  0.634873628616333\n",
      "Epoch:  3 | Step:  14 | Val Loss:  0.5730322599411011\n",
      "Epoch:  3 | Step:  15 | Val Loss:  0.5869513154029846\n",
      "Epoch:  3 | Step:  16 | Val Loss:  0.5634922981262207\n",
      "Epoch:  3 | Step:  17 | Val Loss:  0.4663398861885071\n",
      "Epoch:  3 | Step:  18 | Val Loss:  0.4982531666755676\n",
      "Epoch:  3 | Step:  19 | Val Loss:  0.2815009355545044\n",
      "Epoch:  3 | Step:  20 | Val Loss:  0.5004462003707886\n",
      "Epoch:  3 | Step:  21 | Val Loss:  0.8273496031761169\n",
      "Epoch:  3 | Step:  22 | Val Loss:  0.6061711311340332\n",
      "Epoch:  3 | Step:  23 | Val Loss:  0.8073340654373169\n",
      "Epoch:  3 | Step:  24 | Val Loss:  0.3972586393356323\n",
      "Epoch:  3 | Step:  25 | Val Loss:  0.9272843599319458\n",
      "Epoch:  3 | Step:  26 | Val Loss:  0.7898555994033813\n",
      "Epoch:  3 | Step:  27 | Val Loss:  0.7089642286300659\n",
      "Epoch:  3 | Step:  28 | Val Loss:  0.7420345544815063\n",
      "Epoch:  3 | Step:  29 | Val Loss:  0.6466813087463379\n",
      "Epoch:  3 | Step:  30 | Val Loss:  0.57503342628479\n",
      "Epoch:  3 | Step:  31 | Val Loss:  0.8354353904724121\n",
      "Epoch:  3 | Step:  32 | Val Loss:  0.8962757587432861\n",
      "Epoch:  3 | Step:  33 | Val Loss:  0.4954832196235657\n",
      "Epoch:  3 | Step:  34 | Val Loss:  0.8977727890014648\n",
      "Epoch:  3 | Step:  35 | Val Loss:  0.8637402653694153\n",
      "Epoch:  3 | Step:  36 | Val Loss:  0.4040448069572449\n",
      "Epoch:  3 | Step:  37 | Val Loss:  0.7060835361480713\n",
      "Epoch:  3 | Step:  38 | Val Loss:  0.6475751996040344\n",
      "Epoch:  3 | Step:  39 | Val Loss:  0.4188082218170166\n",
      "Epoch:  3 | Step:  40 | Val Loss:  0.7068423628807068\n",
      "Epoch:  3 | Step:  41 | Val Loss:  0.40223219990730286\n",
      "Epoch:  3 | Step:  42 | Val Loss:  0.7946745157241821\n",
      "Epoch:  3 | Step:  43 | Val Loss:  0.7443324327468872\n",
      "Epoch:  3 | Step:  44 | Val Loss:  0.7339404821395874\n",
      "Epoch:  3 | Step:  45 | Val Loss:  0.5068178176879883\n",
      "Epoch:  3 | Step:  46 | Val Loss:  0.9379647970199585\n",
      "Epoch:  3 | Step:  47 | Val Loss:  0.5030317306518555\n",
      "Epoch:  3 | Step:  48 | Val Loss:  0.6744786500930786\n",
      "Epoch:  3 | Step:  49 | Val Loss:  0.6189868450164795\n",
      "Epoch:  3 | Step:  50 | Val Loss:  0.5581930875778198\n",
      "Epoch:  3 | Step:  51 | Val Loss:  0.7211812734603882\n",
      "Epoch:  3 | Step:  52 | Val Loss:  0.708181619644165\n",
      "Epoch:  3 | Step:  53 | Val Loss:  0.745011031627655\n",
      "Epoch:  3 | Step:  54 | Val Loss:  0.7739453911781311\n",
      "Epoch:  3 | Step:  55 | Val Loss:  0.640399158000946\n",
      "Epoch:  3 | Step:  56 | Val Loss:  0.7135205268859863\n",
      "Epoch:  3 | Step:  57 | Val Loss:  0.5855510234832764\n",
      "Epoch:  3 | Step:  58 | Val Loss:  0.6400247812271118\n",
      "Epoch:  3 | Step:  59 | Val Loss:  0.6701359748840332\n",
      "Epoch:  3 | Step:  60 | Val Loss:  0.8796308040618896\n",
      "Epoch:  3 | Step:  61 | Val Loss:  0.6355118751525879\n",
      "Epoch:  3 | Step:  62 | Val Loss:  0.9005499482154846\n",
      "Epoch:  3 | Step:  63 | Val Loss:  0.5362672209739685\n",
      "Epoch:  3 | Step:  64 | Val Loss:  0.5669394731521606\n",
      "Epoch:  3 | Step:  65 | Val Loss:  0.6802530884742737\n",
      "Epoch:  3 | Step:  66 | Val Loss:  0.7203205823898315\n",
      "Epoch:  3 | Step:  67 | Val Loss:  0.5869030356407166\n",
      "Epoch:  3 | Step:  68 | Val Loss:  0.6426982879638672\n",
      "Epoch:  3 | Step:  69 | Val Loss:  0.6783499717712402\n",
      "Epoch:  3 | Step:  70 | Val Loss:  0.4459109306335449\n",
      "Epoch:  3 | Step:  71 | Val Loss:  0.6344002485275269\n",
      "Epoch:  3 | Step:  72 | Val Loss:  0.4894850552082062\n",
      "Epoch:  3 | Step:  73 | Val Loss:  0.6506950855255127\n",
      "Epoch:  3 | Step:  74 | Val Loss:  0.4723302125930786\n",
      "Epoch:  3 | Step:  75 | Val Loss:  0.5366400480270386\n",
      "Epoch:  3 | Step:  76 | Val Loss:  0.6241397857666016\n",
      "Epoch:  3 | Step:  77 | Val Loss:  0.6435894966125488\n",
      "Epoch:  3 | Step:  78 | Val Loss:  0.6779344081878662\n",
      "Epoch:  3 | Step:  79 | Val Loss:  0.5871638059616089\n",
      "Epoch:  3 | Step:  80 | Val Loss:  0.6727081537246704\n",
      "Epoch:  3 | Step:  81 | Val Loss:  0.6505528688430786\n",
      "Epoch:  3 | Step:  82 | Val Loss:  0.6890138387680054\n",
      "Epoch:  3 | Step:  83 | Val Loss:  0.35466715693473816\n",
      "Epoch:  3 | Step:  84 | Val Loss:  0.6252867579460144\n",
      "Epoch:  3 | Step:  85 | Val Loss:  0.6936480402946472\n",
      "Epoch:  3 | Step:  86 | Val Loss:  0.46213144063949585\n",
      "Epoch:  3 | Step:  87 | Val Loss:  0.882828950881958\n",
      "Epoch:  3 | Step:  88 | Val Loss:  0.735941469669342\n",
      "Epoch:  3 | Step:  89 | Val Loss:  0.5725535154342651\n",
      "Epoch:  3 | Step:  90 | Val Loss:  0.6378311514854431\n",
      "Epoch:  3 | Step:  91 | Val Loss:  0.5337417721748352\n",
      "Epoch:  3 | Step:  92 | Val Loss:  0.9028114080429077\n",
      "Epoch:  3 | Step:  93 | Val Loss:  0.8039808869361877\n",
      "Epoch:  3 | Step:  94 | Val Loss:  0.6627117395401001\n",
      "Epoch:  3 | Step:  95 | Val Loss:  0.8147719502449036\n",
      "Epoch:  3 | Step:  96 | Val Loss:  0.7988818883895874\n",
      "Epoch:  3 | Step:  97 | Val Loss:  0.783591628074646\n",
      "Epoch:  3 | Step:  98 | Val Loss:  0.4865742325782776\n",
      "Epoch:  3 | Step:  99 | Val Loss:  0.6586768627166748\n",
      "Epoch:  3 | Step:  100 | Val Loss:  0.45883989334106445\n",
      "Epoch:  3 | Step:  101 | Val Loss:  0.7119395732879639\n",
      "Epoch:  3 | Step:  102 | Val Loss:  0.8533034324645996\n",
      "Epoch:  3 | Step:  103 | Val Loss:  0.8004730939865112\n",
      "Epoch:  3 | Step:  104 | Val Loss:  0.7849191427230835\n",
      "Epoch:  3 | Step:  105 | Val Loss:  0.5769357681274414\n",
      "Epoch:  3 | Step:  106 | Val Loss:  0.7082810997962952\n",
      "Epoch:  3 | Step:  107 | Val Loss:  0.7426426410675049\n",
      "Epoch:  3 | Step:  108 | Val Loss:  0.6656031608581543\n",
      "Epoch:  3 | Step:  109 | Val Loss:  0.6334167122840881\n",
      "Epoch:  3 | Step:  110 | Val Loss:  0.6476515531539917\n",
      "Epoch:  3 | Step:  111 | Val Loss:  0.937397301197052\n",
      "Epoch:  3 | Step:  112 | Val Loss:  0.7449367046356201\n",
      "Epoch:  3 | Step:  113 | Val Loss:  0.7490223050117493\n",
      "Epoch:  3 | Step:  114 | Val Loss:  0.7202173471450806\n",
      "Epoch:  3 | Step:  115 | Val Loss:  0.8636698722839355\n",
      "Epoch:  3 | Step:  116 | Val Loss:  0.48053109645843506\n",
      "Epoch:  3 | Step:  117 | Val Loss:  0.5706383585929871\n",
      "Epoch:  3 | Step:  118 | Val Loss:  0.7718023657798767\n",
      "Epoch:  3 | Step:  119 | Val Loss:  0.6805722713470459\n",
      "Epoch:  3 | Step:  120 | Val Loss:  0.5353307723999023\n",
      "Epoch:  3 | Step:  121 | Val Loss:  0.54791259765625\n",
      "Epoch:  3 | Step:  122 | Val Loss:  0.6652189493179321\n",
      "Epoch:  3 | Step:  123 | Val Loss:  0.5156217813491821\n",
      "Epoch:  3 | Step:  124 | Val Loss:  0.7185978889465332\n",
      "Epoch:  3 | Step:  125 | Val Loss:  0.6145536303520203\n",
      "Epoch:  3 | Train Loss:  tensor(0.6173, device='cuda:0') | Val Loss:  tensor(0.6640, device='cuda:0')\n",
      "Epoch:  4 | Step:  500 | Train Loss:  0.4651568830013275\n",
      "Epoch:  4 | Step:  1 | Val Loss:  0.6615675687789917\n",
      "Epoch:  4 | Step:  2 | Val Loss:  0.6378583908081055\n",
      "Epoch:  4 | Step:  3 | Val Loss:  0.7255129814147949\n",
      "Epoch:  4 | Step:  4 | Val Loss:  0.5346757769584656\n",
      "Epoch:  4 | Step:  5 | Val Loss:  0.6798615455627441\n",
      "Epoch:  4 | Step:  6 | Val Loss:  0.6062143445014954\n",
      "Epoch:  4 | Step:  7 | Val Loss:  0.5742904543876648\n",
      "Epoch:  4 | Step:  8 | Val Loss:  0.5445256233215332\n",
      "Epoch:  4 | Step:  9 | Val Loss:  0.7010403275489807\n",
      "Epoch:  4 | Step:  10 | Val Loss:  0.8457565307617188\n",
      "Epoch:  4 | Step:  11 | Val Loss:  0.4806874096393585\n",
      "Epoch:  4 | Step:  12 | Val Loss:  0.7191282510757446\n",
      "Epoch:  4 | Step:  13 | Val Loss:  0.7660866379737854\n",
      "Epoch:  4 | Step:  14 | Val Loss:  0.5779151916503906\n",
      "Epoch:  4 | Step:  15 | Val Loss:  0.6088569760322571\n",
      "Epoch:  4 | Step:  16 | Val Loss:  0.6033851504325867\n",
      "Epoch:  4 | Step:  17 | Val Loss:  0.5631470680236816\n",
      "Epoch:  4 | Step:  18 | Val Loss:  0.5669432878494263\n",
      "Epoch:  4 | Step:  19 | Val Loss:  0.6198805570602417\n",
      "Epoch:  4 | Step:  20 | Val Loss:  0.7052488327026367\n",
      "Epoch:  4 | Step:  21 | Val Loss:  0.8713222742080688\n",
      "Epoch:  4 | Step:  22 | Val Loss:  0.4804539680480957\n",
      "Epoch:  4 | Step:  23 | Val Loss:  0.8016241788864136\n",
      "Epoch:  4 | Step:  24 | Val Loss:  1.0951635837554932\n",
      "Epoch:  4 | Step:  25 | Val Loss:  0.5272525548934937\n",
      "Epoch:  4 | Step:  26 | Val Loss:  0.8289560079574585\n",
      "Epoch:  4 | Step:  27 | Val Loss:  0.4907299280166626\n",
      "Epoch:  4 | Step:  28 | Val Loss:  0.6785111427307129\n",
      "Epoch:  4 | Step:  29 | Val Loss:  0.800723135471344\n",
      "Epoch:  4 | Step:  30 | Val Loss:  0.45184263586997986\n",
      "Epoch:  4 | Step:  31 | Val Loss:  0.8179391026496887\n",
      "Epoch:  4 | Step:  32 | Val Loss:  0.5573363304138184\n",
      "Epoch:  4 | Step:  33 | Val Loss:  0.46362000703811646\n",
      "Epoch:  4 | Step:  34 | Val Loss:  0.6638563871383667\n",
      "Epoch:  4 | Step:  35 | Val Loss:  0.47075849771499634\n",
      "Epoch:  4 | Step:  36 | Val Loss:  0.8386833667755127\n",
      "Epoch:  4 | Step:  37 | Val Loss:  0.6274274587631226\n",
      "Epoch:  4 | Step:  38 | Val Loss:  0.7385239601135254\n",
      "Epoch:  4 | Step:  39 | Val Loss:  0.6842477321624756\n",
      "Epoch:  4 | Step:  40 | Val Loss:  0.7796975374221802\n",
      "Epoch:  4 | Step:  41 | Val Loss:  0.5753703117370605\n",
      "Epoch:  4 | Step:  42 | Val Loss:  0.5093443393707275\n",
      "Epoch:  4 | Step:  43 | Val Loss:  0.919856607913971\n",
      "Epoch:  4 | Step:  44 | Val Loss:  0.7509711980819702\n",
      "Epoch:  4 | Step:  45 | Val Loss:  0.772544264793396\n",
      "Epoch:  4 | Step:  46 | Val Loss:  0.6880492568016052\n",
      "Epoch:  4 | Step:  47 | Val Loss:  0.7473257780075073\n",
      "Epoch:  4 | Step:  48 | Val Loss:  0.6723186373710632\n",
      "Epoch:  4 | Step:  49 | Val Loss:  0.6838983297348022\n",
      "Epoch:  4 | Step:  50 | Val Loss:  0.7265819311141968\n",
      "Epoch:  4 | Step:  51 | Val Loss:  0.5918903350830078\n",
      "Epoch:  4 | Step:  52 | Val Loss:  0.9564467668533325\n",
      "Epoch:  4 | Step:  53 | Val Loss:  0.3490571975708008\n",
      "Epoch:  4 | Step:  54 | Val Loss:  0.5646510124206543\n",
      "Epoch:  4 | Step:  55 | Val Loss:  0.6212952136993408\n",
      "Epoch:  4 | Step:  56 | Val Loss:  0.5574649572372437\n",
      "Epoch:  4 | Step:  57 | Val Loss:  0.6600719094276428\n",
      "Epoch:  4 | Step:  58 | Val Loss:  0.7080888748168945\n",
      "Epoch:  4 | Step:  59 | Val Loss:  0.2757871150970459\n",
      "Epoch:  4 | Step:  60 | Val Loss:  0.5086886882781982\n",
      "Epoch:  4 | Step:  61 | Val Loss:  0.6610939502716064\n",
      "Epoch:  4 | Step:  62 | Val Loss:  0.5346581935882568\n",
      "Epoch:  4 | Step:  63 | Val Loss:  0.626233696937561\n",
      "Epoch:  4 | Step:  64 | Val Loss:  0.5196512341499329\n",
      "Epoch:  4 | Step:  65 | Val Loss:  0.7513253092765808\n",
      "Epoch:  4 | Step:  66 | Val Loss:  0.6851608753204346\n",
      "Epoch:  4 | Step:  67 | Val Loss:  0.6487292647361755\n",
      "Epoch:  4 | Step:  68 | Val Loss:  0.33371639251708984\n",
      "Epoch:  4 | Step:  69 | Val Loss:  0.5045425295829773\n",
      "Epoch:  4 | Step:  70 | Val Loss:  0.6702473163604736\n",
      "Epoch:  4 | Step:  71 | Val Loss:  0.6545044183731079\n",
      "Epoch:  4 | Step:  72 | Val Loss:  0.44296082854270935\n",
      "Epoch:  4 | Step:  73 | Val Loss:  0.513164758682251\n",
      "Epoch:  4 | Step:  74 | Val Loss:  0.46219396591186523\n",
      "Epoch:  4 | Step:  75 | Val Loss:  0.9148000478744507\n",
      "Epoch:  4 | Step:  76 | Val Loss:  0.7580665349960327\n",
      "Epoch:  4 | Step:  77 | Val Loss:  0.6941772699356079\n",
      "Epoch:  4 | Step:  78 | Val Loss:  0.6155319213867188\n",
      "Epoch:  4 | Step:  79 | Val Loss:  0.5051834583282471\n",
      "Epoch:  4 | Step:  80 | Val Loss:  0.5876795053482056\n",
      "Epoch:  4 | Step:  81 | Val Loss:  0.6299103498458862\n",
      "Epoch:  4 | Step:  82 | Val Loss:  0.6643584966659546\n",
      "Epoch:  4 | Step:  83 | Val Loss:  0.5937987565994263\n",
      "Epoch:  4 | Step:  84 | Val Loss:  0.5110327005386353\n",
      "Epoch:  4 | Step:  85 | Val Loss:  0.6984665393829346\n",
      "Epoch:  4 | Step:  86 | Val Loss:  0.5117406845092773\n",
      "Epoch:  4 | Step:  87 | Val Loss:  0.5554872751235962\n",
      "Epoch:  4 | Step:  88 | Val Loss:  0.47018712759017944\n",
      "Epoch:  4 | Step:  89 | Val Loss:  0.6200480461120605\n",
      "Epoch:  4 | Step:  90 | Val Loss:  0.7334144115447998\n",
      "Epoch:  4 | Step:  91 | Val Loss:  0.899668276309967\n",
      "Epoch:  4 | Step:  92 | Val Loss:  0.4422069191932678\n",
      "Epoch:  4 | Step:  93 | Val Loss:  0.7347885370254517\n",
      "Epoch:  4 | Step:  94 | Val Loss:  0.8179388642311096\n",
      "Epoch:  4 | Step:  95 | Val Loss:  0.5803385972976685\n",
      "Epoch:  4 | Step:  96 | Val Loss:  0.6716755032539368\n",
      "Epoch:  4 | Step:  97 | Val Loss:  1.0040768384933472\n",
      "Epoch:  4 | Step:  98 | Val Loss:  0.6609936952590942\n",
      "Epoch:  4 | Step:  99 | Val Loss:  0.7056898474693298\n",
      "Epoch:  4 | Step:  100 | Val Loss:  0.5690243244171143\n",
      "Epoch:  4 | Step:  101 | Val Loss:  0.5352252721786499\n",
      "Epoch:  4 | Step:  102 | Val Loss:  0.7061589360237122\n",
      "Epoch:  4 | Step:  103 | Val Loss:  0.667841911315918\n",
      "Epoch:  4 | Step:  104 | Val Loss:  0.4128641188144684\n",
      "Epoch:  4 | Step:  105 | Val Loss:  0.7641100287437439\n",
      "Epoch:  4 | Step:  106 | Val Loss:  0.9950414896011353\n",
      "Epoch:  4 | Step:  107 | Val Loss:  0.646325945854187\n",
      "Epoch:  4 | Step:  108 | Val Loss:  0.60283362865448\n",
      "Epoch:  4 | Step:  109 | Val Loss:  0.8582502007484436\n",
      "Epoch:  4 | Step:  110 | Val Loss:  0.6671351194381714\n",
      "Epoch:  4 | Step:  111 | Val Loss:  0.7938734292984009\n",
      "Epoch:  4 | Step:  112 | Val Loss:  0.8084216713905334\n",
      "Epoch:  4 | Step:  113 | Val Loss:  0.9241620302200317\n",
      "Epoch:  4 | Step:  114 | Val Loss:  0.6759500503540039\n",
      "Epoch:  4 | Step:  115 | Val Loss:  0.49633723497390747\n",
      "Epoch:  4 | Step:  116 | Val Loss:  0.5189231038093567\n",
      "Epoch:  4 | Step:  117 | Val Loss:  0.8495456576347351\n",
      "Epoch:  4 | Step:  118 | Val Loss:  0.8926709294319153\n",
      "Epoch:  4 | Step:  119 | Val Loss:  0.6728534698486328\n",
      "Epoch:  4 | Step:  120 | Val Loss:  0.6713278889656067\n",
      "Epoch:  4 | Step:  121 | Val Loss:  0.6996083855628967\n",
      "Epoch:  4 | Step:  122 | Val Loss:  0.5858253836631775\n",
      "Epoch:  4 | Step:  123 | Val Loss:  0.5276596546173096\n",
      "Epoch:  4 | Step:  124 | Val Loss:  0.8464970588684082\n",
      "Epoch:  4 | Step:  125 | Val Loss:  0.5580493211746216\n",
      "Epoch:  4 | Train Loss:  tensor(0.6063, device='cuda:0') | Val Loss:  tensor(0.6541, device='cuda:0')\n",
      "Epoch:  5 | Step:  500 | Train Loss:  0.40113314986228943\n",
      "Epoch:  5 | Step:  1 | Val Loss:  0.6063129901885986\n",
      "Epoch:  5 | Step:  2 | Val Loss:  0.5969772934913635\n",
      "Epoch:  5 | Step:  3 | Val Loss:  0.6536122560501099\n",
      "Epoch:  5 | Step:  4 | Val Loss:  0.5741540193557739\n",
      "Epoch:  5 | Step:  5 | Val Loss:  0.7662644982337952\n",
      "Epoch:  5 | Step:  6 | Val Loss:  0.5574182868003845\n",
      "Epoch:  5 | Step:  7 | Val Loss:  0.571287989616394\n",
      "Epoch:  5 | Step:  8 | Val Loss:  0.8699418306350708\n",
      "Epoch:  5 | Step:  9 | Val Loss:  0.40382760763168335\n",
      "Epoch:  5 | Step:  10 | Val Loss:  0.7799278497695923\n",
      "Epoch:  5 | Step:  11 | Val Loss:  0.4356552064418793\n",
      "Epoch:  5 | Step:  12 | Val Loss:  0.7566790580749512\n",
      "Epoch:  5 | Step:  13 | Val Loss:  0.40746334195137024\n",
      "Epoch:  5 | Step:  14 | Val Loss:  0.6157435178756714\n",
      "Epoch:  5 | Step:  15 | Val Loss:  0.4743838906288147\n",
      "Epoch:  5 | Step:  16 | Val Loss:  0.492351233959198\n",
      "Epoch:  5 | Step:  17 | Val Loss:  0.5304199457168579\n",
      "Epoch:  5 | Step:  18 | Val Loss:  0.3666061460971832\n",
      "Epoch:  5 | Step:  19 | Val Loss:  0.6564865112304688\n",
      "Epoch:  5 | Step:  20 | Val Loss:  0.6039580702781677\n",
      "Epoch:  5 | Step:  21 | Val Loss:  0.8492306470870972\n",
      "Epoch:  5 | Step:  22 | Val Loss:  0.8266386389732361\n",
      "Epoch:  5 | Step:  23 | Val Loss:  0.4203341603279114\n",
      "Epoch:  5 | Step:  24 | Val Loss:  0.622611403465271\n",
      "Epoch:  5 | Step:  25 | Val Loss:  0.7805852890014648\n",
      "Epoch:  5 | Step:  26 | Val Loss:  0.7628354430198669\n",
      "Epoch:  5 | Step:  27 | Val Loss:  0.6066159009933472\n",
      "Epoch:  5 | Step:  28 | Val Loss:  0.7299230098724365\n",
      "Epoch:  5 | Step:  29 | Val Loss:  0.5429378747940063\n",
      "Epoch:  5 | Step:  30 | Val Loss:  0.515451967716217\n",
      "Epoch:  5 | Step:  31 | Val Loss:  0.6418898105621338\n",
      "Epoch:  5 | Step:  32 | Val Loss:  0.38769054412841797\n",
      "Epoch:  5 | Step:  33 | Val Loss:  0.37395429611206055\n",
      "Epoch:  5 | Step:  34 | Val Loss:  0.6881110668182373\n",
      "Epoch:  5 | Step:  35 | Val Loss:  0.7649796605110168\n",
      "Epoch:  5 | Step:  36 | Val Loss:  0.6642220616340637\n",
      "Epoch:  5 | Step:  37 | Val Loss:  0.6146296262741089\n",
      "Epoch:  5 | Step:  38 | Val Loss:  0.8704838752746582\n",
      "Epoch:  5 | Step:  39 | Val Loss:  0.8145464658737183\n",
      "Epoch:  5 | Step:  40 | Val Loss:  0.6062887907028198\n",
      "Epoch:  5 | Step:  41 | Val Loss:  0.6814768314361572\n",
      "Epoch:  5 | Step:  42 | Val Loss:  0.5568605065345764\n",
      "Epoch:  5 | Step:  43 | Val Loss:  0.49501049518585205\n",
      "Epoch:  5 | Step:  44 | Val Loss:  0.8246047496795654\n",
      "Epoch:  5 | Step:  45 | Val Loss:  0.7633357644081116\n",
      "Epoch:  5 | Step:  46 | Val Loss:  0.6137884855270386\n",
      "Epoch:  5 | Step:  47 | Val Loss:  0.6685596704483032\n",
      "Epoch:  5 | Step:  48 | Val Loss:  0.9185901284217834\n",
      "Epoch:  5 | Step:  49 | Val Loss:  0.5715340971946716\n",
      "Epoch:  5 | Step:  50 | Val Loss:  0.6084760427474976\n",
      "Epoch:  5 | Step:  51 | Val Loss:  0.5078709125518799\n",
      "Epoch:  5 | Step:  52 | Val Loss:  0.8351518511772156\n",
      "Epoch:  5 | Step:  53 | Val Loss:  0.7007524967193604\n",
      "Epoch:  5 | Step:  54 | Val Loss:  0.411008358001709\n",
      "Epoch:  5 | Step:  55 | Val Loss:  0.6036313772201538\n",
      "Epoch:  5 | Step:  56 | Val Loss:  0.5903636813163757\n",
      "Epoch:  5 | Step:  57 | Val Loss:  0.6241270303726196\n",
      "Epoch:  5 | Step:  58 | Val Loss:  0.5263235569000244\n",
      "Epoch:  5 | Step:  59 | Val Loss:  0.8147588968276978\n",
      "Epoch:  5 | Step:  60 | Val Loss:  0.6235406398773193\n",
      "Epoch:  5 | Step:  61 | Val Loss:  0.7821729183197021\n",
      "Epoch:  5 | Step:  62 | Val Loss:  0.6792985200881958\n",
      "Epoch:  5 | Step:  63 | Val Loss:  0.6528265476226807\n",
      "Epoch:  5 | Step:  64 | Val Loss:  0.9951496720314026\n",
      "Epoch:  5 | Step:  65 | Val Loss:  0.5052495002746582\n",
      "Epoch:  5 | Step:  66 | Val Loss:  0.3227040767669678\n",
      "Epoch:  5 | Step:  67 | Val Loss:  0.6111606955528259\n",
      "Epoch:  5 | Step:  68 | Val Loss:  0.4807756841182709\n",
      "Epoch:  5 | Step:  69 | Val Loss:  0.6575239896774292\n",
      "Epoch:  5 | Step:  70 | Val Loss:  0.4088779091835022\n",
      "Epoch:  5 | Step:  71 | Val Loss:  0.6573023796081543\n",
      "Epoch:  5 | Step:  72 | Val Loss:  0.7952197194099426\n",
      "Epoch:  5 | Step:  73 | Val Loss:  0.631105899810791\n",
      "Epoch:  5 | Step:  74 | Val Loss:  0.5247126817703247\n",
      "Epoch:  5 | Step:  75 | Val Loss:  0.6180264949798584\n",
      "Epoch:  5 | Step:  76 | Val Loss:  1.0413331985473633\n",
      "Epoch:  5 | Step:  77 | Val Loss:  0.44813358783721924\n",
      "Epoch:  5 | Step:  78 | Val Loss:  0.5338612794876099\n",
      "Epoch:  5 | Step:  79 | Val Loss:  0.7816120386123657\n",
      "Epoch:  5 | Step:  80 | Val Loss:  0.9840730428695679\n",
      "Epoch:  5 | Step:  81 | Val Loss:  0.9011595845222473\n",
      "Epoch:  5 | Step:  82 | Val Loss:  0.5967819094657898\n",
      "Epoch:  5 | Step:  83 | Val Loss:  0.5871669054031372\n",
      "Epoch:  5 | Step:  84 | Val Loss:  0.7845054864883423\n",
      "Epoch:  5 | Step:  85 | Val Loss:  0.4103815257549286\n",
      "Epoch:  5 | Step:  86 | Val Loss:  0.8332105875015259\n",
      "Epoch:  5 | Step:  87 | Val Loss:  0.6661118268966675\n",
      "Epoch:  5 | Step:  88 | Val Loss:  0.609317421913147\n",
      "Epoch:  5 | Step:  89 | Val Loss:  0.6933423280715942\n",
      "Epoch:  5 | Step:  90 | Val Loss:  0.6182288527488708\n",
      "Epoch:  5 | Step:  91 | Val Loss:  0.6208015084266663\n",
      "Epoch:  5 | Step:  92 | Val Loss:  0.6043342351913452\n",
      "Epoch:  5 | Step:  93 | Val Loss:  0.5128823518753052\n",
      "Epoch:  5 | Step:  94 | Val Loss:  0.7201993465423584\n",
      "Epoch:  5 | Step:  95 | Val Loss:  0.6047932505607605\n",
      "Epoch:  5 | Step:  96 | Val Loss:  0.7051042318344116\n",
      "Epoch:  5 | Step:  97 | Val Loss:  0.7746769785881042\n",
      "Epoch:  5 | Step:  98 | Val Loss:  0.821799635887146\n",
      "Epoch:  5 | Step:  99 | Val Loss:  0.49951261281967163\n",
      "Epoch:  5 | Step:  100 | Val Loss:  0.8334035873413086\n",
      "Epoch:  5 | Step:  101 | Val Loss:  0.6116800308227539\n",
      "Epoch:  5 | Step:  102 | Val Loss:  0.6696258783340454\n",
      "Epoch:  5 | Step:  103 | Val Loss:  0.45194655656814575\n",
      "Epoch:  5 | Step:  104 | Val Loss:  0.8296335935592651\n",
      "Epoch:  5 | Step:  105 | Val Loss:  0.7147951126098633\n",
      "Epoch:  5 | Step:  106 | Val Loss:  0.7293193340301514\n",
      "Epoch:  5 | Step:  107 | Val Loss:  0.8465480804443359\n",
      "Epoch:  5 | Step:  108 | Val Loss:  0.5675181150436401\n",
      "Epoch:  5 | Step:  109 | Val Loss:  0.5439643263816833\n",
      "Epoch:  5 | Step:  110 | Val Loss:  0.6346502304077148\n",
      "Epoch:  5 | Step:  111 | Val Loss:  0.8430284261703491\n",
      "Epoch:  5 | Step:  112 | Val Loss:  0.46655112504959106\n",
      "Epoch:  5 | Step:  113 | Val Loss:  0.5592118501663208\n",
      "Epoch:  5 | Step:  114 | Val Loss:  0.6240711808204651\n",
      "Epoch:  5 | Step:  115 | Val Loss:  0.4043588936328888\n",
      "Epoch:  5 | Step:  116 | Val Loss:  0.6592013835906982\n",
      "Epoch:  5 | Step:  117 | Val Loss:  0.6578773856163025\n",
      "Epoch:  5 | Step:  118 | Val Loss:  0.7101437449455261\n",
      "Epoch:  5 | Step:  119 | Val Loss:  0.7920494675636292\n",
      "Epoch:  5 | Step:  120 | Val Loss:  0.5940068960189819\n",
      "Epoch:  5 | Step:  121 | Val Loss:  0.8040146231651306\n",
      "Epoch:  5 | Step:  122 | Val Loss:  0.7583235502243042\n",
      "Epoch:  5 | Step:  123 | Val Loss:  0.4799243211746216\n",
      "Epoch:  5 | Step:  124 | Val Loss:  0.5783216953277588\n",
      "Epoch:  5 | Step:  125 | Val Loss:  0.8405578136444092\n",
      "Epoch:  5 | Train Loss:  tensor(0.5974, device='cuda:0') | Val Loss:  tensor(0.6450, device='cuda:0')\n",
      "Epoch:  6 | Step:  500 | Train Loss:  0.6930729746818542\n",
      "Epoch:  6 | Step:  1 | Val Loss:  0.4880417585372925\n",
      "Epoch:  6 | Step:  2 | Val Loss:  0.6306495070457458\n",
      "Epoch:  6 | Step:  3 | Val Loss:  0.5035629868507385\n",
      "Epoch:  6 | Step:  4 | Val Loss:  0.5386772751808167\n",
      "Epoch:  6 | Step:  5 | Val Loss:  0.6781842708587646\n",
      "Epoch:  6 | Step:  6 | Val Loss:  0.6387348175048828\n",
      "Epoch:  6 | Step:  7 | Val Loss:  0.8028693795204163\n",
      "Epoch:  6 | Step:  8 | Val Loss:  0.6038089990615845\n",
      "Epoch:  6 | Step:  9 | Val Loss:  0.6229459047317505\n",
      "Epoch:  6 | Step:  10 | Val Loss:  0.8122466802597046\n",
      "Epoch:  6 | Step:  11 | Val Loss:  0.9457435011863708\n",
      "Epoch:  6 | Step:  12 | Val Loss:  0.4697313904762268\n",
      "Epoch:  6 | Step:  13 | Val Loss:  0.37451404333114624\n",
      "Epoch:  6 | Step:  14 | Val Loss:  0.7476606965065002\n",
      "Epoch:  6 | Step:  15 | Val Loss:  0.5256450772285461\n",
      "Epoch:  6 | Step:  16 | Val Loss:  0.5529767274856567\n",
      "Epoch:  6 | Step:  17 | Val Loss:  0.7879581451416016\n",
      "Epoch:  6 | Step:  18 | Val Loss:  0.5500392913818359\n",
      "Epoch:  6 | Step:  19 | Val Loss:  0.5710195899009705\n",
      "Epoch:  6 | Step:  20 | Val Loss:  0.7132382392883301\n",
      "Epoch:  6 | Step:  21 | Val Loss:  0.5250171422958374\n",
      "Epoch:  6 | Step:  22 | Val Loss:  0.6717170476913452\n",
      "Epoch:  6 | Step:  23 | Val Loss:  0.30057504773139954\n",
      "Epoch:  6 | Step:  24 | Val Loss:  0.8409658670425415\n",
      "Epoch:  6 | Step:  25 | Val Loss:  0.6429364085197449\n",
      "Epoch:  6 | Step:  26 | Val Loss:  0.5810820460319519\n",
      "Epoch:  6 | Step:  27 | Val Loss:  0.5368218421936035\n",
      "Epoch:  6 | Step:  28 | Val Loss:  0.7167502641677856\n",
      "Epoch:  6 | Step:  29 | Val Loss:  0.5376777648925781\n",
      "Epoch:  6 | Step:  30 | Val Loss:  0.6109004020690918\n",
      "Epoch:  6 | Step:  31 | Val Loss:  0.5669153928756714\n",
      "Epoch:  6 | Step:  32 | Val Loss:  0.5748686790466309\n",
      "Epoch:  6 | Step:  33 | Val Loss:  0.5569921731948853\n",
      "Epoch:  6 | Step:  34 | Val Loss:  0.4407162070274353\n",
      "Epoch:  6 | Step:  35 | Val Loss:  0.6506637334823608\n",
      "Epoch:  6 | Step:  36 | Val Loss:  0.7744773626327515\n",
      "Epoch:  6 | Step:  37 | Val Loss:  0.6204094290733337\n",
      "Epoch:  6 | Step:  38 | Val Loss:  0.501185417175293\n",
      "Epoch:  6 | Step:  39 | Val Loss:  0.7048479318618774\n",
      "Epoch:  6 | Step:  40 | Val Loss:  0.6813600063323975\n",
      "Epoch:  6 | Step:  41 | Val Loss:  0.5441886186599731\n",
      "Epoch:  6 | Step:  42 | Val Loss:  0.7200558185577393\n",
      "Epoch:  6 | Step:  43 | Val Loss:  0.5109123587608337\n",
      "Epoch:  6 | Step:  44 | Val Loss:  0.8505985736846924\n",
      "Epoch:  6 | Step:  45 | Val Loss:  0.615150511264801\n",
      "Epoch:  6 | Step:  46 | Val Loss:  0.7336344122886658\n",
      "Epoch:  6 | Step:  47 | Val Loss:  0.7081103324890137\n",
      "Epoch:  6 | Step:  48 | Val Loss:  0.7321816682815552\n",
      "Epoch:  6 | Step:  49 | Val Loss:  0.5393890142440796\n",
      "Epoch:  6 | Step:  50 | Val Loss:  0.8005845546722412\n",
      "Epoch:  6 | Step:  51 | Val Loss:  0.5999473929405212\n",
      "Epoch:  6 | Step:  52 | Val Loss:  0.9451055526733398\n",
      "Epoch:  6 | Step:  53 | Val Loss:  0.4501798748970032\n",
      "Epoch:  6 | Step:  54 | Val Loss:  0.55440354347229\n",
      "Epoch:  6 | Step:  55 | Val Loss:  0.7837666869163513\n",
      "Epoch:  6 | Step:  56 | Val Loss:  0.5405084490776062\n",
      "Epoch:  6 | Step:  57 | Val Loss:  0.5486589670181274\n",
      "Epoch:  6 | Step:  58 | Val Loss:  0.5500359535217285\n",
      "Epoch:  6 | Step:  59 | Val Loss:  0.8558229207992554\n",
      "Epoch:  6 | Step:  60 | Val Loss:  0.8633491396903992\n",
      "Epoch:  6 | Step:  61 | Val Loss:  0.9259533882141113\n",
      "Epoch:  6 | Step:  62 | Val Loss:  0.6961695551872253\n",
      "Epoch:  6 | Step:  63 | Val Loss:  0.6738963723182678\n",
      "Epoch:  6 | Step:  64 | Val Loss:  0.6549586057662964\n",
      "Epoch:  6 | Step:  65 | Val Loss:  0.5931926369667053\n",
      "Epoch:  6 | Step:  66 | Val Loss:  0.6151415109634399\n",
      "Epoch:  6 | Step:  67 | Val Loss:  0.5891530513763428\n",
      "Epoch:  6 | Step:  68 | Val Loss:  0.6108629703521729\n",
      "Epoch:  6 | Step:  69 | Val Loss:  0.7858631610870361\n",
      "Epoch:  6 | Step:  70 | Val Loss:  0.6127071380615234\n",
      "Epoch:  6 | Step:  71 | Val Loss:  0.7637573480606079\n",
      "Epoch:  6 | Step:  72 | Val Loss:  0.302604615688324\n",
      "Epoch:  6 | Step:  73 | Val Loss:  0.6022319793701172\n",
      "Epoch:  6 | Step:  74 | Val Loss:  0.8426793813705444\n",
      "Epoch:  6 | Step:  75 | Val Loss:  0.5929142832756042\n",
      "Epoch:  6 | Step:  76 | Val Loss:  0.4745720624923706\n",
      "Epoch:  6 | Step:  77 | Val Loss:  0.6416000127792358\n",
      "Epoch:  6 | Step:  78 | Val Loss:  0.7769842743873596\n",
      "Epoch:  6 | Step:  79 | Val Loss:  0.4481556713581085\n",
      "Epoch:  6 | Step:  80 | Val Loss:  0.5878464579582214\n",
      "Epoch:  6 | Step:  81 | Val Loss:  0.5056561231613159\n",
      "Epoch:  6 | Step:  82 | Val Loss:  0.8861669301986694\n",
      "Epoch:  6 | Step:  83 | Val Loss:  0.6600264310836792\n",
      "Epoch:  6 | Step:  84 | Val Loss:  0.6521117091178894\n",
      "Epoch:  6 | Step:  85 | Val Loss:  0.7809168100357056\n",
      "Epoch:  6 | Step:  86 | Val Loss:  0.5931504964828491\n",
      "Epoch:  6 | Step:  87 | Val Loss:  0.7206671237945557\n",
      "Epoch:  6 | Step:  88 | Val Loss:  0.8110761642456055\n",
      "Epoch:  6 | Step:  89 | Val Loss:  0.5926598310470581\n",
      "Epoch:  6 | Step:  90 | Val Loss:  0.5640811324119568\n",
      "Epoch:  6 | Step:  91 | Val Loss:  0.9173625707626343\n",
      "Epoch:  6 | Step:  92 | Val Loss:  0.683704137802124\n",
      "Epoch:  6 | Step:  93 | Val Loss:  0.4434591829776764\n",
      "Epoch:  6 | Step:  94 | Val Loss:  0.6819782853126526\n",
      "Epoch:  6 | Step:  95 | Val Loss:  0.7341485619544983\n",
      "Epoch:  6 | Step:  96 | Val Loss:  0.5405234694480896\n",
      "Epoch:  6 | Step:  97 | Val Loss:  0.6476202011108398\n",
      "Epoch:  6 | Step:  98 | Val Loss:  0.7745362520217896\n",
      "Epoch:  6 | Step:  99 | Val Loss:  0.6175656318664551\n",
      "Epoch:  6 | Step:  100 | Val Loss:  0.5868310928344727\n",
      "Epoch:  6 | Step:  101 | Val Loss:  0.5357773900032043\n",
      "Epoch:  6 | Step:  102 | Val Loss:  0.7245981693267822\n",
      "Epoch:  6 | Step:  103 | Val Loss:  0.5235779285430908\n",
      "Epoch:  6 | Step:  104 | Val Loss:  0.8141298890113831\n",
      "Epoch:  6 | Step:  105 | Val Loss:  0.5749304294586182\n",
      "Epoch:  6 | Step:  106 | Val Loss:  0.5927711725234985\n",
      "Epoch:  6 | Step:  107 | Val Loss:  0.6613836288452148\n",
      "Epoch:  6 | Step:  108 | Val Loss:  0.5097764730453491\n",
      "Epoch:  6 | Step:  109 | Val Loss:  0.4384436011314392\n",
      "Epoch:  6 | Step:  110 | Val Loss:  0.5614969730377197\n",
      "Epoch:  6 | Step:  111 | Val Loss:  0.7645072340965271\n",
      "Epoch:  6 | Step:  112 | Val Loss:  0.46142834424972534\n",
      "Epoch:  6 | Step:  113 | Val Loss:  0.6078000068664551\n",
      "Epoch:  6 | Step:  114 | Val Loss:  0.760619044303894\n",
      "Epoch:  6 | Step:  115 | Val Loss:  0.5743073225021362\n",
      "Epoch:  6 | Step:  116 | Val Loss:  0.5341185331344604\n",
      "Epoch:  6 | Step:  117 | Val Loss:  0.648461103439331\n",
      "Epoch:  6 | Step:  118 | Val Loss:  0.5929878950119019\n",
      "Epoch:  6 | Step:  119 | Val Loss:  0.5636395812034607\n",
      "Epoch:  6 | Step:  120 | Val Loss:  0.4305580258369446\n",
      "Epoch:  6 | Step:  121 | Val Loss:  0.5000983476638794\n",
      "Epoch:  6 | Step:  122 | Val Loss:  0.7939976453781128\n",
      "Epoch:  6 | Step:  123 | Val Loss:  0.8821589946746826\n",
      "Epoch:  6 | Step:  124 | Val Loss:  0.6288546323776245\n",
      "Epoch:  6 | Step:  125 | Val Loss:  0.7795303463935852\n",
      "Epoch:  6 | Train Loss:  tensor(0.5900, device='cuda:0') | Val Loss:  tensor(0.6375, device='cuda:0')\n",
      "Epoch:  7 | Step:  500 | Train Loss:  0.7399448752403259\n",
      "Epoch:  7 | Step:  1 | Val Loss:  0.7652917504310608\n",
      "Epoch:  7 | Step:  2 | Val Loss:  0.6317139863967896\n",
      "Epoch:  7 | Step:  3 | Val Loss:  0.6427892446517944\n",
      "Epoch:  7 | Step:  4 | Val Loss:  0.5087735652923584\n",
      "Epoch:  7 | Step:  5 | Val Loss:  0.6890367269515991\n",
      "Epoch:  7 | Step:  6 | Val Loss:  0.524105429649353\n",
      "Epoch:  7 | Step:  7 | Val Loss:  0.5865560173988342\n",
      "Epoch:  7 | Step:  8 | Val Loss:  0.869861364364624\n",
      "Epoch:  7 | Step:  9 | Val Loss:  0.7633750438690186\n",
      "Epoch:  7 | Step:  10 | Val Loss:  0.48238682746887207\n",
      "Epoch:  7 | Step:  11 | Val Loss:  0.8084465265274048\n",
      "Epoch:  7 | Step:  12 | Val Loss:  0.7102820873260498\n",
      "Epoch:  7 | Step:  13 | Val Loss:  0.7510148286819458\n",
      "Epoch:  7 | Step:  14 | Val Loss:  0.4772163927555084\n",
      "Epoch:  7 | Step:  15 | Val Loss:  0.6278706789016724\n",
      "Epoch:  7 | Step:  16 | Val Loss:  0.564906895160675\n",
      "Epoch:  7 | Step:  17 | Val Loss:  0.8031395077705383\n",
      "Epoch:  7 | Step:  18 | Val Loss:  0.6887816786766052\n",
      "Epoch:  7 | Step:  19 | Val Loss:  0.8665322661399841\n",
      "Epoch:  7 | Step:  20 | Val Loss:  0.6662863492965698\n",
      "Epoch:  7 | Step:  21 | Val Loss:  0.7314454317092896\n",
      "Epoch:  7 | Step:  22 | Val Loss:  0.7929645776748657\n",
      "Epoch:  7 | Step:  23 | Val Loss:  0.6513694524765015\n",
      "Epoch:  7 | Step:  24 | Val Loss:  0.6790341734886169\n",
      "Epoch:  7 | Step:  25 | Val Loss:  0.9226969480514526\n",
      "Epoch:  7 | Step:  26 | Val Loss:  0.7660847306251526\n",
      "Epoch:  7 | Step:  27 | Val Loss:  0.6276980638504028\n",
      "Epoch:  7 | Step:  28 | Val Loss:  0.49450579285621643\n",
      "Epoch:  7 | Step:  29 | Val Loss:  0.5071648359298706\n",
      "Epoch:  7 | Step:  30 | Val Loss:  0.5563291907310486\n",
      "Epoch:  7 | Step:  31 | Val Loss:  0.9093767404556274\n",
      "Epoch:  7 | Step:  32 | Val Loss:  0.4137462377548218\n",
      "Epoch:  7 | Step:  33 | Val Loss:  0.5800514221191406\n",
      "Epoch:  7 | Step:  34 | Val Loss:  0.7020376920700073\n",
      "Epoch:  7 | Step:  35 | Val Loss:  0.47214505076408386\n",
      "Epoch:  7 | Step:  36 | Val Loss:  0.7687712907791138\n",
      "Epoch:  7 | Step:  37 | Val Loss:  0.7816752195358276\n",
      "Epoch:  7 | Step:  38 | Val Loss:  0.8196332454681396\n",
      "Epoch:  7 | Step:  39 | Val Loss:  0.3416799306869507\n",
      "Epoch:  7 | Step:  40 | Val Loss:  0.651030957698822\n",
      "Epoch:  7 | Step:  41 | Val Loss:  0.42876654863357544\n",
      "Epoch:  7 | Step:  42 | Val Loss:  0.5734918117523193\n",
      "Epoch:  7 | Step:  43 | Val Loss:  0.5404094457626343\n",
      "Epoch:  7 | Step:  44 | Val Loss:  0.42653101682662964\n",
      "Epoch:  7 | Step:  45 | Val Loss:  0.46662914752960205\n",
      "Epoch:  7 | Step:  46 | Val Loss:  0.4460088312625885\n",
      "Epoch:  7 | Step:  47 | Val Loss:  0.6356984376907349\n",
      "Epoch:  7 | Step:  48 | Val Loss:  0.7149892449378967\n",
      "Epoch:  7 | Step:  49 | Val Loss:  0.5223696231842041\n",
      "Epoch:  7 | Step:  50 | Val Loss:  0.5499342679977417\n",
      "Epoch:  7 | Step:  51 | Val Loss:  0.5885621309280396\n",
      "Epoch:  7 | Step:  52 | Val Loss:  0.3348172605037689\n",
      "Epoch:  7 | Step:  53 | Val Loss:  0.7024500370025635\n",
      "Epoch:  7 | Step:  54 | Val Loss:  0.7512644529342651\n",
      "Epoch:  7 | Step:  55 | Val Loss:  0.346269816160202\n",
      "Epoch:  7 | Step:  56 | Val Loss:  0.47069042921066284\n",
      "Epoch:  7 | Step:  57 | Val Loss:  0.4092722237110138\n",
      "Epoch:  7 | Step:  58 | Val Loss:  0.5640354156494141\n",
      "Epoch:  7 | Step:  59 | Val Loss:  0.587414562702179\n",
      "Epoch:  7 | Step:  60 | Val Loss:  0.7252318263053894\n",
      "Epoch:  7 | Step:  61 | Val Loss:  0.6009920835494995\n",
      "Epoch:  7 | Step:  62 | Val Loss:  0.8030003905296326\n",
      "Epoch:  7 | Step:  63 | Val Loss:  0.5401039123535156\n",
      "Epoch:  7 | Step:  64 | Val Loss:  0.7232964634895325\n",
      "Epoch:  7 | Step:  65 | Val Loss:  0.6862547397613525\n",
      "Epoch:  7 | Step:  66 | Val Loss:  0.49547988176345825\n",
      "Epoch:  7 | Step:  67 | Val Loss:  0.515548825263977\n",
      "Epoch:  7 | Step:  68 | Val Loss:  0.6367940902709961\n",
      "Epoch:  7 | Step:  69 | Val Loss:  0.6239111423492432\n",
      "Epoch:  7 | Step:  70 | Val Loss:  0.4756293296813965\n",
      "Epoch:  7 | Step:  71 | Val Loss:  0.8394205570220947\n",
      "Epoch:  7 | Step:  72 | Val Loss:  0.7028294205665588\n",
      "Epoch:  7 | Step:  73 | Val Loss:  0.7545397281646729\n",
      "Epoch:  7 | Step:  74 | Val Loss:  0.7776476144790649\n",
      "Epoch:  7 | Step:  75 | Val Loss:  0.37691712379455566\n",
      "Epoch:  7 | Step:  76 | Val Loss:  0.8049542903900146\n",
      "Epoch:  7 | Step:  77 | Val Loss:  0.8697638511657715\n",
      "Epoch:  7 | Step:  78 | Val Loss:  0.2820292115211487\n",
      "Epoch:  7 | Step:  79 | Val Loss:  0.8371652364730835\n",
      "Epoch:  7 | Step:  80 | Val Loss:  0.5287807583808899\n",
      "Epoch:  7 | Step:  81 | Val Loss:  0.4949764013290405\n",
      "Epoch:  7 | Step:  82 | Val Loss:  0.5698920488357544\n",
      "Epoch:  7 | Step:  83 | Val Loss:  0.6623146533966064\n",
      "Epoch:  7 | Step:  84 | Val Loss:  0.5079625844955444\n",
      "Epoch:  7 | Step:  85 | Val Loss:  0.5152426958084106\n",
      "Epoch:  7 | Step:  86 | Val Loss:  0.6668964624404907\n",
      "Epoch:  7 | Step:  87 | Val Loss:  0.4929365813732147\n",
      "Epoch:  7 | Step:  88 | Val Loss:  0.6305098533630371\n",
      "Epoch:  7 | Step:  89 | Val Loss:  0.5987412929534912\n",
      "Epoch:  7 | Step:  90 | Val Loss:  0.8161195516586304\n",
      "Epoch:  7 | Step:  91 | Val Loss:  0.5891880989074707\n",
      "Epoch:  7 | Step:  92 | Val Loss:  0.564757227897644\n",
      "Epoch:  7 | Step:  93 | Val Loss:  0.7994207143783569\n",
      "Epoch:  7 | Step:  94 | Val Loss:  0.6752772331237793\n",
      "Epoch:  7 | Step:  95 | Val Loss:  0.8910685181617737\n",
      "Epoch:  7 | Step:  96 | Val Loss:  0.6321630477905273\n",
      "Epoch:  7 | Step:  97 | Val Loss:  0.464540958404541\n",
      "Epoch:  7 | Step:  98 | Val Loss:  0.8272358775138855\n",
      "Epoch:  7 | Step:  99 | Val Loss:  0.43119481205940247\n",
      "Epoch:  7 | Step:  100 | Val Loss:  0.7935717701911926\n",
      "Epoch:  7 | Step:  101 | Val Loss:  0.432670533657074\n",
      "Epoch:  7 | Step:  102 | Val Loss:  0.7729141712188721\n",
      "Epoch:  7 | Step:  103 | Val Loss:  0.6941918134689331\n",
      "Epoch:  7 | Step:  104 | Val Loss:  0.5132652521133423\n",
      "Epoch:  7 | Step:  105 | Val Loss:  0.8338900804519653\n",
      "Epoch:  7 | Step:  106 | Val Loss:  0.41058260202407837\n",
      "Epoch:  7 | Step:  107 | Val Loss:  0.5184289216995239\n",
      "Epoch:  7 | Step:  108 | Val Loss:  0.8120404481887817\n",
      "Epoch:  7 | Step:  109 | Val Loss:  0.5787018537521362\n",
      "Epoch:  7 | Step:  110 | Val Loss:  0.8102741241455078\n",
      "Epoch:  7 | Step:  111 | Val Loss:  0.7961457371711731\n",
      "Epoch:  7 | Step:  112 | Val Loss:  0.5304092764854431\n",
      "Epoch:  7 | Step:  113 | Val Loss:  0.38141024112701416\n",
      "Epoch:  7 | Step:  114 | Val Loss:  0.5081306099891663\n",
      "Epoch:  7 | Step:  115 | Val Loss:  0.41388872265815735\n",
      "Epoch:  7 | Step:  116 | Val Loss:  0.6562000513076782\n",
      "Epoch:  7 | Step:  117 | Val Loss:  0.41428831219673157\n",
      "Epoch:  7 | Step:  118 | Val Loss:  0.7504581212997437\n",
      "Epoch:  7 | Step:  119 | Val Loss:  0.8363739252090454\n",
      "Epoch:  7 | Step:  120 | Val Loss:  0.5972434878349304\n",
      "Epoch:  7 | Step:  121 | Val Loss:  0.6838501691818237\n",
      "Epoch:  7 | Step:  122 | Val Loss:  0.6377217769622803\n",
      "Epoch:  7 | Step:  123 | Val Loss:  0.6667281985282898\n",
      "Epoch:  7 | Step:  124 | Val Loss:  0.8177837133407593\n",
      "Epoch:  7 | Step:  125 | Val Loss:  0.5880756378173828\n",
      "Epoch:  7 | Train Loss:  tensor(0.5835, device='cuda:0') | Val Loss:  tensor(0.6283, device='cuda:0')\n",
      "Epoch:  8 | Step:  500 | Train Loss:  0.4505160450935364\n",
      "Epoch:  8 | Step:  1 | Val Loss:  0.4813874363899231\n",
      "Epoch:  8 | Step:  2 | Val Loss:  0.5228844881057739\n",
      "Epoch:  8 | Step:  3 | Val Loss:  0.6463807821273804\n",
      "Epoch:  8 | Step:  4 | Val Loss:  0.3476364016532898\n",
      "Epoch:  8 | Step:  5 | Val Loss:  0.761002779006958\n",
      "Epoch:  8 | Step:  6 | Val Loss:  0.4815225601196289\n",
      "Epoch:  8 | Step:  7 | Val Loss:  0.42455196380615234\n",
      "Epoch:  8 | Step:  8 | Val Loss:  0.5117160081863403\n",
      "Epoch:  8 | Step:  9 | Val Loss:  0.8703207969665527\n",
      "Epoch:  8 | Step:  10 | Val Loss:  0.44328731298446655\n",
      "Epoch:  8 | Step:  11 | Val Loss:  0.5761723518371582\n",
      "Epoch:  8 | Step:  12 | Val Loss:  0.651350200176239\n",
      "Epoch:  8 | Step:  13 | Val Loss:  0.6176663041114807\n",
      "Epoch:  8 | Step:  14 | Val Loss:  0.5097838640213013\n",
      "Epoch:  8 | Step:  15 | Val Loss:  0.6771988272666931\n",
      "Epoch:  8 | Step:  16 | Val Loss:  0.8056423664093018\n",
      "Epoch:  8 | Step:  17 | Val Loss:  0.7220787405967712\n",
      "Epoch:  8 | Step:  18 | Val Loss:  0.6710938215255737\n",
      "Epoch:  8 | Step:  19 | Val Loss:  0.3607913851737976\n",
      "Epoch:  8 | Step:  20 | Val Loss:  0.5452965497970581\n",
      "Epoch:  8 | Step:  21 | Val Loss:  0.6842451095581055\n",
      "Epoch:  8 | Step:  22 | Val Loss:  0.4651913046836853\n",
      "Epoch:  8 | Step:  23 | Val Loss:  0.801180362701416\n",
      "Epoch:  8 | Step:  24 | Val Loss:  0.5340853929519653\n",
      "Epoch:  8 | Step:  25 | Val Loss:  0.7044563293457031\n",
      "Epoch:  8 | Step:  26 | Val Loss:  0.8425309062004089\n",
      "Epoch:  8 | Step:  27 | Val Loss:  0.7769484519958496\n",
      "Epoch:  8 | Step:  28 | Val Loss:  0.6380935907363892\n",
      "Epoch:  8 | Step:  29 | Val Loss:  0.5675075650215149\n",
      "Epoch:  8 | Step:  30 | Val Loss:  0.618126392364502\n",
      "Epoch:  8 | Step:  31 | Val Loss:  0.4553123116493225\n",
      "Epoch:  8 | Step:  32 | Val Loss:  0.598721981048584\n",
      "Epoch:  8 | Step:  33 | Val Loss:  0.7397691607475281\n",
      "Epoch:  8 | Step:  34 | Val Loss:  0.6239099502563477\n",
      "Epoch:  8 | Step:  35 | Val Loss:  0.7109625339508057\n",
      "Epoch:  8 | Step:  36 | Val Loss:  0.44946998357772827\n",
      "Epoch:  8 | Step:  37 | Val Loss:  0.7214725613594055\n",
      "Epoch:  8 | Step:  38 | Val Loss:  0.9230501651763916\n",
      "Epoch:  8 | Step:  39 | Val Loss:  0.6765035390853882\n",
      "Epoch:  8 | Step:  40 | Val Loss:  0.6230083703994751\n",
      "Epoch:  8 | Step:  41 | Val Loss:  0.553729236125946\n",
      "Epoch:  8 | Step:  42 | Val Loss:  0.45068755745887756\n",
      "Epoch:  8 | Step:  43 | Val Loss:  0.49450406432151794\n",
      "Epoch:  8 | Step:  44 | Val Loss:  0.8259567618370056\n",
      "Epoch:  8 | Step:  45 | Val Loss:  0.6965658664703369\n",
      "Epoch:  8 | Step:  46 | Val Loss:  0.5659164786338806\n",
      "Epoch:  8 | Step:  47 | Val Loss:  0.7278540730476379\n",
      "Epoch:  8 | Step:  48 | Val Loss:  0.45325395464897156\n",
      "Epoch:  8 | Step:  49 | Val Loss:  0.6138637065887451\n",
      "Epoch:  8 | Step:  50 | Val Loss:  0.5245126485824585\n",
      "Epoch:  8 | Step:  51 | Val Loss:  0.6022261381149292\n",
      "Epoch:  8 | Step:  52 | Val Loss:  0.438997358083725\n",
      "Epoch:  8 | Step:  53 | Val Loss:  0.5882000923156738\n",
      "Epoch:  8 | Step:  54 | Val Loss:  0.6816153526306152\n",
      "Epoch:  8 | Step:  55 | Val Loss:  0.47659212350845337\n",
      "Epoch:  8 | Step:  56 | Val Loss:  0.686766505241394\n",
      "Epoch:  8 | Step:  57 | Val Loss:  0.6200893521308899\n",
      "Epoch:  8 | Step:  58 | Val Loss:  0.8504319190979004\n",
      "Epoch:  8 | Step:  59 | Val Loss:  0.6607739925384521\n",
      "Epoch:  8 | Step:  60 | Val Loss:  0.775209903717041\n",
      "Epoch:  8 | Step:  61 | Val Loss:  0.5395666360855103\n",
      "Epoch:  8 | Step:  62 | Val Loss:  0.6897155046463013\n",
      "Epoch:  8 | Step:  63 | Val Loss:  0.3989979028701782\n",
      "Epoch:  8 | Step:  64 | Val Loss:  0.7923122048377991\n",
      "Epoch:  8 | Step:  65 | Val Loss:  0.6645706295967102\n",
      "Epoch:  8 | Step:  66 | Val Loss:  0.5027209520339966\n",
      "Epoch:  8 | Step:  67 | Val Loss:  0.4368223547935486\n",
      "Epoch:  8 | Step:  68 | Val Loss:  0.8413257598876953\n",
      "Epoch:  8 | Step:  69 | Val Loss:  0.5950248837471008\n",
      "Epoch:  8 | Step:  70 | Val Loss:  0.707421600818634\n",
      "Epoch:  8 | Step:  71 | Val Loss:  0.5756332874298096\n",
      "Epoch:  8 | Step:  72 | Val Loss:  0.792435884475708\n",
      "Epoch:  8 | Step:  73 | Val Loss:  0.6464331746101379\n",
      "Epoch:  8 | Step:  74 | Val Loss:  0.5636090040206909\n",
      "Epoch:  8 | Step:  75 | Val Loss:  0.5795823931694031\n",
      "Epoch:  8 | Step:  76 | Val Loss:  0.8011658191680908\n",
      "Epoch:  8 | Step:  77 | Val Loss:  0.4471924901008606\n",
      "Epoch:  8 | Step:  78 | Val Loss:  0.5709336996078491\n",
      "Epoch:  8 | Step:  79 | Val Loss:  0.7153390645980835\n",
      "Epoch:  8 | Step:  80 | Val Loss:  0.7132470607757568\n",
      "Epoch:  8 | Step:  81 | Val Loss:  0.5415900945663452\n",
      "Epoch:  8 | Step:  82 | Val Loss:  0.7829117774963379\n",
      "Epoch:  8 | Step:  83 | Val Loss:  0.5749146938323975\n",
      "Epoch:  8 | Step:  84 | Val Loss:  0.6289039850234985\n",
      "Epoch:  8 | Step:  85 | Val Loss:  0.676082193851471\n",
      "Epoch:  8 | Step:  86 | Val Loss:  0.7056276798248291\n",
      "Epoch:  8 | Step:  87 | Val Loss:  0.5087336301803589\n",
      "Epoch:  8 | Step:  88 | Val Loss:  0.3573518991470337\n",
      "Epoch:  8 | Step:  89 | Val Loss:  0.896526575088501\n",
      "Epoch:  8 | Step:  90 | Val Loss:  0.6202249526977539\n",
      "Epoch:  8 | Step:  91 | Val Loss:  0.43442463874816895\n",
      "Epoch:  8 | Step:  92 | Val Loss:  0.48972272872924805\n",
      "Epoch:  8 | Step:  93 | Val Loss:  0.717941403388977\n",
      "Epoch:  8 | Step:  94 | Val Loss:  0.5355852246284485\n",
      "Epoch:  8 | Step:  95 | Val Loss:  0.4937622547149658\n",
      "Epoch:  8 | Step:  96 | Val Loss:  0.6666337251663208\n",
      "Epoch:  8 | Step:  97 | Val Loss:  0.5603477954864502\n",
      "Epoch:  8 | Step:  98 | Val Loss:  0.5664150714874268\n",
      "Epoch:  8 | Step:  99 | Val Loss:  0.6070181727409363\n",
      "Epoch:  8 | Step:  100 | Val Loss:  0.6979823112487793\n",
      "Epoch:  8 | Step:  101 | Val Loss:  0.546723484992981\n",
      "Epoch:  8 | Step:  102 | Val Loss:  0.6709473729133606\n",
      "Epoch:  8 | Step:  103 | Val Loss:  0.6199151277542114\n",
      "Epoch:  8 | Step:  104 | Val Loss:  0.538415789604187\n",
      "Epoch:  8 | Step:  105 | Val Loss:  0.7821081876754761\n",
      "Epoch:  8 | Step:  106 | Val Loss:  0.573104739189148\n",
      "Epoch:  8 | Step:  107 | Val Loss:  0.709707498550415\n",
      "Epoch:  8 | Step:  108 | Val Loss:  0.5321271419525146\n",
      "Epoch:  8 | Step:  109 | Val Loss:  0.4162675142288208\n",
      "Epoch:  8 | Step:  110 | Val Loss:  0.47815805673599243\n",
      "Epoch:  8 | Step:  111 | Val Loss:  0.5262957215309143\n",
      "Epoch:  8 | Step:  112 | Val Loss:  0.7081973552703857\n",
      "Epoch:  8 | Step:  113 | Val Loss:  0.77000892162323\n",
      "Epoch:  8 | Step:  114 | Val Loss:  0.6438297033309937\n",
      "Epoch:  8 | Step:  115 | Val Loss:  0.6997269988059998\n",
      "Epoch:  8 | Step:  116 | Val Loss:  0.7255662679672241\n",
      "Epoch:  8 | Step:  117 | Val Loss:  0.7250581979751587\n",
      "Epoch:  8 | Step:  118 | Val Loss:  0.39188897609710693\n",
      "Epoch:  8 | Step:  119 | Val Loss:  0.5572457909584045\n",
      "Epoch:  8 | Step:  120 | Val Loss:  0.8417392373085022\n",
      "Epoch:  8 | Step:  121 | Val Loss:  0.307567834854126\n",
      "Epoch:  8 | Step:  122 | Val Loss:  0.8399642705917358\n",
      "Epoch:  8 | Step:  123 | Val Loss:  0.642092764377594\n",
      "Epoch:  8 | Step:  124 | Val Loss:  0.8544276356697083\n",
      "Epoch:  8 | Step:  125 | Val Loss:  0.837088942527771\n",
      "Epoch:  8 | Train Loss:  tensor(0.5777, device='cuda:0') | Val Loss:  tensor(0.6212, device='cuda:0')\n",
      "Epoch:  9 | Step:  500 | Train Loss:  0.5244203805923462\n",
      "Epoch:  9 | Step:  1 | Val Loss:  0.6113584637641907\n",
      "Epoch:  9 | Step:  2 | Val Loss:  0.46860915422439575\n",
      "Epoch:  9 | Step:  3 | Val Loss:  0.6475542187690735\n",
      "Epoch:  9 | Step:  4 | Val Loss:  0.8646246194839478\n",
      "Epoch:  9 | Step:  5 | Val Loss:  0.6939104795455933\n",
      "Epoch:  9 | Step:  6 | Val Loss:  0.6627503633499146\n",
      "Epoch:  9 | Step:  7 | Val Loss:  0.6316732168197632\n",
      "Epoch:  9 | Step:  8 | Val Loss:  0.6242592930793762\n",
      "Epoch:  9 | Step:  9 | Val Loss:  0.5803807377815247\n",
      "Epoch:  9 | Step:  10 | Val Loss:  0.610339343547821\n",
      "Epoch:  9 | Step:  11 | Val Loss:  0.612488865852356\n",
      "Epoch:  9 | Step:  12 | Val Loss:  0.35469067096710205\n",
      "Epoch:  9 | Step:  13 | Val Loss:  0.5950773358345032\n",
      "Epoch:  9 | Step:  14 | Val Loss:  0.7253236174583435\n",
      "Epoch:  9 | Step:  15 | Val Loss:  0.43441951274871826\n",
      "Epoch:  9 | Step:  16 | Val Loss:  0.4994407892227173\n",
      "Epoch:  9 | Step:  17 | Val Loss:  0.49128690361976624\n",
      "Epoch:  9 | Step:  18 | Val Loss:  0.6715953350067139\n",
      "Epoch:  9 | Step:  19 | Val Loss:  0.8611262440681458\n",
      "Epoch:  9 | Step:  20 | Val Loss:  0.6642059683799744\n",
      "Epoch:  9 | Step:  21 | Val Loss:  0.3937975764274597\n",
      "Epoch:  9 | Step:  22 | Val Loss:  0.546073853969574\n",
      "Epoch:  9 | Step:  23 | Val Loss:  0.6086797714233398\n",
      "Epoch:  9 | Step:  24 | Val Loss:  0.5789468884468079\n",
      "Epoch:  9 | Step:  25 | Val Loss:  0.5639784932136536\n",
      "Epoch:  9 | Step:  26 | Val Loss:  0.5667076706886292\n",
      "Epoch:  9 | Step:  27 | Val Loss:  0.6421589255332947\n",
      "Epoch:  9 | Step:  28 | Val Loss:  0.5560381412506104\n",
      "Epoch:  9 | Step:  29 | Val Loss:  0.5634574890136719\n",
      "Epoch:  9 | Step:  30 | Val Loss:  0.5074707269668579\n",
      "Epoch:  9 | Step:  31 | Val Loss:  0.6087328195571899\n",
      "Epoch:  9 | Step:  32 | Val Loss:  0.679085910320282\n",
      "Epoch:  9 | Step:  33 | Val Loss:  0.6297297477722168\n",
      "Epoch:  9 | Step:  34 | Val Loss:  0.568587601184845\n",
      "Epoch:  9 | Step:  35 | Val Loss:  0.6075044870376587\n",
      "Epoch:  9 | Step:  36 | Val Loss:  0.29561516642570496\n",
      "Epoch:  9 | Step:  37 | Val Loss:  0.6063346862792969\n",
      "Epoch:  9 | Step:  38 | Val Loss:  0.6859956979751587\n",
      "Epoch:  9 | Step:  39 | Val Loss:  0.4043562412261963\n",
      "Epoch:  9 | Step:  40 | Val Loss:  0.9926880598068237\n",
      "Epoch:  9 | Step:  41 | Val Loss:  0.6341723799705505\n",
      "Epoch:  9 | Step:  42 | Val Loss:  0.5591426491737366\n",
      "Epoch:  9 | Step:  43 | Val Loss:  0.6933168172836304\n",
      "Epoch:  9 | Step:  44 | Val Loss:  0.5709405541419983\n",
      "Epoch:  9 | Step:  45 | Val Loss:  0.5563936233520508\n",
      "Epoch:  9 | Step:  46 | Val Loss:  0.708685040473938\n",
      "Epoch:  9 | Step:  47 | Val Loss:  0.6494191288948059\n",
      "Epoch:  9 | Step:  48 | Val Loss:  0.7830604314804077\n",
      "Epoch:  9 | Step:  49 | Val Loss:  0.86365807056427\n",
      "Epoch:  9 | Step:  50 | Val Loss:  0.5530880689620972\n",
      "Epoch:  9 | Step:  51 | Val Loss:  0.3927098512649536\n",
      "Epoch:  9 | Step:  52 | Val Loss:  0.754502534866333\n",
      "Epoch:  9 | Step:  53 | Val Loss:  0.6442149877548218\n",
      "Epoch:  9 | Step:  54 | Val Loss:  0.6523429155349731\n",
      "Epoch:  9 | Step:  55 | Val Loss:  0.5774078369140625\n",
      "Epoch:  9 | Step:  56 | Val Loss:  0.6218541264533997\n",
      "Epoch:  9 | Step:  57 | Val Loss:  0.547184944152832\n",
      "Epoch:  9 | Step:  58 | Val Loss:  0.49761199951171875\n",
      "Epoch:  9 | Step:  59 | Val Loss:  0.8075014352798462\n",
      "Epoch:  9 | Step:  60 | Val Loss:  0.4161868691444397\n",
      "Epoch:  9 | Step:  61 | Val Loss:  0.6517783403396606\n",
      "Epoch:  9 | Step:  62 | Val Loss:  0.6674965023994446\n",
      "Epoch:  9 | Step:  63 | Val Loss:  0.5256075263023376\n",
      "Epoch:  9 | Step:  64 | Val Loss:  0.6862735152244568\n",
      "Epoch:  9 | Step:  65 | Val Loss:  0.5867787599563599\n",
      "Epoch:  9 | Step:  66 | Val Loss:  0.4809700846672058\n",
      "Epoch:  9 | Step:  67 | Val Loss:  0.40134644508361816\n",
      "Epoch:  9 | Step:  68 | Val Loss:  0.5935450792312622\n",
      "Epoch:  9 | Step:  69 | Val Loss:  0.6295678615570068\n",
      "Epoch:  9 | Step:  70 | Val Loss:  0.39646875858306885\n",
      "Epoch:  9 | Step:  71 | Val Loss:  0.8181987404823303\n",
      "Epoch:  9 | Step:  72 | Val Loss:  0.5991058349609375\n",
      "Epoch:  9 | Step:  73 | Val Loss:  0.658795952796936\n",
      "Epoch:  9 | Step:  74 | Val Loss:  0.6928623914718628\n",
      "Epoch:  9 | Step:  75 | Val Loss:  0.40197205543518066\n",
      "Epoch:  9 | Step:  76 | Val Loss:  0.7159532308578491\n",
      "Epoch:  9 | Step:  77 | Val Loss:  0.9161760807037354\n",
      "Epoch:  9 | Step:  78 | Val Loss:  0.4482671022415161\n",
      "Epoch:  9 | Step:  79 | Val Loss:  0.5247784852981567\n",
      "Epoch:  9 | Step:  80 | Val Loss:  0.5172946453094482\n",
      "Epoch:  9 | Step:  81 | Val Loss:  0.5899317264556885\n",
      "Epoch:  9 | Step:  82 | Val Loss:  0.5881232023239136\n",
      "Epoch:  9 | Step:  83 | Val Loss:  0.658614993095398\n",
      "Epoch:  9 | Step:  84 | Val Loss:  0.6423409581184387\n",
      "Epoch:  9 | Step:  85 | Val Loss:  0.40854209661483765\n",
      "Epoch:  9 | Step:  86 | Val Loss:  0.6881756782531738\n",
      "Epoch:  9 | Step:  87 | Val Loss:  0.551199197769165\n",
      "Epoch:  9 | Step:  88 | Val Loss:  0.6654139757156372\n",
      "Epoch:  9 | Step:  89 | Val Loss:  0.5533157587051392\n",
      "Epoch:  9 | Step:  90 | Val Loss:  0.5892304182052612\n",
      "Epoch:  9 | Step:  91 | Val Loss:  0.8921748995780945\n",
      "Epoch:  9 | Step:  92 | Val Loss:  0.7647165060043335\n",
      "Epoch:  9 | Step:  93 | Val Loss:  0.6008408069610596\n",
      "Epoch:  9 | Step:  94 | Val Loss:  0.7645841836929321\n",
      "Epoch:  9 | Step:  95 | Val Loss:  0.2997448742389679\n",
      "Epoch:  9 | Step:  96 | Val Loss:  0.666509211063385\n",
      "Epoch:  9 | Step:  97 | Val Loss:  0.5130404233932495\n",
      "Epoch:  9 | Step:  98 | Val Loss:  0.5835639834403992\n",
      "Epoch:  9 | Step:  99 | Val Loss:  0.5713974237442017\n",
      "Epoch:  9 | Step:  100 | Val Loss:  0.639182448387146\n",
      "Epoch:  9 | Step:  101 | Val Loss:  0.6085538864135742\n",
      "Epoch:  9 | Step:  102 | Val Loss:  0.5712472200393677\n",
      "Epoch:  9 | Step:  103 | Val Loss:  0.7086153030395508\n",
      "Epoch:  9 | Step:  104 | Val Loss:  0.6388004422187805\n",
      "Epoch:  9 | Step:  105 | Val Loss:  0.6232627630233765\n",
      "Epoch:  9 | Step:  106 | Val Loss:  0.5767005085945129\n",
      "Epoch:  9 | Step:  107 | Val Loss:  0.5122441053390503\n",
      "Epoch:  9 | Step:  108 | Val Loss:  0.5373764634132385\n",
      "Epoch:  9 | Step:  109 | Val Loss:  0.8645598292350769\n",
      "Epoch:  9 | Step:  110 | Val Loss:  0.7520416975021362\n",
      "Epoch:  9 | Step:  111 | Val Loss:  0.8698758482933044\n",
      "Epoch:  9 | Step:  112 | Val Loss:  0.5913393497467041\n",
      "Epoch:  9 | Step:  113 | Val Loss:  0.782768964767456\n",
      "Epoch:  9 | Step:  114 | Val Loss:  0.7568502426147461\n",
      "Epoch:  9 | Step:  115 | Val Loss:  0.616473913192749\n",
      "Epoch:  9 | Step:  116 | Val Loss:  0.7945283651351929\n",
      "Epoch:  9 | Step:  117 | Val Loss:  0.6820341348648071\n",
      "Epoch:  9 | Step:  118 | Val Loss:  0.6972575783729553\n",
      "Epoch:  9 | Step:  119 | Val Loss:  0.47477269172668457\n",
      "Epoch:  9 | Step:  120 | Val Loss:  0.8301100730895996\n",
      "Epoch:  9 | Step:  121 | Val Loss:  0.6801641583442688\n",
      "Epoch:  9 | Step:  122 | Val Loss:  0.6764221787452698\n",
      "Epoch:  9 | Step:  123 | Val Loss:  0.49054428935050964\n",
      "Epoch:  9 | Step:  124 | Val Loss:  0.7067652940750122\n",
      "Epoch:  9 | Step:  125 | Val Loss:  0.5139249563217163\n",
      "Epoch:  9 | Train Loss:  tensor(0.5731, device='cuda:0') | Val Loss:  tensor(0.6168, device='cuda:0')\n",
      "Epoch:  10 | Step:  500 | Train Loss:  0.5894757509231567\n",
      "Epoch:  10 | Step:  1 | Val Loss:  0.5343786478042603\n",
      "Epoch:  10 | Step:  2 | Val Loss:  0.47982460260391235\n",
      "Epoch:  10 | Step:  3 | Val Loss:  0.6257716417312622\n",
      "Epoch:  10 | Step:  4 | Val Loss:  0.5947473049163818\n",
      "Epoch:  10 | Step:  5 | Val Loss:  0.6112427115440369\n",
      "Epoch:  10 | Step:  6 | Val Loss:  0.612099289894104\n",
      "Epoch:  10 | Step:  7 | Val Loss:  0.6571671366691589\n",
      "Epoch:  10 | Step:  8 | Val Loss:  0.6977895498275757\n",
      "Epoch:  10 | Step:  9 | Val Loss:  0.6871815323829651\n",
      "Epoch:  10 | Step:  10 | Val Loss:  0.9617451429367065\n",
      "Epoch:  10 | Step:  11 | Val Loss:  0.6641979217529297\n",
      "Epoch:  10 | Step:  12 | Val Loss:  0.6310265064239502\n",
      "Epoch:  10 | Step:  13 | Val Loss:  0.5760462284088135\n",
      "Epoch:  10 | Step:  14 | Val Loss:  0.48313722014427185\n",
      "Epoch:  10 | Step:  15 | Val Loss:  0.7118101716041565\n",
      "Epoch:  10 | Step:  16 | Val Loss:  0.6985506415367126\n",
      "Epoch:  10 | Step:  17 | Val Loss:  0.7735822200775146\n",
      "Epoch:  10 | Step:  18 | Val Loss:  0.8572169542312622\n",
      "Epoch:  10 | Step:  19 | Val Loss:  0.837746262550354\n",
      "Epoch:  10 | Step:  20 | Val Loss:  0.7468011975288391\n",
      "Epoch:  10 | Step:  21 | Val Loss:  0.5000629425048828\n",
      "Epoch:  10 | Step:  22 | Val Loss:  0.7026443481445312\n",
      "Epoch:  10 | Step:  23 | Val Loss:  0.4938804507255554\n",
      "Epoch:  10 | Step:  24 | Val Loss:  0.49789297580718994\n",
      "Epoch:  10 | Step:  25 | Val Loss:  0.8266580104827881\n",
      "Epoch:  10 | Step:  26 | Val Loss:  0.5515570640563965\n",
      "Epoch:  10 | Step:  27 | Val Loss:  0.562064528465271\n",
      "Epoch:  10 | Step:  28 | Val Loss:  0.4317101836204529\n",
      "Epoch:  10 | Step:  29 | Val Loss:  0.4008897840976715\n",
      "Epoch:  10 | Step:  30 | Val Loss:  0.5934303402900696\n",
      "Epoch:  10 | Step:  31 | Val Loss:  0.5635896325111389\n",
      "Epoch:  10 | Step:  32 | Val Loss:  0.6068953275680542\n",
      "Epoch:  10 | Step:  33 | Val Loss:  0.6239412426948547\n",
      "Epoch:  10 | Step:  34 | Val Loss:  0.7342008352279663\n",
      "Epoch:  10 | Step:  35 | Val Loss:  0.4092645049095154\n",
      "Epoch:  10 | Step:  36 | Val Loss:  0.6199640035629272\n",
      "Epoch:  10 | Step:  37 | Val Loss:  0.46417665481567383\n",
      "Epoch:  10 | Step:  38 | Val Loss:  0.5763179659843445\n",
      "Epoch:  10 | Step:  39 | Val Loss:  0.5658057928085327\n",
      "Epoch:  10 | Step:  40 | Val Loss:  0.5399808883666992\n",
      "Epoch:  10 | Step:  41 | Val Loss:  0.5828045606613159\n",
      "Epoch:  10 | Step:  42 | Val Loss:  0.9646346569061279\n",
      "Epoch:  10 | Step:  43 | Val Loss:  0.5795819759368896\n",
      "Epoch:  10 | Step:  44 | Val Loss:  0.7748397588729858\n",
      "Epoch:  10 | Step:  45 | Val Loss:  0.7829618453979492\n",
      "Epoch:  10 | Step:  46 | Val Loss:  0.512252926826477\n",
      "Epoch:  10 | Step:  47 | Val Loss:  0.48613834381103516\n",
      "Epoch:  10 | Step:  48 | Val Loss:  0.5779361724853516\n",
      "Epoch:  10 | Step:  49 | Val Loss:  0.5471614599227905\n",
      "Epoch:  10 | Step:  50 | Val Loss:  0.8711552619934082\n",
      "Epoch:  10 | Step:  51 | Val Loss:  0.48931923508644104\n",
      "Epoch:  10 | Step:  52 | Val Loss:  0.6365566253662109\n",
      "Epoch:  10 | Step:  53 | Val Loss:  0.6238608956336975\n",
      "Epoch:  10 | Step:  54 | Val Loss:  0.6272252202033997\n",
      "Epoch:  10 | Step:  55 | Val Loss:  0.7013881206512451\n",
      "Epoch:  10 | Step:  56 | Val Loss:  0.7268351316452026\n",
      "Epoch:  10 | Step:  57 | Val Loss:  0.7047572731971741\n",
      "Epoch:  10 | Step:  58 | Val Loss:  0.7557355165481567\n",
      "Epoch:  10 | Step:  59 | Val Loss:  0.5535144209861755\n",
      "Epoch:  10 | Step:  60 | Val Loss:  0.6685454845428467\n",
      "Epoch:  10 | Step:  61 | Val Loss:  0.5618163347244263\n",
      "Epoch:  10 | Step:  62 | Val Loss:  0.6117109060287476\n",
      "Epoch:  10 | Step:  63 | Val Loss:  0.6810814142227173\n",
      "Epoch:  10 | Step:  64 | Val Loss:  0.6052348613739014\n",
      "Epoch:  10 | Step:  65 | Val Loss:  0.6578631401062012\n",
      "Epoch:  10 | Step:  66 | Val Loss:  0.4574376046657562\n",
      "Epoch:  10 | Step:  67 | Val Loss:  0.504101037979126\n",
      "Epoch:  10 | Step:  68 | Val Loss:  0.5913779735565186\n",
      "Epoch:  10 | Step:  69 | Val Loss:  0.38043415546417236\n",
      "Epoch:  10 | Step:  70 | Val Loss:  0.5576305389404297\n",
      "Epoch:  10 | Step:  71 | Val Loss:  0.3649026155471802\n",
      "Epoch:  10 | Step:  72 | Val Loss:  0.63889479637146\n",
      "Epoch:  10 | Step:  73 | Val Loss:  0.6388693451881409\n",
      "Epoch:  10 | Step:  74 | Val Loss:  0.3545878827571869\n",
      "Epoch:  10 | Step:  75 | Val Loss:  0.44933396577835083\n",
      "Epoch:  10 | Step:  76 | Val Loss:  0.7274601459503174\n",
      "Epoch:  10 | Step:  77 | Val Loss:  0.5162742137908936\n",
      "Epoch:  10 | Step:  78 | Val Loss:  0.5418499708175659\n",
      "Epoch:  10 | Step:  79 | Val Loss:  0.5933026075363159\n",
      "Epoch:  10 | Step:  80 | Val Loss:  0.6203123331069946\n",
      "Epoch:  10 | Step:  81 | Val Loss:  0.5453636050224304\n",
      "Epoch:  10 | Step:  82 | Val Loss:  0.4864954650402069\n",
      "Epoch:  10 | Step:  83 | Val Loss:  0.5860990285873413\n",
      "Epoch:  10 | Step:  84 | Val Loss:  0.720455527305603\n",
      "Epoch:  10 | Step:  85 | Val Loss:  0.5365129709243774\n",
      "Epoch:  10 | Step:  86 | Val Loss:  0.3861822187900543\n",
      "Epoch:  10 | Step:  87 | Val Loss:  0.6642808318138123\n",
      "Epoch:  10 | Step:  88 | Val Loss:  0.5240582823753357\n",
      "Epoch:  10 | Step:  89 | Val Loss:  0.7610740661621094\n",
      "Epoch:  10 | Step:  90 | Val Loss:  0.6220799684524536\n",
      "Epoch:  10 | Step:  91 | Val Loss:  0.5191417932510376\n",
      "Epoch:  10 | Step:  92 | Val Loss:  0.4941539168357849\n",
      "Epoch:  10 | Step:  93 | Val Loss:  0.6688294410705566\n",
      "Epoch:  10 | Step:  94 | Val Loss:  0.6255769729614258\n",
      "Epoch:  10 | Step:  95 | Val Loss:  0.6481713056564331\n",
      "Epoch:  10 | Step:  96 | Val Loss:  0.6170903444290161\n",
      "Epoch:  10 | Step:  97 | Val Loss:  0.643317699432373\n",
      "Epoch:  10 | Step:  98 | Val Loss:  0.7006477117538452\n",
      "Epoch:  10 | Step:  99 | Val Loss:  0.5881757736206055\n",
      "Epoch:  10 | Step:  100 | Val Loss:  0.3189540505409241\n",
      "Epoch:  10 | Step:  101 | Val Loss:  0.5266350507736206\n",
      "Epoch:  10 | Step:  102 | Val Loss:  0.5912909507751465\n",
      "Epoch:  10 | Step:  103 | Val Loss:  0.5295453667640686\n",
      "Epoch:  10 | Step:  104 | Val Loss:  0.8401893377304077\n",
      "Epoch:  10 | Step:  105 | Val Loss:  0.7532931566238403\n",
      "Epoch:  10 | Step:  106 | Val Loss:  0.5507076978683472\n",
      "Epoch:  10 | Step:  107 | Val Loss:  0.6513323783874512\n",
      "Epoch:  10 | Step:  108 | Val Loss:  0.7781468629837036\n",
      "Epoch:  10 | Step:  109 | Val Loss:  0.8022350072860718\n",
      "Epoch:  10 | Step:  110 | Val Loss:  0.4064571261405945\n",
      "Epoch:  10 | Step:  111 | Val Loss:  0.5697447657585144\n",
      "Epoch:  10 | Step:  112 | Val Loss:  0.531470000743866\n",
      "Epoch:  10 | Step:  113 | Val Loss:  0.6540694832801819\n",
      "Epoch:  10 | Step:  114 | Val Loss:  0.7376039028167725\n",
      "Epoch:  10 | Step:  115 | Val Loss:  0.3611987233161926\n",
      "Epoch:  10 | Step:  116 | Val Loss:  0.6133611798286438\n",
      "Epoch:  10 | Step:  117 | Val Loss:  0.9244089722633362\n",
      "Epoch:  10 | Step:  118 | Val Loss:  0.6876400113105774\n",
      "Epoch:  10 | Step:  119 | Val Loss:  0.7390683889389038\n",
      "Epoch:  10 | Step:  120 | Val Loss:  0.5003114342689514\n",
      "Epoch:  10 | Step:  121 | Val Loss:  0.5703045129776001\n",
      "Epoch:  10 | Step:  122 | Val Loss:  0.6978576183319092\n",
      "Epoch:  10 | Step:  123 | Val Loss:  0.834791898727417\n",
      "Epoch:  10 | Step:  124 | Val Loss:  0.48666539788246155\n",
      "Epoch:  10 | Step:  125 | Val Loss:  0.615768313407898\n",
      "Epoch:  10 | Train Loss:  tensor(0.5685, device='cuda:0') | Val Loss:  tensor(0.6123, device='cuda:0')\n",
      "Epoch:  11 | Step:  500 | Train Loss:  0.5322076678276062\n",
      "Epoch:  11 | Step:  1 | Val Loss:  0.7755832672119141\n",
      "Epoch:  11 | Step:  2 | Val Loss:  0.952620267868042\n",
      "Epoch:  11 | Step:  3 | Val Loss:  0.4738185703754425\n",
      "Epoch:  11 | Step:  4 | Val Loss:  0.42543718218803406\n",
      "Epoch:  11 | Step:  5 | Val Loss:  0.6776860356330872\n",
      "Epoch:  11 | Step:  6 | Val Loss:  0.5975438356399536\n",
      "Epoch:  11 | Step:  7 | Val Loss:  0.7841740846633911\n",
      "Epoch:  11 | Step:  8 | Val Loss:  0.5140222311019897\n",
      "Epoch:  11 | Step:  9 | Val Loss:  0.5570581555366516\n",
      "Epoch:  11 | Step:  10 | Val Loss:  0.7665790319442749\n",
      "Epoch:  11 | Step:  11 | Val Loss:  0.5532667636871338\n",
      "Epoch:  11 | Step:  12 | Val Loss:  0.7022331953048706\n",
      "Epoch:  11 | Step:  13 | Val Loss:  0.433409720659256\n",
      "Epoch:  11 | Step:  14 | Val Loss:  0.6480348706245422\n",
      "Epoch:  11 | Step:  15 | Val Loss:  0.7067969441413879\n",
      "Epoch:  11 | Step:  16 | Val Loss:  0.6516531109809875\n",
      "Epoch:  11 | Step:  17 | Val Loss:  0.68520587682724\n",
      "Epoch:  11 | Step:  18 | Val Loss:  0.7424847483634949\n",
      "Epoch:  11 | Step:  19 | Val Loss:  0.640046238899231\n",
      "Epoch:  11 | Step:  20 | Val Loss:  0.7084364891052246\n",
      "Epoch:  11 | Step:  21 | Val Loss:  0.6019027233123779\n",
      "Epoch:  11 | Step:  22 | Val Loss:  0.5186071991920471\n",
      "Epoch:  11 | Step:  23 | Val Loss:  0.5424357056617737\n",
      "Epoch:  11 | Step:  24 | Val Loss:  0.8873340487480164\n",
      "Epoch:  11 | Step:  25 | Val Loss:  0.48775017261505127\n",
      "Epoch:  11 | Step:  26 | Val Loss:  0.5593264698982239\n",
      "Epoch:  11 | Step:  27 | Val Loss:  0.7842644453048706\n",
      "Epoch:  11 | Step:  28 | Val Loss:  0.5058865547180176\n",
      "Epoch:  11 | Step:  29 | Val Loss:  0.7764492034912109\n",
      "Epoch:  11 | Step:  30 | Val Loss:  0.45886436104774475\n",
      "Epoch:  11 | Step:  31 | Val Loss:  0.5858699083328247\n",
      "Epoch:  11 | Step:  32 | Val Loss:  0.6188995838165283\n",
      "Epoch:  11 | Step:  33 | Val Loss:  0.695264458656311\n",
      "Epoch:  11 | Step:  34 | Val Loss:  0.7752836346626282\n",
      "Epoch:  11 | Step:  35 | Val Loss:  0.47359198331832886\n",
      "Epoch:  11 | Step:  36 | Val Loss:  0.5676206350326538\n",
      "Epoch:  11 | Step:  37 | Val Loss:  0.5551312565803528\n",
      "Epoch:  11 | Step:  38 | Val Loss:  0.5849583148956299\n",
      "Epoch:  11 | Step:  39 | Val Loss:  0.6344448328018188\n",
      "Epoch:  11 | Step:  40 | Val Loss:  0.46254605054855347\n",
      "Epoch:  11 | Step:  41 | Val Loss:  0.5722344517707825\n",
      "Epoch:  11 | Step:  42 | Val Loss:  0.38550710678100586\n",
      "Epoch:  11 | Step:  43 | Val Loss:  0.38164377212524414\n",
      "Epoch:  11 | Step:  44 | Val Loss:  0.4039451777935028\n",
      "Epoch:  11 | Step:  45 | Val Loss:  0.6384222507476807\n",
      "Epoch:  11 | Step:  46 | Val Loss:  0.47188514471054077\n",
      "Epoch:  11 | Step:  47 | Val Loss:  0.42561742663383484\n",
      "Epoch:  11 | Step:  48 | Val Loss:  0.6492955088615417\n",
      "Epoch:  11 | Step:  49 | Val Loss:  0.4630495309829712\n",
      "Epoch:  11 | Step:  50 | Val Loss:  0.7237679362297058\n",
      "Epoch:  11 | Step:  51 | Val Loss:  0.5267928242683411\n",
      "Epoch:  11 | Step:  52 | Val Loss:  0.6555725336074829\n",
      "Epoch:  11 | Step:  53 | Val Loss:  0.4438611567020416\n",
      "Epoch:  11 | Step:  54 | Val Loss:  0.6554688215255737\n",
      "Epoch:  11 | Step:  55 | Val Loss:  0.565075159072876\n",
      "Epoch:  11 | Step:  56 | Val Loss:  0.6035581827163696\n",
      "Epoch:  11 | Step:  57 | Val Loss:  0.6010441184043884\n",
      "Epoch:  11 | Step:  58 | Val Loss:  0.5941135287284851\n",
      "Epoch:  11 | Step:  59 | Val Loss:  0.4921470880508423\n",
      "Epoch:  11 | Step:  60 | Val Loss:  0.6925028562545776\n",
      "Epoch:  11 | Step:  61 | Val Loss:  0.659471869468689\n",
      "Epoch:  11 | Step:  62 | Val Loss:  0.5976594686508179\n",
      "Epoch:  11 | Step:  63 | Val Loss:  0.6152418851852417\n",
      "Epoch:  11 | Step:  64 | Val Loss:  0.7228482961654663\n",
      "Epoch:  11 | Step:  65 | Val Loss:  0.6555945873260498\n",
      "Epoch:  11 | Step:  66 | Val Loss:  0.4421018064022064\n",
      "Epoch:  11 | Step:  67 | Val Loss:  0.6359544396400452\n",
      "Epoch:  11 | Step:  68 | Val Loss:  0.6023310422897339\n",
      "Epoch:  11 | Step:  69 | Val Loss:  0.6448253393173218\n",
      "Epoch:  11 | Step:  70 | Val Loss:  0.7633564472198486\n",
      "Epoch:  11 | Step:  71 | Val Loss:  0.6497211456298828\n",
      "Epoch:  11 | Step:  72 | Val Loss:  0.6285903453826904\n",
      "Epoch:  11 | Step:  73 | Val Loss:  0.5772330164909363\n",
      "Epoch:  11 | Step:  74 | Val Loss:  0.5262782573699951\n",
      "Epoch:  11 | Step:  75 | Val Loss:  0.4722643792629242\n",
      "Epoch:  11 | Step:  76 | Val Loss:  0.6087193489074707\n",
      "Epoch:  11 | Step:  77 | Val Loss:  0.6215199828147888\n",
      "Epoch:  11 | Step:  78 | Val Loss:  0.4377579689025879\n",
      "Epoch:  11 | Step:  79 | Val Loss:  0.6079269647598267\n",
      "Epoch:  11 | Step:  80 | Val Loss:  0.6027100086212158\n",
      "Epoch:  11 | Step:  81 | Val Loss:  0.5625066161155701\n",
      "Epoch:  11 | Step:  82 | Val Loss:  0.5880351066589355\n",
      "Epoch:  11 | Step:  83 | Val Loss:  0.6631696820259094\n",
      "Epoch:  11 | Step:  84 | Val Loss:  0.5551401376724243\n",
      "Epoch:  11 | Step:  85 | Val Loss:  0.7607842087745667\n",
      "Epoch:  11 | Step:  86 | Val Loss:  0.7013508677482605\n",
      "Epoch:  11 | Step:  87 | Val Loss:  0.30271032452583313\n",
      "Epoch:  11 | Step:  88 | Val Loss:  0.6116921901702881\n",
      "Epoch:  11 | Step:  89 | Val Loss:  0.6929827928543091\n",
      "Epoch:  11 | Step:  90 | Val Loss:  0.641486644744873\n",
      "Epoch:  11 | Step:  91 | Val Loss:  0.4415368437767029\n",
      "Epoch:  11 | Step:  92 | Val Loss:  0.5826084613800049\n",
      "Epoch:  11 | Step:  93 | Val Loss:  0.674042284488678\n",
      "Epoch:  11 | Step:  94 | Val Loss:  0.7819312810897827\n",
      "Epoch:  11 | Step:  95 | Val Loss:  0.26405322551727295\n",
      "Epoch:  11 | Step:  96 | Val Loss:  0.882172703742981\n",
      "Epoch:  11 | Step:  97 | Val Loss:  0.6491550803184509\n",
      "Epoch:  11 | Step:  98 | Val Loss:  0.396493136882782\n",
      "Epoch:  11 | Step:  99 | Val Loss:  0.8167562484741211\n",
      "Epoch:  11 | Step:  100 | Val Loss:  0.5906437635421753\n",
      "Epoch:  11 | Step:  101 | Val Loss:  0.6929439306259155\n",
      "Epoch:  11 | Step:  102 | Val Loss:  0.47312939167022705\n",
      "Epoch:  11 | Step:  103 | Val Loss:  0.579034686088562\n",
      "Epoch:  11 | Step:  104 | Val Loss:  0.2888784408569336\n",
      "Epoch:  11 | Step:  105 | Val Loss:  0.5415133833885193\n",
      "Epoch:  11 | Step:  106 | Val Loss:  0.49027347564697266\n",
      "Epoch:  11 | Step:  107 | Val Loss:  0.6194593906402588\n",
      "Epoch:  11 | Step:  108 | Val Loss:  0.49751904606819153\n",
      "Epoch:  11 | Step:  109 | Val Loss:  0.48682838678359985\n",
      "Epoch:  11 | Step:  110 | Val Loss:  0.664422869682312\n",
      "Epoch:  11 | Step:  111 | Val Loss:  0.6604361534118652\n",
      "Epoch:  11 | Step:  112 | Val Loss:  0.883703351020813\n",
      "Epoch:  11 | Step:  113 | Val Loss:  0.6867703199386597\n",
      "Epoch:  11 | Step:  114 | Val Loss:  0.808664858341217\n",
      "Epoch:  11 | Step:  115 | Val Loss:  0.7328907251358032\n",
      "Epoch:  11 | Step:  116 | Val Loss:  0.40761613845825195\n",
      "Epoch:  11 | Step:  117 | Val Loss:  0.6301311254501343\n",
      "Epoch:  11 | Step:  118 | Val Loss:  0.6974065899848938\n",
      "Epoch:  11 | Step:  119 | Val Loss:  0.7213531732559204\n",
      "Epoch:  11 | Step:  120 | Val Loss:  0.7630431652069092\n",
      "Epoch:  11 | Step:  121 | Val Loss:  0.5178080797195435\n",
      "Epoch:  11 | Step:  122 | Val Loss:  0.7612721920013428\n",
      "Epoch:  11 | Step:  123 | Val Loss:  0.414897620677948\n",
      "Epoch:  11 | Step:  124 | Val Loss:  0.854129433631897\n",
      "Epoch:  11 | Step:  125 | Val Loss:  0.6936827898025513\n",
      "Epoch:  11 | Train Loss:  tensor(0.5651, device='cuda:0') | Val Loss:  tensor(0.6065, device='cuda:0')\n",
      "Epoch:  12 | Step:  500 | Train Loss:  0.5310754179954529\n",
      "Epoch:  12 | Step:  1 | Val Loss:  0.6055254936218262\n",
      "Epoch:  12 | Step:  2 | Val Loss:  0.6726424694061279\n",
      "Epoch:  12 | Step:  3 | Val Loss:  0.5515896081924438\n",
      "Epoch:  12 | Step:  4 | Val Loss:  0.6655498743057251\n",
      "Epoch:  12 | Step:  5 | Val Loss:  0.5474564433097839\n",
      "Epoch:  12 | Step:  6 | Val Loss:  0.6714295148849487\n",
      "Epoch:  12 | Step:  7 | Val Loss:  0.682689905166626\n",
      "Epoch:  12 | Step:  8 | Val Loss:  0.4301782250404358\n",
      "Epoch:  12 | Step:  9 | Val Loss:  0.6017163991928101\n",
      "Epoch:  12 | Step:  10 | Val Loss:  0.521345317363739\n",
      "Epoch:  12 | Step:  11 | Val Loss:  0.5778599381446838\n",
      "Epoch:  12 | Step:  12 | Val Loss:  0.49358198046684265\n",
      "Epoch:  12 | Step:  13 | Val Loss:  0.74488365650177\n",
      "Epoch:  12 | Step:  14 | Val Loss:  0.44693952798843384\n",
      "Epoch:  12 | Step:  15 | Val Loss:  0.6652622222900391\n",
      "Epoch:  12 | Step:  16 | Val Loss:  0.5098041892051697\n",
      "Epoch:  12 | Step:  17 | Val Loss:  0.36434808373451233\n",
      "Epoch:  12 | Step:  18 | Val Loss:  0.4770631492137909\n",
      "Epoch:  12 | Step:  19 | Val Loss:  0.6149204969406128\n",
      "Epoch:  12 | Step:  20 | Val Loss:  0.5962740182876587\n",
      "Epoch:  12 | Step:  21 | Val Loss:  0.7109737396240234\n",
      "Epoch:  12 | Step:  22 | Val Loss:  0.8692386150360107\n",
      "Epoch:  12 | Step:  23 | Val Loss:  0.5356091856956482\n",
      "Epoch:  12 | Step:  24 | Val Loss:  0.5698589086532593\n",
      "Epoch:  12 | Step:  25 | Val Loss:  0.4038695693016052\n",
      "Epoch:  12 | Step:  26 | Val Loss:  0.5910449028015137\n",
      "Epoch:  12 | Step:  27 | Val Loss:  0.5436551570892334\n",
      "Epoch:  12 | Step:  28 | Val Loss:  0.7386497259140015\n",
      "Epoch:  12 | Step:  29 | Val Loss:  0.6834375262260437\n",
      "Epoch:  12 | Step:  30 | Val Loss:  0.45945650339126587\n",
      "Epoch:  12 | Step:  31 | Val Loss:  0.6264667510986328\n",
      "Epoch:  12 | Step:  32 | Val Loss:  0.3792095184326172\n",
      "Epoch:  12 | Step:  33 | Val Loss:  0.42768609523773193\n",
      "Epoch:  12 | Step:  34 | Val Loss:  0.5245527625083923\n",
      "Epoch:  12 | Step:  35 | Val Loss:  0.49371397495269775\n",
      "Epoch:  12 | Step:  36 | Val Loss:  0.6676573157310486\n",
      "Epoch:  12 | Step:  37 | Val Loss:  0.7264906764030457\n",
      "Epoch:  12 | Step:  38 | Val Loss:  0.7107823491096497\n",
      "Epoch:  12 | Step:  39 | Val Loss:  0.6840965747833252\n",
      "Epoch:  12 | Step:  40 | Val Loss:  0.7155618667602539\n",
      "Epoch:  12 | Step:  41 | Val Loss:  0.4860081672668457\n",
      "Epoch:  12 | Step:  42 | Val Loss:  0.45343703031539917\n",
      "Epoch:  12 | Step:  43 | Val Loss:  0.2371538281440735\n",
      "Epoch:  12 | Step:  44 | Val Loss:  1.0176544189453125\n",
      "Epoch:  12 | Step:  45 | Val Loss:  0.8069190382957458\n",
      "Epoch:  12 | Step:  46 | Val Loss:  0.7915330529212952\n",
      "Epoch:  12 | Step:  47 | Val Loss:  0.7675531506538391\n",
      "Epoch:  12 | Step:  48 | Val Loss:  0.6807029843330383\n",
      "Epoch:  12 | Step:  49 | Val Loss:  0.5773372650146484\n",
      "Epoch:  12 | Step:  50 | Val Loss:  0.4808681011199951\n",
      "Epoch:  12 | Step:  51 | Val Loss:  0.6117517948150635\n",
      "Epoch:  12 | Step:  52 | Val Loss:  0.6577585935592651\n",
      "Epoch:  12 | Step:  53 | Val Loss:  0.7594228982925415\n",
      "Epoch:  12 | Step:  54 | Val Loss:  0.6916332244873047\n",
      "Epoch:  12 | Step:  55 | Val Loss:  0.8201169371604919\n",
      "Epoch:  12 | Step:  56 | Val Loss:  0.7755297422409058\n",
      "Epoch:  12 | Step:  57 | Val Loss:  0.5092023611068726\n",
      "Epoch:  12 | Step:  58 | Val Loss:  0.6148453950881958\n",
      "Epoch:  12 | Step:  59 | Val Loss:  0.6160863637924194\n",
      "Epoch:  12 | Step:  60 | Val Loss:  0.5747728943824768\n",
      "Epoch:  12 | Step:  61 | Val Loss:  0.5099987983703613\n",
      "Epoch:  12 | Step:  62 | Val Loss:  0.594457745552063\n",
      "Epoch:  12 | Step:  63 | Val Loss:  0.5805165767669678\n",
      "Epoch:  12 | Step:  64 | Val Loss:  0.7559695243835449\n",
      "Epoch:  12 | Step:  65 | Val Loss:  0.580069363117218\n",
      "Epoch:  12 | Step:  66 | Val Loss:  0.5734362602233887\n",
      "Epoch:  12 | Step:  67 | Val Loss:  0.6732046604156494\n",
      "Epoch:  12 | Step:  68 | Val Loss:  0.5811079144477844\n",
      "Epoch:  12 | Step:  69 | Val Loss:  0.5167949199676514\n",
      "Epoch:  12 | Step:  70 | Val Loss:  0.6982003450393677\n",
      "Epoch:  12 | Step:  71 | Val Loss:  0.5856742262840271\n",
      "Epoch:  12 | Step:  72 | Val Loss:  0.48986053466796875\n",
      "Epoch:  12 | Step:  73 | Val Loss:  0.6877835988998413\n",
      "Epoch:  12 | Step:  74 | Val Loss:  0.47143709659576416\n",
      "Epoch:  12 | Step:  75 | Val Loss:  0.6941943168640137\n",
      "Epoch:  12 | Step:  76 | Val Loss:  0.6968971490859985\n",
      "Epoch:  12 | Step:  77 | Val Loss:  0.7270568609237671\n",
      "Epoch:  12 | Step:  78 | Val Loss:  0.5155705213546753\n",
      "Epoch:  12 | Step:  79 | Val Loss:  0.37063586711883545\n",
      "Epoch:  12 | Step:  80 | Val Loss:  0.5838578343391418\n",
      "Epoch:  12 | Step:  81 | Val Loss:  0.7538138628005981\n",
      "Epoch:  12 | Step:  82 | Val Loss:  0.6976243853569031\n",
      "Epoch:  12 | Step:  83 | Val Loss:  0.553191602230072\n",
      "Epoch:  12 | Step:  84 | Val Loss:  0.3666735291481018\n",
      "Epoch:  12 | Step:  85 | Val Loss:  0.5113040208816528\n",
      "Epoch:  12 | Step:  86 | Val Loss:  0.6724980473518372\n",
      "Epoch:  12 | Step:  87 | Val Loss:  0.5488027334213257\n",
      "Epoch:  12 | Step:  88 | Val Loss:  0.5978259444236755\n",
      "Epoch:  12 | Step:  89 | Val Loss:  0.5776329040527344\n",
      "Epoch:  12 | Step:  90 | Val Loss:  0.6517130732536316\n",
      "Epoch:  12 | Step:  91 | Val Loss:  0.543826162815094\n",
      "Epoch:  12 | Step:  92 | Val Loss:  0.7791669368743896\n",
      "Epoch:  12 | Step:  93 | Val Loss:  0.4383794069290161\n",
      "Epoch:  12 | Step:  94 | Val Loss:  0.8736519813537598\n",
      "Epoch:  12 | Step:  95 | Val Loss:  0.5828340649604797\n",
      "Epoch:  12 | Step:  96 | Val Loss:  0.6914998292922974\n",
      "Epoch:  12 | Step:  97 | Val Loss:  0.699569821357727\n",
      "Epoch:  12 | Step:  98 | Val Loss:  0.22935578227043152\n",
      "Epoch:  12 | Step:  99 | Val Loss:  0.7677401304244995\n",
      "Epoch:  12 | Step:  100 | Val Loss:  0.5618163347244263\n",
      "Epoch:  12 | Step:  101 | Val Loss:  0.5711386203765869\n",
      "Epoch:  12 | Step:  102 | Val Loss:  0.6675273180007935\n",
      "Epoch:  12 | Step:  103 | Val Loss:  0.47285428643226624\n",
      "Epoch:  12 | Step:  104 | Val Loss:  0.28096574544906616\n",
      "Epoch:  12 | Step:  105 | Val Loss:  0.5516347289085388\n",
      "Epoch:  12 | Step:  106 | Val Loss:  0.6177718639373779\n",
      "Epoch:  12 | Step:  107 | Val Loss:  0.4952567219734192\n",
      "Epoch:  12 | Step:  108 | Val Loss:  0.6700742244720459\n",
      "Epoch:  12 | Step:  109 | Val Loss:  0.44961878657341003\n",
      "Epoch:  12 | Step:  110 | Val Loss:  0.4727059006690979\n",
      "Epoch:  12 | Step:  111 | Val Loss:  0.8164286017417908\n",
      "Epoch:  12 | Step:  112 | Val Loss:  0.8377813100814819\n",
      "Epoch:  12 | Step:  113 | Val Loss:  0.6002848744392395\n",
      "Epoch:  12 | Step:  114 | Val Loss:  0.7182576656341553\n",
      "Epoch:  12 | Step:  115 | Val Loss:  0.762814998626709\n",
      "Epoch:  12 | Step:  116 | Val Loss:  0.4771287441253662\n",
      "Epoch:  12 | Step:  117 | Val Loss:  0.6500250697135925\n",
      "Epoch:  12 | Step:  118 | Val Loss:  0.5620461106300354\n",
      "Epoch:  12 | Step:  119 | Val Loss:  0.6371166706085205\n",
      "Epoch:  12 | Step:  120 | Val Loss:  0.6568162441253662\n",
      "Epoch:  12 | Step:  121 | Val Loss:  0.6920245885848999\n",
      "Epoch:  12 | Step:  122 | Val Loss:  0.6061563491821289\n",
      "Epoch:  12 | Step:  123 | Val Loss:  0.7016844749450684\n",
      "Epoch:  12 | Step:  124 | Val Loss:  0.6810188293457031\n",
      "Epoch:  12 | Step:  125 | Val Loss:  0.3435285687446594\n",
      "Epoch:  12 | Train Loss:  tensor(0.5614, device='cuda:0') | Val Loss:  tensor(0.6033, device='cuda:0')\n",
      "Epoch:  13 | Step:  500 | Train Loss:  0.7976566553115845\n",
      "Epoch:  13 | Step:  1 | Val Loss:  0.747643768787384\n",
      "Epoch:  13 | Step:  2 | Val Loss:  0.5279050469398499\n",
      "Epoch:  13 | Step:  3 | Val Loss:  0.5483501553535461\n",
      "Epoch:  13 | Step:  4 | Val Loss:  0.5895929336547852\n",
      "Epoch:  13 | Step:  5 | Val Loss:  0.5938928127288818\n",
      "Epoch:  13 | Step:  6 | Val Loss:  0.6455216407775879\n",
      "Epoch:  13 | Step:  7 | Val Loss:  0.7783606052398682\n",
      "Epoch:  13 | Step:  8 | Val Loss:  0.3560984432697296\n",
      "Epoch:  13 | Step:  9 | Val Loss:  0.4379661977291107\n",
      "Epoch:  13 | Step:  10 | Val Loss:  0.6788508296012878\n",
      "Epoch:  13 | Step:  11 | Val Loss:  0.7257660031318665\n",
      "Epoch:  13 | Step:  12 | Val Loss:  0.48433032631874084\n",
      "Epoch:  13 | Step:  13 | Val Loss:  0.6868398785591125\n",
      "Epoch:  13 | Step:  14 | Val Loss:  0.5005265474319458\n",
      "Epoch:  13 | Step:  15 | Val Loss:  0.5235710740089417\n",
      "Epoch:  13 | Step:  16 | Val Loss:  0.35807791352272034\n",
      "Epoch:  13 | Step:  17 | Val Loss:  0.5932979583740234\n",
      "Epoch:  13 | Step:  18 | Val Loss:  0.5970662832260132\n",
      "Epoch:  13 | Step:  19 | Val Loss:  0.42432719469070435\n",
      "Epoch:  13 | Step:  20 | Val Loss:  0.7620479464530945\n",
      "Epoch:  13 | Step:  21 | Val Loss:  0.5208185911178589\n",
      "Epoch:  13 | Step:  22 | Val Loss:  0.7444469928741455\n",
      "Epoch:  13 | Step:  23 | Val Loss:  0.7335706949234009\n",
      "Epoch:  13 | Step:  24 | Val Loss:  0.5730026960372925\n",
      "Epoch:  13 | Step:  25 | Val Loss:  0.7238991260528564\n",
      "Epoch:  13 | Step:  26 | Val Loss:  0.6264164447784424\n",
      "Epoch:  13 | Step:  27 | Val Loss:  0.8080703020095825\n",
      "Epoch:  13 | Step:  28 | Val Loss:  0.4938141703605652\n",
      "Epoch:  13 | Step:  29 | Val Loss:  0.6850801706314087\n",
      "Epoch:  13 | Step:  30 | Val Loss:  0.5948739051818848\n",
      "Epoch:  13 | Step:  31 | Val Loss:  0.5425446033477783\n",
      "Epoch:  13 | Step:  32 | Val Loss:  0.49282169342041016\n",
      "Epoch:  13 | Step:  33 | Val Loss:  0.4838797450065613\n",
      "Epoch:  13 | Step:  34 | Val Loss:  0.6030288934707642\n",
      "Epoch:  13 | Step:  35 | Val Loss:  0.5999306440353394\n",
      "Epoch:  13 | Step:  36 | Val Loss:  0.7808539867401123\n",
      "Epoch:  13 | Step:  37 | Val Loss:  0.5418176054954529\n",
      "Epoch:  13 | Step:  38 | Val Loss:  0.6076077818870544\n",
      "Epoch:  13 | Step:  39 | Val Loss:  0.5579982399940491\n",
      "Epoch:  13 | Step:  40 | Val Loss:  0.8333433866500854\n",
      "Epoch:  13 | Step:  41 | Val Loss:  0.6664318442344666\n",
      "Epoch:  13 | Step:  42 | Val Loss:  0.5966516733169556\n",
      "Epoch:  13 | Step:  43 | Val Loss:  0.6399868130683899\n",
      "Epoch:  13 | Step:  44 | Val Loss:  0.5803521275520325\n",
      "Epoch:  13 | Step:  45 | Val Loss:  0.4687839150428772\n",
      "Epoch:  13 | Step:  46 | Val Loss:  0.5079340934753418\n",
      "Epoch:  13 | Step:  47 | Val Loss:  0.6332347393035889\n",
      "Epoch:  13 | Step:  48 | Val Loss:  0.6665526628494263\n",
      "Epoch:  13 | Step:  49 | Val Loss:  0.45108312368392944\n",
      "Epoch:  13 | Step:  50 | Val Loss:  0.6591206789016724\n",
      "Epoch:  13 | Step:  51 | Val Loss:  0.616240918636322\n",
      "Epoch:  13 | Step:  52 | Val Loss:  0.5685251355171204\n",
      "Epoch:  13 | Step:  53 | Val Loss:  0.7196756601333618\n",
      "Epoch:  13 | Step:  54 | Val Loss:  0.5833542943000793\n",
      "Epoch:  13 | Step:  55 | Val Loss:  0.5848590135574341\n",
      "Epoch:  13 | Step:  56 | Val Loss:  0.7144293785095215\n",
      "Epoch:  13 | Step:  57 | Val Loss:  0.35149282217025757\n",
      "Epoch:  13 | Step:  58 | Val Loss:  0.5912079215049744\n",
      "Epoch:  13 | Step:  59 | Val Loss:  0.6399853229522705\n",
      "Epoch:  13 | Step:  60 | Val Loss:  0.586256742477417\n",
      "Epoch:  13 | Step:  61 | Val Loss:  0.3201426863670349\n",
      "Epoch:  13 | Step:  62 | Val Loss:  0.48394519090652466\n",
      "Epoch:  13 | Step:  63 | Val Loss:  0.5334003567695618\n",
      "Epoch:  13 | Step:  64 | Val Loss:  0.3681863844394684\n",
      "Epoch:  13 | Step:  65 | Val Loss:  0.7572847604751587\n",
      "Epoch:  13 | Step:  66 | Val Loss:  0.5167486667633057\n",
      "Epoch:  13 | Step:  67 | Val Loss:  0.6617860794067383\n",
      "Epoch:  13 | Step:  68 | Val Loss:  0.37987250089645386\n",
      "Epoch:  13 | Step:  69 | Val Loss:  0.5612471103668213\n",
      "Epoch:  13 | Step:  70 | Val Loss:  0.526679277420044\n",
      "Epoch:  13 | Step:  71 | Val Loss:  0.731703519821167\n",
      "Epoch:  13 | Step:  72 | Val Loss:  0.412708580493927\n",
      "Epoch:  13 | Step:  73 | Val Loss:  0.6502615809440613\n",
      "Epoch:  13 | Step:  74 | Val Loss:  0.7432462573051453\n",
      "Epoch:  13 | Step:  75 | Val Loss:  0.5584275722503662\n",
      "Epoch:  13 | Step:  76 | Val Loss:  0.7933703064918518\n",
      "Epoch:  13 | Step:  77 | Val Loss:  0.7287994027137756\n",
      "Epoch:  13 | Step:  78 | Val Loss:  0.6859298944473267\n",
      "Epoch:  13 | Step:  79 | Val Loss:  0.5689549446105957\n",
      "Epoch:  13 | Step:  80 | Val Loss:  0.7732216119766235\n",
      "Epoch:  13 | Step:  81 | Val Loss:  0.6230239868164062\n",
      "Epoch:  13 | Step:  82 | Val Loss:  0.46063292026519775\n",
      "Epoch:  13 | Step:  83 | Val Loss:  0.5577062964439392\n",
      "Epoch:  13 | Step:  84 | Val Loss:  0.5654070377349854\n",
      "Epoch:  13 | Step:  85 | Val Loss:  0.6383016109466553\n",
      "Epoch:  13 | Step:  86 | Val Loss:  0.652772068977356\n",
      "Epoch:  13 | Step:  87 | Val Loss:  0.5732651948928833\n",
      "Epoch:  13 | Step:  88 | Val Loss:  0.764458954334259\n",
      "Epoch:  13 | Step:  89 | Val Loss:  0.7753713130950928\n",
      "Epoch:  13 | Step:  90 | Val Loss:  0.43689030408859253\n",
      "Epoch:  13 | Step:  91 | Val Loss:  0.7350571155548096\n",
      "Epoch:  13 | Step:  92 | Val Loss:  1.0658206939697266\n",
      "Epoch:  13 | Step:  93 | Val Loss:  0.4111291170120239\n",
      "Epoch:  13 | Step:  94 | Val Loss:  0.456770658493042\n",
      "Epoch:  13 | Step:  95 | Val Loss:  0.47137248516082764\n",
      "Epoch:  13 | Step:  96 | Val Loss:  0.23796868324279785\n",
      "Epoch:  13 | Step:  97 | Val Loss:  0.5983915328979492\n",
      "Epoch:  13 | Step:  98 | Val Loss:  0.5243706107139587\n",
      "Epoch:  13 | Step:  99 | Val Loss:  0.5445891618728638\n",
      "Epoch:  13 | Step:  100 | Val Loss:  0.3376837968826294\n",
      "Epoch:  13 | Step:  101 | Val Loss:  0.4336497187614441\n",
      "Epoch:  13 | Step:  102 | Val Loss:  0.6559739112854004\n",
      "Epoch:  13 | Step:  103 | Val Loss:  0.6597571969032288\n",
      "Epoch:  13 | Step:  104 | Val Loss:  0.7116498947143555\n",
      "Epoch:  13 | Step:  105 | Val Loss:  0.6065305471420288\n",
      "Epoch:  13 | Step:  106 | Val Loss:  0.5806819200515747\n",
      "Epoch:  13 | Step:  107 | Val Loss:  0.6160789728164673\n",
      "Epoch:  13 | Step:  108 | Val Loss:  0.5539149045944214\n",
      "Epoch:  13 | Step:  109 | Val Loss:  0.6547163724899292\n",
      "Epoch:  13 | Step:  110 | Val Loss:  0.7212668061256409\n",
      "Epoch:  13 | Step:  111 | Val Loss:  0.6510971784591675\n",
      "Epoch:  13 | Step:  112 | Val Loss:  0.7500196695327759\n",
      "Epoch:  13 | Step:  113 | Val Loss:  0.728691577911377\n",
      "Epoch:  13 | Step:  114 | Val Loss:  0.7320736646652222\n",
      "Epoch:  13 | Step:  115 | Val Loss:  0.4673328995704651\n",
      "Epoch:  13 | Step:  116 | Val Loss:  0.6363487839698792\n",
      "Epoch:  13 | Step:  117 | Val Loss:  0.6966933012008667\n",
      "Epoch:  13 | Step:  118 | Val Loss:  0.5988204479217529\n",
      "Epoch:  13 | Step:  119 | Val Loss:  0.6236890554428101\n",
      "Epoch:  13 | Step:  120 | Val Loss:  0.7165389657020569\n",
      "Epoch:  13 | Step:  121 | Val Loss:  0.6385996341705322\n",
      "Epoch:  13 | Step:  122 | Val Loss:  0.38705572485923767\n",
      "Epoch:  13 | Step:  123 | Val Loss:  0.6740285754203796\n",
      "Epoch:  13 | Step:  124 | Val Loss:  0.7207162380218506\n",
      "Epoch:  13 | Step:  125 | Val Loss:  0.5347151160240173\n",
      "Epoch:  13 | Train Loss:  tensor(0.5589, device='cuda:0') | Val Loss:  tensor(0.5990, device='cuda:0')\n",
      "Epoch:  14 | Step:  500 | Train Loss:  0.5369832515716553\n",
      "Epoch:  14 | Step:  1 | Val Loss:  0.6955820322036743\n",
      "Epoch:  14 | Step:  2 | Val Loss:  0.5743143558502197\n",
      "Epoch:  14 | Step:  3 | Val Loss:  0.37664610147476196\n",
      "Epoch:  14 | Step:  4 | Val Loss:  0.6398403644561768\n",
      "Epoch:  14 | Step:  5 | Val Loss:  0.5541781187057495\n",
      "Epoch:  14 | Step:  6 | Val Loss:  0.6162605285644531\n",
      "Epoch:  14 | Step:  7 | Val Loss:  0.7494771480560303\n",
      "Epoch:  14 | Step:  8 | Val Loss:  0.609514057636261\n",
      "Epoch:  14 | Step:  9 | Val Loss:  0.5811841487884521\n",
      "Epoch:  14 | Step:  10 | Val Loss:  0.5453778505325317\n",
      "Epoch:  14 | Step:  11 | Val Loss:  0.45925232768058777\n",
      "Epoch:  14 | Step:  12 | Val Loss:  0.47239184379577637\n",
      "Epoch:  14 | Step:  13 | Val Loss:  0.5967317223548889\n",
      "Epoch:  14 | Step:  14 | Val Loss:  0.392645001411438\n",
      "Epoch:  14 | Step:  15 | Val Loss:  0.41781875491142273\n",
      "Epoch:  14 | Step:  16 | Val Loss:  0.7310518026351929\n",
      "Epoch:  14 | Step:  17 | Val Loss:  0.7215684652328491\n",
      "Epoch:  14 | Step:  18 | Val Loss:  0.5937551856040955\n",
      "Epoch:  14 | Step:  19 | Val Loss:  0.7211239337921143\n",
      "Epoch:  14 | Step:  20 | Val Loss:  0.4011877179145813\n",
      "Epoch:  14 | Step:  21 | Val Loss:  0.5796924829483032\n",
      "Epoch:  14 | Step:  22 | Val Loss:  0.7622233629226685\n",
      "Epoch:  14 | Step:  23 | Val Loss:  0.6498395800590515\n",
      "Epoch:  14 | Step:  24 | Val Loss:  0.47264227271080017\n",
      "Epoch:  14 | Step:  25 | Val Loss:  0.7524644732475281\n",
      "Epoch:  14 | Step:  26 | Val Loss:  0.7474544048309326\n",
      "Epoch:  14 | Step:  27 | Val Loss:  0.533798336982727\n",
      "Epoch:  14 | Step:  28 | Val Loss:  0.5728296637535095\n",
      "Epoch:  14 | Step:  29 | Val Loss:  0.5288631319999695\n",
      "Epoch:  14 | Step:  30 | Val Loss:  0.5728877782821655\n",
      "Epoch:  14 | Step:  31 | Val Loss:  0.5017718076705933\n",
      "Epoch:  14 | Step:  32 | Val Loss:  0.5605145692825317\n",
      "Epoch:  14 | Step:  33 | Val Loss:  0.38422131538391113\n",
      "Epoch:  14 | Step:  34 | Val Loss:  0.5870569944381714\n",
      "Epoch:  14 | Step:  35 | Val Loss:  0.6089737415313721\n",
      "Epoch:  14 | Step:  36 | Val Loss:  0.83445143699646\n",
      "Epoch:  14 | Step:  37 | Val Loss:  0.6549796462059021\n",
      "Epoch:  14 | Step:  38 | Val Loss:  0.6201042532920837\n",
      "Epoch:  14 | Step:  39 | Val Loss:  0.48359519243240356\n",
      "Epoch:  14 | Step:  40 | Val Loss:  0.5798105001449585\n",
      "Epoch:  14 | Step:  41 | Val Loss:  0.5941085815429688\n",
      "Epoch:  14 | Step:  42 | Val Loss:  0.6562705039978027\n",
      "Epoch:  14 | Step:  43 | Val Loss:  0.6547992825508118\n",
      "Epoch:  14 | Step:  44 | Val Loss:  0.7234213352203369\n",
      "Epoch:  14 | Step:  45 | Val Loss:  0.3432164788246155\n",
      "Epoch:  14 | Step:  46 | Val Loss:  0.6282923221588135\n",
      "Epoch:  14 | Step:  47 | Val Loss:  0.6812499761581421\n",
      "Epoch:  14 | Step:  48 | Val Loss:  0.7097669839859009\n",
      "Epoch:  14 | Step:  49 | Val Loss:  0.8325610160827637\n",
      "Epoch:  14 | Step:  50 | Val Loss:  0.6250213384628296\n",
      "Epoch:  14 | Step:  51 | Val Loss:  0.3972489535808563\n",
      "Epoch:  14 | Step:  52 | Val Loss:  0.8408994078636169\n",
      "Epoch:  14 | Step:  53 | Val Loss:  0.5900903344154358\n",
      "Epoch:  14 | Step:  54 | Val Loss:  0.3909611999988556\n",
      "Epoch:  14 | Step:  55 | Val Loss:  0.6956818103790283\n",
      "Epoch:  14 | Step:  56 | Val Loss:  0.5806666612625122\n",
      "Epoch:  14 | Step:  57 | Val Loss:  0.6177188158035278\n",
      "Epoch:  14 | Step:  58 | Val Loss:  0.5020635724067688\n",
      "Epoch:  14 | Step:  59 | Val Loss:  0.6401879191398621\n",
      "Epoch:  14 | Step:  60 | Val Loss:  0.7456072568893433\n",
      "Epoch:  14 | Step:  61 | Val Loss:  0.8032511472702026\n",
      "Epoch:  14 | Step:  62 | Val Loss:  0.6463176012039185\n",
      "Epoch:  14 | Step:  63 | Val Loss:  0.4471673369407654\n",
      "Epoch:  14 | Step:  64 | Val Loss:  0.3717184066772461\n",
      "Epoch:  14 | Step:  65 | Val Loss:  0.601252019405365\n",
      "Epoch:  14 | Step:  66 | Val Loss:  0.4336296319961548\n",
      "Epoch:  14 | Step:  67 | Val Loss:  0.5981770753860474\n",
      "Epoch:  14 | Step:  68 | Val Loss:  0.5624037384986877\n",
      "Epoch:  14 | Step:  69 | Val Loss:  0.7126052379608154\n",
      "Epoch:  14 | Step:  70 | Val Loss:  0.7743171453475952\n",
      "Epoch:  14 | Step:  71 | Val Loss:  0.5852441787719727\n",
      "Epoch:  14 | Step:  72 | Val Loss:  0.5202120542526245\n",
      "Epoch:  14 | Step:  73 | Val Loss:  0.5313766002655029\n",
      "Epoch:  14 | Step:  74 | Val Loss:  0.6259214878082275\n",
      "Epoch:  14 | Step:  75 | Val Loss:  0.5892003774642944\n",
      "Epoch:  14 | Step:  76 | Val Loss:  0.6007436513900757\n",
      "Epoch:  14 | Step:  77 | Val Loss:  0.5728162527084351\n",
      "Epoch:  14 | Step:  78 | Val Loss:  0.5058746337890625\n",
      "Epoch:  14 | Step:  79 | Val Loss:  0.6234888434410095\n",
      "Epoch:  14 | Step:  80 | Val Loss:  0.683448076248169\n",
      "Epoch:  14 | Step:  81 | Val Loss:  0.5638324022293091\n",
      "Epoch:  14 | Step:  82 | Val Loss:  0.6053282022476196\n",
      "Epoch:  14 | Step:  83 | Val Loss:  0.6086061000823975\n",
      "Epoch:  14 | Step:  84 | Val Loss:  0.7714613676071167\n",
      "Epoch:  14 | Step:  85 | Val Loss:  0.5514366626739502\n",
      "Epoch:  14 | Step:  86 | Val Loss:  0.5117812156677246\n",
      "Epoch:  14 | Step:  87 | Val Loss:  0.6213574409484863\n",
      "Epoch:  14 | Step:  88 | Val Loss:  0.8130208253860474\n",
      "Epoch:  14 | Step:  89 | Val Loss:  0.4427882134914398\n",
      "Epoch:  14 | Step:  90 | Val Loss:  0.7494206428527832\n",
      "Epoch:  14 | Step:  91 | Val Loss:  0.5950479507446289\n",
      "Epoch:  14 | Step:  92 | Val Loss:  0.63849937915802\n",
      "Epoch:  14 | Step:  93 | Val Loss:  0.45982885360717773\n",
      "Epoch:  14 | Step:  94 | Val Loss:  0.5589664578437805\n",
      "Epoch:  14 | Step:  95 | Val Loss:  0.6534404158592224\n",
      "Epoch:  14 | Step:  96 | Val Loss:  0.5870282649993896\n",
      "Epoch:  14 | Step:  97 | Val Loss:  0.5912404656410217\n",
      "Epoch:  14 | Step:  98 | Val Loss:  0.5349574089050293\n",
      "Epoch:  14 | Step:  99 | Val Loss:  0.6915030479431152\n",
      "Epoch:  14 | Step:  100 | Val Loss:  0.5104793310165405\n",
      "Epoch:  14 | Step:  101 | Val Loss:  0.772786021232605\n",
      "Epoch:  14 | Step:  102 | Val Loss:  0.4490690529346466\n",
      "Epoch:  14 | Step:  103 | Val Loss:  0.5073817372322083\n",
      "Epoch:  14 | Step:  104 | Val Loss:  0.43253177404403687\n",
      "Epoch:  14 | Step:  105 | Val Loss:  0.531899094581604\n",
      "Epoch:  14 | Step:  106 | Val Loss:  0.4664517343044281\n",
      "Epoch:  14 | Step:  107 | Val Loss:  0.5598070621490479\n",
      "Epoch:  14 | Step:  108 | Val Loss:  0.5119941234588623\n",
      "Epoch:  14 | Step:  109 | Val Loss:  0.5369898080825806\n",
      "Epoch:  14 | Step:  110 | Val Loss:  0.6741838455200195\n",
      "Epoch:  14 | Step:  111 | Val Loss:  0.6500808000564575\n",
      "Epoch:  14 | Step:  112 | Val Loss:  0.5499323606491089\n",
      "Epoch:  14 | Step:  113 | Val Loss:  0.6979392766952515\n",
      "Epoch:  14 | Step:  114 | Val Loss:  0.672704815864563\n",
      "Epoch:  14 | Step:  115 | Val Loss:  0.7813260555267334\n",
      "Epoch:  14 | Step:  116 | Val Loss:  0.8663094639778137\n",
      "Epoch:  14 | Step:  117 | Val Loss:  0.42681199312210083\n",
      "Epoch:  14 | Step:  118 | Val Loss:  0.4145900011062622\n",
      "Epoch:  14 | Step:  119 | Val Loss:  0.4322453737258911\n",
      "Epoch:  14 | Step:  120 | Val Loss:  0.6199057102203369\n",
      "Epoch:  14 | Step:  121 | Val Loss:  0.801090657711029\n",
      "Epoch:  14 | Step:  122 | Val Loss:  0.5001614093780518\n",
      "Epoch:  14 | Step:  123 | Val Loss:  0.5614104270935059\n",
      "Epoch:  14 | Step:  124 | Val Loss:  0.9003034830093384\n",
      "Epoch:  14 | Step:  125 | Val Loss:  0.616741418838501\n",
      "Epoch:  14 | Train Loss:  tensor(0.5564, device='cuda:0') | Val Loss:  tensor(0.5974, device='cuda:0')\n",
      "Epoch:  15 | Step:  500 | Train Loss:  0.48673713207244873\n",
      "Epoch:  15 | Step:  1 | Val Loss:  0.9110596776008606\n",
      "Epoch:  15 | Step:  2 | Val Loss:  0.681264340877533\n",
      "Epoch:  15 | Step:  3 | Val Loss:  0.6457957029342651\n",
      "Epoch:  15 | Step:  4 | Val Loss:  0.4925908148288727\n",
      "Epoch:  15 | Step:  5 | Val Loss:  0.6990441083908081\n",
      "Epoch:  15 | Step:  6 | Val Loss:  0.7444255352020264\n",
      "Epoch:  15 | Step:  7 | Val Loss:  0.2539044916629791\n",
      "Epoch:  15 | Step:  8 | Val Loss:  0.5747918486595154\n",
      "Epoch:  15 | Step:  9 | Val Loss:  0.6971725821495056\n",
      "Epoch:  15 | Step:  10 | Val Loss:  0.7927692532539368\n",
      "Epoch:  15 | Step:  11 | Val Loss:  0.5348501801490784\n",
      "Epoch:  15 | Step:  12 | Val Loss:  0.646501898765564\n",
      "Epoch:  15 | Step:  13 | Val Loss:  0.7745649814605713\n",
      "Epoch:  15 | Step:  14 | Val Loss:  0.655777096748352\n",
      "Epoch:  15 | Step:  15 | Val Loss:  0.5710725784301758\n",
      "Epoch:  15 | Step:  16 | Val Loss:  0.6497336030006409\n",
      "Epoch:  15 | Step:  17 | Val Loss:  0.617896318435669\n",
      "Epoch:  15 | Step:  18 | Val Loss:  0.6473516821861267\n",
      "Epoch:  15 | Step:  19 | Val Loss:  0.7043669819831848\n",
      "Epoch:  15 | Step:  20 | Val Loss:  0.7154641151428223\n",
      "Epoch:  15 | Step:  21 | Val Loss:  0.7272469997406006\n",
      "Epoch:  15 | Step:  22 | Val Loss:  0.591132640838623\n",
      "Epoch:  15 | Step:  23 | Val Loss:  0.32425224781036377\n",
      "Epoch:  15 | Step:  24 | Val Loss:  0.7398557066917419\n",
      "Epoch:  15 | Step:  25 | Val Loss:  0.89557945728302\n",
      "Epoch:  15 | Step:  26 | Val Loss:  0.38603460788726807\n",
      "Epoch:  15 | Step:  27 | Val Loss:  0.7720136642456055\n",
      "Epoch:  15 | Step:  28 | Val Loss:  0.8234948515892029\n",
      "Epoch:  15 | Step:  29 | Val Loss:  0.6441551446914673\n",
      "Epoch:  15 | Step:  30 | Val Loss:  0.8680154085159302\n",
      "Epoch:  15 | Step:  31 | Val Loss:  0.5591367483139038\n",
      "Epoch:  15 | Step:  32 | Val Loss:  0.6617887020111084\n",
      "Epoch:  15 | Step:  33 | Val Loss:  0.6194275617599487\n",
      "Epoch:  15 | Step:  34 | Val Loss:  0.35614532232284546\n",
      "Epoch:  15 | Step:  35 | Val Loss:  0.5652940273284912\n",
      "Epoch:  15 | Step:  36 | Val Loss:  0.5445126295089722\n",
      "Epoch:  15 | Step:  37 | Val Loss:  0.5515919923782349\n",
      "Epoch:  15 | Step:  38 | Val Loss:  0.6258957386016846\n",
      "Epoch:  15 | Step:  39 | Val Loss:  0.7825722694396973\n",
      "Epoch:  15 | Step:  40 | Val Loss:  0.4230548143386841\n",
      "Epoch:  15 | Step:  41 | Val Loss:  0.8529328107833862\n",
      "Epoch:  15 | Step:  42 | Val Loss:  0.4620167911052704\n",
      "Epoch:  15 | Step:  43 | Val Loss:  0.759717583656311\n",
      "Epoch:  15 | Step:  44 | Val Loss:  0.48911887407302856\n",
      "Epoch:  15 | Step:  45 | Val Loss:  0.45295724272727966\n",
      "Epoch:  15 | Step:  46 | Val Loss:  0.4666282534599304\n",
      "Epoch:  15 | Step:  47 | Val Loss:  0.46583929657936096\n",
      "Epoch:  15 | Step:  48 | Val Loss:  0.6566424369812012\n",
      "Epoch:  15 | Step:  49 | Val Loss:  0.4098607301712036\n",
      "Epoch:  15 | Step:  50 | Val Loss:  0.6323839426040649\n",
      "Epoch:  15 | Step:  51 | Val Loss:  0.5600311756134033\n",
      "Epoch:  15 | Step:  52 | Val Loss:  0.510274350643158\n",
      "Epoch:  15 | Step:  53 | Val Loss:  0.7028821706771851\n",
      "Epoch:  15 | Step:  54 | Val Loss:  0.9316420555114746\n",
      "Epoch:  15 | Step:  55 | Val Loss:  0.3852367401123047\n",
      "Epoch:  15 | Step:  56 | Val Loss:  0.461070716381073\n",
      "Epoch:  15 | Step:  57 | Val Loss:  0.738129734992981\n",
      "Epoch:  15 | Step:  58 | Val Loss:  0.5565216541290283\n",
      "Epoch:  15 | Step:  59 | Val Loss:  0.9066795110702515\n",
      "Epoch:  15 | Step:  60 | Val Loss:  0.6722142696380615\n",
      "Epoch:  15 | Step:  61 | Val Loss:  0.3586469292640686\n",
      "Epoch:  15 | Step:  62 | Val Loss:  0.3375365436077118\n",
      "Epoch:  15 | Step:  63 | Val Loss:  0.553375244140625\n",
      "Epoch:  15 | Step:  64 | Val Loss:  0.4579886198043823\n",
      "Epoch:  15 | Step:  65 | Val Loss:  0.527984082698822\n",
      "Epoch:  15 | Step:  66 | Val Loss:  0.7982912659645081\n",
      "Epoch:  15 | Step:  67 | Val Loss:  0.5550492405891418\n",
      "Epoch:  15 | Step:  68 | Val Loss:  0.4871358871459961\n",
      "Epoch:  15 | Step:  69 | Val Loss:  0.26858633756637573\n",
      "Epoch:  15 | Step:  70 | Val Loss:  0.5755408406257629\n",
      "Epoch:  15 | Step:  71 | Val Loss:  0.5028278231620789\n",
      "Epoch:  15 | Step:  72 | Val Loss:  0.5691491365432739\n",
      "Epoch:  15 | Step:  73 | Val Loss:  0.7419252991676331\n",
      "Epoch:  15 | Step:  74 | Val Loss:  0.3848010301589966\n",
      "Epoch:  15 | Step:  75 | Val Loss:  0.5417495965957642\n",
      "Epoch:  15 | Step:  76 | Val Loss:  0.471994549036026\n",
      "Epoch:  15 | Step:  77 | Val Loss:  0.6321763396263123\n",
      "Epoch:  15 | Step:  78 | Val Loss:  0.8766167163848877\n",
      "Epoch:  15 | Step:  79 | Val Loss:  0.6328927874565125\n",
      "Epoch:  15 | Step:  80 | Val Loss:  0.365755558013916\n",
      "Epoch:  15 | Step:  81 | Val Loss:  0.5647188425064087\n",
      "Epoch:  15 | Step:  82 | Val Loss:  0.5448071360588074\n",
      "Epoch:  15 | Step:  83 | Val Loss:  0.7059258222579956\n",
      "Epoch:  15 | Step:  84 | Val Loss:  0.6233656406402588\n",
      "Epoch:  15 | Step:  85 | Val Loss:  0.7370468378067017\n",
      "Epoch:  15 | Step:  86 | Val Loss:  0.41101282835006714\n",
      "Epoch:  15 | Step:  87 | Val Loss:  0.4532417058944702\n",
      "Epoch:  15 | Step:  88 | Val Loss:  0.44158169627189636\n",
      "Epoch:  15 | Step:  89 | Val Loss:  0.2735205888748169\n",
      "Epoch:  15 | Step:  90 | Val Loss:  0.620474636554718\n",
      "Epoch:  15 | Step:  91 | Val Loss:  0.7911864519119263\n",
      "Epoch:  15 | Step:  92 | Val Loss:  0.5430335402488708\n",
      "Epoch:  15 | Step:  93 | Val Loss:  0.614870548248291\n",
      "Epoch:  15 | Step:  94 | Val Loss:  0.7453675270080566\n",
      "Epoch:  15 | Step:  95 | Val Loss:  0.8000332713127136\n",
      "Epoch:  15 | Step:  96 | Val Loss:  0.4392562508583069\n",
      "Epoch:  15 | Step:  97 | Val Loss:  0.4989975392818451\n",
      "Epoch:  15 | Step:  98 | Val Loss:  0.3752685785293579\n",
      "Epoch:  15 | Step:  99 | Val Loss:  0.7887474298477173\n",
      "Epoch:  15 | Step:  100 | Val Loss:  0.5660092830657959\n",
      "Epoch:  15 | Step:  101 | Val Loss:  0.5163903832435608\n",
      "Epoch:  15 | Step:  102 | Val Loss:  0.8132109642028809\n",
      "Epoch:  15 | Step:  103 | Val Loss:  0.4423452615737915\n",
      "Epoch:  15 | Step:  104 | Val Loss:  0.5149857997894287\n",
      "Epoch:  15 | Step:  105 | Val Loss:  0.6951636672019958\n",
      "Epoch:  15 | Step:  106 | Val Loss:  0.5075245499610901\n",
      "Epoch:  15 | Step:  107 | Val Loss:  0.4630935788154602\n",
      "Epoch:  15 | Step:  108 | Val Loss:  0.5239228010177612\n",
      "Epoch:  15 | Step:  109 | Val Loss:  0.659011960029602\n",
      "Epoch:  15 | Step:  110 | Val Loss:  0.6771642565727234\n",
      "Epoch:  15 | Step:  111 | Val Loss:  0.6207236647605896\n",
      "Epoch:  15 | Step:  112 | Val Loss:  0.6839331984519958\n",
      "Epoch:  15 | Step:  113 | Val Loss:  0.46359729766845703\n",
      "Epoch:  15 | Step:  114 | Val Loss:  0.503312349319458\n",
      "Epoch:  15 | Step:  115 | Val Loss:  0.7044377326965332\n",
      "Epoch:  15 | Step:  116 | Val Loss:  0.5303019285202026\n",
      "Epoch:  15 | Step:  117 | Val Loss:  0.6798563003540039\n",
      "Epoch:  15 | Step:  118 | Val Loss:  0.4513128995895386\n",
      "Epoch:  15 | Step:  119 | Val Loss:  0.4465371370315552\n",
      "Epoch:  15 | Step:  120 | Val Loss:  0.43940699100494385\n",
      "Epoch:  15 | Step:  121 | Val Loss:  0.6320691704750061\n",
      "Epoch:  15 | Step:  122 | Val Loss:  0.5468137860298157\n",
      "Epoch:  15 | Step:  123 | Val Loss:  0.7184390425682068\n",
      "Epoch:  15 | Step:  124 | Val Loss:  0.6799597144126892\n",
      "Epoch:  15 | Step:  125 | Val Loss:  0.54494309425354\n",
      "Epoch:  15 | Train Loss:  tensor(0.5538, device='cuda:0') | Val Loss:  tensor(0.5935, device='cuda:0')\n",
      "Epoch:  16 | Step:  500 | Train Loss:  0.36915329098701477\n",
      "Epoch:  16 | Step:  1 | Val Loss:  0.5618019104003906\n",
      "Epoch:  16 | Step:  2 | Val Loss:  0.5928980708122253\n",
      "Epoch:  16 | Step:  3 | Val Loss:  0.5916000604629517\n",
      "Epoch:  16 | Step:  4 | Val Loss:  0.4168034791946411\n",
      "Epoch:  16 | Step:  5 | Val Loss:  0.45888620615005493\n",
      "Epoch:  16 | Step:  6 | Val Loss:  0.4228670299053192\n",
      "Epoch:  16 | Step:  7 | Val Loss:  0.5531457662582397\n",
      "Epoch:  16 | Step:  8 | Val Loss:  0.5679011344909668\n",
      "Epoch:  16 | Step:  9 | Val Loss:  0.44781017303466797\n",
      "Epoch:  16 | Step:  10 | Val Loss:  0.4131684899330139\n",
      "Epoch:  16 | Step:  11 | Val Loss:  0.6993774175643921\n",
      "Epoch:  16 | Step:  12 | Val Loss:  0.5731521844863892\n",
      "Epoch:  16 | Step:  13 | Val Loss:  0.7634321451187134\n",
      "Epoch:  16 | Step:  14 | Val Loss:  0.643478512763977\n",
      "Epoch:  16 | Step:  15 | Val Loss:  0.6329327821731567\n",
      "Epoch:  16 | Step:  16 | Val Loss:  0.6070478558540344\n",
      "Epoch:  16 | Step:  17 | Val Loss:  0.5339678525924683\n",
      "Epoch:  16 | Step:  18 | Val Loss:  0.5770366191864014\n",
      "Epoch:  16 | Step:  19 | Val Loss:  0.5105818510055542\n",
      "Epoch:  16 | Step:  20 | Val Loss:  0.6988149881362915\n",
      "Epoch:  16 | Step:  21 | Val Loss:  0.5631691813468933\n",
      "Epoch:  16 | Step:  22 | Val Loss:  0.6767693758010864\n",
      "Epoch:  16 | Step:  23 | Val Loss:  0.5079135894775391\n",
      "Epoch:  16 | Step:  24 | Val Loss:  0.5864328145980835\n",
      "Epoch:  16 | Step:  25 | Val Loss:  0.5971395969390869\n",
      "Epoch:  16 | Step:  26 | Val Loss:  0.6749788522720337\n",
      "Epoch:  16 | Step:  27 | Val Loss:  0.5923584699630737\n",
      "Epoch:  16 | Step:  28 | Val Loss:  0.5091969966888428\n",
      "Epoch:  16 | Step:  29 | Val Loss:  0.8927435874938965\n",
      "Epoch:  16 | Step:  30 | Val Loss:  0.7628490328788757\n",
      "Epoch:  16 | Step:  31 | Val Loss:  0.672948956489563\n",
      "Epoch:  16 | Step:  32 | Val Loss:  0.678299069404602\n",
      "Epoch:  16 | Step:  33 | Val Loss:  0.5036318898200989\n",
      "Epoch:  16 | Step:  34 | Val Loss:  0.4758372902870178\n",
      "Epoch:  16 | Step:  35 | Val Loss:  0.607132077217102\n",
      "Epoch:  16 | Step:  36 | Val Loss:  0.5555134415626526\n",
      "Epoch:  16 | Step:  37 | Val Loss:  0.629277229309082\n",
      "Epoch:  16 | Step:  38 | Val Loss:  0.8144104480743408\n",
      "Epoch:  16 | Step:  39 | Val Loss:  0.6598639488220215\n",
      "Epoch:  16 | Step:  40 | Val Loss:  0.6138661503791809\n",
      "Epoch:  16 | Step:  41 | Val Loss:  0.5246109366416931\n",
      "Epoch:  16 | Step:  42 | Val Loss:  0.5345816612243652\n",
      "Epoch:  16 | Step:  43 | Val Loss:  0.5613279938697815\n",
      "Epoch:  16 | Step:  44 | Val Loss:  0.6009000539779663\n",
      "Epoch:  16 | Step:  45 | Val Loss:  0.5725352168083191\n",
      "Epoch:  16 | Step:  46 | Val Loss:  0.5021936893463135\n",
      "Epoch:  16 | Step:  47 | Val Loss:  0.6052354574203491\n",
      "Epoch:  16 | Step:  48 | Val Loss:  0.6816055774688721\n",
      "Epoch:  16 | Step:  49 | Val Loss:  0.52419513463974\n",
      "Epoch:  16 | Step:  50 | Val Loss:  0.8829509019851685\n",
      "Epoch:  16 | Step:  51 | Val Loss:  0.65234375\n",
      "Epoch:  16 | Step:  52 | Val Loss:  0.40770429372787476\n",
      "Epoch:  16 | Step:  53 | Val Loss:  0.8757336735725403\n",
      "Epoch:  16 | Step:  54 | Val Loss:  0.5348856449127197\n",
      "Epoch:  16 | Step:  55 | Val Loss:  0.6648257970809937\n",
      "Epoch:  16 | Step:  56 | Val Loss:  0.6471019983291626\n",
      "Epoch:  16 | Step:  57 | Val Loss:  0.6023432016372681\n",
      "Epoch:  16 | Step:  58 | Val Loss:  0.6955050230026245\n",
      "Epoch:  16 | Step:  59 | Val Loss:  0.4262896180152893\n",
      "Epoch:  16 | Step:  60 | Val Loss:  0.3887079954147339\n",
      "Epoch:  16 | Step:  61 | Val Loss:  0.501173734664917\n",
      "Epoch:  16 | Step:  62 | Val Loss:  0.5154772400856018\n",
      "Epoch:  16 | Step:  63 | Val Loss:  0.585290789604187\n",
      "Epoch:  16 | Step:  64 | Val Loss:  0.7214607000350952\n",
      "Epoch:  16 | Step:  65 | Val Loss:  0.7792003154754639\n",
      "Epoch:  16 | Step:  66 | Val Loss:  0.6507930755615234\n",
      "Epoch:  16 | Step:  67 | Val Loss:  0.4559650719165802\n",
      "Epoch:  16 | Step:  68 | Val Loss:  0.409670889377594\n",
      "Epoch:  16 | Step:  69 | Val Loss:  0.4925271272659302\n",
      "Epoch:  16 | Step:  70 | Val Loss:  0.5135267972946167\n",
      "Epoch:  16 | Step:  71 | Val Loss:  0.6608676314353943\n",
      "Epoch:  16 | Step:  72 | Val Loss:  0.696650505065918\n",
      "Epoch:  16 | Step:  73 | Val Loss:  0.4545392394065857\n",
      "Epoch:  16 | Step:  74 | Val Loss:  0.6781808733940125\n",
      "Epoch:  16 | Step:  75 | Val Loss:  0.5134667754173279\n",
      "Epoch:  16 | Step:  76 | Val Loss:  0.548140823841095\n",
      "Epoch:  16 | Step:  77 | Val Loss:  0.4922529458999634\n",
      "Epoch:  16 | Step:  78 | Val Loss:  0.411399245262146\n",
      "Epoch:  16 | Step:  79 | Val Loss:  0.6383525133132935\n",
      "Epoch:  16 | Step:  80 | Val Loss:  0.6919223666191101\n",
      "Epoch:  16 | Step:  81 | Val Loss:  0.4497237801551819\n",
      "Epoch:  16 | Step:  82 | Val Loss:  0.7784508466720581\n",
      "Epoch:  16 | Step:  83 | Val Loss:  0.5300838947296143\n",
      "Epoch:  16 | Step:  84 | Val Loss:  0.8118121027946472\n",
      "Epoch:  16 | Step:  85 | Val Loss:  0.5058649182319641\n",
      "Epoch:  16 | Step:  86 | Val Loss:  0.5365546941757202\n",
      "Epoch:  16 | Step:  87 | Val Loss:  0.6683575510978699\n",
      "Epoch:  16 | Step:  88 | Val Loss:  0.6452829241752625\n",
      "Epoch:  16 | Step:  89 | Val Loss:  0.7459721565246582\n",
      "Epoch:  16 | Step:  90 | Val Loss:  0.5627421140670776\n",
      "Epoch:  16 | Step:  91 | Val Loss:  0.743094265460968\n",
      "Epoch:  16 | Step:  92 | Val Loss:  0.7147667407989502\n",
      "Epoch:  16 | Step:  93 | Val Loss:  0.5864425897598267\n",
      "Epoch:  16 | Step:  94 | Val Loss:  0.5841519832611084\n",
      "Epoch:  16 | Step:  95 | Val Loss:  0.3993437886238098\n",
      "Epoch:  16 | Step:  96 | Val Loss:  0.4515378475189209\n",
      "Epoch:  16 | Step:  97 | Val Loss:  0.44014599919319153\n",
      "Epoch:  16 | Step:  98 | Val Loss:  0.7085791230201721\n",
      "Epoch:  16 | Step:  99 | Val Loss:  0.4420742988586426\n",
      "Epoch:  16 | Step:  100 | Val Loss:  0.7092874050140381\n",
      "Epoch:  16 | Step:  101 | Val Loss:  0.5179954171180725\n",
      "Epoch:  16 | Step:  102 | Val Loss:  0.6133387088775635\n",
      "Epoch:  16 | Step:  103 | Val Loss:  0.5653793811798096\n",
      "Epoch:  16 | Step:  104 | Val Loss:  0.5196325778961182\n",
      "Epoch:  16 | Step:  105 | Val Loss:  0.6769653558731079\n",
      "Epoch:  16 | Step:  106 | Val Loss:  0.7196917533874512\n",
      "Epoch:  16 | Step:  107 | Val Loss:  0.6062155365943909\n",
      "Epoch:  16 | Step:  108 | Val Loss:  0.5583038330078125\n",
      "Epoch:  16 | Step:  109 | Val Loss:  0.48012179136276245\n",
      "Epoch:  16 | Step:  110 | Val Loss:  0.48959672451019287\n",
      "Epoch:  16 | Step:  111 | Val Loss:  0.5138364434242249\n",
      "Epoch:  16 | Step:  112 | Val Loss:  0.711381733417511\n",
      "Epoch:  16 | Step:  113 | Val Loss:  0.5432379245758057\n",
      "Epoch:  16 | Step:  114 | Val Loss:  0.4658554494380951\n",
      "Epoch:  16 | Step:  115 | Val Loss:  0.6861742734909058\n",
      "Epoch:  16 | Step:  116 | Val Loss:  0.6005331873893738\n",
      "Epoch:  16 | Step:  117 | Val Loss:  0.7750957012176514\n",
      "Epoch:  16 | Step:  118 | Val Loss:  0.6018396019935608\n",
      "Epoch:  16 | Step:  119 | Val Loss:  0.6637755632400513\n",
      "Epoch:  16 | Step:  120 | Val Loss:  0.3844295144081116\n",
      "Epoch:  16 | Step:  121 | Val Loss:  0.6452367305755615\n",
      "Epoch:  16 | Step:  122 | Val Loss:  0.5972616672515869\n",
      "Epoch:  16 | Step:  123 | Val Loss:  0.5670119524002075\n",
      "Epoch:  16 | Step:  124 | Val Loss:  0.6238963603973389\n",
      "Epoch:  16 | Step:  125 | Val Loss:  0.7015834450721741\n",
      "Epoch:  16 | Train Loss:  tensor(0.5512, device='cuda:0') | Val Loss:  tensor(0.5918, device='cuda:0')\n",
      "Epoch:  17 | Step:  500 | Train Loss:  0.5097081661224365\n",
      "Epoch:  17 | Step:  1 | Val Loss:  0.6572559475898743\n",
      "Epoch:  17 | Step:  2 | Val Loss:  0.4990965723991394\n",
      "Epoch:  17 | Step:  3 | Val Loss:  0.7093170881271362\n",
      "Epoch:  17 | Step:  4 | Val Loss:  0.8336431980133057\n",
      "Epoch:  17 | Step:  5 | Val Loss:  0.5392995476722717\n",
      "Epoch:  17 | Step:  6 | Val Loss:  0.5857715010643005\n",
      "Epoch:  17 | Step:  7 | Val Loss:  0.5776659250259399\n",
      "Epoch:  17 | Step:  8 | Val Loss:  0.41781148314476013\n",
      "Epoch:  17 | Step:  9 | Val Loss:  0.5679379105567932\n",
      "Epoch:  17 | Step:  10 | Val Loss:  0.598316490650177\n",
      "Epoch:  17 | Step:  11 | Val Loss:  0.6823994517326355\n",
      "Epoch:  17 | Step:  12 | Val Loss:  0.8037322759628296\n",
      "Epoch:  17 | Step:  13 | Val Loss:  0.5801810026168823\n",
      "Epoch:  17 | Step:  14 | Val Loss:  0.597238302230835\n",
      "Epoch:  17 | Step:  15 | Val Loss:  0.7933850288391113\n",
      "Epoch:  17 | Step:  16 | Val Loss:  0.5895376205444336\n",
      "Epoch:  17 | Step:  17 | Val Loss:  0.791748583316803\n",
      "Epoch:  17 | Step:  18 | Val Loss:  0.5826299786567688\n",
      "Epoch:  17 | Step:  19 | Val Loss:  0.5459461212158203\n",
      "Epoch:  17 | Step:  20 | Val Loss:  0.5270781517028809\n",
      "Epoch:  17 | Step:  21 | Val Loss:  0.7136296033859253\n",
      "Epoch:  17 | Step:  22 | Val Loss:  0.5722328424453735\n",
      "Epoch:  17 | Step:  23 | Val Loss:  0.6015854477882385\n",
      "Epoch:  17 | Step:  24 | Val Loss:  0.8533052206039429\n",
      "Epoch:  17 | Step:  25 | Val Loss:  0.5459136366844177\n",
      "Epoch:  17 | Step:  26 | Val Loss:  0.5315667390823364\n",
      "Epoch:  17 | Step:  27 | Val Loss:  0.5657463073730469\n",
      "Epoch:  17 | Step:  28 | Val Loss:  0.5302798748016357\n",
      "Epoch:  17 | Step:  29 | Val Loss:  0.727123498916626\n",
      "Epoch:  17 | Step:  30 | Val Loss:  0.7465099096298218\n",
      "Epoch:  17 | Step:  31 | Val Loss:  0.5957368016242981\n",
      "Epoch:  17 | Step:  32 | Val Loss:  0.5128916501998901\n",
      "Epoch:  17 | Step:  33 | Val Loss:  0.67125403881073\n",
      "Epoch:  17 | Step:  34 | Val Loss:  0.6933597326278687\n",
      "Epoch:  17 | Step:  35 | Val Loss:  0.5546366572380066\n",
      "Epoch:  17 | Step:  36 | Val Loss:  0.7069884538650513\n",
      "Epoch:  17 | Step:  37 | Val Loss:  0.5021105408668518\n",
      "Epoch:  17 | Step:  38 | Val Loss:  0.46803176403045654\n",
      "Epoch:  17 | Step:  39 | Val Loss:  0.587875247001648\n",
      "Epoch:  17 | Step:  40 | Val Loss:  0.5638151168823242\n",
      "Epoch:  17 | Step:  41 | Val Loss:  0.7330527901649475\n",
      "Epoch:  17 | Step:  42 | Val Loss:  0.5062475204467773\n",
      "Epoch:  17 | Step:  43 | Val Loss:  0.6275631189346313\n",
      "Epoch:  17 | Step:  44 | Val Loss:  0.44060906767845154\n",
      "Epoch:  17 | Step:  45 | Val Loss:  0.6863131523132324\n",
      "Epoch:  17 | Step:  46 | Val Loss:  0.46898844838142395\n",
      "Epoch:  17 | Step:  47 | Val Loss:  0.5693100094795227\n",
      "Epoch:  17 | Step:  48 | Val Loss:  0.6630153059959412\n",
      "Epoch:  17 | Step:  49 | Val Loss:  0.3942936658859253\n",
      "Epoch:  17 | Step:  50 | Val Loss:  0.5823385715484619\n",
      "Epoch:  17 | Step:  51 | Val Loss:  0.5149298906326294\n",
      "Epoch:  17 | Step:  52 | Val Loss:  0.720587968826294\n",
      "Epoch:  17 | Step:  53 | Val Loss:  0.5781813859939575\n",
      "Epoch:  17 | Step:  54 | Val Loss:  0.6097880601882935\n",
      "Epoch:  17 | Step:  55 | Val Loss:  0.6479517221450806\n",
      "Epoch:  17 | Step:  56 | Val Loss:  0.6129961609840393\n",
      "Epoch:  17 | Step:  57 | Val Loss:  0.7343786358833313\n",
      "Epoch:  17 | Step:  58 | Val Loss:  0.415042906999588\n",
      "Epoch:  17 | Step:  59 | Val Loss:  0.4663238823413849\n",
      "Epoch:  17 | Step:  60 | Val Loss:  0.6012165546417236\n",
      "Epoch:  17 | Step:  61 | Val Loss:  0.34586215019226074\n",
      "Epoch:  17 | Step:  62 | Val Loss:  0.6248122453689575\n",
      "Epoch:  17 | Step:  63 | Val Loss:  0.5768385529518127\n",
      "Epoch:  17 | Step:  64 | Val Loss:  0.680208683013916\n",
      "Epoch:  17 | Step:  65 | Val Loss:  0.6429355144500732\n",
      "Epoch:  17 | Step:  66 | Val Loss:  0.5305100679397583\n",
      "Epoch:  17 | Step:  67 | Val Loss:  0.5323134660720825\n",
      "Epoch:  17 | Step:  68 | Val Loss:  0.4973381459712982\n",
      "Epoch:  17 | Step:  69 | Val Loss:  0.6129484176635742\n",
      "Epoch:  17 | Step:  70 | Val Loss:  0.6379987001419067\n",
      "Epoch:  17 | Step:  71 | Val Loss:  0.5387948751449585\n",
      "Epoch:  17 | Step:  72 | Val Loss:  0.6423496007919312\n",
      "Epoch:  17 | Step:  73 | Val Loss:  0.5123143196105957\n",
      "Epoch:  17 | Step:  74 | Val Loss:  0.4053346812725067\n",
      "Epoch:  17 | Step:  75 | Val Loss:  0.7730050086975098\n",
      "Epoch:  17 | Step:  76 | Val Loss:  0.7174056172370911\n",
      "Epoch:  17 | Step:  77 | Val Loss:  0.5334903597831726\n",
      "Epoch:  17 | Step:  78 | Val Loss:  0.7623412013053894\n",
      "Epoch:  17 | Step:  79 | Val Loss:  0.7278311252593994\n",
      "Epoch:  17 | Step:  80 | Val Loss:  0.5070362687110901\n",
      "Epoch:  17 | Step:  81 | Val Loss:  0.37131237983703613\n",
      "Epoch:  17 | Step:  82 | Val Loss:  0.3579391539096832\n",
      "Epoch:  17 | Step:  83 | Val Loss:  0.5796434879302979\n",
      "Epoch:  17 | Step:  84 | Val Loss:  0.6465566158294678\n",
      "Epoch:  17 | Step:  85 | Val Loss:  0.6358879804611206\n",
      "Epoch:  17 | Step:  86 | Val Loss:  0.6747078895568848\n",
      "Epoch:  17 | Step:  87 | Val Loss:  0.4580446183681488\n",
      "Epoch:  17 | Step:  88 | Val Loss:  0.5703258514404297\n",
      "Epoch:  17 | Step:  89 | Val Loss:  0.7265340685844421\n",
      "Epoch:  17 | Step:  90 | Val Loss:  0.5713934898376465\n",
      "Epoch:  17 | Step:  91 | Val Loss:  0.8469910621643066\n",
      "Epoch:  17 | Step:  92 | Val Loss:  0.5693396329879761\n",
      "Epoch:  17 | Step:  93 | Val Loss:  0.48274868726730347\n",
      "Epoch:  17 | Step:  94 | Val Loss:  0.6837328672409058\n",
      "Epoch:  17 | Step:  95 | Val Loss:  0.4815056324005127\n",
      "Epoch:  17 | Step:  96 | Val Loss:  0.5076064467430115\n",
      "Epoch:  17 | Step:  97 | Val Loss:  0.43941614031791687\n",
      "Epoch:  17 | Step:  98 | Val Loss:  0.479836106300354\n",
      "Epoch:  17 | Step:  99 | Val Loss:  0.5798237323760986\n",
      "Epoch:  17 | Step:  100 | Val Loss:  0.5953574180603027\n",
      "Epoch:  17 | Step:  101 | Val Loss:  0.4400642514228821\n",
      "Epoch:  17 | Step:  102 | Val Loss:  0.3432679772377014\n",
      "Epoch:  17 | Step:  103 | Val Loss:  0.7567665576934814\n",
      "Epoch:  17 | Step:  104 | Val Loss:  0.7018669843673706\n",
      "Epoch:  17 | Step:  105 | Val Loss:  0.65602707862854\n",
      "Epoch:  17 | Step:  106 | Val Loss:  0.5743453502655029\n",
      "Epoch:  17 | Step:  107 | Val Loss:  0.2897276282310486\n",
      "Epoch:  17 | Step:  108 | Val Loss:  0.5092557668685913\n",
      "Epoch:  17 | Step:  109 | Val Loss:  0.29598498344421387\n",
      "Epoch:  17 | Step:  110 | Val Loss:  0.5351991057395935\n",
      "Epoch:  17 | Step:  111 | Val Loss:  0.5155155658721924\n",
      "Epoch:  17 | Step:  112 | Val Loss:  0.7522404193878174\n",
      "Epoch:  17 | Step:  113 | Val Loss:  0.4448891878128052\n",
      "Epoch:  17 | Step:  114 | Val Loss:  0.6054352521896362\n",
      "Epoch:  17 | Step:  115 | Val Loss:  0.7046780586242676\n",
      "Epoch:  17 | Step:  116 | Val Loss:  0.7139147520065308\n",
      "Epoch:  17 | Step:  117 | Val Loss:  0.6004989147186279\n",
      "Epoch:  17 | Step:  118 | Val Loss:  0.5673178434371948\n",
      "Epoch:  17 | Step:  119 | Val Loss:  0.6445073485374451\n",
      "Epoch:  17 | Step:  120 | Val Loss:  0.6238136887550354\n",
      "Epoch:  17 | Step:  121 | Val Loss:  0.5314753651618958\n",
      "Epoch:  17 | Step:  122 | Val Loss:  0.6438354849815369\n",
      "Epoch:  17 | Step:  123 | Val Loss:  0.49604156613349915\n",
      "Epoch:  17 | Step:  124 | Val Loss:  0.5523853898048401\n",
      "Epoch:  17 | Step:  125 | Val Loss:  0.591555118560791\n",
      "Epoch:  17 | Train Loss:  tensor(0.5491, device='cuda:0') | Val Loss:  tensor(0.5876, device='cuda:0')\n",
      "Epoch:  18 | Step:  500 | Train Loss:  0.6122424602508545\n",
      "Epoch:  18 | Step:  1 | Val Loss:  0.5685861706733704\n",
      "Epoch:  18 | Step:  2 | Val Loss:  0.6980312466621399\n",
      "Epoch:  18 | Step:  3 | Val Loss:  0.7457363605499268\n",
      "Epoch:  18 | Step:  4 | Val Loss:  0.7527832388877869\n",
      "Epoch:  18 | Step:  5 | Val Loss:  0.6876819729804993\n",
      "Epoch:  18 | Step:  6 | Val Loss:  0.4184556305408478\n",
      "Epoch:  18 | Step:  7 | Val Loss:  0.7360097169876099\n",
      "Epoch:  18 | Step:  8 | Val Loss:  0.8060325980186462\n",
      "Epoch:  18 | Step:  9 | Val Loss:  0.36846575140953064\n",
      "Epoch:  18 | Step:  10 | Val Loss:  0.6076322793960571\n",
      "Epoch:  18 | Step:  11 | Val Loss:  0.34084129333496094\n",
      "Epoch:  18 | Step:  12 | Val Loss:  0.5940604209899902\n",
      "Epoch:  18 | Step:  13 | Val Loss:  0.7078452110290527\n",
      "Epoch:  18 | Step:  14 | Val Loss:  0.6058999300003052\n",
      "Epoch:  18 | Step:  15 | Val Loss:  0.6133705377578735\n",
      "Epoch:  18 | Step:  16 | Val Loss:  0.5790185928344727\n",
      "Epoch:  18 | Step:  17 | Val Loss:  0.3994178771972656\n",
      "Epoch:  18 | Step:  18 | Val Loss:  0.7994713187217712\n",
      "Epoch:  18 | Step:  19 | Val Loss:  0.7212041616439819\n",
      "Epoch:  18 | Step:  20 | Val Loss:  0.43123626708984375\n",
      "Epoch:  18 | Step:  21 | Val Loss:  0.5684788823127747\n",
      "Epoch:  18 | Step:  22 | Val Loss:  0.5801467895507812\n",
      "Epoch:  18 | Step:  23 | Val Loss:  0.5200696587562561\n",
      "Epoch:  18 | Step:  24 | Val Loss:  0.594694972038269\n",
      "Epoch:  18 | Step:  25 | Val Loss:  0.6571829915046692\n",
      "Epoch:  18 | Step:  26 | Val Loss:  0.41790950298309326\n",
      "Epoch:  18 | Step:  27 | Val Loss:  0.6006053686141968\n",
      "Epoch:  18 | Step:  28 | Val Loss:  0.4473632872104645\n",
      "Epoch:  18 | Step:  29 | Val Loss:  0.590657114982605\n",
      "Epoch:  18 | Step:  30 | Val Loss:  0.5828967094421387\n",
      "Epoch:  18 | Step:  31 | Val Loss:  0.6748734712600708\n",
      "Epoch:  18 | Step:  32 | Val Loss:  0.49979040026664734\n",
      "Epoch:  18 | Step:  33 | Val Loss:  0.6574019193649292\n",
      "Epoch:  18 | Step:  34 | Val Loss:  0.48519429564476013\n",
      "Epoch:  18 | Step:  35 | Val Loss:  0.46329694986343384\n",
      "Epoch:  18 | Step:  36 | Val Loss:  0.703924298286438\n",
      "Epoch:  18 | Step:  37 | Val Loss:  0.37213796377182007\n",
      "Epoch:  18 | Step:  38 | Val Loss:  0.5692816972732544\n",
      "Epoch:  18 | Step:  39 | Val Loss:  0.6450912356376648\n",
      "Epoch:  18 | Step:  40 | Val Loss:  0.46967387199401855\n",
      "Epoch:  18 | Step:  41 | Val Loss:  0.5926958322525024\n",
      "Epoch:  18 | Step:  42 | Val Loss:  0.6646956205368042\n",
      "Epoch:  18 | Step:  43 | Val Loss:  0.4222293198108673\n",
      "Epoch:  18 | Step:  44 | Val Loss:  0.7170459032058716\n",
      "Epoch:  18 | Step:  45 | Val Loss:  0.6678382158279419\n",
      "Epoch:  18 | Step:  46 | Val Loss:  0.6559092998504639\n",
      "Epoch:  18 | Step:  47 | Val Loss:  0.6058166027069092\n",
      "Epoch:  18 | Step:  48 | Val Loss:  0.5563548803329468\n",
      "Epoch:  18 | Step:  49 | Val Loss:  0.477908730506897\n",
      "Epoch:  18 | Step:  50 | Val Loss:  0.676641583442688\n",
      "Epoch:  18 | Step:  51 | Val Loss:  0.6581971049308777\n",
      "Epoch:  18 | Step:  52 | Val Loss:  0.5943862199783325\n",
      "Epoch:  18 | Step:  53 | Val Loss:  0.5699936747550964\n",
      "Epoch:  18 | Step:  54 | Val Loss:  0.5171071290969849\n",
      "Epoch:  18 | Step:  55 | Val Loss:  0.35858097672462463\n",
      "Epoch:  18 | Step:  56 | Val Loss:  0.5529156923294067\n",
      "Epoch:  18 | Step:  57 | Val Loss:  0.5330536961555481\n",
      "Epoch:  18 | Step:  58 | Val Loss:  0.5699137449264526\n",
      "Epoch:  18 | Step:  59 | Val Loss:  0.599909245967865\n",
      "Epoch:  18 | Step:  60 | Val Loss:  0.48335886001586914\n",
      "Epoch:  18 | Step:  61 | Val Loss:  0.569461464881897\n",
      "Epoch:  18 | Step:  62 | Val Loss:  0.549432635307312\n",
      "Epoch:  18 | Step:  63 | Val Loss:  0.7029365301132202\n",
      "Epoch:  18 | Step:  64 | Val Loss:  0.855078935623169\n",
      "Epoch:  18 | Step:  65 | Val Loss:  0.6046502590179443\n",
      "Epoch:  18 | Step:  66 | Val Loss:  0.44524192810058594\n",
      "Epoch:  18 | Step:  67 | Val Loss:  0.5861430168151855\n",
      "Epoch:  18 | Step:  68 | Val Loss:  0.8130581974983215\n",
      "Epoch:  18 | Step:  69 | Val Loss:  0.5749707221984863\n",
      "Epoch:  18 | Step:  70 | Val Loss:  0.5333739519119263\n",
      "Epoch:  18 | Step:  71 | Val Loss:  0.5142546892166138\n",
      "Epoch:  18 | Step:  72 | Val Loss:  0.5959435701370239\n",
      "Epoch:  18 | Step:  73 | Val Loss:  0.7199744582176208\n",
      "Epoch:  18 | Step:  74 | Val Loss:  0.4746740758419037\n",
      "Epoch:  18 | Step:  75 | Val Loss:  0.7185724377632141\n",
      "Epoch:  18 | Step:  76 | Val Loss:  0.4155609905719757\n",
      "Epoch:  18 | Step:  77 | Val Loss:  0.46259641647338867\n",
      "Epoch:  18 | Step:  78 | Val Loss:  0.6753745079040527\n",
      "Epoch:  18 | Step:  79 | Val Loss:  0.6661840677261353\n",
      "Epoch:  18 | Step:  80 | Val Loss:  0.46760180592536926\n",
      "Epoch:  18 | Step:  81 | Val Loss:  0.3862989842891693\n",
      "Epoch:  18 | Step:  82 | Val Loss:  0.565575897693634\n",
      "Epoch:  18 | Step:  83 | Val Loss:  0.6239263415336609\n",
      "Epoch:  18 | Step:  84 | Val Loss:  0.46757590770721436\n",
      "Epoch:  18 | Step:  85 | Val Loss:  0.4446912407875061\n",
      "Epoch:  18 | Step:  86 | Val Loss:  0.6938778758049011\n",
      "Epoch:  18 | Step:  87 | Val Loss:  0.6078782677650452\n",
      "Epoch:  18 | Step:  88 | Val Loss:  0.5676355361938477\n",
      "Epoch:  18 | Step:  89 | Val Loss:  0.6038079857826233\n",
      "Epoch:  18 | Step:  90 | Val Loss:  0.8519165515899658\n",
      "Epoch:  18 | Step:  91 | Val Loss:  0.9186500310897827\n",
      "Epoch:  18 | Step:  92 | Val Loss:  0.5295082330703735\n",
      "Epoch:  18 | Step:  93 | Val Loss:  0.514034628868103\n",
      "Epoch:  18 | Step:  94 | Val Loss:  0.6549984216690063\n",
      "Epoch:  18 | Step:  95 | Val Loss:  0.599398136138916\n",
      "Epoch:  18 | Step:  96 | Val Loss:  0.5724514722824097\n",
      "Epoch:  18 | Step:  97 | Val Loss:  0.752536952495575\n",
      "Epoch:  18 | Step:  98 | Val Loss:  0.4563465714454651\n",
      "Epoch:  18 | Step:  99 | Val Loss:  0.5180306434631348\n",
      "Epoch:  18 | Step:  100 | Val Loss:  0.554722249507904\n",
      "Epoch:  18 | Step:  101 | Val Loss:  0.39488330483436584\n",
      "Epoch:  18 | Step:  102 | Val Loss:  0.37938427925109863\n",
      "Epoch:  18 | Step:  103 | Val Loss:  0.6566165089607239\n",
      "Epoch:  18 | Step:  104 | Val Loss:  0.6129128932952881\n",
      "Epoch:  18 | Step:  105 | Val Loss:  0.6721652746200562\n",
      "Epoch:  18 | Step:  106 | Val Loss:  0.49495893716812134\n",
      "Epoch:  18 | Step:  107 | Val Loss:  0.4218161404132843\n",
      "Epoch:  18 | Step:  108 | Val Loss:  0.5825893878936768\n",
      "Epoch:  18 | Step:  109 | Val Loss:  0.5127254724502563\n",
      "Epoch:  18 | Step:  110 | Val Loss:  0.724838376045227\n",
      "Epoch:  18 | Step:  111 | Val Loss:  0.6573038101196289\n",
      "Epoch:  18 | Step:  112 | Val Loss:  0.5646737813949585\n",
      "Epoch:  18 | Step:  113 | Val Loss:  0.5521752834320068\n",
      "Epoch:  18 | Step:  114 | Val Loss:  0.5652124285697937\n",
      "Epoch:  18 | Step:  115 | Val Loss:  0.7380160093307495\n",
      "Epoch:  18 | Step:  116 | Val Loss:  0.7778998017311096\n",
      "Epoch:  18 | Step:  117 | Val Loss:  0.558394193649292\n",
      "Epoch:  18 | Step:  118 | Val Loss:  0.38132724165916443\n",
      "Epoch:  18 | Step:  119 | Val Loss:  0.7047513723373413\n",
      "Epoch:  18 | Step:  120 | Val Loss:  0.5120916962623596\n",
      "Epoch:  18 | Step:  121 | Val Loss:  0.4670637249946594\n",
      "Epoch:  18 | Step:  122 | Val Loss:  0.7323200702667236\n",
      "Epoch:  18 | Step:  123 | Val Loss:  0.5781044960021973\n",
      "Epoch:  18 | Step:  124 | Val Loss:  0.6955451965332031\n",
      "Epoch:  18 | Step:  125 | Val Loss:  0.7012133002281189\n",
      "Epoch:  18 | Train Loss:  tensor(0.5474, device='cuda:0') | Val Loss:  tensor(0.5858, device='cuda:0')\n",
      "Epoch:  19 | Step:  500 | Train Loss:  0.5403828024864197\n",
      "Epoch:  19 | Step:  1 | Val Loss:  0.5508323311805725\n",
      "Epoch:  19 | Step:  2 | Val Loss:  0.6295487284660339\n",
      "Epoch:  19 | Step:  3 | Val Loss:  0.4457557201385498\n",
      "Epoch:  19 | Step:  4 | Val Loss:  0.596569299697876\n",
      "Epoch:  19 | Step:  5 | Val Loss:  0.4608883857727051\n",
      "Epoch:  19 | Step:  6 | Val Loss:  0.4722556471824646\n",
      "Epoch:  19 | Step:  7 | Val Loss:  0.4504118859767914\n",
      "Epoch:  19 | Step:  8 | Val Loss:  0.40018370747566223\n",
      "Epoch:  19 | Step:  9 | Val Loss:  0.49453267455101013\n",
      "Epoch:  19 | Step:  10 | Val Loss:  0.5786924958229065\n",
      "Epoch:  19 | Step:  11 | Val Loss:  0.5190972089767456\n",
      "Epoch:  19 | Step:  12 | Val Loss:  0.5517243146896362\n",
      "Epoch:  19 | Step:  13 | Val Loss:  0.6657083034515381\n",
      "Epoch:  19 | Step:  14 | Val Loss:  0.4364873766899109\n",
      "Epoch:  19 | Step:  15 | Val Loss:  0.7251810431480408\n",
      "Epoch:  19 | Step:  16 | Val Loss:  0.4843505024909973\n",
      "Epoch:  19 | Step:  17 | Val Loss:  0.6109607815742493\n",
      "Epoch:  19 | Step:  18 | Val Loss:  0.5912575721740723\n",
      "Epoch:  19 | Step:  19 | Val Loss:  0.4503135681152344\n",
      "Epoch:  19 | Step:  20 | Val Loss:  0.36145952343940735\n",
      "Epoch:  19 | Step:  21 | Val Loss:  0.7861810922622681\n",
      "Epoch:  19 | Step:  22 | Val Loss:  0.7174948453903198\n",
      "Epoch:  19 | Step:  23 | Val Loss:  0.37027278542518616\n",
      "Epoch:  19 | Step:  24 | Val Loss:  0.5910094976425171\n",
      "Epoch:  19 | Step:  25 | Val Loss:  0.5717383027076721\n",
      "Epoch:  19 | Step:  26 | Val Loss:  0.6148614883422852\n",
      "Epoch:  19 | Step:  27 | Val Loss:  0.7062016129493713\n",
      "Epoch:  19 | Step:  28 | Val Loss:  0.7577614784240723\n",
      "Epoch:  19 | Step:  29 | Val Loss:  0.7819454669952393\n",
      "Epoch:  19 | Step:  30 | Val Loss:  0.5471383333206177\n",
      "Epoch:  19 | Step:  31 | Val Loss:  0.7420655488967896\n",
      "Epoch:  19 | Step:  32 | Val Loss:  0.6653848886489868\n",
      "Epoch:  19 | Step:  33 | Val Loss:  0.4527994394302368\n",
      "Epoch:  19 | Step:  34 | Val Loss:  0.6438010334968567\n",
      "Epoch:  19 | Step:  35 | Val Loss:  0.538779616355896\n",
      "Epoch:  19 | Step:  36 | Val Loss:  0.446147084236145\n",
      "Epoch:  19 | Step:  37 | Val Loss:  0.6133487224578857\n",
      "Epoch:  19 | Step:  38 | Val Loss:  0.588356614112854\n",
      "Epoch:  19 | Step:  39 | Val Loss:  0.5836602449417114\n",
      "Epoch:  19 | Step:  40 | Val Loss:  0.8925184011459351\n",
      "Epoch:  19 | Step:  41 | Val Loss:  0.5928922891616821\n",
      "Epoch:  19 | Step:  42 | Val Loss:  0.5859405994415283\n",
      "Epoch:  19 | Step:  43 | Val Loss:  0.5655410885810852\n",
      "Epoch:  19 | Step:  44 | Val Loss:  0.5601104497909546\n",
      "Epoch:  19 | Step:  45 | Val Loss:  0.6846693158149719\n",
      "Epoch:  19 | Step:  46 | Val Loss:  0.5579935312271118\n",
      "Epoch:  19 | Step:  47 | Val Loss:  0.6087075471878052\n",
      "Epoch:  19 | Step:  48 | Val Loss:  0.46371227502822876\n",
      "Epoch:  19 | Step:  49 | Val Loss:  0.5419566035270691\n",
      "Epoch:  19 | Step:  50 | Val Loss:  0.5961485505104065\n",
      "Epoch:  19 | Step:  51 | Val Loss:  0.7011661529541016\n",
      "Epoch:  19 | Step:  52 | Val Loss:  0.4388854205608368\n",
      "Epoch:  19 | Step:  53 | Val Loss:  0.5248571634292603\n",
      "Epoch:  19 | Step:  54 | Val Loss:  0.592598557472229\n",
      "Epoch:  19 | Step:  55 | Val Loss:  0.6279829740524292\n",
      "Epoch:  19 | Step:  56 | Val Loss:  0.5957382917404175\n",
      "Epoch:  19 | Step:  57 | Val Loss:  0.5415385365486145\n",
      "Epoch:  19 | Step:  58 | Val Loss:  0.6127923130989075\n",
      "Epoch:  19 | Step:  59 | Val Loss:  0.6207607984542847\n",
      "Epoch:  19 | Step:  60 | Val Loss:  0.5729542970657349\n",
      "Epoch:  19 | Step:  61 | Val Loss:  0.48174577951431274\n",
      "Epoch:  19 | Step:  62 | Val Loss:  0.5486835241317749\n",
      "Epoch:  19 | Step:  63 | Val Loss:  0.720111608505249\n",
      "Epoch:  19 | Step:  64 | Val Loss:  0.5917155742645264\n",
      "Epoch:  19 | Step:  65 | Val Loss:  0.4020814299583435\n",
      "Epoch:  19 | Step:  66 | Val Loss:  0.5665677785873413\n",
      "Epoch:  19 | Step:  67 | Val Loss:  0.7629513740539551\n",
      "Epoch:  19 | Step:  68 | Val Loss:  0.6813985109329224\n",
      "Epoch:  19 | Step:  69 | Val Loss:  0.5788544416427612\n",
      "Epoch:  19 | Step:  70 | Val Loss:  0.5943398475646973\n",
      "Epoch:  19 | Step:  71 | Val Loss:  0.5498000383377075\n",
      "Epoch:  19 | Step:  72 | Val Loss:  0.5421878099441528\n",
      "Epoch:  19 | Step:  73 | Val Loss:  0.5749920606613159\n",
      "Epoch:  19 | Step:  74 | Val Loss:  0.6810975074768066\n",
      "Epoch:  19 | Step:  75 | Val Loss:  0.6312136054039001\n",
      "Epoch:  19 | Step:  76 | Val Loss:  0.4469097852706909\n",
      "Epoch:  19 | Step:  77 | Val Loss:  0.5466336607933044\n",
      "Epoch:  19 | Step:  78 | Val Loss:  0.5570361614227295\n",
      "Epoch:  19 | Step:  79 | Val Loss:  0.6943924427032471\n",
      "Epoch:  19 | Step:  80 | Val Loss:  0.7953627705574036\n",
      "Epoch:  19 | Step:  81 | Val Loss:  0.5964236259460449\n",
      "Epoch:  19 | Step:  82 | Val Loss:  0.49966979026794434\n",
      "Epoch:  19 | Step:  83 | Val Loss:  0.5273787975311279\n",
      "Epoch:  19 | Step:  84 | Val Loss:  0.5356149673461914\n",
      "Epoch:  19 | Step:  85 | Val Loss:  0.45593273639678955\n",
      "Epoch:  19 | Step:  86 | Val Loss:  0.6410009860992432\n",
      "Epoch:  19 | Step:  87 | Val Loss:  0.6226416230201721\n",
      "Epoch:  19 | Step:  88 | Val Loss:  0.6829091310501099\n",
      "Epoch:  19 | Step:  89 | Val Loss:  0.6038036346435547\n",
      "Epoch:  19 | Step:  90 | Val Loss:  0.4227752685546875\n",
      "Epoch:  19 | Step:  91 | Val Loss:  0.7051573991775513\n",
      "Epoch:  19 | Step:  92 | Val Loss:  0.5726892948150635\n",
      "Epoch:  19 | Step:  93 | Val Loss:  0.6060316562652588\n",
      "Epoch:  19 | Step:  94 | Val Loss:  0.6051385998725891\n",
      "Epoch:  19 | Step:  95 | Val Loss:  0.5873856544494629\n",
      "Epoch:  19 | Step:  96 | Val Loss:  0.48057836294174194\n",
      "Epoch:  19 | Step:  97 | Val Loss:  0.6848866939544678\n",
      "Epoch:  19 | Step:  98 | Val Loss:  0.5033336877822876\n",
      "Epoch:  19 | Step:  99 | Val Loss:  0.7746590971946716\n",
      "Epoch:  19 | Step:  100 | Val Loss:  0.6616925597190857\n",
      "Epoch:  19 | Step:  101 | Val Loss:  0.5734807252883911\n",
      "Epoch:  19 | Step:  102 | Val Loss:  0.5296146869659424\n",
      "Epoch:  19 | Step:  103 | Val Loss:  0.5628111958503723\n",
      "Epoch:  19 | Step:  104 | Val Loss:  0.6408084630966187\n",
      "Epoch:  19 | Step:  105 | Val Loss:  0.6342968940734863\n",
      "Epoch:  19 | Step:  106 | Val Loss:  0.5562913417816162\n",
      "Epoch:  19 | Step:  107 | Val Loss:  0.8403153419494629\n",
      "Epoch:  19 | Step:  108 | Val Loss:  0.5631974339485168\n",
      "Epoch:  19 | Step:  109 | Val Loss:  0.5565651655197144\n",
      "Epoch:  19 | Step:  110 | Val Loss:  0.7622160315513611\n",
      "Epoch:  19 | Step:  111 | Val Loss:  0.44654130935668945\n",
      "Epoch:  19 | Step:  112 | Val Loss:  0.49013662338256836\n",
      "Epoch:  19 | Step:  113 | Val Loss:  0.6494896411895752\n",
      "Epoch:  19 | Step:  114 | Val Loss:  0.6198259592056274\n",
      "Epoch:  19 | Step:  115 | Val Loss:  0.5117563605308533\n",
      "Epoch:  19 | Step:  116 | Val Loss:  0.4428391754627228\n",
      "Epoch:  19 | Step:  117 | Val Loss:  0.4622245728969574\n",
      "Epoch:  19 | Step:  118 | Val Loss:  0.6183768510818481\n",
      "Epoch:  19 | Step:  119 | Val Loss:  0.850753128528595\n",
      "Epoch:  19 | Step:  120 | Val Loss:  0.7395517826080322\n",
      "Epoch:  19 | Step:  121 | Val Loss:  0.6857181787490845\n",
      "Epoch:  19 | Step:  122 | Val Loss:  0.5643504858016968\n",
      "Epoch:  19 | Step:  123 | Val Loss:  0.4289501905441284\n",
      "Epoch:  19 | Step:  124 | Val Loss:  0.5226635932922363\n",
      "Epoch:  19 | Step:  125 | Val Loss:  0.29020315408706665\n",
      "Epoch:  19 | Train Loss:  tensor(0.5452, device='cuda:0') | Val Loss:  tensor(0.5835, device='cuda:0')\n",
      "Epoch:  20 | Step:  500 | Train Loss:  0.28571414947509766\n",
      "Epoch:  20 | Step:  1 | Val Loss:  0.6409001350402832\n",
      "Epoch:  20 | Step:  2 | Val Loss:  0.23802560567855835\n",
      "Epoch:  20 | Step:  3 | Val Loss:  0.5781576633453369\n",
      "Epoch:  20 | Step:  4 | Val Loss:  0.7062591910362244\n",
      "Epoch:  20 | Step:  5 | Val Loss:  0.8301408290863037\n",
      "Epoch:  20 | Step:  6 | Val Loss:  0.5730909109115601\n",
      "Epoch:  20 | Step:  7 | Val Loss:  0.6688954830169678\n",
      "Epoch:  20 | Step:  8 | Val Loss:  0.7579689025878906\n",
      "Epoch:  20 | Step:  9 | Val Loss:  0.5096724629402161\n",
      "Epoch:  20 | Step:  10 | Val Loss:  0.7557012438774109\n",
      "Epoch:  20 | Step:  11 | Val Loss:  0.6894509792327881\n",
      "Epoch:  20 | Step:  12 | Val Loss:  0.7090225219726562\n",
      "Epoch:  20 | Step:  13 | Val Loss:  0.6966056823730469\n",
      "Epoch:  20 | Step:  14 | Val Loss:  0.6100028157234192\n",
      "Epoch:  20 | Step:  15 | Val Loss:  0.44667813181877136\n",
      "Epoch:  20 | Step:  16 | Val Loss:  0.4560398459434509\n",
      "Epoch:  20 | Step:  17 | Val Loss:  0.7007570862770081\n",
      "Epoch:  20 | Step:  18 | Val Loss:  0.6509116888046265\n",
      "Epoch:  20 | Step:  19 | Val Loss:  0.5217071771621704\n",
      "Epoch:  20 | Step:  20 | Val Loss:  0.8874609470367432\n",
      "Epoch:  20 | Step:  21 | Val Loss:  0.7261667847633362\n",
      "Epoch:  20 | Step:  22 | Val Loss:  0.4635482728481293\n",
      "Epoch:  20 | Step:  23 | Val Loss:  0.6784763932228088\n",
      "Epoch:  20 | Step:  24 | Val Loss:  0.64427250623703\n",
      "Epoch:  20 | Step:  25 | Val Loss:  0.4610806107521057\n",
      "Epoch:  20 | Step:  26 | Val Loss:  0.5395896434783936\n",
      "Epoch:  20 | Step:  27 | Val Loss:  0.5918165445327759\n",
      "Epoch:  20 | Step:  28 | Val Loss:  0.7390424013137817\n",
      "Epoch:  20 | Step:  29 | Val Loss:  0.46303558349609375\n",
      "Epoch:  20 | Step:  30 | Val Loss:  0.5889180302619934\n",
      "Epoch:  20 | Step:  31 | Val Loss:  0.7324692606925964\n",
      "Epoch:  20 | Step:  32 | Val Loss:  0.39598384499549866\n",
      "Epoch:  20 | Step:  33 | Val Loss:  0.6912976503372192\n",
      "Epoch:  20 | Step:  34 | Val Loss:  0.5506635904312134\n",
      "Epoch:  20 | Step:  35 | Val Loss:  0.8020651936531067\n",
      "Epoch:  20 | Step:  36 | Val Loss:  0.44929251074790955\n",
      "Epoch:  20 | Step:  37 | Val Loss:  0.753788948059082\n",
      "Epoch:  20 | Step:  38 | Val Loss:  0.473160982131958\n",
      "Epoch:  20 | Step:  39 | Val Loss:  0.42910563945770264\n",
      "Epoch:  20 | Step:  40 | Val Loss:  0.6017833352088928\n",
      "Epoch:  20 | Step:  41 | Val Loss:  0.46244508028030396\n",
      "Epoch:  20 | Step:  42 | Val Loss:  0.30699416995048523\n",
      "Epoch:  20 | Step:  43 | Val Loss:  0.6189063787460327\n",
      "Epoch:  20 | Step:  44 | Val Loss:  0.4147173762321472\n",
      "Epoch:  20 | Step:  45 | Val Loss:  0.5838229656219482\n",
      "Epoch:  20 | Step:  46 | Val Loss:  0.6075661182403564\n",
      "Epoch:  20 | Step:  47 | Val Loss:  0.5800925493240356\n",
      "Epoch:  20 | Step:  48 | Val Loss:  0.5881984233856201\n",
      "Epoch:  20 | Step:  49 | Val Loss:  0.5009000897407532\n",
      "Epoch:  20 | Step:  50 | Val Loss:  0.5413066148757935\n",
      "Epoch:  20 | Step:  51 | Val Loss:  0.5176253318786621\n",
      "Epoch:  20 | Step:  52 | Val Loss:  0.5480438470840454\n",
      "Epoch:  20 | Step:  53 | Val Loss:  0.7411952614784241\n",
      "Epoch:  20 | Step:  54 | Val Loss:  0.4437417984008789\n",
      "Epoch:  20 | Step:  55 | Val Loss:  0.43443211913108826\n",
      "Epoch:  20 | Step:  56 | Val Loss:  0.5255146026611328\n",
      "Epoch:  20 | Step:  57 | Val Loss:  0.4874460995197296\n",
      "Epoch:  20 | Step:  58 | Val Loss:  0.6128189563751221\n",
      "Epoch:  20 | Step:  59 | Val Loss:  0.47043895721435547\n",
      "Epoch:  20 | Step:  60 | Val Loss:  0.5286306142807007\n",
      "Epoch:  20 | Step:  61 | Val Loss:  0.5489312410354614\n",
      "Epoch:  20 | Step:  62 | Val Loss:  0.4375320076942444\n",
      "Epoch:  20 | Step:  63 | Val Loss:  0.6505114436149597\n",
      "Epoch:  20 | Step:  64 | Val Loss:  0.5474399328231812\n",
      "Epoch:  20 | Step:  65 | Val Loss:  0.7010172605514526\n",
      "Epoch:  20 | Step:  66 | Val Loss:  0.6822869181632996\n",
      "Epoch:  20 | Step:  67 | Val Loss:  0.5954665541648865\n",
      "Epoch:  20 | Step:  68 | Val Loss:  0.6446155309677124\n",
      "Epoch:  20 | Step:  69 | Val Loss:  0.37786805629730225\n",
      "Epoch:  20 | Step:  70 | Val Loss:  0.8930875658988953\n",
      "Epoch:  20 | Step:  71 | Val Loss:  0.5813186764717102\n",
      "Epoch:  20 | Step:  72 | Val Loss:  0.5573446154594421\n",
      "Epoch:  20 | Step:  73 | Val Loss:  0.7090436220169067\n",
      "Epoch:  20 | Step:  74 | Val Loss:  0.6060012578964233\n",
      "Epoch:  20 | Step:  75 | Val Loss:  0.5684952139854431\n",
      "Epoch:  20 | Step:  76 | Val Loss:  0.6407145261764526\n",
      "Epoch:  20 | Step:  77 | Val Loss:  0.5427718162536621\n",
      "Epoch:  20 | Step:  78 | Val Loss:  0.5163203477859497\n",
      "Epoch:  20 | Step:  79 | Val Loss:  0.4356048107147217\n",
      "Epoch:  20 | Step:  80 | Val Loss:  0.5557500123977661\n",
      "Epoch:  20 | Step:  81 | Val Loss:  0.3599405288696289\n",
      "Epoch:  20 | Step:  82 | Val Loss:  0.6475502252578735\n",
      "Epoch:  20 | Step:  83 | Val Loss:  0.5307408571243286\n",
      "Epoch:  20 | Step:  84 | Val Loss:  0.7031279802322388\n",
      "Epoch:  20 | Step:  85 | Val Loss:  0.5586511492729187\n",
      "Epoch:  20 | Step:  86 | Val Loss:  0.45399871468544006\n",
      "Epoch:  20 | Step:  87 | Val Loss:  0.5251748561859131\n",
      "Epoch:  20 | Step:  88 | Val Loss:  0.6624674797058105\n",
      "Epoch:  20 | Step:  89 | Val Loss:  0.5661373734474182\n",
      "Epoch:  20 | Step:  90 | Val Loss:  0.5518341064453125\n",
      "Epoch:  20 | Step:  91 | Val Loss:  0.5015660524368286\n",
      "Epoch:  20 | Step:  92 | Val Loss:  0.707221269607544\n",
      "Epoch:  20 | Step:  93 | Val Loss:  0.5716980695724487\n",
      "Epoch:  20 | Step:  94 | Val Loss:  0.7414641976356506\n",
      "Epoch:  20 | Step:  95 | Val Loss:  0.395294189453125\n",
      "Epoch:  20 | Step:  96 | Val Loss:  0.5324310064315796\n",
      "Epoch:  20 | Step:  97 | Val Loss:  0.6424551010131836\n",
      "Epoch:  20 | Step:  98 | Val Loss:  0.5318783521652222\n",
      "Epoch:  20 | Step:  99 | Val Loss:  0.6124640703201294\n",
      "Epoch:  20 | Step:  100 | Val Loss:  0.38805004954338074\n",
      "Epoch:  20 | Step:  101 | Val Loss:  0.8191992044448853\n",
      "Epoch:  20 | Step:  102 | Val Loss:  0.5154755115509033\n",
      "Epoch:  20 | Step:  103 | Val Loss:  0.3620475232601166\n",
      "Epoch:  20 | Step:  104 | Val Loss:  0.32947981357574463\n",
      "Epoch:  20 | Step:  105 | Val Loss:  0.4593467712402344\n",
      "Epoch:  20 | Step:  106 | Val Loss:  0.31168413162231445\n",
      "Epoch:  20 | Step:  107 | Val Loss:  0.6719538569450378\n",
      "Epoch:  20 | Step:  108 | Val Loss:  0.7521919012069702\n",
      "Epoch:  20 | Step:  109 | Val Loss:  0.4355763792991638\n",
      "Epoch:  20 | Step:  110 | Val Loss:  0.5130553245544434\n",
      "Epoch:  20 | Step:  111 | Val Loss:  0.7212303876876831\n",
      "Epoch:  20 | Step:  112 | Val Loss:  0.5608765482902527\n",
      "Epoch:  20 | Step:  113 | Val Loss:  0.8759641647338867\n",
      "Epoch:  20 | Step:  114 | Val Loss:  0.8868613839149475\n",
      "Epoch:  20 | Step:  115 | Val Loss:  0.6518520712852478\n",
      "Epoch:  20 | Step:  116 | Val Loss:  0.4854840934276581\n",
      "Epoch:  20 | Step:  117 | Val Loss:  0.4831377863883972\n",
      "Epoch:  20 | Step:  118 | Val Loss:  0.4923698902130127\n",
      "Epoch:  20 | Step:  119 | Val Loss:  0.7021405696868896\n",
      "Epoch:  20 | Step:  120 | Val Loss:  0.6574187874794006\n",
      "Epoch:  20 | Step:  121 | Val Loss:  0.5145450830459595\n",
      "Epoch:  20 | Step:  122 | Val Loss:  0.6310152411460876\n",
      "Epoch:  20 | Step:  123 | Val Loss:  0.5638468861579895\n",
      "Epoch:  20 | Step:  124 | Val Loss:  0.6525769233703613\n",
      "Epoch:  20 | Step:  125 | Val Loss:  0.6428005695343018\n",
      "Epoch:  20 | Train Loss:  tensor(0.5432, device='cuda:0') | Val Loss:  tensor(0.5807, device='cuda:0')\n",
      "Epoch:  21 | Step:  500 | Train Loss:  0.6653469800949097\n",
      "Epoch:  21 | Step:  1 | Val Loss:  0.7287631034851074\n",
      "Epoch:  21 | Step:  2 | Val Loss:  0.6230685710906982\n",
      "Epoch:  21 | Step:  3 | Val Loss:  0.5522582530975342\n",
      "Epoch:  21 | Step:  4 | Val Loss:  0.6388370990753174\n",
      "Epoch:  21 | Step:  5 | Val Loss:  0.43758127093315125\n",
      "Epoch:  21 | Step:  6 | Val Loss:  0.6068503260612488\n",
      "Epoch:  21 | Step:  7 | Val Loss:  0.4417451024055481\n",
      "Epoch:  21 | Step:  8 | Val Loss:  0.7267553210258484\n",
      "Epoch:  21 | Step:  9 | Val Loss:  0.4072979986667633\n",
      "Epoch:  21 | Step:  10 | Val Loss:  0.5803236961364746\n",
      "Epoch:  21 | Step:  11 | Val Loss:  0.5911084413528442\n",
      "Epoch:  21 | Step:  12 | Val Loss:  0.5564330816268921\n",
      "Epoch:  21 | Step:  13 | Val Loss:  0.5984808206558228\n",
      "Epoch:  21 | Step:  14 | Val Loss:  0.5514193177223206\n",
      "Epoch:  21 | Step:  15 | Val Loss:  0.5507131814956665\n",
      "Epoch:  21 | Step:  16 | Val Loss:  0.5966675877571106\n",
      "Epoch:  21 | Step:  17 | Val Loss:  0.6315007209777832\n",
      "Epoch:  21 | Step:  18 | Val Loss:  0.3396846652030945\n",
      "Epoch:  21 | Step:  19 | Val Loss:  0.6851297616958618\n",
      "Epoch:  21 | Step:  20 | Val Loss:  0.5367177128791809\n",
      "Epoch:  21 | Step:  21 | Val Loss:  0.560562014579773\n",
      "Epoch:  21 | Step:  22 | Val Loss:  0.5239590406417847\n",
      "Epoch:  21 | Step:  23 | Val Loss:  0.6527654528617859\n",
      "Epoch:  21 | Step:  24 | Val Loss:  0.6048524379730225\n",
      "Epoch:  21 | Step:  25 | Val Loss:  0.40945714712142944\n",
      "Epoch:  21 | Step:  26 | Val Loss:  0.808005690574646\n",
      "Epoch:  21 | Step:  27 | Val Loss:  0.7483042478561401\n",
      "Epoch:  21 | Step:  28 | Val Loss:  0.6689371466636658\n",
      "Epoch:  21 | Step:  29 | Val Loss:  0.5449609756469727\n",
      "Epoch:  21 | Step:  30 | Val Loss:  0.5931211709976196\n",
      "Epoch:  21 | Step:  31 | Val Loss:  0.35355132818222046\n",
      "Epoch:  21 | Step:  32 | Val Loss:  0.6220372915267944\n",
      "Epoch:  21 | Step:  33 | Val Loss:  0.7156323194503784\n",
      "Epoch:  21 | Step:  34 | Val Loss:  0.6509886980056763\n",
      "Epoch:  21 | Step:  35 | Val Loss:  0.6709722876548767\n",
      "Epoch:  21 | Step:  36 | Val Loss:  0.6529990434646606\n",
      "Epoch:  21 | Step:  37 | Val Loss:  0.5583128929138184\n",
      "Epoch:  21 | Step:  38 | Val Loss:  0.7526507377624512\n",
      "Epoch:  21 | Step:  39 | Val Loss:  0.6518439054489136\n",
      "Epoch:  21 | Step:  40 | Val Loss:  0.7906314730644226\n",
      "Epoch:  21 | Step:  41 | Val Loss:  0.523516833782196\n",
      "Epoch:  21 | Step:  42 | Val Loss:  0.5906010866165161\n",
      "Epoch:  21 | Step:  43 | Val Loss:  0.6426733136177063\n",
      "Epoch:  21 | Step:  44 | Val Loss:  0.46916913986206055\n",
      "Epoch:  21 | Step:  45 | Val Loss:  0.5644006133079529\n",
      "Epoch:  21 | Step:  46 | Val Loss:  0.6196737289428711\n",
      "Epoch:  21 | Step:  47 | Val Loss:  0.5577119588851929\n",
      "Epoch:  21 | Step:  48 | Val Loss:  0.4866999387741089\n",
      "Epoch:  21 | Step:  49 | Val Loss:  0.7333110570907593\n",
      "Epoch:  21 | Step:  50 | Val Loss:  0.5544008016586304\n",
      "Epoch:  21 | Step:  51 | Val Loss:  0.7273974418640137\n",
      "Epoch:  21 | Step:  52 | Val Loss:  0.5067535638809204\n",
      "Epoch:  21 | Step:  53 | Val Loss:  0.5878776907920837\n",
      "Epoch:  21 | Step:  54 | Val Loss:  0.5817140340805054\n",
      "Epoch:  21 | Step:  55 | Val Loss:  0.8360934853553772\n",
      "Epoch:  21 | Step:  56 | Val Loss:  0.39795565605163574\n",
      "Epoch:  21 | Step:  57 | Val Loss:  0.8269973993301392\n",
      "Epoch:  21 | Step:  58 | Val Loss:  0.580247700214386\n",
      "Epoch:  21 | Step:  59 | Val Loss:  0.5370211005210876\n",
      "Epoch:  21 | Step:  60 | Val Loss:  0.5238176584243774\n",
      "Epoch:  21 | Step:  61 | Val Loss:  0.5620842576026917\n",
      "Epoch:  21 | Step:  62 | Val Loss:  0.6853140592575073\n",
      "Epoch:  21 | Step:  63 | Val Loss:  0.6298272609710693\n",
      "Epoch:  21 | Step:  64 | Val Loss:  0.5280009508132935\n",
      "Epoch:  21 | Step:  65 | Val Loss:  0.4752817153930664\n",
      "Epoch:  21 | Step:  66 | Val Loss:  0.7942745685577393\n",
      "Epoch:  21 | Step:  67 | Val Loss:  0.5358446836471558\n",
      "Epoch:  21 | Step:  68 | Val Loss:  0.5960773229598999\n",
      "Epoch:  21 | Step:  69 | Val Loss:  0.47885051369667053\n",
      "Epoch:  21 | Step:  70 | Val Loss:  0.6396827697753906\n",
      "Epoch:  21 | Step:  71 | Val Loss:  0.5680842995643616\n",
      "Epoch:  21 | Step:  72 | Val Loss:  0.5159050822257996\n",
      "Epoch:  21 | Step:  73 | Val Loss:  0.423179566860199\n",
      "Epoch:  21 | Step:  74 | Val Loss:  0.3980066180229187\n",
      "Epoch:  21 | Step:  75 | Val Loss:  0.6265247464179993\n",
      "Epoch:  21 | Step:  76 | Val Loss:  0.4720316529273987\n",
      "Epoch:  21 | Step:  77 | Val Loss:  0.5335471630096436\n",
      "Epoch:  21 | Step:  78 | Val Loss:  0.6245365142822266\n",
      "Epoch:  21 | Step:  79 | Val Loss:  0.720494270324707\n",
      "Epoch:  21 | Step:  80 | Val Loss:  0.4472498595714569\n",
      "Epoch:  21 | Step:  81 | Val Loss:  0.30508142709732056\n",
      "Epoch:  21 | Step:  82 | Val Loss:  0.38207054138183594\n",
      "Epoch:  21 | Step:  83 | Val Loss:  0.616435706615448\n",
      "Epoch:  21 | Step:  84 | Val Loss:  0.8417917490005493\n",
      "Epoch:  21 | Step:  85 | Val Loss:  0.41900813579559326\n",
      "Epoch:  21 | Step:  86 | Val Loss:  0.511719286441803\n",
      "Epoch:  21 | Step:  87 | Val Loss:  0.6326507329940796\n",
      "Epoch:  21 | Step:  88 | Val Loss:  0.43039393424987793\n",
      "Epoch:  21 | Step:  89 | Val Loss:  0.5264726281166077\n",
      "Epoch:  21 | Step:  90 | Val Loss:  0.4376915693283081\n",
      "Epoch:  21 | Step:  91 | Val Loss:  0.4773828685283661\n",
      "Epoch:  21 | Step:  92 | Val Loss:  0.5557767748832703\n",
      "Epoch:  21 | Step:  93 | Val Loss:  0.6527210474014282\n",
      "Epoch:  21 | Step:  94 | Val Loss:  0.5828979015350342\n",
      "Epoch:  21 | Step:  95 | Val Loss:  0.4776657223701477\n",
      "Epoch:  21 | Step:  96 | Val Loss:  0.6433690786361694\n",
      "Epoch:  21 | Step:  97 | Val Loss:  0.39364463090896606\n",
      "Epoch:  21 | Step:  98 | Val Loss:  0.7427047491073608\n",
      "Epoch:  21 | Step:  99 | Val Loss:  0.594201922416687\n",
      "Epoch:  21 | Step:  100 | Val Loss:  0.5366249680519104\n",
      "Epoch:  21 | Step:  101 | Val Loss:  0.6168555617332458\n",
      "Epoch:  21 | Step:  102 | Val Loss:  0.72456955909729\n",
      "Epoch:  21 | Step:  103 | Val Loss:  0.5346506834030151\n",
      "Epoch:  21 | Step:  104 | Val Loss:  0.6307806968688965\n",
      "Epoch:  21 | Step:  105 | Val Loss:  0.42396456003189087\n",
      "Epoch:  21 | Step:  106 | Val Loss:  0.546043872833252\n",
      "Epoch:  21 | Step:  107 | Val Loss:  0.5064600110054016\n",
      "Epoch:  21 | Step:  108 | Val Loss:  0.4582710862159729\n",
      "Epoch:  21 | Step:  109 | Val Loss:  0.6332664489746094\n",
      "Epoch:  21 | Step:  110 | Val Loss:  0.5971028804779053\n",
      "Epoch:  21 | Step:  111 | Val Loss:  0.6173357963562012\n",
      "Epoch:  21 | Step:  112 | Val Loss:  0.5783933997154236\n",
      "Epoch:  21 | Step:  113 | Val Loss:  0.3228399157524109\n",
      "Epoch:  21 | Step:  114 | Val Loss:  0.49986469745635986\n",
      "Epoch:  21 | Step:  115 | Val Loss:  0.6011831760406494\n",
      "Epoch:  21 | Step:  116 | Val Loss:  0.5785232782363892\n",
      "Epoch:  21 | Step:  117 | Val Loss:  0.6875718832015991\n",
      "Epoch:  21 | Step:  118 | Val Loss:  0.47893276810646057\n",
      "Epoch:  21 | Step:  119 | Val Loss:  0.5084083080291748\n",
      "Epoch:  21 | Step:  120 | Val Loss:  0.6072946190834045\n",
      "Epoch:  21 | Step:  121 | Val Loss:  0.6898635625839233\n",
      "Epoch:  21 | Step:  122 | Val Loss:  0.8528347611427307\n",
      "Epoch:  21 | Step:  123 | Val Loss:  0.5611152052879333\n",
      "Epoch:  21 | Step:  124 | Val Loss:  0.5849793553352356\n",
      "Epoch:  21 | Step:  125 | Val Loss:  0.6077288389205933\n",
      "Epoch:  21 | Train Loss:  tensor(0.5414, device='cuda:0') | Val Loss:  tensor(0.5778, device='cuda:0')\n",
      "Epoch:  22 | Step:  500 | Train Loss:  0.7164795398712158\n",
      "Epoch:  22 | Step:  1 | Val Loss:  0.5394189357757568\n",
      "Epoch:  22 | Step:  2 | Val Loss:  0.43538105487823486\n",
      "Epoch:  22 | Step:  3 | Val Loss:  0.45813947916030884\n",
      "Epoch:  22 | Step:  4 | Val Loss:  0.6588711738586426\n",
      "Epoch:  22 | Step:  5 | Val Loss:  0.5875099897384644\n",
      "Epoch:  22 | Step:  6 | Val Loss:  0.5731043815612793\n",
      "Epoch:  22 | Step:  7 | Val Loss:  0.6058012247085571\n",
      "Epoch:  22 | Step:  8 | Val Loss:  0.4043167531490326\n",
      "Epoch:  22 | Step:  9 | Val Loss:  0.7749156951904297\n",
      "Epoch:  22 | Step:  10 | Val Loss:  0.47519779205322266\n",
      "Epoch:  22 | Step:  11 | Val Loss:  0.41358083486557007\n",
      "Epoch:  22 | Step:  12 | Val Loss:  0.48806723952293396\n",
      "Epoch:  22 | Step:  13 | Val Loss:  0.7557799816131592\n",
      "Epoch:  22 | Step:  14 | Val Loss:  0.6319337487220764\n",
      "Epoch:  22 | Step:  15 | Val Loss:  0.5849413871765137\n",
      "Epoch:  22 | Step:  16 | Val Loss:  0.547468900680542\n",
      "Epoch:  22 | Step:  17 | Val Loss:  0.7512354850769043\n",
      "Epoch:  22 | Step:  18 | Val Loss:  0.6235343813896179\n",
      "Epoch:  22 | Step:  19 | Val Loss:  0.394497275352478\n",
      "Epoch:  22 | Step:  20 | Val Loss:  0.5860151052474976\n",
      "Epoch:  22 | Step:  21 | Val Loss:  0.4120868444442749\n",
      "Epoch:  22 | Step:  22 | Val Loss:  0.6578887701034546\n",
      "Epoch:  22 | Step:  23 | Val Loss:  0.728081226348877\n",
      "Epoch:  22 | Step:  24 | Val Loss:  0.3899933695793152\n",
      "Epoch:  22 | Step:  25 | Val Loss:  0.5497058629989624\n",
      "Epoch:  22 | Step:  26 | Val Loss:  0.7136564254760742\n",
      "Epoch:  22 | Step:  27 | Val Loss:  0.46344053745269775\n",
      "Epoch:  22 | Step:  28 | Val Loss:  0.47407740354537964\n",
      "Epoch:  22 | Step:  29 | Val Loss:  0.7711567878723145\n",
      "Epoch:  22 | Step:  30 | Val Loss:  0.5184025764465332\n",
      "Epoch:  22 | Step:  31 | Val Loss:  0.5438796281814575\n",
      "Epoch:  22 | Step:  32 | Val Loss:  0.6252138018608093\n",
      "Epoch:  22 | Step:  33 | Val Loss:  0.6576352119445801\n",
      "Epoch:  22 | Step:  34 | Val Loss:  0.7321692109107971\n",
      "Epoch:  22 | Step:  35 | Val Loss:  0.6275116205215454\n",
      "Epoch:  22 | Step:  36 | Val Loss:  0.6389007568359375\n",
      "Epoch:  22 | Step:  37 | Val Loss:  0.4593254625797272\n",
      "Epoch:  22 | Step:  38 | Val Loss:  0.40965473651885986\n",
      "Epoch:  22 | Step:  39 | Val Loss:  0.5109411478042603\n",
      "Epoch:  22 | Step:  40 | Val Loss:  0.7127872705459595\n",
      "Epoch:  22 | Step:  41 | Val Loss:  0.5798242092132568\n",
      "Epoch:  22 | Step:  42 | Val Loss:  0.6202058792114258\n",
      "Epoch:  22 | Step:  43 | Val Loss:  0.5208114385604858\n",
      "Epoch:  22 | Step:  44 | Val Loss:  0.5302497744560242\n",
      "Epoch:  22 | Step:  45 | Val Loss:  0.49389833211898804\n",
      "Epoch:  22 | Step:  46 | Val Loss:  0.6240589022636414\n",
      "Epoch:  22 | Step:  47 | Val Loss:  0.47610199451446533\n",
      "Epoch:  22 | Step:  48 | Val Loss:  0.7684934139251709\n",
      "Epoch:  22 | Step:  49 | Val Loss:  0.44986093044281006\n",
      "Epoch:  22 | Step:  50 | Val Loss:  0.4904797673225403\n",
      "Epoch:  22 | Step:  51 | Val Loss:  0.5161418914794922\n",
      "Epoch:  22 | Step:  52 | Val Loss:  0.37806135416030884\n",
      "Epoch:  22 | Step:  53 | Val Loss:  0.43173933029174805\n",
      "Epoch:  22 | Step:  54 | Val Loss:  0.6108171939849854\n",
      "Epoch:  22 | Step:  55 | Val Loss:  0.5550152063369751\n",
      "Epoch:  22 | Step:  56 | Val Loss:  0.6477029323577881\n",
      "Epoch:  22 | Step:  57 | Val Loss:  0.4250974655151367\n",
      "Epoch:  22 | Step:  58 | Val Loss:  0.6210355758666992\n",
      "Epoch:  22 | Step:  59 | Val Loss:  0.6554079055786133\n",
      "Epoch:  22 | Step:  60 | Val Loss:  0.6041374802589417\n",
      "Epoch:  22 | Step:  61 | Val Loss:  0.63680100440979\n",
      "Epoch:  22 | Step:  62 | Val Loss:  0.6684839725494385\n",
      "Epoch:  22 | Step:  63 | Val Loss:  0.5104097127914429\n",
      "Epoch:  22 | Step:  64 | Val Loss:  0.5187417268753052\n",
      "Epoch:  22 | Step:  65 | Val Loss:  0.42195117473602295\n",
      "Epoch:  22 | Step:  66 | Val Loss:  0.613320529460907\n",
      "Epoch:  22 | Step:  67 | Val Loss:  0.5839029550552368\n",
      "Epoch:  22 | Step:  68 | Val Loss:  0.637551486492157\n",
      "Epoch:  22 | Step:  69 | Val Loss:  0.6206477880477905\n",
      "Epoch:  22 | Step:  70 | Val Loss:  0.6894649267196655\n",
      "Epoch:  22 | Step:  71 | Val Loss:  0.5553039312362671\n",
      "Epoch:  22 | Step:  72 | Val Loss:  0.4922283887863159\n",
      "Epoch:  22 | Step:  73 | Val Loss:  0.6692352294921875\n",
      "Epoch:  22 | Step:  74 | Val Loss:  0.4291820526123047\n",
      "Epoch:  22 | Step:  75 | Val Loss:  0.4200654625892639\n",
      "Epoch:  22 | Step:  76 | Val Loss:  0.4782688021659851\n",
      "Epoch:  22 | Step:  77 | Val Loss:  0.648827075958252\n",
      "Epoch:  22 | Step:  78 | Val Loss:  0.5591915249824524\n",
      "Epoch:  22 | Step:  79 | Val Loss:  0.5919947624206543\n",
      "Epoch:  22 | Step:  80 | Val Loss:  0.6537055373191833\n",
      "Epoch:  22 | Step:  81 | Val Loss:  0.4346109628677368\n",
      "Epoch:  22 | Step:  82 | Val Loss:  0.5483732223510742\n",
      "Epoch:  22 | Step:  83 | Val Loss:  0.72423255443573\n",
      "Epoch:  22 | Step:  84 | Val Loss:  0.51829993724823\n",
      "Epoch:  22 | Step:  85 | Val Loss:  0.9406888484954834\n",
      "Epoch:  22 | Step:  86 | Val Loss:  0.6229866743087769\n",
      "Epoch:  22 | Step:  87 | Val Loss:  0.5652039647102356\n",
      "Epoch:  22 | Step:  88 | Val Loss:  0.5701825618743896\n",
      "Epoch:  22 | Step:  89 | Val Loss:  0.7881479263305664\n",
      "Epoch:  22 | Step:  90 | Val Loss:  0.5439629554748535\n",
      "Epoch:  22 | Step:  91 | Val Loss:  0.5482957363128662\n",
      "Epoch:  22 | Step:  92 | Val Loss:  0.5767616033554077\n",
      "Epoch:  22 | Step:  93 | Val Loss:  0.34318825602531433\n",
      "Epoch:  22 | Step:  94 | Val Loss:  0.45660239458084106\n",
      "Epoch:  22 | Step:  95 | Val Loss:  0.5852091312408447\n",
      "Epoch:  22 | Step:  96 | Val Loss:  0.5290788412094116\n",
      "Epoch:  22 | Step:  97 | Val Loss:  0.6640805006027222\n",
      "Epoch:  22 | Step:  98 | Val Loss:  0.7059537172317505\n",
      "Epoch:  22 | Step:  99 | Val Loss:  0.6420063972473145\n",
      "Epoch:  22 | Step:  100 | Val Loss:  0.5963466763496399\n",
      "Epoch:  22 | Step:  101 | Val Loss:  0.398955762386322\n",
      "Epoch:  22 | Step:  102 | Val Loss:  0.8200643062591553\n",
      "Epoch:  22 | Step:  103 | Val Loss:  0.7529224157333374\n",
      "Epoch:  22 | Step:  104 | Val Loss:  0.6108457446098328\n",
      "Epoch:  22 | Step:  105 | Val Loss:  0.45547330379486084\n",
      "Epoch:  22 | Step:  106 | Val Loss:  0.5341128706932068\n",
      "Epoch:  22 | Step:  107 | Val Loss:  0.47295576333999634\n",
      "Epoch:  22 | Step:  108 | Val Loss:  0.7803068161010742\n",
      "Epoch:  22 | Step:  109 | Val Loss:  0.7669356465339661\n",
      "Epoch:  22 | Step:  110 | Val Loss:  0.4519718885421753\n",
      "Epoch:  22 | Step:  111 | Val Loss:  0.5414588451385498\n",
      "Epoch:  22 | Step:  112 | Val Loss:  0.40870633721351624\n",
      "Epoch:  22 | Step:  113 | Val Loss:  0.697797417640686\n",
      "Epoch:  22 | Step:  114 | Val Loss:  0.5165567994117737\n",
      "Epoch:  22 | Step:  115 | Val Loss:  0.5279141664505005\n",
      "Epoch:  22 | Step:  116 | Val Loss:  0.618293285369873\n",
      "Epoch:  22 | Step:  117 | Val Loss:  0.6777925491333008\n",
      "Epoch:  22 | Step:  118 | Val Loss:  0.6862579584121704\n",
      "Epoch:  22 | Step:  119 | Val Loss:  0.7070461511611938\n",
      "Epoch:  22 | Step:  120 | Val Loss:  0.7797926664352417\n",
      "Epoch:  22 | Step:  121 | Val Loss:  0.44155246019363403\n",
      "Epoch:  22 | Step:  122 | Val Loss:  0.49433884024620056\n",
      "Epoch:  22 | Step:  123 | Val Loss:  0.4916561543941498\n",
      "Epoch:  22 | Step:  124 | Val Loss:  0.581647515296936\n",
      "Epoch:  22 | Step:  125 | Val Loss:  0.560861349105835\n",
      "Epoch:  22 | Train Loss:  tensor(0.5397, device='cuda:0') | Val Loss:  tensor(0.5757, device='cuda:0')\n",
      "Epoch:  23 | Step:  500 | Train Loss:  0.3916294574737549\n",
      "Epoch:  23 | Step:  1 | Val Loss:  0.5865058898925781\n",
      "Epoch:  23 | Step:  2 | Val Loss:  0.5123450756072998\n",
      "Epoch:  23 | Step:  3 | Val Loss:  0.5693860054016113\n",
      "Epoch:  23 | Step:  4 | Val Loss:  0.5605526566505432\n",
      "Epoch:  23 | Step:  5 | Val Loss:  0.6340141892433167\n",
      "Epoch:  23 | Step:  6 | Val Loss:  0.7673791646957397\n",
      "Epoch:  23 | Step:  7 | Val Loss:  0.4975624680519104\n",
      "Epoch:  23 | Step:  8 | Val Loss:  0.615721583366394\n",
      "Epoch:  23 | Step:  9 | Val Loss:  0.42944443225860596\n",
      "Epoch:  23 | Step:  10 | Val Loss:  0.6947399377822876\n",
      "Epoch:  23 | Step:  11 | Val Loss:  0.5284326076507568\n",
      "Epoch:  23 | Step:  12 | Val Loss:  0.5528768301010132\n",
      "Epoch:  23 | Step:  13 | Val Loss:  0.4694151282310486\n",
      "Epoch:  23 | Step:  14 | Val Loss:  0.6922460794448853\n",
      "Epoch:  23 | Step:  15 | Val Loss:  0.40997540950775146\n",
      "Epoch:  23 | Step:  16 | Val Loss:  0.6596912145614624\n",
      "Epoch:  23 | Step:  17 | Val Loss:  0.6231271028518677\n",
      "Epoch:  23 | Step:  18 | Val Loss:  0.525197446346283\n",
      "Epoch:  23 | Step:  19 | Val Loss:  0.5816299915313721\n",
      "Epoch:  23 | Step:  20 | Val Loss:  0.4611465036869049\n",
      "Epoch:  23 | Step:  21 | Val Loss:  0.543384313583374\n",
      "Epoch:  23 | Step:  22 | Val Loss:  0.6131788492202759\n",
      "Epoch:  23 | Step:  23 | Val Loss:  0.6051294803619385\n",
      "Epoch:  23 | Step:  24 | Val Loss:  0.6751245260238647\n",
      "Epoch:  23 | Step:  25 | Val Loss:  0.5272408723831177\n",
      "Epoch:  23 | Step:  26 | Val Loss:  0.45140689611434937\n",
      "Epoch:  23 | Step:  27 | Val Loss:  0.45207715034484863\n",
      "Epoch:  23 | Step:  28 | Val Loss:  0.42760831117630005\n",
      "Epoch:  23 | Step:  29 | Val Loss:  0.4415678381919861\n",
      "Epoch:  23 | Step:  30 | Val Loss:  0.5102901458740234\n",
      "Epoch:  23 | Step:  31 | Val Loss:  0.5957450866699219\n",
      "Epoch:  23 | Step:  32 | Val Loss:  0.6900148987770081\n",
      "Epoch:  23 | Step:  33 | Val Loss:  0.6456262469291687\n",
      "Epoch:  23 | Step:  34 | Val Loss:  0.6417113542556763\n",
      "Epoch:  23 | Step:  35 | Val Loss:  0.39960792660713196\n",
      "Epoch:  23 | Step:  36 | Val Loss:  0.5940185189247131\n",
      "Epoch:  23 | Step:  37 | Val Loss:  0.6132593154907227\n",
      "Epoch:  23 | Step:  38 | Val Loss:  0.6392136216163635\n",
      "Epoch:  23 | Step:  39 | Val Loss:  0.6951010227203369\n",
      "Epoch:  23 | Step:  40 | Val Loss:  0.4644131362438202\n",
      "Epoch:  23 | Step:  41 | Val Loss:  0.5575180649757385\n",
      "Epoch:  23 | Step:  42 | Val Loss:  0.5709337592124939\n",
      "Epoch:  23 | Step:  43 | Val Loss:  0.7863940000534058\n",
      "Epoch:  23 | Step:  44 | Val Loss:  0.39353540539741516\n",
      "Epoch:  23 | Step:  45 | Val Loss:  0.8214088082313538\n",
      "Epoch:  23 | Step:  46 | Val Loss:  0.5248357057571411\n",
      "Epoch:  23 | Step:  47 | Val Loss:  0.6321980953216553\n",
      "Epoch:  23 | Step:  48 | Val Loss:  0.7584028244018555\n",
      "Epoch:  23 | Step:  49 | Val Loss:  0.7813947200775146\n",
      "Epoch:  23 | Step:  50 | Val Loss:  0.6546622514724731\n",
      "Epoch:  23 | Step:  51 | Val Loss:  0.5401104092597961\n",
      "Epoch:  23 | Step:  52 | Val Loss:  0.5394008159637451\n",
      "Epoch:  23 | Step:  53 | Val Loss:  0.8277193307876587\n",
      "Epoch:  23 | Step:  54 | Val Loss:  0.38761961460113525\n",
      "Epoch:  23 | Step:  55 | Val Loss:  0.6176841855049133\n",
      "Epoch:  23 | Step:  56 | Val Loss:  0.6263641715049744\n",
      "Epoch:  23 | Step:  57 | Val Loss:  0.43153488636016846\n",
      "Epoch:  23 | Step:  58 | Val Loss:  0.46782439947128296\n",
      "Epoch:  23 | Step:  59 | Val Loss:  0.6162000298500061\n",
      "Epoch:  23 | Step:  60 | Val Loss:  0.5654838681221008\n",
      "Epoch:  23 | Step:  61 | Val Loss:  0.5528524518013\n",
      "Epoch:  23 | Step:  62 | Val Loss:  0.6222473382949829\n",
      "Epoch:  23 | Step:  63 | Val Loss:  0.5485045313835144\n",
      "Epoch:  23 | Step:  64 | Val Loss:  0.5439325571060181\n",
      "Epoch:  23 | Step:  65 | Val Loss:  0.5706995129585266\n",
      "Epoch:  23 | Step:  66 | Val Loss:  0.5008025169372559\n",
      "Epoch:  23 | Step:  67 | Val Loss:  0.4889490604400635\n",
      "Epoch:  23 | Step:  68 | Val Loss:  0.5021889209747314\n",
      "Epoch:  23 | Step:  69 | Val Loss:  0.5374189019203186\n",
      "Epoch:  23 | Step:  70 | Val Loss:  0.6746063232421875\n",
      "Epoch:  23 | Step:  71 | Val Loss:  0.5003976821899414\n",
      "Epoch:  23 | Step:  72 | Val Loss:  0.46013227105140686\n",
      "Epoch:  23 | Step:  73 | Val Loss:  0.661418080329895\n",
      "Epoch:  23 | Step:  74 | Val Loss:  0.5641993284225464\n",
      "Epoch:  23 | Step:  75 | Val Loss:  0.6597909927368164\n",
      "Epoch:  23 | Step:  76 | Val Loss:  0.63297438621521\n",
      "Epoch:  23 | Step:  77 | Val Loss:  0.5477649569511414\n",
      "Epoch:  23 | Step:  78 | Val Loss:  0.5767104029655457\n",
      "Epoch:  23 | Step:  79 | Val Loss:  0.5552740097045898\n",
      "Epoch:  23 | Step:  80 | Val Loss:  0.43425190448760986\n",
      "Epoch:  23 | Step:  81 | Val Loss:  0.8421034812927246\n",
      "Epoch:  23 | Step:  82 | Val Loss:  0.3616425395011902\n",
      "Epoch:  23 | Step:  83 | Val Loss:  0.5008572340011597\n",
      "Epoch:  23 | Step:  84 | Val Loss:  0.5774533748626709\n",
      "Epoch:  23 | Step:  85 | Val Loss:  0.5684518814086914\n",
      "Epoch:  23 | Step:  86 | Val Loss:  0.49832290410995483\n",
      "Epoch:  23 | Step:  87 | Val Loss:  0.5074324607849121\n",
      "Epoch:  23 | Step:  88 | Val Loss:  0.8159987926483154\n",
      "Epoch:  23 | Step:  89 | Val Loss:  0.5155826807022095\n",
      "Epoch:  23 | Step:  90 | Val Loss:  0.6121541261672974\n",
      "Epoch:  23 | Step:  91 | Val Loss:  0.6148419976234436\n",
      "Epoch:  23 | Step:  92 | Val Loss:  0.6332836747169495\n",
      "Epoch:  23 | Step:  93 | Val Loss:  0.514090895652771\n",
      "Epoch:  23 | Step:  94 | Val Loss:  0.7108131647109985\n",
      "Epoch:  23 | Step:  95 | Val Loss:  0.4902364909648895\n",
      "Epoch:  23 | Step:  96 | Val Loss:  0.6028980612754822\n",
      "Epoch:  23 | Step:  97 | Val Loss:  0.48742008209228516\n",
      "Epoch:  23 | Step:  98 | Val Loss:  0.493015855550766\n",
      "Epoch:  23 | Step:  99 | Val Loss:  0.6823554039001465\n",
      "Epoch:  23 | Step:  100 | Val Loss:  0.441267192363739\n",
      "Epoch:  23 | Step:  101 | Val Loss:  0.6884918212890625\n",
      "Epoch:  23 | Step:  102 | Val Loss:  0.6370083093643188\n",
      "Epoch:  23 | Step:  103 | Val Loss:  0.28417396545410156\n",
      "Epoch:  23 | Step:  104 | Val Loss:  0.7842521667480469\n",
      "Epoch:  23 | Step:  105 | Val Loss:  0.4436982274055481\n",
      "Epoch:  23 | Step:  106 | Val Loss:  0.575371503829956\n",
      "Epoch:  23 | Step:  107 | Val Loss:  0.5729202032089233\n",
      "Epoch:  23 | Step:  108 | Val Loss:  0.68055260181427\n",
      "Epoch:  23 | Step:  109 | Val Loss:  0.6559521555900574\n",
      "Epoch:  23 | Step:  110 | Val Loss:  0.5724401473999023\n",
      "Epoch:  23 | Step:  111 | Val Loss:  0.4498758614063263\n",
      "Epoch:  23 | Step:  112 | Val Loss:  0.6457309722900391\n",
      "Epoch:  23 | Step:  113 | Val Loss:  0.5331869125366211\n",
      "Epoch:  23 | Step:  114 | Val Loss:  0.6549370288848877\n",
      "Epoch:  23 | Step:  115 | Val Loss:  0.7650846838951111\n",
      "Epoch:  23 | Step:  116 | Val Loss:  0.5071347951889038\n",
      "Epoch:  23 | Step:  117 | Val Loss:  0.4692581295967102\n",
      "Epoch:  23 | Step:  118 | Val Loss:  0.7394571304321289\n",
      "Epoch:  23 | Step:  119 | Val Loss:  0.39188921451568604\n",
      "Epoch:  23 | Step:  120 | Val Loss:  0.7180300951004028\n",
      "Epoch:  23 | Step:  121 | Val Loss:  0.46663451194763184\n",
      "Epoch:  23 | Step:  122 | Val Loss:  0.6015632152557373\n",
      "Epoch:  23 | Step:  123 | Val Loss:  0.48093220591545105\n",
      "Epoch:  23 | Step:  124 | Val Loss:  0.7475148439407349\n",
      "Epoch:  23 | Step:  125 | Val Loss:  0.3638322651386261\n",
      "Epoch:  23 | Train Loss:  tensor(0.5376, device='cuda:0') | Val Loss:  tensor(0.5739, device='cuda:0')\n",
      "Epoch:  24 | Step:  500 | Train Loss:  0.5012937784194946\n",
      "Epoch:  24 | Step:  1 | Val Loss:  0.4496716260910034\n",
      "Epoch:  24 | Step:  2 | Val Loss:  0.5977181792259216\n",
      "Epoch:  24 | Step:  3 | Val Loss:  0.35581210255622864\n",
      "Epoch:  24 | Step:  4 | Val Loss:  0.5915573835372925\n",
      "Epoch:  24 | Step:  5 | Val Loss:  0.6682475805282593\n",
      "Epoch:  24 | Step:  6 | Val Loss:  0.5164262056350708\n",
      "Epoch:  24 | Step:  7 | Val Loss:  0.42039287090301514\n",
      "Epoch:  24 | Step:  8 | Val Loss:  0.6319854259490967\n",
      "Epoch:  24 | Step:  9 | Val Loss:  0.6096935272216797\n",
      "Epoch:  24 | Step:  10 | Val Loss:  0.7006792426109314\n",
      "Epoch:  24 | Step:  11 | Val Loss:  0.49593669176101685\n",
      "Epoch:  24 | Step:  12 | Val Loss:  0.5145879983901978\n",
      "Epoch:  24 | Step:  13 | Val Loss:  0.6882584691047668\n",
      "Epoch:  24 | Step:  14 | Val Loss:  0.5955607891082764\n",
      "Epoch:  24 | Step:  15 | Val Loss:  0.7437050342559814\n",
      "Epoch:  24 | Step:  16 | Val Loss:  0.5112515687942505\n",
      "Epoch:  24 | Step:  17 | Val Loss:  0.6042029857635498\n",
      "Epoch:  24 | Step:  18 | Val Loss:  0.524775505065918\n",
      "Epoch:  24 | Step:  19 | Val Loss:  0.6278223991394043\n",
      "Epoch:  24 | Step:  20 | Val Loss:  0.4841190576553345\n",
      "Epoch:  24 | Step:  21 | Val Loss:  0.4405748248100281\n",
      "Epoch:  24 | Step:  22 | Val Loss:  0.5010055303573608\n",
      "Epoch:  24 | Step:  23 | Val Loss:  0.600292444229126\n",
      "Epoch:  24 | Step:  24 | Val Loss:  0.6578919887542725\n",
      "Epoch:  24 | Step:  25 | Val Loss:  0.6230987906455994\n",
      "Epoch:  24 | Step:  26 | Val Loss:  0.432485967874527\n",
      "Epoch:  24 | Step:  27 | Val Loss:  0.6004311442375183\n",
      "Epoch:  24 | Step:  28 | Val Loss:  0.6167271733283997\n",
      "Epoch:  24 | Step:  29 | Val Loss:  0.4934123754501343\n",
      "Epoch:  24 | Step:  30 | Val Loss:  0.5913063287734985\n",
      "Epoch:  24 | Step:  31 | Val Loss:  0.512078046798706\n",
      "Epoch:  24 | Step:  32 | Val Loss:  0.6218162178993225\n",
      "Epoch:  24 | Step:  33 | Val Loss:  0.4780096411705017\n",
      "Epoch:  24 | Step:  34 | Val Loss:  0.4763619899749756\n",
      "Epoch:  24 | Step:  35 | Val Loss:  0.43936917185783386\n",
      "Epoch:  24 | Step:  36 | Val Loss:  0.47397762537002563\n",
      "Epoch:  24 | Step:  37 | Val Loss:  0.46821045875549316\n",
      "Epoch:  24 | Step:  38 | Val Loss:  0.5889602899551392\n",
      "Epoch:  24 | Step:  39 | Val Loss:  0.4710897207260132\n",
      "Epoch:  24 | Step:  40 | Val Loss:  0.44382351636886597\n",
      "Epoch:  24 | Step:  41 | Val Loss:  0.4406037926673889\n",
      "Epoch:  24 | Step:  42 | Val Loss:  0.3239304721355438\n",
      "Epoch:  24 | Step:  43 | Val Loss:  0.7464674711227417\n",
      "Epoch:  24 | Step:  44 | Val Loss:  0.6560239195823669\n",
      "Epoch:  24 | Step:  45 | Val Loss:  0.5807958841323853\n",
      "Epoch:  24 | Step:  46 | Val Loss:  0.4293326735496521\n",
      "Epoch:  24 | Step:  47 | Val Loss:  0.510373055934906\n",
      "Epoch:  24 | Step:  48 | Val Loss:  0.38043439388275146\n",
      "Epoch:  24 | Step:  49 | Val Loss:  0.5653051137924194\n",
      "Epoch:  24 | Step:  50 | Val Loss:  0.5510220527648926\n",
      "Epoch:  24 | Step:  51 | Val Loss:  0.5072236657142639\n",
      "Epoch:  24 | Step:  52 | Val Loss:  0.8346781134605408\n",
      "Epoch:  24 | Step:  53 | Val Loss:  0.4239259958267212\n",
      "Epoch:  24 | Step:  54 | Val Loss:  0.5956774353981018\n",
      "Epoch:  24 | Step:  55 | Val Loss:  0.8258131742477417\n",
      "Epoch:  24 | Step:  56 | Val Loss:  0.45574742555618286\n",
      "Epoch:  24 | Step:  57 | Val Loss:  0.5047232508659363\n",
      "Epoch:  24 | Step:  58 | Val Loss:  0.6753752827644348\n",
      "Epoch:  24 | Step:  59 | Val Loss:  0.6493580341339111\n",
      "Epoch:  24 | Step:  60 | Val Loss:  0.42915305495262146\n",
      "Epoch:  24 | Step:  61 | Val Loss:  0.6859373450279236\n",
      "Epoch:  24 | Step:  62 | Val Loss:  0.6019116640090942\n",
      "Epoch:  24 | Step:  63 | Val Loss:  0.5939620733261108\n",
      "Epoch:  24 | Step:  64 | Val Loss:  0.6063252687454224\n",
      "Epoch:  24 | Step:  65 | Val Loss:  0.6822740435600281\n",
      "Epoch:  24 | Step:  66 | Val Loss:  0.6313302516937256\n",
      "Epoch:  24 | Step:  67 | Val Loss:  0.5111645460128784\n",
      "Epoch:  24 | Step:  68 | Val Loss:  0.4859662652015686\n",
      "Epoch:  24 | Step:  69 | Val Loss:  0.6927387714385986\n",
      "Epoch:  24 | Step:  70 | Val Loss:  0.576852023601532\n",
      "Epoch:  24 | Step:  71 | Val Loss:  0.42793434858322144\n",
      "Epoch:  24 | Step:  72 | Val Loss:  0.7405415773391724\n",
      "Epoch:  24 | Step:  73 | Val Loss:  0.42268526554107666\n",
      "Epoch:  24 | Step:  74 | Val Loss:  0.7310377359390259\n",
      "Epoch:  24 | Step:  75 | Val Loss:  0.6245461702346802\n",
      "Epoch:  24 | Step:  76 | Val Loss:  0.621739387512207\n",
      "Epoch:  24 | Step:  77 | Val Loss:  0.6401107311248779\n",
      "Epoch:  24 | Step:  78 | Val Loss:  0.6378954648971558\n",
      "Epoch:  24 | Step:  79 | Val Loss:  0.6661841869354248\n",
      "Epoch:  24 | Step:  80 | Val Loss:  0.6218432188034058\n",
      "Epoch:  24 | Step:  81 | Val Loss:  0.7169368267059326\n",
      "Epoch:  24 | Step:  82 | Val Loss:  0.6042507886886597\n",
      "Epoch:  24 | Step:  83 | Val Loss:  0.580490231513977\n",
      "Epoch:  24 | Step:  84 | Val Loss:  0.7956490516662598\n",
      "Epoch:  24 | Step:  85 | Val Loss:  0.6700079441070557\n",
      "Epoch:  24 | Step:  86 | Val Loss:  0.4518068730831146\n",
      "Epoch:  24 | Step:  87 | Val Loss:  0.5679078698158264\n",
      "Epoch:  24 | Step:  88 | Val Loss:  0.6609988212585449\n",
      "Epoch:  24 | Step:  89 | Val Loss:  0.6976230144500732\n",
      "Epoch:  24 | Step:  90 | Val Loss:  0.4990048408508301\n",
      "Epoch:  24 | Step:  91 | Val Loss:  0.8530300259590149\n",
      "Epoch:  24 | Step:  92 | Val Loss:  0.5500560998916626\n",
      "Epoch:  24 | Step:  93 | Val Loss:  0.5861176252365112\n",
      "Epoch:  24 | Step:  94 | Val Loss:  0.7829091548919678\n",
      "Epoch:  24 | Step:  95 | Val Loss:  0.6901912093162537\n",
      "Epoch:  24 | Step:  96 | Val Loss:  0.5516061782836914\n",
      "Epoch:  24 | Step:  97 | Val Loss:  0.6426907181739807\n",
      "Epoch:  24 | Step:  98 | Val Loss:  0.6727571487426758\n",
      "Epoch:  24 | Step:  99 | Val Loss:  0.5111604332923889\n",
      "Epoch:  24 | Step:  100 | Val Loss:  0.3631434738636017\n",
      "Epoch:  24 | Step:  101 | Val Loss:  0.5138649344444275\n",
      "Epoch:  24 | Step:  102 | Val Loss:  0.3273335099220276\n",
      "Epoch:  24 | Step:  103 | Val Loss:  0.6773602366447449\n",
      "Epoch:  24 | Step:  104 | Val Loss:  0.5309146642684937\n",
      "Epoch:  24 | Step:  105 | Val Loss:  0.59233558177948\n",
      "Epoch:  24 | Step:  106 | Val Loss:  0.6739628911018372\n",
      "Epoch:  24 | Step:  107 | Val Loss:  0.7122093439102173\n",
      "Epoch:  24 | Step:  108 | Val Loss:  0.5880258083343506\n",
      "Epoch:  24 | Step:  109 | Val Loss:  0.7912079095840454\n",
      "Epoch:  24 | Step:  110 | Val Loss:  0.7559695243835449\n",
      "Epoch:  24 | Step:  111 | Val Loss:  0.3905937671661377\n",
      "Epoch:  24 | Step:  112 | Val Loss:  0.38509827852249146\n",
      "Epoch:  24 | Step:  113 | Val Loss:  0.5425331592559814\n",
      "Epoch:  24 | Step:  114 | Val Loss:  0.4923687279224396\n",
      "Epoch:  24 | Step:  115 | Val Loss:  0.5252977013587952\n",
      "Epoch:  24 | Step:  116 | Val Loss:  0.7861983776092529\n",
      "Epoch:  24 | Step:  117 | Val Loss:  0.5021026134490967\n",
      "Epoch:  24 | Step:  118 | Val Loss:  0.3077065348625183\n",
      "Epoch:  24 | Step:  119 | Val Loss:  0.4554397761821747\n",
      "Epoch:  24 | Step:  120 | Val Loss:  0.5808129906654358\n",
      "Epoch:  24 | Step:  121 | Val Loss:  0.5847960114479065\n",
      "Epoch:  24 | Step:  122 | Val Loss:  0.573067307472229\n",
      "Epoch:  24 | Step:  123 | Val Loss:  0.4930912256240845\n",
      "Epoch:  24 | Step:  124 | Val Loss:  0.5556585788726807\n",
      "Epoch:  24 | Step:  125 | Val Loss:  0.4910951256752014\n",
      "Epoch:  24 | Train Loss:  tensor(0.5362, device='cuda:0') | Val Loss:  tensor(0.5714, device='cuda:0')\n",
      "Epoch:  25 | Step:  500 | Train Loss:  0.6196326017379761\n",
      "Epoch:  25 | Step:  1 | Val Loss:  0.5325796604156494\n",
      "Epoch:  25 | Step:  2 | Val Loss:  0.5874423980712891\n",
      "Epoch:  25 | Step:  3 | Val Loss:  0.6356288194656372\n",
      "Epoch:  25 | Step:  4 | Val Loss:  0.6158775091171265\n",
      "Epoch:  25 | Step:  5 | Val Loss:  0.5581340789794922\n",
      "Epoch:  25 | Step:  6 | Val Loss:  0.5978279113769531\n",
      "Epoch:  25 | Step:  7 | Val Loss:  0.5127725601196289\n",
      "Epoch:  25 | Step:  8 | Val Loss:  0.47743457555770874\n",
      "Epoch:  25 | Step:  9 | Val Loss:  0.4845481514930725\n",
      "Epoch:  25 | Step:  10 | Val Loss:  0.6341882944107056\n",
      "Epoch:  25 | Step:  11 | Val Loss:  0.7555549144744873\n",
      "Epoch:  25 | Step:  12 | Val Loss:  0.6447429060935974\n",
      "Epoch:  25 | Step:  13 | Val Loss:  0.6199666857719421\n",
      "Epoch:  25 | Step:  14 | Val Loss:  0.7656149864196777\n",
      "Epoch:  25 | Step:  15 | Val Loss:  0.7057802081108093\n",
      "Epoch:  25 | Step:  16 | Val Loss:  0.5724916458129883\n",
      "Epoch:  25 | Step:  17 | Val Loss:  0.37596195936203003\n",
      "Epoch:  25 | Step:  18 | Val Loss:  0.6383346319198608\n",
      "Epoch:  25 | Step:  19 | Val Loss:  0.44655469059944153\n",
      "Epoch:  25 | Step:  20 | Val Loss:  0.6062010526657104\n",
      "Epoch:  25 | Step:  21 | Val Loss:  0.621512770652771\n",
      "Epoch:  25 | Step:  22 | Val Loss:  0.5317364931106567\n",
      "Epoch:  25 | Step:  23 | Val Loss:  0.6018544435501099\n",
      "Epoch:  25 | Step:  24 | Val Loss:  0.37877583503723145\n",
      "Epoch:  25 | Step:  25 | Val Loss:  0.6507006883621216\n",
      "Epoch:  25 | Step:  26 | Val Loss:  0.5661357641220093\n",
      "Epoch:  25 | Step:  27 | Val Loss:  0.3827664852142334\n",
      "Epoch:  25 | Step:  28 | Val Loss:  0.5771485567092896\n",
      "Epoch:  25 | Step:  29 | Val Loss:  0.6232671737670898\n",
      "Epoch:  25 | Step:  30 | Val Loss:  0.3279038667678833\n",
      "Epoch:  25 | Step:  31 | Val Loss:  0.5269417762756348\n",
      "Epoch:  25 | Step:  32 | Val Loss:  0.5959467887878418\n",
      "Epoch:  25 | Step:  33 | Val Loss:  0.6402261257171631\n",
      "Epoch:  25 | Step:  34 | Val Loss:  0.5915398001670837\n",
      "Epoch:  25 | Step:  35 | Val Loss:  0.6920269727706909\n",
      "Epoch:  25 | Step:  36 | Val Loss:  0.5153805017471313\n",
      "Epoch:  25 | Step:  37 | Val Loss:  0.4688906669616699\n",
      "Epoch:  25 | Step:  38 | Val Loss:  0.5064013600349426\n",
      "Epoch:  25 | Step:  39 | Val Loss:  0.5858330726623535\n",
      "Epoch:  25 | Step:  40 | Val Loss:  0.527603268623352\n",
      "Epoch:  25 | Step:  41 | Val Loss:  0.4837227165699005\n",
      "Epoch:  25 | Step:  42 | Val Loss:  0.654617965221405\n",
      "Epoch:  25 | Step:  43 | Val Loss:  0.5016059875488281\n",
      "Epoch:  25 | Step:  44 | Val Loss:  0.6136348843574524\n",
      "Epoch:  25 | Step:  45 | Val Loss:  0.6806337237358093\n",
      "Epoch:  25 | Step:  46 | Val Loss:  0.8115249872207642\n",
      "Epoch:  25 | Step:  47 | Val Loss:  0.46191227436065674\n",
      "Epoch:  25 | Step:  48 | Val Loss:  0.5415840148925781\n",
      "Epoch:  25 | Step:  49 | Val Loss:  0.5314443707466125\n",
      "Epoch:  25 | Step:  50 | Val Loss:  0.6500444412231445\n",
      "Epoch:  25 | Step:  51 | Val Loss:  0.3297654986381531\n",
      "Epoch:  25 | Step:  52 | Val Loss:  0.5864980220794678\n",
      "Epoch:  25 | Step:  53 | Val Loss:  0.5883394479751587\n",
      "Epoch:  25 | Step:  54 | Val Loss:  0.6820204257965088\n",
      "Epoch:  25 | Step:  55 | Val Loss:  0.5006271600723267\n",
      "Epoch:  25 | Step:  56 | Val Loss:  0.7989631295204163\n",
      "Epoch:  25 | Step:  57 | Val Loss:  0.5590669512748718\n",
      "Epoch:  25 | Step:  58 | Val Loss:  0.5109843015670776\n",
      "Epoch:  25 | Step:  59 | Val Loss:  0.6683403253555298\n",
      "Epoch:  25 | Step:  60 | Val Loss:  0.6416837573051453\n",
      "Epoch:  25 | Step:  61 | Val Loss:  0.5996286869049072\n",
      "Epoch:  25 | Step:  62 | Val Loss:  0.5958419442176819\n",
      "Epoch:  25 | Step:  63 | Val Loss:  0.29309213161468506\n",
      "Epoch:  25 | Step:  64 | Val Loss:  0.5418379306793213\n",
      "Epoch:  25 | Step:  65 | Val Loss:  0.42255184054374695\n",
      "Epoch:  25 | Step:  66 | Val Loss:  0.41779157519340515\n",
      "Epoch:  25 | Step:  67 | Val Loss:  0.4577380418777466\n",
      "Epoch:  25 | Step:  68 | Val Loss:  0.36297890543937683\n",
      "Epoch:  25 | Step:  69 | Val Loss:  0.5184823274612427\n",
      "Epoch:  25 | Step:  70 | Val Loss:  0.7629882097244263\n",
      "Epoch:  25 | Step:  71 | Val Loss:  0.5699731707572937\n",
      "Epoch:  25 | Step:  72 | Val Loss:  0.505302906036377\n",
      "Epoch:  25 | Step:  73 | Val Loss:  0.389213502407074\n",
      "Epoch:  25 | Step:  74 | Val Loss:  0.41374364495277405\n",
      "Epoch:  25 | Step:  75 | Val Loss:  0.6008081436157227\n",
      "Epoch:  25 | Step:  76 | Val Loss:  0.6256715655326843\n",
      "Epoch:  25 | Step:  77 | Val Loss:  0.607439398765564\n",
      "Epoch:  25 | Step:  78 | Val Loss:  0.5640491843223572\n",
      "Epoch:  25 | Step:  79 | Val Loss:  0.62789386510849\n",
      "Epoch:  25 | Step:  80 | Val Loss:  0.5019104480743408\n",
      "Epoch:  25 | Step:  81 | Val Loss:  0.35818392038345337\n",
      "Epoch:  25 | Step:  82 | Val Loss:  0.651667594909668\n",
      "Epoch:  25 | Step:  83 | Val Loss:  0.6269881725311279\n",
      "Epoch:  25 | Step:  84 | Val Loss:  0.4792499840259552\n",
      "Epoch:  25 | Step:  85 | Val Loss:  0.7079715728759766\n",
      "Epoch:  25 | Step:  86 | Val Loss:  0.506033182144165\n",
      "Epoch:  25 | Step:  87 | Val Loss:  0.40124279260635376\n",
      "Epoch:  25 | Step:  88 | Val Loss:  0.6946763396263123\n",
      "Epoch:  25 | Step:  89 | Val Loss:  0.49961766600608826\n",
      "Epoch:  25 | Step:  90 | Val Loss:  0.3858964443206787\n",
      "Epoch:  25 | Step:  91 | Val Loss:  0.6113044619560242\n",
      "Epoch:  25 | Step:  92 | Val Loss:  0.5381624698638916\n",
      "Epoch:  25 | Step:  93 | Val Loss:  0.5694780349731445\n",
      "Epoch:  25 | Step:  94 | Val Loss:  0.47963231801986694\n",
      "Epoch:  25 | Step:  95 | Val Loss:  0.6174414157867432\n",
      "Epoch:  25 | Step:  96 | Val Loss:  0.36707115173339844\n",
      "Epoch:  25 | Step:  97 | Val Loss:  0.5818769931793213\n",
      "Epoch:  25 | Step:  98 | Val Loss:  0.5609815120697021\n",
      "Epoch:  25 | Step:  99 | Val Loss:  0.5612802505493164\n",
      "Epoch:  25 | Step:  100 | Val Loss:  0.6959786415100098\n",
      "Epoch:  25 | Step:  101 | Val Loss:  0.47336751222610474\n",
      "Epoch:  25 | Step:  102 | Val Loss:  0.5296226739883423\n",
      "Epoch:  25 | Step:  103 | Val Loss:  0.45810872316360474\n",
      "Epoch:  25 | Step:  104 | Val Loss:  0.6844907999038696\n",
      "Epoch:  25 | Step:  105 | Val Loss:  0.8403120040893555\n",
      "Epoch:  25 | Step:  106 | Val Loss:  0.5068656802177429\n",
      "Epoch:  25 | Step:  107 | Val Loss:  0.65230393409729\n",
      "Epoch:  25 | Step:  108 | Val Loss:  0.5195977091789246\n",
      "Epoch:  25 | Step:  109 | Val Loss:  0.6877312660217285\n",
      "Epoch:  25 | Step:  110 | Val Loss:  0.6830516457557678\n",
      "Epoch:  25 | Step:  111 | Val Loss:  0.7954376935958862\n",
      "Epoch:  25 | Step:  112 | Val Loss:  0.42174679040908813\n",
      "Epoch:  25 | Step:  113 | Val Loss:  0.5756399631500244\n",
      "Epoch:  25 | Step:  114 | Val Loss:  0.5442003011703491\n",
      "Epoch:  25 | Step:  115 | Val Loss:  0.7176648378372192\n",
      "Epoch:  25 | Step:  116 | Val Loss:  0.6329663991928101\n",
      "Epoch:  25 | Step:  117 | Val Loss:  0.674281120300293\n",
      "Epoch:  25 | Step:  118 | Val Loss:  0.43195271492004395\n",
      "Epoch:  25 | Step:  119 | Val Loss:  0.7314399480819702\n",
      "Epoch:  25 | Step:  120 | Val Loss:  0.7114284038543701\n",
      "Epoch:  25 | Step:  121 | Val Loss:  0.5370054244995117\n",
      "Epoch:  25 | Step:  122 | Val Loss:  0.587894082069397\n",
      "Epoch:  25 | Step:  123 | Val Loss:  0.48384687304496765\n",
      "Epoch:  25 | Step:  124 | Val Loss:  0.6027628183364868\n",
      "Epoch:  25 | Step:  125 | Val Loss:  0.7673592567443848\n",
      "Epoch:  25 | Train Loss:  tensor(0.5339, device='cuda:0') | Val Loss:  tensor(0.5680, device='cuda:0')\n",
      "Epoch:  26 | Step:  500 | Train Loss:  0.7790927886962891\n",
      "Epoch:  26 | Step:  1 | Val Loss:  0.5731379985809326\n",
      "Epoch:  26 | Step:  2 | Val Loss:  0.6555730104446411\n",
      "Epoch:  26 | Step:  3 | Val Loss:  0.5568575859069824\n",
      "Epoch:  26 | Step:  4 | Val Loss:  0.41883325576782227\n",
      "Epoch:  26 | Step:  5 | Val Loss:  0.5567314624786377\n",
      "Epoch:  26 | Step:  6 | Val Loss:  0.4984436631202698\n",
      "Epoch:  26 | Step:  7 | Val Loss:  0.612550675868988\n",
      "Epoch:  26 | Step:  8 | Val Loss:  0.31392890214920044\n",
      "Epoch:  26 | Step:  9 | Val Loss:  0.5929298996925354\n",
      "Epoch:  26 | Step:  10 | Val Loss:  0.5688035488128662\n",
      "Epoch:  26 | Step:  11 | Val Loss:  0.5229291319847107\n",
      "Epoch:  26 | Step:  12 | Val Loss:  0.5209174752235413\n",
      "Epoch:  26 | Step:  13 | Val Loss:  0.4952283799648285\n",
      "Epoch:  26 | Step:  14 | Val Loss:  0.5657916069030762\n",
      "Epoch:  26 | Step:  15 | Val Loss:  0.6808353662490845\n",
      "Epoch:  26 | Step:  16 | Val Loss:  0.5797057747840881\n",
      "Epoch:  26 | Step:  17 | Val Loss:  0.3848204016685486\n",
      "Epoch:  26 | Step:  18 | Val Loss:  0.6444228887557983\n",
      "Epoch:  26 | Step:  19 | Val Loss:  0.5473685264587402\n",
      "Epoch:  26 | Step:  20 | Val Loss:  0.33248454332351685\n",
      "Epoch:  26 | Step:  21 | Val Loss:  0.5620938539505005\n",
      "Epoch:  26 | Step:  22 | Val Loss:  0.477655827999115\n",
      "Epoch:  26 | Step:  23 | Val Loss:  0.54430091381073\n",
      "Epoch:  26 | Step:  24 | Val Loss:  0.48138830065727234\n",
      "Epoch:  26 | Step:  25 | Val Loss:  0.5874649286270142\n",
      "Epoch:  26 | Step:  26 | Val Loss:  0.4436895251274109\n",
      "Epoch:  26 | Step:  27 | Val Loss:  0.5826345682144165\n",
      "Epoch:  26 | Step:  28 | Val Loss:  0.4947810769081116\n",
      "Epoch:  26 | Step:  29 | Val Loss:  0.6219378709793091\n",
      "Epoch:  26 | Step:  30 | Val Loss:  0.5696651935577393\n",
      "Epoch:  26 | Step:  31 | Val Loss:  0.6880688667297363\n",
      "Epoch:  26 | Step:  32 | Val Loss:  0.6303356289863586\n",
      "Epoch:  26 | Step:  33 | Val Loss:  0.5222132205963135\n",
      "Epoch:  26 | Step:  34 | Val Loss:  0.57051682472229\n",
      "Epoch:  26 | Step:  35 | Val Loss:  0.4501681923866272\n",
      "Epoch:  26 | Step:  36 | Val Loss:  0.6729278564453125\n",
      "Epoch:  26 | Step:  37 | Val Loss:  0.5815573930740356\n",
      "Epoch:  26 | Step:  38 | Val Loss:  0.5827897787094116\n",
      "Epoch:  26 | Step:  39 | Val Loss:  0.626284122467041\n",
      "Epoch:  26 | Step:  40 | Val Loss:  0.6100163459777832\n",
      "Epoch:  26 | Step:  41 | Val Loss:  0.714779257774353\n",
      "Epoch:  26 | Step:  42 | Val Loss:  0.70209801197052\n",
      "Epoch:  26 | Step:  43 | Val Loss:  0.5963878631591797\n",
      "Epoch:  26 | Step:  44 | Val Loss:  0.5923159122467041\n",
      "Epoch:  26 | Step:  45 | Val Loss:  0.5170944929122925\n",
      "Epoch:  26 | Step:  46 | Val Loss:  0.787772536277771\n",
      "Epoch:  26 | Step:  47 | Val Loss:  0.6812247633934021\n",
      "Epoch:  26 | Step:  48 | Val Loss:  0.47680389881134033\n",
      "Epoch:  26 | Step:  49 | Val Loss:  0.5318996906280518\n",
      "Epoch:  26 | Step:  50 | Val Loss:  0.847187876701355\n",
      "Epoch:  26 | Step:  51 | Val Loss:  0.5921345949172974\n",
      "Epoch:  26 | Step:  52 | Val Loss:  0.5103901624679565\n",
      "Epoch:  26 | Step:  53 | Val Loss:  0.5455740690231323\n",
      "Epoch:  26 | Step:  54 | Val Loss:  0.72627192735672\n",
      "Epoch:  26 | Step:  55 | Val Loss:  0.5374850034713745\n",
      "Epoch:  26 | Step:  56 | Val Loss:  0.7003546953201294\n",
      "Epoch:  26 | Step:  57 | Val Loss:  0.6007277369499207\n",
      "Epoch:  26 | Step:  58 | Val Loss:  0.5700703859329224\n",
      "Epoch:  26 | Step:  59 | Val Loss:  0.5020637512207031\n",
      "Epoch:  26 | Step:  60 | Val Loss:  0.6291944980621338\n",
      "Epoch:  26 | Step:  61 | Val Loss:  0.5932320356369019\n",
      "Epoch:  26 | Step:  62 | Val Loss:  0.6877710819244385\n",
      "Epoch:  26 | Step:  63 | Val Loss:  0.45567601919174194\n",
      "Epoch:  26 | Step:  64 | Val Loss:  0.6892943978309631\n",
      "Epoch:  26 | Step:  65 | Val Loss:  0.7083439826965332\n",
      "Epoch:  26 | Step:  66 | Val Loss:  0.6365000009536743\n",
      "Epoch:  26 | Step:  67 | Val Loss:  0.6365970373153687\n",
      "Epoch:  26 | Step:  68 | Val Loss:  0.586783766746521\n",
      "Epoch:  26 | Step:  69 | Val Loss:  0.6590784192085266\n",
      "Epoch:  26 | Step:  70 | Val Loss:  0.5145838260650635\n",
      "Epoch:  26 | Step:  71 | Val Loss:  0.7093839049339294\n",
      "Epoch:  26 | Step:  72 | Val Loss:  0.8877373337745667\n",
      "Epoch:  26 | Step:  73 | Val Loss:  0.47176116704940796\n",
      "Epoch:  26 | Step:  74 | Val Loss:  0.4379236102104187\n",
      "Epoch:  26 | Step:  75 | Val Loss:  0.4960840940475464\n",
      "Epoch:  26 | Step:  76 | Val Loss:  0.596060037612915\n",
      "Epoch:  26 | Step:  77 | Val Loss:  0.511117696762085\n",
      "Epoch:  26 | Step:  78 | Val Loss:  0.4654857814311981\n",
      "Epoch:  26 | Step:  79 | Val Loss:  0.4694754481315613\n",
      "Epoch:  26 | Step:  80 | Val Loss:  0.4910670816898346\n",
      "Epoch:  26 | Step:  81 | Val Loss:  0.6031461954116821\n",
      "Epoch:  26 | Step:  82 | Val Loss:  0.49746954441070557\n",
      "Epoch:  26 | Step:  83 | Val Loss:  0.6291536092758179\n",
      "Epoch:  26 | Step:  84 | Val Loss:  0.6013819575309753\n",
      "Epoch:  26 | Step:  85 | Val Loss:  0.5109896659851074\n",
      "Epoch:  26 | Step:  86 | Val Loss:  0.6092919111251831\n",
      "Epoch:  26 | Step:  87 | Val Loss:  0.6051155924797058\n",
      "Epoch:  26 | Step:  88 | Val Loss:  0.4732634127140045\n",
      "Epoch:  26 | Step:  89 | Val Loss:  0.6732593774795532\n",
      "Epoch:  26 | Step:  90 | Val Loss:  0.5157618522644043\n",
      "Epoch:  26 | Step:  91 | Val Loss:  0.516375720500946\n",
      "Epoch:  26 | Step:  92 | Val Loss:  0.4757283926010132\n",
      "Epoch:  26 | Step:  93 | Val Loss:  0.7459821701049805\n",
      "Epoch:  26 | Step:  94 | Val Loss:  0.5966095924377441\n",
      "Epoch:  26 | Step:  95 | Val Loss:  0.551101565361023\n",
      "Epoch:  26 | Step:  96 | Val Loss:  0.7148032188415527\n",
      "Epoch:  26 | Step:  97 | Val Loss:  0.7319408059120178\n",
      "Epoch:  26 | Step:  98 | Val Loss:  0.6985548138618469\n",
      "Epoch:  26 | Step:  99 | Val Loss:  0.29700255393981934\n",
      "Epoch:  26 | Step:  100 | Val Loss:  0.4913487136363983\n",
      "Epoch:  26 | Step:  101 | Val Loss:  0.5907189249992371\n",
      "Epoch:  26 | Step:  102 | Val Loss:  0.5293728113174438\n",
      "Epoch:  26 | Step:  103 | Val Loss:  0.6300939321517944\n",
      "Epoch:  26 | Step:  104 | Val Loss:  0.6447658538818359\n",
      "Epoch:  26 | Step:  105 | Val Loss:  0.7455823421478271\n",
      "Epoch:  26 | Step:  106 | Val Loss:  0.5992900133132935\n",
      "Epoch:  26 | Step:  107 | Val Loss:  0.5485223531723022\n",
      "Epoch:  26 | Step:  108 | Val Loss:  0.43913596868515015\n",
      "Epoch:  26 | Step:  109 | Val Loss:  0.4957757890224457\n",
      "Epoch:  26 | Step:  110 | Val Loss:  0.4158813953399658\n",
      "Epoch:  26 | Step:  111 | Val Loss:  0.6287219524383545\n",
      "Epoch:  26 | Step:  112 | Val Loss:  0.3832775056362152\n",
      "Epoch:  26 | Step:  113 | Val Loss:  0.6415820121765137\n",
      "Epoch:  26 | Step:  114 | Val Loss:  0.5177745819091797\n",
      "Epoch:  26 | Step:  115 | Val Loss:  0.5708233714103699\n",
      "Epoch:  26 | Step:  116 | Val Loss:  0.41573280096054077\n",
      "Epoch:  26 | Step:  117 | Val Loss:  0.5151296257972717\n",
      "Epoch:  26 | Step:  118 | Val Loss:  0.5720750093460083\n",
      "Epoch:  26 | Step:  119 | Val Loss:  0.41089242696762085\n",
      "Epoch:  26 | Step:  120 | Val Loss:  0.512714684009552\n",
      "Epoch:  26 | Step:  121 | Val Loss:  0.48996514081954956\n",
      "Epoch:  26 | Step:  122 | Val Loss:  0.3831706941127777\n",
      "Epoch:  26 | Step:  123 | Val Loss:  0.3979630470275879\n",
      "Epoch:  26 | Step:  124 | Val Loss:  0.6912370324134827\n",
      "Epoch:  26 | Step:  125 | Val Loss:  0.2940857410430908\n",
      "Epoch:  26 | Train Loss:  tensor(0.5318, device='cuda:0') | Val Loss:  tensor(0.5647, device='cuda:0')\n",
      "Epoch:  27 | Step:  500 | Train Loss:  0.6060057878494263\n",
      "Epoch:  27 | Step:  1 | Val Loss:  0.6682183146476746\n",
      "Epoch:  27 | Step:  2 | Val Loss:  0.5337867736816406\n",
      "Epoch:  27 | Step:  3 | Val Loss:  0.6249489784240723\n",
      "Epoch:  27 | Step:  4 | Val Loss:  0.463008314371109\n",
      "Epoch:  27 | Step:  5 | Val Loss:  0.7021815776824951\n",
      "Epoch:  27 | Step:  6 | Val Loss:  0.6253149509429932\n",
      "Epoch:  27 | Step:  7 | Val Loss:  0.5172934532165527\n",
      "Epoch:  27 | Step:  8 | Val Loss:  0.3828849792480469\n",
      "Epoch:  27 | Step:  9 | Val Loss:  0.3468615412712097\n",
      "Epoch:  27 | Step:  10 | Val Loss:  0.4011985659599304\n",
      "Epoch:  27 | Step:  11 | Val Loss:  0.5367894172668457\n",
      "Epoch:  27 | Step:  12 | Val Loss:  0.4749756157398224\n",
      "Epoch:  27 | Step:  13 | Val Loss:  0.6080013513565063\n",
      "Epoch:  27 | Step:  14 | Val Loss:  0.40545693039894104\n",
      "Epoch:  27 | Step:  15 | Val Loss:  0.5396596193313599\n",
      "Epoch:  27 | Step:  16 | Val Loss:  0.5197218060493469\n",
      "Epoch:  27 | Step:  17 | Val Loss:  0.34924960136413574\n",
      "Epoch:  27 | Step:  18 | Val Loss:  0.6599097847938538\n",
      "Epoch:  27 | Step:  19 | Val Loss:  0.6895560622215271\n",
      "Epoch:  27 | Step:  20 | Val Loss:  0.4854043126106262\n",
      "Epoch:  27 | Step:  21 | Val Loss:  0.5200263857841492\n",
      "Epoch:  27 | Step:  22 | Val Loss:  0.7100896835327148\n",
      "Epoch:  27 | Step:  23 | Val Loss:  0.4827130138874054\n",
      "Epoch:  27 | Step:  24 | Val Loss:  0.6459304094314575\n",
      "Epoch:  27 | Step:  25 | Val Loss:  0.4603736996650696\n",
      "Epoch:  27 | Step:  26 | Val Loss:  0.38760703802108765\n",
      "Epoch:  27 | Step:  27 | Val Loss:  0.630639374256134\n",
      "Epoch:  27 | Step:  28 | Val Loss:  0.4906357526779175\n",
      "Epoch:  27 | Step:  29 | Val Loss:  0.575896143913269\n",
      "Epoch:  27 | Step:  30 | Val Loss:  0.6639251708984375\n",
      "Epoch:  27 | Step:  31 | Val Loss:  0.4691435694694519\n",
      "Epoch:  27 | Step:  32 | Val Loss:  0.5742572546005249\n",
      "Epoch:  27 | Step:  33 | Val Loss:  0.5521970987319946\n",
      "Epoch:  27 | Step:  34 | Val Loss:  0.5784704685211182\n",
      "Epoch:  27 | Step:  35 | Val Loss:  0.6026527881622314\n",
      "Epoch:  27 | Step:  36 | Val Loss:  0.45664453506469727\n",
      "Epoch:  27 | Step:  37 | Val Loss:  0.5358038544654846\n",
      "Epoch:  27 | Step:  38 | Val Loss:  0.703136682510376\n",
      "Epoch:  27 | Step:  39 | Val Loss:  0.5423868894577026\n",
      "Epoch:  27 | Step:  40 | Val Loss:  0.5692663788795471\n",
      "Epoch:  27 | Step:  41 | Val Loss:  0.7906891107559204\n",
      "Epoch:  27 | Step:  42 | Val Loss:  0.45328986644744873\n",
      "Epoch:  27 | Step:  43 | Val Loss:  0.7010093331336975\n",
      "Epoch:  27 | Step:  44 | Val Loss:  0.5232495069503784\n",
      "Epoch:  27 | Step:  45 | Val Loss:  0.36531418561935425\n",
      "Epoch:  27 | Step:  46 | Val Loss:  0.5750740170478821\n",
      "Epoch:  27 | Step:  47 | Val Loss:  0.590370774269104\n",
      "Epoch:  27 | Step:  48 | Val Loss:  0.5943396091461182\n",
      "Epoch:  27 | Step:  49 | Val Loss:  0.600657045841217\n",
      "Epoch:  27 | Step:  50 | Val Loss:  0.4266793131828308\n",
      "Epoch:  27 | Step:  51 | Val Loss:  0.6239192485809326\n",
      "Epoch:  27 | Step:  52 | Val Loss:  0.720880925655365\n",
      "Epoch:  27 | Step:  53 | Val Loss:  0.768560528755188\n",
      "Epoch:  27 | Step:  54 | Val Loss:  0.6410562992095947\n",
      "Epoch:  27 | Step:  55 | Val Loss:  0.6703631281852722\n",
      "Epoch:  27 | Step:  56 | Val Loss:  0.5328177809715271\n",
      "Epoch:  27 | Step:  57 | Val Loss:  0.5335464477539062\n",
      "Epoch:  27 | Step:  58 | Val Loss:  0.7468489408493042\n",
      "Epoch:  27 | Step:  59 | Val Loss:  0.660248875617981\n",
      "Epoch:  27 | Step:  60 | Val Loss:  0.6302934288978577\n",
      "Epoch:  27 | Step:  61 | Val Loss:  0.42539840936660767\n",
      "Epoch:  27 | Step:  62 | Val Loss:  0.49083513021469116\n",
      "Epoch:  27 | Step:  63 | Val Loss:  0.6876060962677002\n",
      "Epoch:  27 | Step:  64 | Val Loss:  0.6990584135055542\n",
      "Epoch:  27 | Step:  65 | Val Loss:  0.6420568227767944\n",
      "Epoch:  27 | Step:  66 | Val Loss:  0.5688512325286865\n",
      "Epoch:  27 | Step:  67 | Val Loss:  0.38142943382263184\n",
      "Epoch:  27 | Step:  68 | Val Loss:  0.6328389644622803\n",
      "Epoch:  27 | Step:  69 | Val Loss:  0.44061383605003357\n",
      "Epoch:  27 | Step:  70 | Val Loss:  0.48984163999557495\n",
      "Epoch:  27 | Step:  71 | Val Loss:  0.43362826108932495\n",
      "Epoch:  27 | Step:  72 | Val Loss:  0.5017786026000977\n",
      "Epoch:  27 | Step:  73 | Val Loss:  0.5105152726173401\n",
      "Epoch:  27 | Step:  74 | Val Loss:  0.46394020318984985\n",
      "Epoch:  27 | Step:  75 | Val Loss:  0.6791473031044006\n",
      "Epoch:  27 | Step:  76 | Val Loss:  0.6409486532211304\n",
      "Epoch:  27 | Step:  77 | Val Loss:  0.4565253257751465\n",
      "Epoch:  27 | Step:  78 | Val Loss:  0.49143660068511963\n",
      "Epoch:  27 | Step:  79 | Val Loss:  0.5323735475540161\n",
      "Epoch:  27 | Step:  80 | Val Loss:  0.5136592388153076\n",
      "Epoch:  27 | Step:  81 | Val Loss:  0.457030326128006\n",
      "Epoch:  27 | Step:  82 | Val Loss:  0.7543880343437195\n",
      "Epoch:  27 | Step:  83 | Val Loss:  0.3916546106338501\n",
      "Epoch:  27 | Step:  84 | Val Loss:  0.5318474173545837\n",
      "Epoch:  27 | Step:  85 | Val Loss:  0.8345208168029785\n",
      "Epoch:  27 | Step:  86 | Val Loss:  0.6300745606422424\n",
      "Epoch:  27 | Step:  87 | Val Loss:  0.5312883853912354\n",
      "Epoch:  27 | Step:  88 | Val Loss:  0.6107735633850098\n",
      "Epoch:  27 | Step:  89 | Val Loss:  0.5198988914489746\n",
      "Epoch:  27 | Step:  90 | Val Loss:  0.4154115915298462\n",
      "Epoch:  27 | Step:  91 | Val Loss:  0.5524438619613647\n",
      "Epoch:  27 | Step:  92 | Val Loss:  0.5131581425666809\n",
      "Epoch:  27 | Step:  93 | Val Loss:  0.5219225883483887\n",
      "Epoch:  27 | Step:  94 | Val Loss:  0.6785012483596802\n",
      "Epoch:  27 | Step:  95 | Val Loss:  0.6863031387329102\n",
      "Epoch:  27 | Step:  96 | Val Loss:  0.7578728199005127\n",
      "Epoch:  27 | Step:  97 | Val Loss:  0.6685622930526733\n",
      "Epoch:  27 | Step:  98 | Val Loss:  0.24472206830978394\n",
      "Epoch:  27 | Step:  99 | Val Loss:  0.492125928401947\n",
      "Epoch:  27 | Step:  100 | Val Loss:  0.36590781807899475\n",
      "Epoch:  27 | Step:  101 | Val Loss:  0.4098755121231079\n",
      "Epoch:  27 | Step:  102 | Val Loss:  0.6204017400741577\n",
      "Epoch:  27 | Step:  103 | Val Loss:  0.6791984438896179\n",
      "Epoch:  27 | Step:  104 | Val Loss:  0.5189554691314697\n",
      "Epoch:  27 | Step:  105 | Val Loss:  0.5975527763366699\n",
      "Epoch:  27 | Step:  106 | Val Loss:  0.5995652079582214\n",
      "Epoch:  27 | Step:  107 | Val Loss:  0.6256951689720154\n",
      "Epoch:  27 | Step:  108 | Val Loss:  0.637597918510437\n",
      "Epoch:  27 | Step:  109 | Val Loss:  0.5153998136520386\n",
      "Epoch:  27 | Step:  110 | Val Loss:  0.5218407511711121\n",
      "Epoch:  27 | Step:  111 | Val Loss:  0.6220381259918213\n",
      "Epoch:  27 | Step:  112 | Val Loss:  0.500376284122467\n",
      "Epoch:  27 | Step:  113 | Val Loss:  0.5756574869155884\n",
      "Epoch:  27 | Step:  114 | Val Loss:  0.5360569953918457\n",
      "Epoch:  27 | Step:  115 | Val Loss:  0.6938179731369019\n",
      "Epoch:  27 | Step:  116 | Val Loss:  0.7202562093734741\n",
      "Epoch:  27 | Step:  117 | Val Loss:  0.5492715835571289\n",
      "Epoch:  27 | Step:  118 | Val Loss:  0.6701840162277222\n",
      "Epoch:  27 | Step:  119 | Val Loss:  0.6374306678771973\n",
      "Epoch:  27 | Step:  120 | Val Loss:  0.6127548217773438\n",
      "Epoch:  27 | Step:  121 | Val Loss:  0.5759705305099487\n",
      "Epoch:  27 | Step:  122 | Val Loss:  0.6646791696548462\n",
      "Epoch:  27 | Step:  123 | Val Loss:  0.4739253520965576\n",
      "Epoch:  27 | Step:  124 | Val Loss:  0.4936728775501251\n",
      "Epoch:  27 | Step:  125 | Val Loss:  0.46190154552459717\n",
      "Epoch:  27 | Train Loss:  tensor(0.5290, device='cuda:0') | Val Loss:  tensor(0.5615, device='cuda:0')\n",
      "Epoch:  28 | Step:  500 | Train Loss:  0.6388466358184814\n",
      "Epoch:  28 | Step:  1 | Val Loss:  0.541407585144043\n",
      "Epoch:  28 | Step:  2 | Val Loss:  0.5829182267189026\n",
      "Epoch:  28 | Step:  3 | Val Loss:  0.5586699843406677\n",
      "Epoch:  28 | Step:  4 | Val Loss:  0.5094482898712158\n",
      "Epoch:  28 | Step:  5 | Val Loss:  0.6602914333343506\n",
      "Epoch:  28 | Step:  6 | Val Loss:  0.7665789723396301\n",
      "Epoch:  28 | Step:  7 | Val Loss:  0.5247935652732849\n",
      "Epoch:  28 | Step:  8 | Val Loss:  0.6929783821105957\n",
      "Epoch:  28 | Step:  9 | Val Loss:  0.349861741065979\n",
      "Epoch:  28 | Step:  10 | Val Loss:  0.5671600103378296\n",
      "Epoch:  28 | Step:  11 | Val Loss:  0.6966919898986816\n",
      "Epoch:  28 | Step:  12 | Val Loss:  0.2700519859790802\n",
      "Epoch:  28 | Step:  13 | Val Loss:  0.4152916669845581\n",
      "Epoch:  28 | Step:  14 | Val Loss:  0.5566670894622803\n",
      "Epoch:  28 | Step:  15 | Val Loss:  0.5014292597770691\n",
      "Epoch:  28 | Step:  16 | Val Loss:  0.4393313229084015\n",
      "Epoch:  28 | Step:  17 | Val Loss:  0.5877569913864136\n",
      "Epoch:  28 | Step:  18 | Val Loss:  0.6710817813873291\n",
      "Epoch:  28 | Step:  19 | Val Loss:  0.6970318555831909\n",
      "Epoch:  28 | Step:  20 | Val Loss:  0.4933440685272217\n",
      "Epoch:  28 | Step:  21 | Val Loss:  0.638248860836029\n",
      "Epoch:  28 | Step:  22 | Val Loss:  0.4787742495536804\n",
      "Epoch:  28 | Step:  23 | Val Loss:  0.5901225209236145\n",
      "Epoch:  28 | Step:  24 | Val Loss:  0.7404282093048096\n",
      "Epoch:  28 | Step:  25 | Val Loss:  0.4956914484500885\n",
      "Epoch:  28 | Step:  26 | Val Loss:  0.6671510934829712\n",
      "Epoch:  28 | Step:  27 | Val Loss:  0.7574076056480408\n",
      "Epoch:  28 | Step:  28 | Val Loss:  0.6561264991760254\n",
      "Epoch:  28 | Step:  29 | Val Loss:  0.38535943627357483\n",
      "Epoch:  28 | Step:  30 | Val Loss:  0.6209014654159546\n",
      "Epoch:  28 | Step:  31 | Val Loss:  0.5333600044250488\n",
      "Epoch:  28 | Step:  32 | Val Loss:  0.8174985647201538\n",
      "Epoch:  28 | Step:  33 | Val Loss:  0.4305936098098755\n",
      "Epoch:  28 | Step:  34 | Val Loss:  0.6243596076965332\n",
      "Epoch:  28 | Step:  35 | Val Loss:  0.7870054841041565\n",
      "Epoch:  28 | Step:  36 | Val Loss:  0.5803694725036621\n",
      "Epoch:  28 | Step:  37 | Val Loss:  0.5648708343505859\n",
      "Epoch:  28 | Step:  38 | Val Loss:  0.3317575752735138\n",
      "Epoch:  28 | Step:  39 | Val Loss:  0.4847815930843353\n",
      "Epoch:  28 | Step:  40 | Val Loss:  0.6170870065689087\n",
      "Epoch:  28 | Step:  41 | Val Loss:  0.47073283791542053\n",
      "Epoch:  28 | Step:  42 | Val Loss:  0.3768532872200012\n",
      "Epoch:  28 | Step:  43 | Val Loss:  0.5923401117324829\n",
      "Epoch:  28 | Step:  44 | Val Loss:  0.4422367811203003\n",
      "Epoch:  28 | Step:  45 | Val Loss:  0.6308766007423401\n",
      "Epoch:  28 | Step:  46 | Val Loss:  0.7092391848564148\n",
      "Epoch:  28 | Step:  47 | Val Loss:  0.5172349810600281\n",
      "Epoch:  28 | Step:  48 | Val Loss:  0.3696820139884949\n",
      "Epoch:  28 | Step:  49 | Val Loss:  0.6630643606185913\n",
      "Epoch:  28 | Step:  50 | Val Loss:  0.4882938265800476\n",
      "Epoch:  28 | Step:  51 | Val Loss:  0.5414602756500244\n",
      "Epoch:  28 | Step:  52 | Val Loss:  0.6665856838226318\n",
      "Epoch:  28 | Step:  53 | Val Loss:  0.6468594074249268\n",
      "Epoch:  28 | Step:  54 | Val Loss:  0.3760085105895996\n",
      "Epoch:  28 | Step:  55 | Val Loss:  0.7228298783302307\n",
      "Epoch:  28 | Step:  56 | Val Loss:  0.5936379432678223\n",
      "Epoch:  28 | Step:  57 | Val Loss:  0.33770814538002014\n",
      "Epoch:  28 | Step:  58 | Val Loss:  0.5438257455825806\n",
      "Epoch:  28 | Step:  59 | Val Loss:  0.6522834300994873\n",
      "Epoch:  28 | Step:  60 | Val Loss:  0.5472458600997925\n",
      "Epoch:  28 | Step:  61 | Val Loss:  0.5514230728149414\n",
      "Epoch:  28 | Step:  62 | Val Loss:  0.44915011525154114\n",
      "Epoch:  28 | Step:  63 | Val Loss:  0.644243061542511\n",
      "Epoch:  28 | Step:  64 | Val Loss:  0.5001785159111023\n",
      "Epoch:  28 | Step:  65 | Val Loss:  0.4963797330856323\n",
      "Epoch:  28 | Step:  66 | Val Loss:  0.6350855231285095\n",
      "Epoch:  28 | Step:  67 | Val Loss:  0.6098489165306091\n",
      "Epoch:  28 | Step:  68 | Val Loss:  0.49083346128463745\n",
      "Epoch:  28 | Step:  69 | Val Loss:  0.5032948851585388\n",
      "Epoch:  28 | Step:  70 | Val Loss:  0.663845419883728\n",
      "Epoch:  28 | Step:  71 | Val Loss:  0.5035717487335205\n",
      "Epoch:  28 | Step:  72 | Val Loss:  0.4900497794151306\n",
      "Epoch:  28 | Step:  73 | Val Loss:  0.68706214427948\n",
      "Epoch:  28 | Step:  74 | Val Loss:  0.5321966409683228\n",
      "Epoch:  28 | Step:  75 | Val Loss:  0.5690656900405884\n",
      "Epoch:  28 | Step:  76 | Val Loss:  0.49085113406181335\n",
      "Epoch:  28 | Step:  77 | Val Loss:  0.6923646926879883\n",
      "Epoch:  28 | Step:  78 | Val Loss:  0.5567541718482971\n",
      "Epoch:  28 | Step:  79 | Val Loss:  0.4817790389060974\n",
      "Epoch:  28 | Step:  80 | Val Loss:  0.5117366313934326\n",
      "Epoch:  28 | Step:  81 | Val Loss:  0.49381253123283386\n",
      "Epoch:  28 | Step:  82 | Val Loss:  0.4585055112838745\n",
      "Epoch:  28 | Step:  83 | Val Loss:  0.5727463960647583\n",
      "Epoch:  28 | Step:  84 | Val Loss:  0.5830649137496948\n",
      "Epoch:  28 | Step:  85 | Val Loss:  0.5787116289138794\n",
      "Epoch:  28 | Step:  86 | Val Loss:  0.4717159569263458\n",
      "Epoch:  28 | Step:  87 | Val Loss:  0.6390674114227295\n",
      "Epoch:  28 | Step:  88 | Val Loss:  0.5535399913787842\n",
      "Epoch:  28 | Step:  89 | Val Loss:  0.6858187913894653\n",
      "Epoch:  28 | Step:  90 | Val Loss:  0.5567266345024109\n",
      "Epoch:  28 | Step:  91 | Val Loss:  0.5734623074531555\n",
      "Epoch:  28 | Step:  92 | Val Loss:  0.7145512104034424\n",
      "Epoch:  28 | Step:  93 | Val Loss:  0.530558168888092\n",
      "Epoch:  28 | Step:  94 | Val Loss:  0.7129836082458496\n",
      "Epoch:  28 | Step:  95 | Val Loss:  0.44058188796043396\n",
      "Epoch:  28 | Step:  96 | Val Loss:  0.6780542135238647\n",
      "Epoch:  28 | Step:  97 | Val Loss:  0.4067680239677429\n",
      "Epoch:  28 | Step:  98 | Val Loss:  0.347365140914917\n",
      "Epoch:  28 | Step:  99 | Val Loss:  0.49293550848960876\n",
      "Epoch:  28 | Step:  100 | Val Loss:  0.5745086669921875\n",
      "Epoch:  28 | Step:  101 | Val Loss:  0.7563551664352417\n",
      "Epoch:  28 | Step:  102 | Val Loss:  0.6669889092445374\n",
      "Epoch:  28 | Step:  103 | Val Loss:  0.6784802675247192\n",
      "Epoch:  28 | Step:  104 | Val Loss:  0.5455641746520996\n",
      "Epoch:  28 | Step:  105 | Val Loss:  0.521277129650116\n",
      "Epoch:  28 | Step:  106 | Val Loss:  0.6024960875511169\n",
      "Epoch:  28 | Step:  107 | Val Loss:  0.42962646484375\n",
      "Epoch:  28 | Step:  108 | Val Loss:  0.6514194011688232\n",
      "Epoch:  28 | Step:  109 | Val Loss:  0.46530574560165405\n",
      "Epoch:  28 | Step:  110 | Val Loss:  0.49378493428230286\n",
      "Epoch:  28 | Step:  111 | Val Loss:  0.494962602853775\n",
      "Epoch:  28 | Step:  112 | Val Loss:  0.4116869866847992\n",
      "Epoch:  28 | Step:  113 | Val Loss:  0.46110671758651733\n",
      "Epoch:  28 | Step:  114 | Val Loss:  0.4470309019088745\n",
      "Epoch:  28 | Step:  115 | Val Loss:  0.5053768157958984\n",
      "Epoch:  28 | Step:  116 | Val Loss:  0.40423786640167236\n",
      "Epoch:  28 | Step:  117 | Val Loss:  0.5637305378913879\n",
      "Epoch:  28 | Step:  118 | Val Loss:  0.4429314434528351\n",
      "Epoch:  28 | Step:  119 | Val Loss:  0.7437626123428345\n",
      "Epoch:  28 | Step:  120 | Val Loss:  0.5504353046417236\n",
      "Epoch:  28 | Step:  121 | Val Loss:  0.5119184255599976\n",
      "Epoch:  28 | Step:  122 | Val Loss:  0.7713751196861267\n",
      "Epoch:  28 | Step:  123 | Val Loss:  0.4665645956993103\n",
      "Epoch:  28 | Step:  124 | Val Loss:  0.5749223232269287\n",
      "Epoch:  28 | Step:  125 | Val Loss:  0.5903644561767578\n",
      "Epoch:  28 | Train Loss:  tensor(0.5262, device='cuda:0') | Val Loss:  tensor(0.5572, device='cuda:0')\n",
      "Epoch:  29 | Step:  500 | Train Loss:  0.48267680406570435\n",
      "Epoch:  29 | Step:  1 | Val Loss:  0.5208073854446411\n",
      "Epoch:  29 | Step:  2 | Val Loss:  0.4480377435684204\n",
      "Epoch:  29 | Step:  3 | Val Loss:  0.6590977907180786\n",
      "Epoch:  29 | Step:  4 | Val Loss:  0.505407452583313\n",
      "Epoch:  29 | Step:  5 | Val Loss:  0.6013176441192627\n",
      "Epoch:  29 | Step:  6 | Val Loss:  0.6138311624526978\n",
      "Epoch:  29 | Step:  7 | Val Loss:  0.5401697158813477\n",
      "Epoch:  29 | Step:  8 | Val Loss:  0.6899901032447815\n",
      "Epoch:  29 | Step:  9 | Val Loss:  0.7373450994491577\n",
      "Epoch:  29 | Step:  10 | Val Loss:  0.5266197323799133\n",
      "Epoch:  29 | Step:  11 | Val Loss:  0.6359543800354004\n",
      "Epoch:  29 | Step:  12 | Val Loss:  0.4170341491699219\n",
      "Epoch:  29 | Step:  13 | Val Loss:  0.32509925961494446\n",
      "Epoch:  29 | Step:  14 | Val Loss:  0.5547659397125244\n",
      "Epoch:  29 | Step:  15 | Val Loss:  0.37481266260147095\n",
      "Epoch:  29 | Step:  16 | Val Loss:  0.5933483839035034\n",
      "Epoch:  29 | Step:  17 | Val Loss:  0.6103026866912842\n",
      "Epoch:  29 | Step:  18 | Val Loss:  0.5793657898902893\n",
      "Epoch:  29 | Step:  19 | Val Loss:  0.47187769412994385\n",
      "Epoch:  29 | Step:  20 | Val Loss:  0.6948435306549072\n",
      "Epoch:  29 | Step:  21 | Val Loss:  0.4857128858566284\n",
      "Epoch:  29 | Step:  22 | Val Loss:  0.48889458179473877\n",
      "Epoch:  29 | Step:  23 | Val Loss:  0.585210919380188\n",
      "Epoch:  29 | Step:  24 | Val Loss:  0.6022835969924927\n",
      "Epoch:  29 | Step:  25 | Val Loss:  0.5030711889266968\n",
      "Epoch:  29 | Step:  26 | Val Loss:  0.5195629596710205\n",
      "Epoch:  29 | Step:  27 | Val Loss:  0.6186079382896423\n",
      "Epoch:  29 | Step:  28 | Val Loss:  0.5580103993415833\n",
      "Epoch:  29 | Step:  29 | Val Loss:  0.685771107673645\n",
      "Epoch:  29 | Step:  30 | Val Loss:  0.7515882253646851\n",
      "Epoch:  29 | Step:  31 | Val Loss:  0.6644191741943359\n",
      "Epoch:  29 | Step:  32 | Val Loss:  0.5198793411254883\n",
      "Epoch:  29 | Step:  33 | Val Loss:  0.47546207904815674\n",
      "Epoch:  29 | Step:  34 | Val Loss:  0.653226375579834\n",
      "Epoch:  29 | Step:  35 | Val Loss:  0.6623758673667908\n",
      "Epoch:  29 | Step:  36 | Val Loss:  0.5007290840148926\n",
      "Epoch:  29 | Step:  37 | Val Loss:  0.44706571102142334\n",
      "Epoch:  29 | Step:  38 | Val Loss:  0.4648231267929077\n",
      "Epoch:  29 | Step:  39 | Val Loss:  0.4815657436847687\n",
      "Epoch:  29 | Step:  40 | Val Loss:  0.49591881036758423\n",
      "Epoch:  29 | Step:  41 | Val Loss:  0.5667749047279358\n",
      "Epoch:  29 | Step:  42 | Val Loss:  0.5003021955490112\n",
      "Epoch:  29 | Step:  43 | Val Loss:  0.5760800242424011\n",
      "Epoch:  29 | Step:  44 | Val Loss:  0.47463059425354004\n",
      "Epoch:  29 | Step:  45 | Val Loss:  0.5287380218505859\n",
      "Epoch:  29 | Step:  46 | Val Loss:  0.62068772315979\n",
      "Epoch:  29 | Step:  47 | Val Loss:  0.4890083074569702\n",
      "Epoch:  29 | Step:  48 | Val Loss:  0.37891173362731934\n",
      "Epoch:  29 | Step:  49 | Val Loss:  0.6127191185951233\n",
      "Epoch:  29 | Step:  50 | Val Loss:  0.7041079998016357\n",
      "Epoch:  29 | Step:  51 | Val Loss:  0.4702288508415222\n",
      "Epoch:  29 | Step:  52 | Val Loss:  0.5514841079711914\n",
      "Epoch:  29 | Step:  53 | Val Loss:  0.6444776058197021\n",
      "Epoch:  29 | Step:  54 | Val Loss:  0.6282704472541809\n",
      "Epoch:  29 | Step:  55 | Val Loss:  0.624550461769104\n",
      "Epoch:  29 | Step:  56 | Val Loss:  0.3770429193973541\n",
      "Epoch:  29 | Step:  57 | Val Loss:  0.47293025255203247\n",
      "Epoch:  29 | Step:  58 | Val Loss:  0.4154279828071594\n",
      "Epoch:  29 | Step:  59 | Val Loss:  0.5732361078262329\n",
      "Epoch:  29 | Step:  60 | Val Loss:  0.6606109142303467\n",
      "Epoch:  29 | Step:  61 | Val Loss:  0.40110868215560913\n",
      "Epoch:  29 | Step:  62 | Val Loss:  0.5494349002838135\n",
      "Epoch:  29 | Step:  63 | Val Loss:  0.39599400758743286\n",
      "Epoch:  29 | Step:  64 | Val Loss:  0.6414626836776733\n",
      "Epoch:  29 | Step:  65 | Val Loss:  0.6794471740722656\n",
      "Epoch:  29 | Step:  66 | Val Loss:  0.6377006769180298\n",
      "Epoch:  29 | Step:  67 | Val Loss:  0.5211701393127441\n",
      "Epoch:  29 | Step:  68 | Val Loss:  0.6531532406806946\n",
      "Epoch:  29 | Step:  69 | Val Loss:  0.6334949731826782\n",
      "Epoch:  29 | Step:  70 | Val Loss:  0.3944026827812195\n",
      "Epoch:  29 | Step:  71 | Val Loss:  0.5898799300193787\n",
      "Epoch:  29 | Step:  72 | Val Loss:  0.4834405481815338\n",
      "Epoch:  29 | Step:  73 | Val Loss:  0.454725444316864\n",
      "Epoch:  29 | Step:  74 | Val Loss:  0.5885834693908691\n",
      "Epoch:  29 | Step:  75 | Val Loss:  0.5056036710739136\n",
      "Epoch:  29 | Step:  76 | Val Loss:  0.6523101329803467\n",
      "Epoch:  29 | Step:  77 | Val Loss:  0.5218566060066223\n",
      "Epoch:  29 | Step:  78 | Val Loss:  0.6263889074325562\n",
      "Epoch:  29 | Step:  79 | Val Loss:  0.4665892422199249\n",
      "Epoch:  29 | Step:  80 | Val Loss:  0.40699052810668945\n",
      "Epoch:  29 | Step:  81 | Val Loss:  0.634636640548706\n",
      "Epoch:  29 | Step:  82 | Val Loss:  0.727056086063385\n",
      "Epoch:  29 | Step:  83 | Val Loss:  0.49034634232521057\n",
      "Epoch:  29 | Step:  84 | Val Loss:  0.44764482975006104\n",
      "Epoch:  29 | Step:  85 | Val Loss:  0.5259086489677429\n",
      "Epoch:  29 | Step:  86 | Val Loss:  0.6035981178283691\n",
      "Epoch:  29 | Step:  87 | Val Loss:  0.4875820279121399\n",
      "Epoch:  29 | Step:  88 | Val Loss:  0.62226402759552\n",
      "Epoch:  29 | Step:  89 | Val Loss:  0.392255961894989\n",
      "Epoch:  29 | Step:  90 | Val Loss:  0.6281849145889282\n",
      "Epoch:  29 | Step:  91 | Val Loss:  0.6249359250068665\n",
      "Epoch:  29 | Step:  92 | Val Loss:  0.7230435609817505\n",
      "Epoch:  29 | Step:  93 | Val Loss:  0.7771766781806946\n",
      "Epoch:  29 | Step:  94 | Val Loss:  0.573487401008606\n",
      "Epoch:  29 | Step:  95 | Val Loss:  0.522533655166626\n",
      "Epoch:  29 | Step:  96 | Val Loss:  0.6424899101257324\n",
      "Epoch:  29 | Step:  97 | Val Loss:  0.5063177347183228\n",
      "Epoch:  29 | Step:  98 | Val Loss:  0.6933158040046692\n",
      "Epoch:  29 | Step:  99 | Val Loss:  0.5115896463394165\n",
      "Epoch:  29 | Step:  100 | Val Loss:  0.46593526005744934\n",
      "Epoch:  29 | Step:  101 | Val Loss:  0.5800740718841553\n",
      "Epoch:  29 | Step:  102 | Val Loss:  0.5417075753211975\n",
      "Epoch:  29 | Step:  103 | Val Loss:  0.5445579290390015\n",
      "Epoch:  29 | Step:  104 | Val Loss:  0.3697361350059509\n",
      "Epoch:  29 | Step:  105 | Val Loss:  0.5795642137527466\n",
      "Epoch:  29 | Step:  106 | Val Loss:  0.6160446405410767\n",
      "Epoch:  29 | Step:  107 | Val Loss:  0.6900426149368286\n",
      "Epoch:  29 | Step:  108 | Val Loss:  0.48310551047325134\n",
      "Epoch:  29 | Step:  109 | Val Loss:  0.45592057704925537\n",
      "Epoch:  29 | Step:  110 | Val Loss:  0.6949207186698914\n",
      "Epoch:  29 | Step:  111 | Val Loss:  0.4614329934120178\n",
      "Epoch:  29 | Step:  112 | Val Loss:  0.3942750096321106\n",
      "Epoch:  29 | Step:  113 | Val Loss:  0.5457994341850281\n",
      "Epoch:  29 | Step:  114 | Val Loss:  0.49411749839782715\n",
      "Epoch:  29 | Step:  115 | Val Loss:  0.46026986837387085\n",
      "Epoch:  29 | Step:  116 | Val Loss:  0.45700445771217346\n",
      "Epoch:  29 | Step:  117 | Val Loss:  0.5457956194877625\n",
      "Epoch:  29 | Step:  118 | Val Loss:  0.5824122428894043\n",
      "Epoch:  29 | Step:  119 | Val Loss:  0.6383463144302368\n",
      "Epoch:  29 | Step:  120 | Val Loss:  0.6213364601135254\n",
      "Epoch:  29 | Step:  121 | Val Loss:  0.5719553232192993\n",
      "Epoch:  29 | Step:  122 | Val Loss:  0.466098427772522\n",
      "Epoch:  29 | Step:  123 | Val Loss:  0.6706109642982483\n",
      "Epoch:  29 | Step:  124 | Val Loss:  0.45124050974845886\n",
      "Epoch:  29 | Step:  125 | Val Loss:  0.6194218397140503\n",
      "Epoch:  29 | Train Loss:  tensor(0.5231, device='cuda:0') | Val Loss:  tensor(0.5526, device='cuda:0')\n",
      "Epoch:  30 | Step:  500 | Train Loss:  0.3748009204864502\n",
      "Epoch:  30 | Step:  1 | Val Loss:  0.58702552318573\n",
      "Epoch:  30 | Step:  2 | Val Loss:  0.5423281192779541\n",
      "Epoch:  30 | Step:  3 | Val Loss:  0.5524652004241943\n",
      "Epoch:  30 | Step:  4 | Val Loss:  0.4951912760734558\n",
      "Epoch:  30 | Step:  5 | Val Loss:  0.44088590145111084\n",
      "Epoch:  30 | Step:  6 | Val Loss:  0.47903427481651306\n",
      "Epoch:  30 | Step:  7 | Val Loss:  0.4392789304256439\n",
      "Epoch:  30 | Step:  8 | Val Loss:  0.662266194820404\n",
      "Epoch:  30 | Step:  9 | Val Loss:  0.4588274359703064\n",
      "Epoch:  30 | Step:  10 | Val Loss:  0.658820629119873\n",
      "Epoch:  30 | Step:  11 | Val Loss:  0.5336363315582275\n",
      "Epoch:  30 | Step:  12 | Val Loss:  0.6783918142318726\n",
      "Epoch:  30 | Step:  13 | Val Loss:  0.29233890771865845\n",
      "Epoch:  30 | Step:  14 | Val Loss:  0.5227951407432556\n",
      "Epoch:  30 | Step:  15 | Val Loss:  0.6449524164199829\n",
      "Epoch:  30 | Step:  16 | Val Loss:  0.5632383823394775\n",
      "Epoch:  30 | Step:  17 | Val Loss:  0.44682246446609497\n",
      "Epoch:  30 | Step:  18 | Val Loss:  0.4486374855041504\n",
      "Epoch:  30 | Step:  19 | Val Loss:  0.6496485471725464\n",
      "Epoch:  30 | Step:  20 | Val Loss:  0.6748205423355103\n",
      "Epoch:  30 | Step:  21 | Val Loss:  0.5270659923553467\n",
      "Epoch:  30 | Step:  22 | Val Loss:  0.39163076877593994\n",
      "Epoch:  30 | Step:  23 | Val Loss:  0.6550291776657104\n",
      "Epoch:  30 | Step:  24 | Val Loss:  0.645724356174469\n",
      "Epoch:  30 | Step:  25 | Val Loss:  0.36855047941207886\n",
      "Epoch:  30 | Step:  26 | Val Loss:  0.47392207384109497\n",
      "Epoch:  30 | Step:  27 | Val Loss:  0.46511057019233704\n",
      "Epoch:  30 | Step:  28 | Val Loss:  0.5949779748916626\n",
      "Epoch:  30 | Step:  29 | Val Loss:  0.6201345920562744\n",
      "Epoch:  30 | Step:  30 | Val Loss:  0.6063985824584961\n",
      "Epoch:  30 | Step:  31 | Val Loss:  0.38273221254348755\n",
      "Epoch:  30 | Step:  32 | Val Loss:  0.49022233486175537\n",
      "Epoch:  30 | Step:  33 | Val Loss:  0.49899041652679443\n",
      "Epoch:  30 | Step:  34 | Val Loss:  0.7271519899368286\n",
      "Epoch:  30 | Step:  35 | Val Loss:  0.4194777309894562\n",
      "Epoch:  30 | Step:  36 | Val Loss:  0.6334426999092102\n",
      "Epoch:  30 | Step:  37 | Val Loss:  0.6689777374267578\n",
      "Epoch:  30 | Step:  38 | Val Loss:  0.4865866005420685\n",
      "Epoch:  30 | Step:  39 | Val Loss:  0.5281261205673218\n",
      "Epoch:  30 | Step:  40 | Val Loss:  0.5760724544525146\n",
      "Epoch:  30 | Step:  41 | Val Loss:  0.44353312253952026\n",
      "Epoch:  30 | Step:  42 | Val Loss:  0.4627365469932556\n",
      "Epoch:  30 | Step:  43 | Val Loss:  0.6370620727539062\n",
      "Epoch:  30 | Step:  44 | Val Loss:  0.6705857515335083\n",
      "Epoch:  30 | Step:  45 | Val Loss:  0.4541993737220764\n",
      "Epoch:  30 | Step:  46 | Val Loss:  0.4742831587791443\n",
      "Epoch:  30 | Step:  47 | Val Loss:  0.4444737434387207\n",
      "Epoch:  30 | Step:  48 | Val Loss:  0.5296522378921509\n",
      "Epoch:  30 | Step:  49 | Val Loss:  0.420767605304718\n",
      "Epoch:  30 | Step:  50 | Val Loss:  0.6362341642379761\n",
      "Epoch:  30 | Step:  51 | Val Loss:  0.529254674911499\n",
      "Epoch:  30 | Step:  52 | Val Loss:  0.6021606922149658\n",
      "Epoch:  30 | Step:  53 | Val Loss:  0.4813505709171295\n",
      "Epoch:  30 | Step:  54 | Val Loss:  0.6262511014938354\n",
      "Epoch:  30 | Step:  55 | Val Loss:  0.6041947603225708\n",
      "Epoch:  30 | Step:  56 | Val Loss:  0.702053427696228\n",
      "Epoch:  30 | Step:  57 | Val Loss:  0.5646352767944336\n",
      "Epoch:  30 | Step:  58 | Val Loss:  0.5837292671203613\n",
      "Epoch:  30 | Step:  59 | Val Loss:  0.7106479406356812\n",
      "Epoch:  30 | Step:  60 | Val Loss:  0.5805929899215698\n",
      "Epoch:  30 | Step:  61 | Val Loss:  0.8293634653091431\n",
      "Epoch:  30 | Step:  62 | Val Loss:  0.4998883903026581\n",
      "Epoch:  30 | Step:  63 | Val Loss:  0.4894482493400574\n",
      "Epoch:  30 | Step:  64 | Val Loss:  0.3850955367088318\n",
      "Epoch:  30 | Step:  65 | Val Loss:  0.621505856513977\n",
      "Epoch:  30 | Step:  66 | Val Loss:  0.5587217807769775\n",
      "Epoch:  30 | Step:  67 | Val Loss:  0.5210495591163635\n",
      "Epoch:  30 | Step:  68 | Val Loss:  0.38951531052589417\n",
      "Epoch:  30 | Step:  69 | Val Loss:  0.7551260590553284\n",
      "Epoch:  30 | Step:  70 | Val Loss:  0.4341886639595032\n",
      "Epoch:  30 | Step:  71 | Val Loss:  0.6298220157623291\n",
      "Epoch:  30 | Step:  72 | Val Loss:  0.696237325668335\n",
      "Epoch:  30 | Step:  73 | Val Loss:  0.69089674949646\n",
      "Epoch:  30 | Step:  74 | Val Loss:  0.5934683084487915\n",
      "Epoch:  30 | Step:  75 | Val Loss:  0.512503981590271\n",
      "Epoch:  30 | Step:  76 | Val Loss:  0.6127970218658447\n",
      "Epoch:  30 | Step:  77 | Val Loss:  0.7173975706100464\n",
      "Epoch:  30 | Step:  78 | Val Loss:  0.7768336534500122\n",
      "Epoch:  30 | Step:  79 | Val Loss:  0.44758880138397217\n",
      "Epoch:  30 | Step:  80 | Val Loss:  0.6115890741348267\n",
      "Epoch:  30 | Step:  81 | Val Loss:  0.574815571308136\n",
      "Epoch:  30 | Step:  82 | Val Loss:  0.4975145757198334\n",
      "Epoch:  30 | Step:  83 | Val Loss:  0.43158602714538574\n",
      "Epoch:  30 | Step:  84 | Val Loss:  0.6030749678611755\n",
      "Epoch:  30 | Step:  85 | Val Loss:  0.5347495079040527\n",
      "Epoch:  30 | Step:  86 | Val Loss:  0.4629366993904114\n",
      "Epoch:  30 | Step:  87 | Val Loss:  0.5055047869682312\n",
      "Epoch:  30 | Step:  88 | Val Loss:  0.6613757610321045\n",
      "Epoch:  30 | Step:  89 | Val Loss:  0.5406365394592285\n",
      "Epoch:  30 | Step:  90 | Val Loss:  0.5791240930557251\n",
      "Epoch:  30 | Step:  91 | Val Loss:  0.45441001653671265\n",
      "Epoch:  30 | Step:  92 | Val Loss:  0.46139034628868103\n",
      "Epoch:  30 | Step:  93 | Val Loss:  0.592945396900177\n",
      "Epoch:  30 | Step:  94 | Val Loss:  0.6854772567749023\n",
      "Epoch:  30 | Step:  95 | Val Loss:  0.5227371454238892\n",
      "Epoch:  30 | Step:  96 | Val Loss:  0.4918712377548218\n",
      "Epoch:  30 | Step:  97 | Val Loss:  0.6001390218734741\n",
      "Epoch:  30 | Step:  98 | Val Loss:  0.7542473673820496\n",
      "Epoch:  30 | Step:  99 | Val Loss:  0.41926008462905884\n",
      "Epoch:  30 | Step:  100 | Val Loss:  0.43849247694015503\n",
      "Epoch:  30 | Step:  101 | Val Loss:  0.6276543736457825\n",
      "Epoch:  30 | Step:  102 | Val Loss:  0.6752374172210693\n",
      "Epoch:  30 | Step:  103 | Val Loss:  0.5233871340751648\n",
      "Epoch:  30 | Step:  104 | Val Loss:  0.5800176858901978\n",
      "Epoch:  30 | Step:  105 | Val Loss:  0.5881107449531555\n",
      "Epoch:  30 | Step:  106 | Val Loss:  0.3183326721191406\n",
      "Epoch:  30 | Step:  107 | Val Loss:  0.5310516357421875\n",
      "Epoch:  30 | Step:  108 | Val Loss:  0.44697338342666626\n",
      "Epoch:  30 | Step:  109 | Val Loss:  0.6719468832015991\n",
      "Epoch:  30 | Step:  110 | Val Loss:  0.4543052911758423\n",
      "Epoch:  30 | Step:  111 | Val Loss:  0.770580530166626\n",
      "Epoch:  30 | Step:  112 | Val Loss:  0.4461781084537506\n",
      "Epoch:  30 | Step:  113 | Val Loss:  0.49567314982414246\n",
      "Epoch:  30 | Step:  114 | Val Loss:  0.47262030839920044\n",
      "Epoch:  30 | Step:  115 | Val Loss:  0.7540184259414673\n",
      "Epoch:  30 | Step:  116 | Val Loss:  0.3319855034351349\n",
      "Epoch:  30 | Step:  117 | Val Loss:  0.5517522692680359\n",
      "Epoch:  30 | Step:  118 | Val Loss:  0.46230384707450867\n",
      "Epoch:  30 | Step:  119 | Val Loss:  0.5570447444915771\n",
      "Epoch:  30 | Step:  120 | Val Loss:  0.5310626029968262\n",
      "Epoch:  30 | Step:  121 | Val Loss:  0.6085633039474487\n",
      "Epoch:  30 | Step:  122 | Val Loss:  0.4807407855987549\n",
      "Epoch:  30 | Step:  123 | Val Loss:  0.5040915012359619\n",
      "Epoch:  30 | Step:  124 | Val Loss:  0.3337458074092865\n",
      "Epoch:  30 | Step:  125 | Val Loss:  0.5745522975921631\n",
      "Epoch:  30 | Train Loss:  tensor(0.5201, device='cuda:0') | Val Loss:  tensor(0.5485, device='cuda:0')\n",
      "Epoch:  31 | Step:  500 | Train Loss:  0.6878843903541565\n",
      "Epoch:  31 | Step:  1 | Val Loss:  0.5302914381027222\n",
      "Epoch:  31 | Step:  2 | Val Loss:  0.6951566934585571\n",
      "Epoch:  31 | Step:  3 | Val Loss:  0.5647025108337402\n",
      "Epoch:  31 | Step:  4 | Val Loss:  0.5541420578956604\n",
      "Epoch:  31 | Step:  5 | Val Loss:  0.423984169960022\n",
      "Epoch:  31 | Step:  6 | Val Loss:  0.4389494061470032\n",
      "Epoch:  31 | Step:  7 | Val Loss:  0.4457700848579407\n",
      "Epoch:  31 | Step:  8 | Val Loss:  0.6519428491592407\n",
      "Epoch:  31 | Step:  9 | Val Loss:  0.5654556751251221\n",
      "Epoch:  31 | Step:  10 | Val Loss:  0.6171104907989502\n",
      "Epoch:  31 | Step:  11 | Val Loss:  0.6349500417709351\n",
      "Epoch:  31 | Step:  12 | Val Loss:  0.661780595779419\n",
      "Epoch:  31 | Step:  13 | Val Loss:  0.5683125853538513\n",
      "Epoch:  31 | Step:  14 | Val Loss:  0.5358666181564331\n",
      "Epoch:  31 | Step:  15 | Val Loss:  0.5840727686882019\n",
      "Epoch:  31 | Step:  16 | Val Loss:  0.4401475787162781\n",
      "Epoch:  31 | Step:  17 | Val Loss:  0.3896005153656006\n",
      "Epoch:  31 | Step:  18 | Val Loss:  0.6317405700683594\n",
      "Epoch:  31 | Step:  19 | Val Loss:  0.5347865223884583\n",
      "Epoch:  31 | Step:  20 | Val Loss:  0.6657312512397766\n",
      "Epoch:  31 | Step:  21 | Val Loss:  0.44020533561706543\n",
      "Epoch:  31 | Step:  22 | Val Loss:  0.5466233491897583\n",
      "Epoch:  31 | Step:  23 | Val Loss:  0.37364834547042847\n",
      "Epoch:  31 | Step:  24 | Val Loss:  0.38134491443634033\n",
      "Epoch:  31 | Step:  25 | Val Loss:  0.4703545868396759\n",
      "Epoch:  31 | Step:  26 | Val Loss:  0.569000244140625\n",
      "Epoch:  31 | Step:  27 | Val Loss:  0.5655171275138855\n",
      "Epoch:  31 | Step:  28 | Val Loss:  0.7044035196304321\n",
      "Epoch:  31 | Step:  29 | Val Loss:  0.5519794821739197\n",
      "Epoch:  31 | Step:  30 | Val Loss:  0.6125179529190063\n",
      "Epoch:  31 | Step:  31 | Val Loss:  0.5283752679824829\n",
      "Epoch:  31 | Step:  32 | Val Loss:  0.42445769906044006\n",
      "Epoch:  31 | Step:  33 | Val Loss:  0.5651028156280518\n",
      "Epoch:  31 | Step:  34 | Val Loss:  0.4818052649497986\n",
      "Epoch:  31 | Step:  35 | Val Loss:  0.5183334350585938\n",
      "Epoch:  31 | Step:  36 | Val Loss:  0.44722557067871094\n",
      "Epoch:  31 | Step:  37 | Val Loss:  0.6557720899581909\n",
      "Epoch:  31 | Step:  38 | Val Loss:  0.4671420753002167\n",
      "Epoch:  31 | Step:  39 | Val Loss:  0.5760771036148071\n",
      "Epoch:  31 | Step:  40 | Val Loss:  0.4469096064567566\n",
      "Epoch:  31 | Step:  41 | Val Loss:  0.5175877213478088\n",
      "Epoch:  31 | Step:  42 | Val Loss:  0.4539928138256073\n",
      "Epoch:  31 | Step:  43 | Val Loss:  0.6458867788314819\n",
      "Epoch:  31 | Step:  44 | Val Loss:  0.4537386894226074\n",
      "Epoch:  31 | Step:  45 | Val Loss:  0.5497375726699829\n",
      "Epoch:  31 | Step:  46 | Val Loss:  0.5473577976226807\n",
      "Epoch:  31 | Step:  47 | Val Loss:  0.7004296779632568\n",
      "Epoch:  31 | Step:  48 | Val Loss:  0.6766283512115479\n",
      "Epoch:  31 | Step:  49 | Val Loss:  0.4175350069999695\n",
      "Epoch:  31 | Step:  50 | Val Loss:  0.5961225628852844\n",
      "Epoch:  31 | Step:  51 | Val Loss:  0.5847411751747131\n",
      "Epoch:  31 | Step:  52 | Val Loss:  0.7832708954811096\n",
      "Epoch:  31 | Step:  53 | Val Loss:  0.4471060037612915\n",
      "Epoch:  31 | Step:  54 | Val Loss:  0.6177085638046265\n",
      "Epoch:  31 | Step:  55 | Val Loss:  0.48119521141052246\n",
      "Epoch:  31 | Step:  56 | Val Loss:  0.5486716032028198\n",
      "Epoch:  31 | Step:  57 | Val Loss:  0.5233787298202515\n",
      "Epoch:  31 | Step:  58 | Val Loss:  0.611412763595581\n",
      "Epoch:  31 | Step:  59 | Val Loss:  0.46687349677085876\n",
      "Epoch:  31 | Step:  60 | Val Loss:  0.5824295282363892\n",
      "Epoch:  31 | Step:  61 | Val Loss:  0.537325382232666\n",
      "Epoch:  31 | Step:  62 | Val Loss:  0.4209407567977905\n",
      "Epoch:  31 | Step:  63 | Val Loss:  0.5654449462890625\n",
      "Epoch:  31 | Step:  64 | Val Loss:  0.6553002595901489\n",
      "Epoch:  31 | Step:  65 | Val Loss:  0.38228166103363037\n",
      "Epoch:  31 | Step:  66 | Val Loss:  0.7275621891021729\n",
      "Epoch:  31 | Step:  67 | Val Loss:  0.6297746896743774\n",
      "Epoch:  31 | Step:  68 | Val Loss:  0.5132579207420349\n",
      "Epoch:  31 | Step:  69 | Val Loss:  0.7087043523788452\n",
      "Epoch:  31 | Step:  70 | Val Loss:  0.4270001947879791\n",
      "Epoch:  31 | Step:  71 | Val Loss:  0.5166826248168945\n",
      "Epoch:  31 | Step:  72 | Val Loss:  0.5218071937561035\n",
      "Epoch:  31 | Step:  73 | Val Loss:  0.5495156049728394\n",
      "Epoch:  31 | Step:  74 | Val Loss:  0.4260050356388092\n",
      "Epoch:  31 | Step:  75 | Val Loss:  0.6511340141296387\n",
      "Epoch:  31 | Step:  76 | Val Loss:  0.6228898763656616\n",
      "Epoch:  31 | Step:  77 | Val Loss:  0.5162811875343323\n",
      "Epoch:  31 | Step:  78 | Val Loss:  0.6554718017578125\n",
      "Epoch:  31 | Step:  79 | Val Loss:  0.5859319567680359\n",
      "Epoch:  31 | Step:  80 | Val Loss:  0.3205142021179199\n",
      "Epoch:  31 | Step:  81 | Val Loss:  0.5299038887023926\n",
      "Epoch:  31 | Step:  82 | Val Loss:  0.44760799407958984\n",
      "Epoch:  31 | Step:  83 | Val Loss:  0.891131579875946\n",
      "Epoch:  31 | Step:  84 | Val Loss:  0.5776705741882324\n",
      "Epoch:  31 | Step:  85 | Val Loss:  0.5588290691375732\n",
      "Epoch:  31 | Step:  86 | Val Loss:  0.5852561593055725\n",
      "Epoch:  31 | Step:  87 | Val Loss:  0.5227445363998413\n",
      "Epoch:  31 | Step:  88 | Val Loss:  0.7041260004043579\n",
      "Epoch:  31 | Step:  89 | Val Loss:  0.5761943459510803\n",
      "Epoch:  31 | Step:  90 | Val Loss:  0.4676724672317505\n",
      "Epoch:  31 | Step:  91 | Val Loss:  0.48858752846717834\n",
      "Epoch:  31 | Step:  92 | Val Loss:  0.5582058429718018\n",
      "Epoch:  31 | Step:  93 | Val Loss:  0.5836731195449829\n",
      "Epoch:  31 | Step:  94 | Val Loss:  0.5410302877426147\n",
      "Epoch:  31 | Step:  95 | Val Loss:  0.5445275902748108\n",
      "Epoch:  31 | Step:  96 | Val Loss:  0.3449113667011261\n",
      "Epoch:  31 | Step:  97 | Val Loss:  0.5461499691009521\n",
      "Epoch:  31 | Step:  98 | Val Loss:  0.6906145811080933\n",
      "Epoch:  31 | Step:  99 | Val Loss:  0.4484623670578003\n",
      "Epoch:  31 | Step:  100 | Val Loss:  0.5108263492584229\n",
      "Epoch:  31 | Step:  101 | Val Loss:  0.5529853105545044\n",
      "Epoch:  31 | Step:  102 | Val Loss:  0.38219940662384033\n",
      "Epoch:  31 | Step:  103 | Val Loss:  0.5535404682159424\n",
      "Epoch:  31 | Step:  104 | Val Loss:  0.4062339663505554\n",
      "Epoch:  31 | Step:  105 | Val Loss:  0.46168971061706543\n",
      "Epoch:  31 | Step:  106 | Val Loss:  0.5090148448944092\n",
      "Epoch:  31 | Step:  107 | Val Loss:  0.4631025493144989\n",
      "Epoch:  31 | Step:  108 | Val Loss:  0.5492082238197327\n",
      "Epoch:  31 | Step:  109 | Val Loss:  0.7114118933677673\n",
      "Epoch:  31 | Step:  110 | Val Loss:  0.460031121969223\n",
      "Epoch:  31 | Step:  111 | Val Loss:  0.6782448887825012\n",
      "Epoch:  31 | Step:  112 | Val Loss:  0.4236089289188385\n",
      "Epoch:  31 | Step:  113 | Val Loss:  0.5323038101196289\n",
      "Epoch:  31 | Step:  114 | Val Loss:  0.5530978441238403\n",
      "Epoch:  31 | Step:  115 | Val Loss:  0.4619213342666626\n",
      "Epoch:  31 | Step:  116 | Val Loss:  0.5970924496650696\n",
      "Epoch:  31 | Step:  117 | Val Loss:  0.580762505531311\n",
      "Epoch:  31 | Step:  118 | Val Loss:  0.5936952233314514\n",
      "Epoch:  31 | Step:  119 | Val Loss:  0.43851837515830994\n",
      "Epoch:  31 | Step:  120 | Val Loss:  0.7188887000083923\n",
      "Epoch:  31 | Step:  121 | Val Loss:  0.521639347076416\n",
      "Epoch:  31 | Step:  122 | Val Loss:  0.6005491018295288\n",
      "Epoch:  31 | Step:  123 | Val Loss:  0.5972164869308472\n",
      "Epoch:  31 | Step:  124 | Val Loss:  0.5183898210525513\n",
      "Epoch:  31 | Step:  125 | Val Loss:  0.4654793441295624\n",
      "Epoch:  31 | Train Loss:  tensor(0.5171, device='cuda:0') | Val Loss:  tensor(0.5444, device='cuda:0')\n",
      "Epoch:  32 | Step:  500 | Train Loss:  0.4598080515861511\n",
      "Epoch:  32 | Step:  1 | Val Loss:  0.4969612956047058\n",
      "Epoch:  32 | Step:  2 | Val Loss:  0.4827720522880554\n",
      "Epoch:  32 | Step:  3 | Val Loss:  0.4799395203590393\n",
      "Epoch:  32 | Step:  4 | Val Loss:  0.5078089237213135\n",
      "Epoch:  32 | Step:  5 | Val Loss:  0.494005024433136\n",
      "Epoch:  32 | Step:  6 | Val Loss:  0.385209858417511\n",
      "Epoch:  32 | Step:  7 | Val Loss:  0.44037485122680664\n",
      "Epoch:  32 | Step:  8 | Val Loss:  0.18582946062088013\n",
      "Epoch:  32 | Step:  9 | Val Loss:  0.4867337942123413\n",
      "Epoch:  32 | Step:  10 | Val Loss:  0.5638319253921509\n",
      "Epoch:  32 | Step:  11 | Val Loss:  0.6514824628829956\n",
      "Epoch:  32 | Step:  12 | Val Loss:  0.6699891090393066\n",
      "Epoch:  32 | Step:  13 | Val Loss:  0.6051527261734009\n",
      "Epoch:  32 | Step:  14 | Val Loss:  0.4376061260700226\n",
      "Epoch:  32 | Step:  15 | Val Loss:  0.5912230610847473\n",
      "Epoch:  32 | Step:  16 | Val Loss:  0.39328524470329285\n",
      "Epoch:  32 | Step:  17 | Val Loss:  0.5998122096061707\n",
      "Epoch:  32 | Step:  18 | Val Loss:  0.6150416135787964\n",
      "Epoch:  32 | Step:  19 | Val Loss:  0.5160062313079834\n",
      "Epoch:  32 | Step:  20 | Val Loss:  0.8298636674880981\n",
      "Epoch:  32 | Step:  21 | Val Loss:  0.40882769227027893\n",
      "Epoch:  32 | Step:  22 | Val Loss:  0.7335366010665894\n",
      "Epoch:  32 | Step:  23 | Val Loss:  0.36787116527557373\n",
      "Epoch:  32 | Step:  24 | Val Loss:  0.6091015338897705\n",
      "Epoch:  32 | Step:  25 | Val Loss:  0.7314223051071167\n",
      "Epoch:  32 | Step:  26 | Val Loss:  0.480812668800354\n",
      "Epoch:  32 | Step:  27 | Val Loss:  0.5525673627853394\n",
      "Epoch:  32 | Step:  28 | Val Loss:  0.5813511610031128\n",
      "Epoch:  32 | Step:  29 | Val Loss:  0.5206030607223511\n",
      "Epoch:  32 | Step:  30 | Val Loss:  0.5893946886062622\n",
      "Epoch:  32 | Step:  31 | Val Loss:  0.5441615581512451\n",
      "Epoch:  32 | Step:  32 | Val Loss:  0.5230658054351807\n",
      "Epoch:  32 | Step:  33 | Val Loss:  0.5693690180778503\n",
      "Epoch:  32 | Step:  34 | Val Loss:  0.5953869819641113\n",
      "Epoch:  32 | Step:  35 | Val Loss:  0.6551445722579956\n",
      "Epoch:  32 | Step:  36 | Val Loss:  0.6104270815849304\n",
      "Epoch:  32 | Step:  37 | Val Loss:  0.5525343418121338\n",
      "Epoch:  32 | Step:  38 | Val Loss:  0.47722524404525757\n",
      "Epoch:  32 | Step:  39 | Val Loss:  0.45209890604019165\n",
      "Epoch:  32 | Step:  40 | Val Loss:  0.5410082340240479\n",
      "Epoch:  32 | Step:  41 | Val Loss:  0.6380341053009033\n",
      "Epoch:  32 | Step:  42 | Val Loss:  0.4538814425468445\n",
      "Epoch:  32 | Step:  43 | Val Loss:  0.3885289132595062\n",
      "Epoch:  32 | Step:  44 | Val Loss:  0.5584427118301392\n",
      "Epoch:  32 | Step:  45 | Val Loss:  0.4619171619415283\n",
      "Epoch:  32 | Step:  46 | Val Loss:  0.577125072479248\n",
      "Epoch:  32 | Step:  47 | Val Loss:  0.5118895173072815\n",
      "Epoch:  32 | Step:  48 | Val Loss:  0.4678097665309906\n",
      "Epoch:  32 | Step:  49 | Val Loss:  0.37843483686447144\n",
      "Epoch:  32 | Step:  50 | Val Loss:  0.5352877378463745\n",
      "Epoch:  32 | Step:  51 | Val Loss:  0.6546846032142639\n",
      "Epoch:  32 | Step:  52 | Val Loss:  0.5977965593338013\n",
      "Epoch:  32 | Step:  53 | Val Loss:  0.5795711278915405\n",
      "Epoch:  32 | Step:  54 | Val Loss:  0.4439489543437958\n",
      "Epoch:  32 | Step:  55 | Val Loss:  0.5039911270141602\n",
      "Epoch:  32 | Step:  56 | Val Loss:  0.5303897857666016\n",
      "Epoch:  32 | Step:  57 | Val Loss:  0.46121343970298767\n",
      "Epoch:  32 | Step:  58 | Val Loss:  0.4165171980857849\n",
      "Epoch:  32 | Step:  59 | Val Loss:  0.5335600972175598\n",
      "Epoch:  32 | Step:  60 | Val Loss:  0.4783661365509033\n",
      "Epoch:  32 | Step:  61 | Val Loss:  0.5684318542480469\n",
      "Epoch:  32 | Step:  62 | Val Loss:  0.5671185255050659\n",
      "Epoch:  32 | Step:  63 | Val Loss:  0.44860976934432983\n",
      "Epoch:  32 | Step:  64 | Val Loss:  0.5582811832427979\n",
      "Epoch:  32 | Step:  65 | Val Loss:  0.5833637714385986\n",
      "Epoch:  32 | Step:  66 | Val Loss:  0.6776531338691711\n",
      "Epoch:  32 | Step:  67 | Val Loss:  0.47831228375434875\n",
      "Epoch:  32 | Step:  68 | Val Loss:  0.5413526892662048\n",
      "Epoch:  32 | Step:  69 | Val Loss:  0.6814901232719421\n",
      "Epoch:  32 | Step:  70 | Val Loss:  0.5078672170639038\n",
      "Epoch:  32 | Step:  71 | Val Loss:  0.4815465211868286\n",
      "Epoch:  32 | Step:  72 | Val Loss:  0.6062513589859009\n",
      "Epoch:  32 | Step:  73 | Val Loss:  0.6465189456939697\n",
      "Epoch:  32 | Step:  74 | Val Loss:  0.450502872467041\n",
      "Epoch:  32 | Step:  75 | Val Loss:  0.5441470146179199\n",
      "Epoch:  32 | Step:  76 | Val Loss:  0.5451908707618713\n",
      "Epoch:  32 | Step:  77 | Val Loss:  0.6755704879760742\n",
      "Epoch:  32 | Step:  78 | Val Loss:  0.7407865524291992\n",
      "Epoch:  32 | Step:  79 | Val Loss:  0.46814191341400146\n",
      "Epoch:  32 | Step:  80 | Val Loss:  0.5109001398086548\n",
      "Epoch:  32 | Step:  81 | Val Loss:  0.5782711505889893\n",
      "Epoch:  32 | Step:  82 | Val Loss:  0.38268738985061646\n",
      "Epoch:  32 | Step:  83 | Val Loss:  0.6174566745758057\n",
      "Epoch:  32 | Step:  84 | Val Loss:  0.2868906855583191\n",
      "Epoch:  32 | Step:  85 | Val Loss:  0.5056639909744263\n",
      "Epoch:  32 | Step:  86 | Val Loss:  0.5844196081161499\n",
      "Epoch:  32 | Step:  87 | Val Loss:  0.48102450370788574\n",
      "Epoch:  32 | Step:  88 | Val Loss:  0.4862058460712433\n",
      "Epoch:  32 | Step:  89 | Val Loss:  0.5138603448867798\n",
      "Epoch:  32 | Step:  90 | Val Loss:  0.46057724952697754\n",
      "Epoch:  32 | Step:  91 | Val Loss:  0.7625575661659241\n",
      "Epoch:  32 | Step:  92 | Val Loss:  0.7078530788421631\n",
      "Epoch:  32 | Step:  93 | Val Loss:  0.5331966280937195\n",
      "Epoch:  32 | Step:  94 | Val Loss:  0.7026677131652832\n",
      "Epoch:  32 | Step:  95 | Val Loss:  0.6903809309005737\n",
      "Epoch:  32 | Step:  96 | Val Loss:  0.5638267397880554\n",
      "Epoch:  32 | Step:  97 | Val Loss:  0.7236118316650391\n",
      "Epoch:  32 | Step:  98 | Val Loss:  0.4527101516723633\n",
      "Epoch:  32 | Step:  99 | Val Loss:  0.49198082089424133\n",
      "Epoch:  32 | Step:  100 | Val Loss:  0.5103733539581299\n",
      "Epoch:  32 | Step:  101 | Val Loss:  0.4522431492805481\n",
      "Epoch:  32 | Step:  102 | Val Loss:  0.6406444907188416\n",
      "Epoch:  32 | Step:  103 | Val Loss:  0.4367627501487732\n",
      "Epoch:  32 | Step:  104 | Val Loss:  0.7450131177902222\n",
      "Epoch:  32 | Step:  105 | Val Loss:  0.4502212405204773\n",
      "Epoch:  32 | Step:  106 | Val Loss:  0.6680539846420288\n",
      "Epoch:  32 | Step:  107 | Val Loss:  0.5924035310745239\n",
      "Epoch:  32 | Step:  108 | Val Loss:  0.31049779057502747\n",
      "Epoch:  32 | Step:  109 | Val Loss:  0.3941574692726135\n",
      "Epoch:  32 | Step:  110 | Val Loss:  0.49926042556762695\n",
      "Epoch:  32 | Step:  111 | Val Loss:  0.5067903399467468\n",
      "Epoch:  32 | Step:  112 | Val Loss:  0.7376315593719482\n",
      "Epoch:  32 | Step:  113 | Val Loss:  0.6068953275680542\n",
      "Epoch:  32 | Step:  114 | Val Loss:  0.6225398778915405\n",
      "Epoch:  32 | Step:  115 | Val Loss:  0.4542537331581116\n",
      "Epoch:  32 | Step:  116 | Val Loss:  0.40906238555908203\n",
      "Epoch:  32 | Step:  117 | Val Loss:  0.7118905782699585\n",
      "Epoch:  32 | Step:  118 | Val Loss:  0.497916579246521\n",
      "Epoch:  32 | Step:  119 | Val Loss:  0.44030171632766724\n",
      "Epoch:  32 | Step:  120 | Val Loss:  0.3685542941093445\n",
      "Epoch:  32 | Step:  121 | Val Loss:  0.7010653614997864\n",
      "Epoch:  32 | Step:  122 | Val Loss:  0.6548872590065002\n",
      "Epoch:  32 | Step:  123 | Val Loss:  0.5364415645599365\n",
      "Epoch:  32 | Step:  124 | Val Loss:  0.6540669202804565\n",
      "Epoch:  32 | Step:  125 | Val Loss:  0.4091632664203644\n",
      "Epoch:  32 | Train Loss:  tensor(0.5147, device='cuda:0') | Val Loss:  tensor(0.5402, device='cuda:0')\n",
      "Epoch:  33 | Step:  500 | Train Loss:  0.6469460725784302\n",
      "Epoch:  33 | Step:  1 | Val Loss:  0.5079646110534668\n",
      "Epoch:  33 | Step:  2 | Val Loss:  0.458940327167511\n",
      "Epoch:  33 | Step:  3 | Val Loss:  0.5130054354667664\n",
      "Epoch:  33 | Step:  4 | Val Loss:  0.5688173770904541\n",
      "Epoch:  33 | Step:  5 | Val Loss:  0.4856799840927124\n",
      "Epoch:  33 | Step:  6 | Val Loss:  0.6213966608047485\n",
      "Epoch:  33 | Step:  7 | Val Loss:  0.5097372531890869\n",
      "Epoch:  33 | Step:  8 | Val Loss:  0.48852357268333435\n",
      "Epoch:  33 | Step:  9 | Val Loss:  0.4515261650085449\n",
      "Epoch:  33 | Step:  10 | Val Loss:  0.48418769240379333\n",
      "Epoch:  33 | Step:  11 | Val Loss:  0.6583028435707092\n",
      "Epoch:  33 | Step:  12 | Val Loss:  0.5929183959960938\n",
      "Epoch:  33 | Step:  13 | Val Loss:  0.6660040020942688\n",
      "Epoch:  33 | Step:  14 | Val Loss:  0.5444343090057373\n",
      "Epoch:  33 | Step:  15 | Val Loss:  0.5724061131477356\n",
      "Epoch:  33 | Step:  16 | Val Loss:  0.5972676873207092\n",
      "Epoch:  33 | Step:  17 | Val Loss:  0.5266225337982178\n",
      "Epoch:  33 | Step:  18 | Val Loss:  0.5757759809494019\n",
      "Epoch:  33 | Step:  19 | Val Loss:  0.5569417476654053\n",
      "Epoch:  33 | Step:  20 | Val Loss:  0.6039667129516602\n",
      "Epoch:  33 | Step:  21 | Val Loss:  0.6881735324859619\n",
      "Epoch:  33 | Step:  22 | Val Loss:  0.44606223702430725\n",
      "Epoch:  33 | Step:  23 | Val Loss:  0.7553936839103699\n",
      "Epoch:  33 | Step:  24 | Val Loss:  0.40047943592071533\n",
      "Epoch:  33 | Step:  25 | Val Loss:  0.7018702626228333\n",
      "Epoch:  33 | Step:  26 | Val Loss:  0.5738314390182495\n",
      "Epoch:  33 | Step:  27 | Val Loss:  0.5850034952163696\n",
      "Epoch:  33 | Step:  28 | Val Loss:  0.5337588787078857\n",
      "Epoch:  33 | Step:  29 | Val Loss:  0.46682286262512207\n",
      "Epoch:  33 | Step:  30 | Val Loss:  0.7947105765342712\n",
      "Epoch:  33 | Step:  31 | Val Loss:  0.5180537104606628\n",
      "Epoch:  33 | Step:  32 | Val Loss:  0.7826273441314697\n",
      "Epoch:  33 | Step:  33 | Val Loss:  0.5102705955505371\n",
      "Epoch:  33 | Step:  34 | Val Loss:  0.5713115334510803\n",
      "Epoch:  33 | Step:  35 | Val Loss:  0.6171576976776123\n",
      "Epoch:  33 | Step:  36 | Val Loss:  0.3393998444080353\n",
      "Epoch:  33 | Step:  37 | Val Loss:  0.6746334433555603\n",
      "Epoch:  33 | Step:  38 | Val Loss:  0.41313087940216064\n",
      "Epoch:  33 | Step:  39 | Val Loss:  0.48733529448509216\n",
      "Epoch:  33 | Step:  40 | Val Loss:  0.5566505789756775\n",
      "Epoch:  33 | Step:  41 | Val Loss:  0.48150473833084106\n",
      "Epoch:  33 | Step:  42 | Val Loss:  0.5357037782669067\n",
      "Epoch:  33 | Step:  43 | Val Loss:  0.4575507938861847\n",
      "Epoch:  33 | Step:  44 | Val Loss:  0.5797547101974487\n",
      "Epoch:  33 | Step:  45 | Val Loss:  0.5341167449951172\n",
      "Epoch:  33 | Step:  46 | Val Loss:  0.45794808864593506\n",
      "Epoch:  33 | Step:  47 | Val Loss:  0.44104456901550293\n",
      "Epoch:  33 | Step:  48 | Val Loss:  0.4982617497444153\n",
      "Epoch:  33 | Step:  49 | Val Loss:  0.45064476132392883\n",
      "Epoch:  33 | Step:  50 | Val Loss:  0.5081007480621338\n",
      "Epoch:  33 | Step:  51 | Val Loss:  0.5571999549865723\n",
      "Epoch:  33 | Step:  52 | Val Loss:  0.4589528441429138\n",
      "Epoch:  33 | Step:  53 | Val Loss:  0.6387573480606079\n",
      "Epoch:  33 | Step:  54 | Val Loss:  0.48638057708740234\n",
      "Epoch:  33 | Step:  55 | Val Loss:  0.5212302207946777\n",
      "Epoch:  33 | Step:  56 | Val Loss:  0.5830806493759155\n",
      "Epoch:  33 | Step:  57 | Val Loss:  0.41561663150787354\n",
      "Epoch:  33 | Step:  58 | Val Loss:  0.5576931834220886\n",
      "Epoch:  33 | Step:  59 | Val Loss:  0.6283438205718994\n",
      "Epoch:  33 | Step:  60 | Val Loss:  0.611101508140564\n",
      "Epoch:  33 | Step:  61 | Val Loss:  0.36825141310691833\n",
      "Epoch:  33 | Step:  62 | Val Loss:  0.6021427512168884\n",
      "Epoch:  33 | Step:  63 | Val Loss:  0.5419541597366333\n",
      "Epoch:  33 | Step:  64 | Val Loss:  0.5763418674468994\n",
      "Epoch:  33 | Step:  65 | Val Loss:  0.5682584047317505\n",
      "Epoch:  33 | Step:  66 | Val Loss:  0.47264164686203003\n",
      "Epoch:  33 | Step:  67 | Val Loss:  0.4745347499847412\n",
      "Epoch:  33 | Step:  68 | Val Loss:  0.5724029541015625\n",
      "Epoch:  33 | Step:  69 | Val Loss:  0.5894370079040527\n",
      "Epoch:  33 | Step:  70 | Val Loss:  0.4759998023509979\n",
      "Epoch:  33 | Step:  71 | Val Loss:  0.6715849041938782\n",
      "Epoch:  33 | Step:  72 | Val Loss:  0.5338764190673828\n",
      "Epoch:  33 | Step:  73 | Val Loss:  0.4497336149215698\n",
      "Epoch:  33 | Step:  74 | Val Loss:  0.6045175790786743\n",
      "Epoch:  33 | Step:  75 | Val Loss:  0.4483954906463623\n",
      "Epoch:  33 | Step:  76 | Val Loss:  0.3709890842437744\n",
      "Epoch:  33 | Step:  77 | Val Loss:  0.39082852005958557\n",
      "Epoch:  33 | Step:  78 | Val Loss:  0.661251425743103\n",
      "Epoch:  33 | Step:  79 | Val Loss:  0.5003349781036377\n",
      "Epoch:  33 | Step:  80 | Val Loss:  0.5448287725448608\n",
      "Epoch:  33 | Step:  81 | Val Loss:  0.6157596111297607\n",
      "Epoch:  33 | Step:  82 | Val Loss:  0.5256862640380859\n",
      "Epoch:  33 | Step:  83 | Val Loss:  0.5020146369934082\n",
      "Epoch:  33 | Step:  84 | Val Loss:  0.61527019739151\n",
      "Epoch:  33 | Step:  85 | Val Loss:  0.5100597143173218\n",
      "Epoch:  33 | Step:  86 | Val Loss:  0.5449408888816833\n",
      "Epoch:  33 | Step:  87 | Val Loss:  0.5589463710784912\n",
      "Epoch:  33 | Step:  88 | Val Loss:  0.6233587265014648\n",
      "Epoch:  33 | Step:  89 | Val Loss:  0.41580185294151306\n",
      "Epoch:  33 | Step:  90 | Val Loss:  0.7107706665992737\n",
      "Epoch:  33 | Step:  91 | Val Loss:  0.6581929922103882\n",
      "Epoch:  33 | Step:  92 | Val Loss:  0.6290631294250488\n",
      "Epoch:  33 | Step:  93 | Val Loss:  0.5353401899337769\n",
      "Epoch:  33 | Step:  94 | Val Loss:  0.5885791182518005\n",
      "Epoch:  33 | Step:  95 | Val Loss:  0.4131443500518799\n",
      "Epoch:  33 | Step:  96 | Val Loss:  0.4966142773628235\n",
      "Epoch:  33 | Step:  97 | Val Loss:  0.3770553767681122\n",
      "Epoch:  33 | Step:  98 | Val Loss:  0.5833257436752319\n",
      "Epoch:  33 | Step:  99 | Val Loss:  0.3491855263710022\n",
      "Epoch:  33 | Step:  100 | Val Loss:  0.5068070888519287\n",
      "Epoch:  33 | Step:  101 | Val Loss:  0.6152788400650024\n",
      "Epoch:  33 | Step:  102 | Val Loss:  0.5511690378189087\n",
      "Epoch:  33 | Step:  103 | Val Loss:  0.6165513396263123\n",
      "Epoch:  33 | Step:  104 | Val Loss:  0.463774710893631\n",
      "Epoch:  33 | Step:  105 | Val Loss:  0.5219588279724121\n",
      "Epoch:  33 | Step:  106 | Val Loss:  0.4851013422012329\n",
      "Epoch:  33 | Step:  107 | Val Loss:  0.6559076905250549\n",
      "Epoch:  33 | Step:  108 | Val Loss:  0.6417529582977295\n",
      "Epoch:  33 | Step:  109 | Val Loss:  0.5113855600357056\n",
      "Epoch:  33 | Step:  110 | Val Loss:  0.4523404538631439\n",
      "Epoch:  33 | Step:  111 | Val Loss:  0.49369028210639954\n",
      "Epoch:  33 | Step:  112 | Val Loss:  0.5439441800117493\n",
      "Epoch:  33 | Step:  113 | Val Loss:  0.35515666007995605\n",
      "Epoch:  33 | Step:  114 | Val Loss:  0.5650407075881958\n",
      "Epoch:  33 | Step:  115 | Val Loss:  0.4454215168952942\n",
      "Epoch:  33 | Step:  116 | Val Loss:  0.6528673768043518\n",
      "Epoch:  33 | Step:  117 | Val Loss:  0.3330373764038086\n",
      "Epoch:  33 | Step:  118 | Val Loss:  0.6244415044784546\n",
      "Epoch:  33 | Step:  119 | Val Loss:  0.5006046891212463\n",
      "Epoch:  33 | Step:  120 | Val Loss:  0.5685967803001404\n",
      "Epoch:  33 | Step:  121 | Val Loss:  0.4816662669181824\n",
      "Epoch:  33 | Step:  122 | Val Loss:  0.5382285118103027\n",
      "Epoch:  33 | Step:  123 | Val Loss:  0.5144515633583069\n",
      "Epoch:  33 | Step:  124 | Val Loss:  0.477453351020813\n",
      "Epoch:  33 | Step:  125 | Val Loss:  0.47905173897743225\n",
      "Epoch:  33 | Train Loss:  tensor(0.5120, device='cuda:0') | Val Loss:  tensor(0.5366, device='cuda:0')\n",
      "Epoch:  34 | Step:  500 | Train Loss:  0.5092298984527588\n",
      "Epoch:  34 | Step:  1 | Val Loss:  0.3362935781478882\n",
      "Epoch:  34 | Step:  2 | Val Loss:  0.5545845627784729\n",
      "Epoch:  34 | Step:  3 | Val Loss:  0.4762011468410492\n",
      "Epoch:  34 | Step:  4 | Val Loss:  0.366066038608551\n",
      "Epoch:  34 | Step:  5 | Val Loss:  0.4894309639930725\n",
      "Epoch:  34 | Step:  6 | Val Loss:  0.36973753571510315\n",
      "Epoch:  34 | Step:  7 | Val Loss:  0.29557937383651733\n",
      "Epoch:  34 | Step:  8 | Val Loss:  0.5081576108932495\n",
      "Epoch:  34 | Step:  9 | Val Loss:  0.5645139217376709\n",
      "Epoch:  34 | Step:  10 | Val Loss:  0.509445071220398\n",
      "Epoch:  34 | Step:  11 | Val Loss:  0.5153806805610657\n",
      "Epoch:  34 | Step:  12 | Val Loss:  0.5356398820877075\n",
      "Epoch:  34 | Step:  13 | Val Loss:  0.5140258073806763\n",
      "Epoch:  34 | Step:  14 | Val Loss:  0.4271562099456787\n",
      "Epoch:  34 | Step:  15 | Val Loss:  0.6860791444778442\n",
      "Epoch:  34 | Step:  16 | Val Loss:  0.360368013381958\n",
      "Epoch:  34 | Step:  17 | Val Loss:  0.5927094221115112\n",
      "Epoch:  34 | Step:  18 | Val Loss:  0.5985560417175293\n",
      "Epoch:  34 | Step:  19 | Val Loss:  0.5890176892280579\n",
      "Epoch:  34 | Step:  20 | Val Loss:  0.5484615564346313\n",
      "Epoch:  34 | Step:  21 | Val Loss:  0.46323728561401367\n",
      "Epoch:  34 | Step:  22 | Val Loss:  0.5872341394424438\n",
      "Epoch:  34 | Step:  23 | Val Loss:  0.5428367853164673\n",
      "Epoch:  34 | Step:  24 | Val Loss:  0.6148892045021057\n",
      "Epoch:  34 | Step:  25 | Val Loss:  0.4279720187187195\n",
      "Epoch:  34 | Step:  26 | Val Loss:  0.4906197190284729\n",
      "Epoch:  34 | Step:  27 | Val Loss:  0.5486184358596802\n",
      "Epoch:  34 | Step:  28 | Val Loss:  0.32405203580856323\n",
      "Epoch:  34 | Step:  29 | Val Loss:  0.5773665904998779\n",
      "Epoch:  34 | Step:  30 | Val Loss:  0.5040894746780396\n",
      "Epoch:  34 | Step:  31 | Val Loss:  0.34693169593811035\n",
      "Epoch:  34 | Step:  32 | Val Loss:  0.45263880491256714\n",
      "Epoch:  34 | Step:  33 | Val Loss:  0.42268458008766174\n",
      "Epoch:  34 | Step:  34 | Val Loss:  0.5549517869949341\n",
      "Epoch:  34 | Step:  35 | Val Loss:  0.6076933145523071\n",
      "Epoch:  34 | Step:  36 | Val Loss:  0.7050166130065918\n",
      "Epoch:  34 | Step:  37 | Val Loss:  0.47560280561447144\n",
      "Epoch:  34 | Step:  38 | Val Loss:  0.6273354291915894\n",
      "Epoch:  34 | Step:  39 | Val Loss:  0.4024137258529663\n",
      "Epoch:  34 | Step:  40 | Val Loss:  0.48063525557518005\n",
      "Epoch:  34 | Step:  41 | Val Loss:  0.6081904172897339\n",
      "Epoch:  34 | Step:  42 | Val Loss:  0.5142853260040283\n",
      "Epoch:  34 | Step:  43 | Val Loss:  0.4827936887741089\n",
      "Epoch:  34 | Step:  44 | Val Loss:  0.6365455389022827\n",
      "Epoch:  34 | Step:  45 | Val Loss:  0.5218549966812134\n",
      "Epoch:  34 | Step:  46 | Val Loss:  0.523904025554657\n",
      "Epoch:  34 | Step:  47 | Val Loss:  0.7803182005882263\n",
      "Epoch:  34 | Step:  48 | Val Loss:  0.4280077815055847\n",
      "Epoch:  34 | Step:  49 | Val Loss:  0.5895025730133057\n",
      "Epoch:  34 | Step:  50 | Val Loss:  0.46630939841270447\n",
      "Epoch:  34 | Step:  51 | Val Loss:  0.4554671049118042\n",
      "Epoch:  34 | Step:  52 | Val Loss:  0.46639561653137207\n",
      "Epoch:  34 | Step:  53 | Val Loss:  0.3896870017051697\n",
      "Epoch:  34 | Step:  54 | Val Loss:  0.6656285524368286\n",
      "Epoch:  34 | Step:  55 | Val Loss:  0.3944958448410034\n",
      "Epoch:  34 | Step:  56 | Val Loss:  0.7413849830627441\n",
      "Epoch:  34 | Step:  57 | Val Loss:  0.46088021993637085\n",
      "Epoch:  34 | Step:  58 | Val Loss:  0.6546536684036255\n",
      "Epoch:  34 | Step:  59 | Val Loss:  0.717377245426178\n",
      "Epoch:  34 | Step:  60 | Val Loss:  0.6065093278884888\n",
      "Epoch:  34 | Step:  61 | Val Loss:  0.509153425693512\n",
      "Epoch:  34 | Step:  62 | Val Loss:  0.7560079097747803\n",
      "Epoch:  34 | Step:  63 | Val Loss:  0.540824830532074\n",
      "Epoch:  34 | Step:  64 | Val Loss:  0.4615412652492523\n",
      "Epoch:  34 | Step:  65 | Val Loss:  0.4893549084663391\n",
      "Epoch:  34 | Step:  66 | Val Loss:  0.40122082829475403\n",
      "Epoch:  34 | Step:  67 | Val Loss:  0.7221269607543945\n",
      "Epoch:  34 | Step:  68 | Val Loss:  0.5474956035614014\n",
      "Epoch:  34 | Step:  69 | Val Loss:  0.4522232711315155\n",
      "Epoch:  34 | Step:  70 | Val Loss:  0.5414319038391113\n",
      "Epoch:  34 | Step:  71 | Val Loss:  0.6506232619285583\n",
      "Epoch:  34 | Step:  72 | Val Loss:  0.6695835590362549\n",
      "Epoch:  34 | Step:  73 | Val Loss:  0.6056893467903137\n",
      "Epoch:  34 | Step:  74 | Val Loss:  0.5166645050048828\n",
      "Epoch:  34 | Step:  75 | Val Loss:  0.7107019424438477\n",
      "Epoch:  34 | Step:  76 | Val Loss:  0.5904130339622498\n",
      "Epoch:  34 | Step:  77 | Val Loss:  0.4569864869117737\n",
      "Epoch:  34 | Step:  78 | Val Loss:  0.43313097953796387\n",
      "Epoch:  34 | Step:  79 | Val Loss:  0.607292115688324\n",
      "Epoch:  34 | Step:  80 | Val Loss:  0.6543178558349609\n",
      "Epoch:  34 | Step:  81 | Val Loss:  0.5515052080154419\n",
      "Epoch:  34 | Step:  82 | Val Loss:  0.34640711545944214\n",
      "Epoch:  34 | Step:  83 | Val Loss:  0.4251564145088196\n",
      "Epoch:  34 | Step:  84 | Val Loss:  0.6060813665390015\n",
      "Epoch:  34 | Step:  85 | Val Loss:  0.5676156282424927\n",
      "Epoch:  34 | Step:  86 | Val Loss:  0.5556983947753906\n",
      "Epoch:  34 | Step:  87 | Val Loss:  0.6581266522407532\n",
      "Epoch:  34 | Step:  88 | Val Loss:  0.658626914024353\n",
      "Epoch:  34 | Step:  89 | Val Loss:  0.6735448241233826\n",
      "Epoch:  34 | Step:  90 | Val Loss:  0.5184056758880615\n",
      "Epoch:  34 | Step:  91 | Val Loss:  0.7136822938919067\n",
      "Epoch:  34 | Step:  92 | Val Loss:  0.545295000076294\n",
      "Epoch:  34 | Step:  93 | Val Loss:  0.7259829640388489\n",
      "Epoch:  34 | Step:  94 | Val Loss:  0.340218722820282\n",
      "Epoch:  34 | Step:  95 | Val Loss:  0.5536158084869385\n",
      "Epoch:  34 | Step:  96 | Val Loss:  0.54175865650177\n",
      "Epoch:  34 | Step:  97 | Val Loss:  0.556675910949707\n",
      "Epoch:  34 | Step:  98 | Val Loss:  0.541265606880188\n",
      "Epoch:  34 | Step:  99 | Val Loss:  0.5368064641952515\n",
      "Epoch:  34 | Step:  100 | Val Loss:  0.7084168195724487\n",
      "Epoch:  34 | Step:  101 | Val Loss:  0.47472119331359863\n",
      "Epoch:  34 | Step:  102 | Val Loss:  0.5412349700927734\n",
      "Epoch:  34 | Step:  103 | Val Loss:  0.430789053440094\n",
      "Epoch:  34 | Step:  104 | Val Loss:  0.5228850245475769\n",
      "Epoch:  34 | Step:  105 | Val Loss:  0.675144612789154\n",
      "Epoch:  34 | Step:  106 | Val Loss:  0.459109365940094\n",
      "Epoch:  34 | Step:  107 | Val Loss:  0.38032084703445435\n",
      "Epoch:  34 | Step:  108 | Val Loss:  0.5070598125457764\n",
      "Epoch:  34 | Step:  109 | Val Loss:  0.6042340397834778\n",
      "Epoch:  34 | Step:  110 | Val Loss:  0.6790946125984192\n",
      "Epoch:  34 | Step:  111 | Val Loss:  0.3982928395271301\n",
      "Epoch:  34 | Step:  112 | Val Loss:  0.5853712558746338\n",
      "Epoch:  34 | Step:  113 | Val Loss:  0.45805624127388\n",
      "Epoch:  34 | Step:  114 | Val Loss:  0.5096545815467834\n",
      "Epoch:  34 | Step:  115 | Val Loss:  0.4185676574707031\n",
      "Epoch:  34 | Step:  116 | Val Loss:  0.4804586172103882\n",
      "Epoch:  34 | Step:  117 | Val Loss:  0.45769643783569336\n",
      "Epoch:  34 | Step:  118 | Val Loss:  0.5928300619125366\n",
      "Epoch:  34 | Step:  119 | Val Loss:  0.5956007242202759\n",
      "Epoch:  34 | Step:  120 | Val Loss:  0.5418108105659485\n",
      "Epoch:  34 | Step:  121 | Val Loss:  0.4415181875228882\n",
      "Epoch:  34 | Step:  122 | Val Loss:  0.6077615022659302\n",
      "Epoch:  34 | Step:  123 | Val Loss:  0.5109778642654419\n",
      "Epoch:  34 | Step:  124 | Val Loss:  0.5811420679092407\n",
      "Epoch:  34 | Step:  125 | Val Loss:  0.45894911885261536\n",
      "Epoch:  34 | Train Loss:  tensor(0.5101, device='cuda:0') | Val Loss:  tensor(0.5332, device='cuda:0')\n",
      "Epoch:  35 | Step:  500 | Train Loss:  0.5763270854949951\n",
      "Epoch:  35 | Step:  1 | Val Loss:  0.5143954753875732\n",
      "Epoch:  35 | Step:  2 | Val Loss:  0.37872767448425293\n",
      "Epoch:  35 | Step:  3 | Val Loss:  0.5756110548973083\n",
      "Epoch:  35 | Step:  4 | Val Loss:  0.5118303894996643\n",
      "Epoch:  35 | Step:  5 | Val Loss:  0.6098846197128296\n",
      "Epoch:  35 | Step:  6 | Val Loss:  0.5532692670822144\n",
      "Epoch:  35 | Step:  7 | Val Loss:  0.5293067097663879\n",
      "Epoch:  35 | Step:  8 | Val Loss:  0.46477770805358887\n",
      "Epoch:  35 | Step:  9 | Val Loss:  0.6202529668807983\n",
      "Epoch:  35 | Step:  10 | Val Loss:  0.5120638608932495\n",
      "Epoch:  35 | Step:  11 | Val Loss:  0.41805532574653625\n",
      "Epoch:  35 | Step:  12 | Val Loss:  0.6830140352249146\n",
      "Epoch:  35 | Step:  13 | Val Loss:  0.4243927299976349\n",
      "Epoch:  35 | Step:  14 | Val Loss:  0.5893358588218689\n",
      "Epoch:  35 | Step:  15 | Val Loss:  0.5830360054969788\n",
      "Epoch:  35 | Step:  16 | Val Loss:  0.5989844799041748\n",
      "Epoch:  35 | Step:  17 | Val Loss:  0.626287579536438\n",
      "Epoch:  35 | Step:  18 | Val Loss:  0.5734033584594727\n",
      "Epoch:  35 | Step:  19 | Val Loss:  0.5743541717529297\n",
      "Epoch:  35 | Step:  20 | Val Loss:  0.5315969586372375\n",
      "Epoch:  35 | Step:  21 | Val Loss:  0.3279764950275421\n",
      "Epoch:  35 | Step:  22 | Val Loss:  0.537021279335022\n",
      "Epoch:  35 | Step:  23 | Val Loss:  0.4006340503692627\n",
      "Epoch:  35 | Step:  24 | Val Loss:  0.5514879822731018\n",
      "Epoch:  35 | Step:  25 | Val Loss:  0.44286271929740906\n",
      "Epoch:  35 | Step:  26 | Val Loss:  0.5849345922470093\n",
      "Epoch:  35 | Step:  27 | Val Loss:  0.5212234258651733\n",
      "Epoch:  35 | Step:  28 | Val Loss:  0.5982198119163513\n",
      "Epoch:  35 | Step:  29 | Val Loss:  0.5664055347442627\n",
      "Epoch:  35 | Step:  30 | Val Loss:  0.38536760210990906\n",
      "Epoch:  35 | Step:  31 | Val Loss:  0.44177547097206116\n",
      "Epoch:  35 | Step:  32 | Val Loss:  0.4556066393852234\n",
      "Epoch:  35 | Step:  33 | Val Loss:  0.5703615546226501\n",
      "Epoch:  35 | Step:  34 | Val Loss:  0.6491177082061768\n",
      "Epoch:  35 | Step:  35 | Val Loss:  0.5752606391906738\n",
      "Epoch:  35 | Step:  36 | Val Loss:  0.4735148847103119\n",
      "Epoch:  35 | Step:  37 | Val Loss:  0.6192665100097656\n",
      "Epoch:  35 | Step:  38 | Val Loss:  0.5161774754524231\n",
      "Epoch:  35 | Step:  39 | Val Loss:  0.596521258354187\n",
      "Epoch:  35 | Step:  40 | Val Loss:  0.586611807346344\n",
      "Epoch:  35 | Step:  41 | Val Loss:  0.5599836111068726\n",
      "Epoch:  35 | Step:  42 | Val Loss:  0.6034576892852783\n",
      "Epoch:  35 | Step:  43 | Val Loss:  0.44092994928359985\n",
      "Epoch:  35 | Step:  44 | Val Loss:  0.6892942786216736\n",
      "Epoch:  35 | Step:  45 | Val Loss:  0.55176842212677\n",
      "Epoch:  35 | Step:  46 | Val Loss:  0.32384634017944336\n",
      "Epoch:  35 | Step:  47 | Val Loss:  0.6742174625396729\n",
      "Epoch:  35 | Step:  48 | Val Loss:  0.4747087061405182\n",
      "Epoch:  35 | Step:  49 | Val Loss:  0.718302309513092\n",
      "Epoch:  35 | Step:  50 | Val Loss:  0.40754395723342896\n",
      "Epoch:  35 | Step:  51 | Val Loss:  0.4843422770500183\n",
      "Epoch:  35 | Step:  52 | Val Loss:  0.5105839967727661\n",
      "Epoch:  35 | Step:  53 | Val Loss:  0.6376123428344727\n",
      "Epoch:  35 | Step:  54 | Val Loss:  0.37366312742233276\n",
      "Epoch:  35 | Step:  55 | Val Loss:  0.5730077028274536\n",
      "Epoch:  35 | Step:  56 | Val Loss:  0.442234605550766\n",
      "Epoch:  35 | Step:  57 | Val Loss:  0.6142209768295288\n",
      "Epoch:  35 | Step:  58 | Val Loss:  0.4250844120979309\n",
      "Epoch:  35 | Step:  59 | Val Loss:  0.6446952819824219\n",
      "Epoch:  35 | Step:  60 | Val Loss:  0.495597779750824\n",
      "Epoch:  35 | Step:  61 | Val Loss:  0.7017905712127686\n",
      "Epoch:  35 | Step:  62 | Val Loss:  0.6509961485862732\n",
      "Epoch:  35 | Step:  63 | Val Loss:  0.5563687086105347\n",
      "Epoch:  35 | Step:  64 | Val Loss:  0.48531925678253174\n",
      "Epoch:  35 | Step:  65 | Val Loss:  0.5433539152145386\n",
      "Epoch:  35 | Step:  66 | Val Loss:  0.5631735324859619\n",
      "Epoch:  35 | Step:  67 | Val Loss:  0.5056992769241333\n",
      "Epoch:  35 | Step:  68 | Val Loss:  0.5798337459564209\n",
      "Epoch:  35 | Step:  69 | Val Loss:  0.5532769560813904\n",
      "Epoch:  35 | Step:  70 | Val Loss:  0.3915133476257324\n",
      "Epoch:  35 | Step:  71 | Val Loss:  0.742696225643158\n",
      "Epoch:  35 | Step:  72 | Val Loss:  0.45564475655555725\n",
      "Epoch:  35 | Step:  73 | Val Loss:  0.671278178691864\n",
      "Epoch:  35 | Step:  74 | Val Loss:  0.5549936294555664\n",
      "Epoch:  35 | Step:  75 | Val Loss:  0.5971143245697021\n",
      "Epoch:  35 | Step:  76 | Val Loss:  0.5543326139450073\n",
      "Epoch:  35 | Step:  77 | Val Loss:  0.46018922328948975\n",
      "Epoch:  35 | Step:  78 | Val Loss:  0.5175518989562988\n",
      "Epoch:  35 | Step:  79 | Val Loss:  0.6412104368209839\n",
      "Epoch:  35 | Step:  80 | Val Loss:  0.584357738494873\n",
      "Epoch:  35 | Step:  81 | Val Loss:  0.34420540928840637\n",
      "Epoch:  35 | Step:  82 | Val Loss:  0.4994174540042877\n",
      "Epoch:  35 | Step:  83 | Val Loss:  0.743515133857727\n",
      "Epoch:  35 | Step:  84 | Val Loss:  0.38650402426719666\n",
      "Epoch:  35 | Step:  85 | Val Loss:  0.48823896050453186\n",
      "Epoch:  35 | Step:  86 | Val Loss:  0.5129443407058716\n",
      "Epoch:  35 | Step:  87 | Val Loss:  0.32067567110061646\n",
      "Epoch:  35 | Step:  88 | Val Loss:  0.5768721103668213\n",
      "Epoch:  35 | Step:  89 | Val Loss:  0.3015504479408264\n",
      "Epoch:  35 | Step:  90 | Val Loss:  0.48017480969429016\n",
      "Epoch:  35 | Step:  91 | Val Loss:  0.4944291114807129\n",
      "Epoch:  35 | Step:  92 | Val Loss:  0.5056251287460327\n",
      "Epoch:  35 | Step:  93 | Val Loss:  0.5380812287330627\n",
      "Epoch:  35 | Step:  94 | Val Loss:  0.6935468912124634\n",
      "Epoch:  35 | Step:  95 | Val Loss:  0.566933274269104\n",
      "Epoch:  35 | Step:  96 | Val Loss:  0.5040934085845947\n",
      "Epoch:  35 | Step:  97 | Val Loss:  0.6209681034088135\n",
      "Epoch:  35 | Step:  98 | Val Loss:  0.5078097581863403\n",
      "Epoch:  35 | Step:  99 | Val Loss:  0.5131186246871948\n",
      "Epoch:  35 | Step:  100 | Val Loss:  0.5381633043289185\n",
      "Epoch:  35 | Step:  101 | Val Loss:  0.47079116106033325\n",
      "Epoch:  35 | Step:  102 | Val Loss:  0.4523147940635681\n",
      "Epoch:  35 | Step:  103 | Val Loss:  0.520844578742981\n",
      "Epoch:  35 | Step:  104 | Val Loss:  0.36019188165664673\n",
      "Epoch:  35 | Step:  105 | Val Loss:  0.4477197825908661\n",
      "Epoch:  35 | Step:  106 | Val Loss:  0.5262971520423889\n",
      "Epoch:  35 | Step:  107 | Val Loss:  0.40921562910079956\n",
      "Epoch:  35 | Step:  108 | Val Loss:  0.48501431941986084\n",
      "Epoch:  35 | Step:  109 | Val Loss:  0.687345027923584\n",
      "Epoch:  35 | Step:  110 | Val Loss:  0.5922437310218811\n",
      "Epoch:  35 | Step:  111 | Val Loss:  0.4659823775291443\n",
      "Epoch:  35 | Step:  112 | Val Loss:  0.49600136280059814\n",
      "Epoch:  35 | Step:  113 | Val Loss:  0.6326898336410522\n",
      "Epoch:  35 | Step:  114 | Val Loss:  0.6530542373657227\n",
      "Epoch:  35 | Step:  115 | Val Loss:  0.42302176356315613\n",
      "Epoch:  35 | Step:  116 | Val Loss:  0.6946771144866943\n",
      "Epoch:  35 | Step:  117 | Val Loss:  0.5273563265800476\n",
      "Epoch:  35 | Step:  118 | Val Loss:  0.36933547258377075\n",
      "Epoch:  35 | Step:  119 | Val Loss:  0.537112295627594\n",
      "Epoch:  35 | Step:  120 | Val Loss:  0.6695793867111206\n",
      "Epoch:  35 | Step:  121 | Val Loss:  0.6381540298461914\n",
      "Epoch:  35 | Step:  122 | Val Loss:  0.3984270393848419\n",
      "Epoch:  35 | Step:  123 | Val Loss:  0.5198282599449158\n",
      "Epoch:  35 | Step:  124 | Val Loss:  0.4060906767845154\n",
      "Epoch:  35 | Step:  125 | Val Loss:  0.5696901679039001\n",
      "Epoch:  35 | Train Loss:  tensor(0.5083, device='cuda:0') | Val Loss:  tensor(0.5305, device='cuda:0')\n",
      "Epoch:  36 | Step:  500 | Train Loss:  0.5656293630599976\n",
      "Epoch:  36 | Step:  1 | Val Loss:  0.4998624324798584\n",
      "Epoch:  36 | Step:  2 | Val Loss:  0.49224936962127686\n",
      "Epoch:  36 | Step:  3 | Val Loss:  0.5330514907836914\n",
      "Epoch:  36 | Step:  4 | Val Loss:  0.41729289293289185\n",
      "Epoch:  36 | Step:  5 | Val Loss:  0.36330628395080566\n",
      "Epoch:  36 | Step:  6 | Val Loss:  0.5755453109741211\n",
      "Epoch:  36 | Step:  7 | Val Loss:  0.6197632551193237\n",
      "Epoch:  36 | Step:  8 | Val Loss:  0.6242058873176575\n",
      "Epoch:  36 | Step:  9 | Val Loss:  0.5973806381225586\n",
      "Epoch:  36 | Step:  10 | Val Loss:  0.6259946823120117\n",
      "Epoch:  36 | Step:  11 | Val Loss:  0.5638670921325684\n",
      "Epoch:  36 | Step:  12 | Val Loss:  0.4434352517127991\n",
      "Epoch:  36 | Step:  13 | Val Loss:  0.5051660537719727\n",
      "Epoch:  36 | Step:  14 | Val Loss:  0.5729908347129822\n",
      "Epoch:  36 | Step:  15 | Val Loss:  0.5402384996414185\n",
      "Epoch:  36 | Step:  16 | Val Loss:  0.47280025482177734\n",
      "Epoch:  36 | Step:  17 | Val Loss:  0.4330645799636841\n",
      "Epoch:  36 | Step:  18 | Val Loss:  0.38344240188598633\n",
      "Epoch:  36 | Step:  19 | Val Loss:  0.46866822242736816\n",
      "Epoch:  36 | Step:  20 | Val Loss:  0.49853992462158203\n",
      "Epoch:  36 | Step:  21 | Val Loss:  0.5793685913085938\n",
      "Epoch:  36 | Step:  22 | Val Loss:  0.5443432331085205\n",
      "Epoch:  36 | Step:  23 | Val Loss:  0.6429523825645447\n",
      "Epoch:  36 | Step:  24 | Val Loss:  0.4584595561027527\n",
      "Epoch:  36 | Step:  25 | Val Loss:  0.476086288690567\n",
      "Epoch:  36 | Step:  26 | Val Loss:  0.5659531950950623\n",
      "Epoch:  36 | Step:  27 | Val Loss:  0.4391928017139435\n",
      "Epoch:  36 | Step:  28 | Val Loss:  0.5349324941635132\n",
      "Epoch:  36 | Step:  29 | Val Loss:  0.5088046789169312\n",
      "Epoch:  36 | Step:  30 | Val Loss:  0.4026640057563782\n",
      "Epoch:  36 | Step:  31 | Val Loss:  0.5341297388076782\n",
      "Epoch:  36 | Step:  32 | Val Loss:  0.41658687591552734\n",
      "Epoch:  36 | Step:  33 | Val Loss:  0.5288388729095459\n",
      "Epoch:  36 | Step:  34 | Val Loss:  0.5979543328285217\n",
      "Epoch:  36 | Step:  35 | Val Loss:  0.5611235499382019\n",
      "Epoch:  36 | Step:  36 | Val Loss:  0.5084556341171265\n",
      "Epoch:  36 | Step:  37 | Val Loss:  0.4958547353744507\n",
      "Epoch:  36 | Step:  38 | Val Loss:  0.5310838222503662\n",
      "Epoch:  36 | Step:  39 | Val Loss:  0.44056862592697144\n",
      "Epoch:  36 | Step:  40 | Val Loss:  0.59417325258255\n",
      "Epoch:  36 | Step:  41 | Val Loss:  0.515913188457489\n",
      "Epoch:  36 | Step:  42 | Val Loss:  0.607525110244751\n",
      "Epoch:  36 | Step:  43 | Val Loss:  0.4686499834060669\n",
      "Epoch:  36 | Step:  44 | Val Loss:  0.3428362011909485\n",
      "Epoch:  36 | Step:  45 | Val Loss:  0.4824545979499817\n",
      "Epoch:  36 | Step:  46 | Val Loss:  0.6228466629981995\n",
      "Epoch:  36 | Step:  47 | Val Loss:  0.485383003950119\n",
      "Epoch:  36 | Step:  48 | Val Loss:  0.43975192308425903\n",
      "Epoch:  36 | Step:  49 | Val Loss:  0.4714622497558594\n",
      "Epoch:  36 | Step:  50 | Val Loss:  0.6694421768188477\n",
      "Epoch:  36 | Step:  51 | Val Loss:  0.37921059131622314\n",
      "Epoch:  36 | Step:  52 | Val Loss:  0.6559969186782837\n",
      "Epoch:  36 | Step:  53 | Val Loss:  0.58072829246521\n",
      "Epoch:  36 | Step:  54 | Val Loss:  0.4418363571166992\n",
      "Epoch:  36 | Step:  55 | Val Loss:  0.5083392858505249\n",
      "Epoch:  36 | Step:  56 | Val Loss:  0.43670767545700073\n",
      "Epoch:  36 | Step:  57 | Val Loss:  0.6347349882125854\n",
      "Epoch:  36 | Step:  58 | Val Loss:  0.5760393142700195\n",
      "Epoch:  36 | Step:  59 | Val Loss:  0.5438714623451233\n",
      "Epoch:  36 | Step:  60 | Val Loss:  0.4125581979751587\n",
      "Epoch:  36 | Step:  61 | Val Loss:  0.5974491834640503\n",
      "Epoch:  36 | Step:  62 | Val Loss:  0.48202475905418396\n",
      "Epoch:  36 | Step:  63 | Val Loss:  0.4296862483024597\n",
      "Epoch:  36 | Step:  64 | Val Loss:  0.5742976665496826\n",
      "Epoch:  36 | Step:  65 | Val Loss:  0.47376951575279236\n",
      "Epoch:  36 | Step:  66 | Val Loss:  0.38619935512542725\n",
      "Epoch:  36 | Step:  67 | Val Loss:  0.4263203740119934\n",
      "Epoch:  36 | Step:  68 | Val Loss:  0.5046958327293396\n",
      "Epoch:  36 | Step:  69 | Val Loss:  0.6387416124343872\n",
      "Epoch:  36 | Step:  70 | Val Loss:  0.5928184390068054\n",
      "Epoch:  36 | Step:  71 | Val Loss:  0.5515075325965881\n",
      "Epoch:  36 | Step:  72 | Val Loss:  0.6041747331619263\n",
      "Epoch:  36 | Step:  73 | Val Loss:  0.665840744972229\n",
      "Epoch:  36 | Step:  74 | Val Loss:  0.6966835856437683\n",
      "Epoch:  36 | Step:  75 | Val Loss:  0.444673091173172\n",
      "Epoch:  36 | Step:  76 | Val Loss:  0.5354632139205933\n",
      "Epoch:  36 | Step:  77 | Val Loss:  0.4276551902294159\n",
      "Epoch:  36 | Step:  78 | Val Loss:  0.5515376329421997\n",
      "Epoch:  36 | Step:  79 | Val Loss:  0.6461362838745117\n",
      "Epoch:  36 | Step:  80 | Val Loss:  0.3097047209739685\n",
      "Epoch:  36 | Step:  81 | Val Loss:  0.4106242060661316\n",
      "Epoch:  36 | Step:  82 | Val Loss:  0.5153710842132568\n",
      "Epoch:  36 | Step:  83 | Val Loss:  0.6599957942962646\n",
      "Epoch:  36 | Step:  84 | Val Loss:  0.7601119875907898\n",
      "Epoch:  36 | Step:  85 | Val Loss:  0.45226651430130005\n",
      "Epoch:  36 | Step:  86 | Val Loss:  0.6272330284118652\n",
      "Epoch:  36 | Step:  87 | Val Loss:  0.5248342752456665\n",
      "Epoch:  36 | Step:  88 | Val Loss:  0.4406280219554901\n",
      "Epoch:  36 | Step:  89 | Val Loss:  0.498471736907959\n",
      "Epoch:  36 | Step:  90 | Val Loss:  0.5253720879554749\n",
      "Epoch:  36 | Step:  91 | Val Loss:  0.5376139879226685\n",
      "Epoch:  36 | Step:  92 | Val Loss:  0.4279181957244873\n",
      "Epoch:  36 | Step:  93 | Val Loss:  0.4149268567562103\n",
      "Epoch:  36 | Step:  94 | Val Loss:  0.47792190313339233\n",
      "Epoch:  36 | Step:  95 | Val Loss:  0.4615170955657959\n",
      "Epoch:  36 | Step:  96 | Val Loss:  0.595727264881134\n",
      "Epoch:  36 | Step:  97 | Val Loss:  0.42124179005622864\n",
      "Epoch:  36 | Step:  98 | Val Loss:  0.5331321954727173\n",
      "Epoch:  36 | Step:  99 | Val Loss:  0.6565447449684143\n",
      "Epoch:  36 | Step:  100 | Val Loss:  0.6297773718833923\n",
      "Epoch:  36 | Step:  101 | Val Loss:  0.40915876626968384\n",
      "Epoch:  36 | Step:  102 | Val Loss:  0.5663934946060181\n",
      "Epoch:  36 | Step:  103 | Val Loss:  0.5425743460655212\n",
      "Epoch:  36 | Step:  104 | Val Loss:  0.47551146149635315\n",
      "Epoch:  36 | Step:  105 | Val Loss:  0.6878165006637573\n",
      "Epoch:  36 | Step:  106 | Val Loss:  0.5005499124526978\n",
      "Epoch:  36 | Step:  107 | Val Loss:  0.389987051486969\n",
      "Epoch:  36 | Step:  108 | Val Loss:  0.5777552127838135\n",
      "Epoch:  36 | Step:  109 | Val Loss:  0.4599885940551758\n",
      "Epoch:  36 | Step:  110 | Val Loss:  0.5572503805160522\n",
      "Epoch:  36 | Step:  111 | Val Loss:  0.7185860872268677\n",
      "Epoch:  36 | Step:  112 | Val Loss:  0.6932578086853027\n",
      "Epoch:  36 | Step:  113 | Val Loss:  0.6860856413841248\n",
      "Epoch:  36 | Step:  114 | Val Loss:  0.675018846988678\n",
      "Epoch:  36 | Step:  115 | Val Loss:  0.5412007570266724\n",
      "Epoch:  36 | Step:  116 | Val Loss:  0.47652381658554077\n",
      "Epoch:  36 | Step:  117 | Val Loss:  0.47229018807411194\n",
      "Epoch:  36 | Step:  118 | Val Loss:  0.6239380836486816\n",
      "Epoch:  36 | Step:  119 | Val Loss:  0.6186778545379639\n",
      "Epoch:  36 | Step:  120 | Val Loss:  0.5085684061050415\n",
      "Epoch:  36 | Step:  121 | Val Loss:  0.6293452978134155\n",
      "Epoch:  36 | Step:  122 | Val Loss:  0.701398491859436\n",
      "Epoch:  36 | Step:  123 | Val Loss:  0.45899832248687744\n",
      "Epoch:  36 | Step:  124 | Val Loss:  0.5820942521095276\n",
      "Epoch:  36 | Step:  125 | Val Loss:  0.44281506538391113\n",
      "Epoch:  36 | Train Loss:  tensor(0.5066, device='cuda:0') | Val Loss:  tensor(0.5278, device='cuda:0')\n",
      "Epoch:  37 | Step:  500 | Train Loss:  0.48433858156204224\n",
      "Epoch:  37 | Step:  1 | Val Loss:  0.6186469793319702\n",
      "Epoch:  37 | Step:  2 | Val Loss:  0.5029332637786865\n",
      "Epoch:  37 | Step:  3 | Val Loss:  0.48185545206069946\n",
      "Epoch:  37 | Step:  4 | Val Loss:  0.37806278467178345\n",
      "Epoch:  37 | Step:  5 | Val Loss:  0.5103416442871094\n",
      "Epoch:  37 | Step:  6 | Val Loss:  0.5906009674072266\n",
      "Epoch:  37 | Step:  7 | Val Loss:  0.49545609951019287\n",
      "Epoch:  37 | Step:  8 | Val Loss:  0.5906755924224854\n",
      "Epoch:  37 | Step:  9 | Val Loss:  0.40077903866767883\n",
      "Epoch:  37 | Step:  10 | Val Loss:  0.4103070795536041\n",
      "Epoch:  37 | Step:  11 | Val Loss:  0.5385156273841858\n",
      "Epoch:  37 | Step:  12 | Val Loss:  0.5637499094009399\n",
      "Epoch:  37 | Step:  13 | Val Loss:  0.534498929977417\n",
      "Epoch:  37 | Step:  14 | Val Loss:  0.49991893768310547\n",
      "Epoch:  37 | Step:  15 | Val Loss:  0.5627627372741699\n",
      "Epoch:  37 | Step:  16 | Val Loss:  0.7067418098449707\n",
      "Epoch:  37 | Step:  17 | Val Loss:  0.4512978792190552\n",
      "Epoch:  37 | Step:  18 | Val Loss:  0.4390089511871338\n",
      "Epoch:  37 | Step:  19 | Val Loss:  0.4529203176498413\n",
      "Epoch:  37 | Step:  20 | Val Loss:  0.5801031589508057\n",
      "Epoch:  37 | Step:  21 | Val Loss:  0.5823748111724854\n",
      "Epoch:  37 | Step:  22 | Val Loss:  0.5163737535476685\n",
      "Epoch:  37 | Step:  23 | Val Loss:  0.3615018129348755\n",
      "Epoch:  37 | Step:  24 | Val Loss:  0.5124726295471191\n",
      "Epoch:  37 | Step:  25 | Val Loss:  0.4816356599330902\n",
      "Epoch:  37 | Step:  26 | Val Loss:  0.5314785242080688\n",
      "Epoch:  37 | Step:  27 | Val Loss:  0.5217163562774658\n",
      "Epoch:  37 | Step:  28 | Val Loss:  0.42310380935668945\n",
      "Epoch:  37 | Step:  29 | Val Loss:  0.5885598063468933\n",
      "Epoch:  37 | Step:  30 | Val Loss:  0.5174320936203003\n",
      "Epoch:  37 | Step:  31 | Val Loss:  0.6634324789047241\n",
      "Epoch:  37 | Step:  32 | Val Loss:  0.304826021194458\n",
      "Epoch:  37 | Step:  33 | Val Loss:  0.746169924736023\n",
      "Epoch:  37 | Step:  34 | Val Loss:  0.5071426630020142\n",
      "Epoch:  37 | Step:  35 | Val Loss:  0.6856824159622192\n",
      "Epoch:  37 | Step:  36 | Val Loss:  0.5296101570129395\n",
      "Epoch:  37 | Step:  37 | Val Loss:  0.38500672578811646\n",
      "Epoch:  37 | Step:  38 | Val Loss:  0.47710442543029785\n",
      "Epoch:  37 | Step:  39 | Val Loss:  0.4055415689945221\n",
      "Epoch:  37 | Step:  40 | Val Loss:  0.6360150575637817\n",
      "Epoch:  37 | Step:  41 | Val Loss:  0.5026092529296875\n",
      "Epoch:  37 | Step:  42 | Val Loss:  0.5505744814872742\n",
      "Epoch:  37 | Step:  43 | Val Loss:  0.5569605231285095\n",
      "Epoch:  37 | Step:  44 | Val Loss:  0.6928865909576416\n",
      "Epoch:  37 | Step:  45 | Val Loss:  0.40954047441482544\n",
      "Epoch:  37 | Step:  46 | Val Loss:  0.4579949676990509\n",
      "Epoch:  37 | Step:  47 | Val Loss:  0.6366409659385681\n",
      "Epoch:  37 | Step:  48 | Val Loss:  0.43274620175361633\n",
      "Epoch:  37 | Step:  49 | Val Loss:  0.45716291666030884\n",
      "Epoch:  37 | Step:  50 | Val Loss:  0.596933126449585\n",
      "Epoch:  37 | Step:  51 | Val Loss:  0.536310076713562\n",
      "Epoch:  37 | Step:  52 | Val Loss:  0.5233791470527649\n",
      "Epoch:  37 | Step:  53 | Val Loss:  0.6128333806991577\n",
      "Epoch:  37 | Step:  54 | Val Loss:  0.5744090676307678\n",
      "Epoch:  37 | Step:  55 | Val Loss:  0.5462671518325806\n",
      "Epoch:  37 | Step:  56 | Val Loss:  0.4647923409938812\n",
      "Epoch:  37 | Step:  57 | Val Loss:  0.5731106996536255\n",
      "Epoch:  37 | Step:  58 | Val Loss:  0.5446369051933289\n",
      "Epoch:  37 | Step:  59 | Val Loss:  0.5024493932723999\n",
      "Epoch:  37 | Step:  60 | Val Loss:  0.6986870765686035\n",
      "Epoch:  37 | Step:  61 | Val Loss:  0.31613337993621826\n",
      "Epoch:  37 | Step:  62 | Val Loss:  0.4493584930896759\n",
      "Epoch:  37 | Step:  63 | Val Loss:  0.5513055324554443\n",
      "Epoch:  37 | Step:  64 | Val Loss:  0.6877994537353516\n",
      "Epoch:  37 | Step:  65 | Val Loss:  0.4403442144393921\n",
      "Epoch:  37 | Step:  66 | Val Loss:  0.5270106196403503\n",
      "Epoch:  37 | Step:  67 | Val Loss:  0.45668143033981323\n",
      "Epoch:  37 | Step:  68 | Val Loss:  0.41749823093414307\n",
      "Epoch:  37 | Step:  69 | Val Loss:  0.47973787784576416\n",
      "Epoch:  37 | Step:  70 | Val Loss:  0.5929313898086548\n",
      "Epoch:  37 | Step:  71 | Val Loss:  0.5518711805343628\n",
      "Epoch:  37 | Step:  72 | Val Loss:  0.638283371925354\n",
      "Epoch:  37 | Step:  73 | Val Loss:  0.5540034770965576\n",
      "Epoch:  37 | Step:  74 | Val Loss:  0.6022820472717285\n",
      "Epoch:  37 | Step:  75 | Val Loss:  0.515894889831543\n",
      "Epoch:  37 | Step:  76 | Val Loss:  0.3783678412437439\n",
      "Epoch:  37 | Step:  77 | Val Loss:  0.5756612420082092\n",
      "Epoch:  37 | Step:  78 | Val Loss:  0.40285059809684753\n",
      "Epoch:  37 | Step:  79 | Val Loss:  0.7024400234222412\n",
      "Epoch:  37 | Step:  80 | Val Loss:  0.3747774362564087\n",
      "Epoch:  37 | Step:  81 | Val Loss:  0.4902593791484833\n",
      "Epoch:  37 | Step:  82 | Val Loss:  0.43771135807037354\n",
      "Epoch:  37 | Step:  83 | Val Loss:  0.48546862602233887\n",
      "Epoch:  37 | Step:  84 | Val Loss:  0.6975328922271729\n",
      "Epoch:  37 | Step:  85 | Val Loss:  0.5176199674606323\n",
      "Epoch:  37 | Step:  86 | Val Loss:  0.4185589551925659\n",
      "Epoch:  37 | Step:  87 | Val Loss:  0.6898791193962097\n",
      "Epoch:  37 | Step:  88 | Val Loss:  0.5879955291748047\n",
      "Epoch:  37 | Step:  89 | Val Loss:  0.42123347520828247\n",
      "Epoch:  37 | Step:  90 | Val Loss:  0.46055638790130615\n",
      "Epoch:  37 | Step:  91 | Val Loss:  0.3479319214820862\n",
      "Epoch:  37 | Step:  92 | Val Loss:  0.6554336547851562\n",
      "Epoch:  37 | Step:  93 | Val Loss:  0.4057537615299225\n",
      "Epoch:  37 | Step:  94 | Val Loss:  0.5875967144966125\n",
      "Epoch:  37 | Step:  95 | Val Loss:  0.6279585361480713\n",
      "Epoch:  37 | Step:  96 | Val Loss:  0.6308550834655762\n",
      "Epoch:  37 | Step:  97 | Val Loss:  0.5724016427993774\n",
      "Epoch:  37 | Step:  98 | Val Loss:  0.7029581069946289\n",
      "Epoch:  37 | Step:  99 | Val Loss:  0.5018914937973022\n",
      "Epoch:  37 | Step:  100 | Val Loss:  0.6457751989364624\n",
      "Epoch:  37 | Step:  101 | Val Loss:  0.6110959649085999\n",
      "Epoch:  37 | Step:  102 | Val Loss:  0.4424239993095398\n",
      "Epoch:  37 | Step:  103 | Val Loss:  0.4138081669807434\n",
      "Epoch:  37 | Step:  104 | Val Loss:  0.5693686604499817\n",
      "Epoch:  37 | Step:  105 | Val Loss:  0.5310332179069519\n",
      "Epoch:  37 | Step:  106 | Val Loss:  0.5802536010742188\n",
      "Epoch:  37 | Step:  107 | Val Loss:  0.40570127964019775\n",
      "Epoch:  37 | Step:  108 | Val Loss:  0.713686466217041\n",
      "Epoch:  37 | Step:  109 | Val Loss:  0.49830445647239685\n",
      "Epoch:  37 | Step:  110 | Val Loss:  0.41213321685791016\n",
      "Epoch:  37 | Step:  111 | Val Loss:  0.6975414156913757\n",
      "Epoch:  37 | Step:  112 | Val Loss:  0.34847867488861084\n",
      "Epoch:  37 | Step:  113 | Val Loss:  0.48153620958328247\n",
      "Epoch:  37 | Step:  114 | Val Loss:  0.42228227853775024\n",
      "Epoch:  37 | Step:  115 | Val Loss:  0.5034774541854858\n",
      "Epoch:  37 | Step:  116 | Val Loss:  0.4800863265991211\n",
      "Epoch:  37 | Step:  117 | Val Loss:  0.5907225012779236\n",
      "Epoch:  37 | Step:  118 | Val Loss:  0.43350380659103394\n",
      "Epoch:  37 | Step:  119 | Val Loss:  0.5146714448928833\n",
      "Epoch:  37 | Step:  120 | Val Loss:  0.6950649619102478\n",
      "Epoch:  37 | Step:  121 | Val Loss:  0.566154420375824\n",
      "Epoch:  37 | Step:  122 | Val Loss:  0.6124613285064697\n",
      "Epoch:  37 | Step:  123 | Val Loss:  0.6093549728393555\n",
      "Epoch:  37 | Step:  124 | Val Loss:  0.5213141441345215\n",
      "Epoch:  37 | Step:  125 | Val Loss:  0.43300533294677734\n",
      "Epoch:  37 | Train Loss:  tensor(0.5053, device='cuda:0') | Val Loss:  tensor(0.5260, device='cuda:0')\n",
      "Epoch:  38 | Step:  500 | Train Loss:  0.653746485710144\n",
      "Epoch:  38 | Step:  1 | Val Loss:  0.41628843545913696\n",
      "Epoch:  38 | Step:  2 | Val Loss:  0.5959093570709229\n",
      "Epoch:  38 | Step:  3 | Val Loss:  0.765044093132019\n",
      "Epoch:  38 | Step:  4 | Val Loss:  0.4133925437927246\n",
      "Epoch:  38 | Step:  5 | Val Loss:  0.45267653465270996\n",
      "Epoch:  38 | Step:  6 | Val Loss:  0.5248095393180847\n",
      "Epoch:  38 | Step:  7 | Val Loss:  0.6416242718696594\n",
      "Epoch:  38 | Step:  8 | Val Loss:  0.5339409708976746\n",
      "Epoch:  38 | Step:  9 | Val Loss:  0.5844074487686157\n",
      "Epoch:  38 | Step:  10 | Val Loss:  0.543308675289154\n",
      "Epoch:  38 | Step:  11 | Val Loss:  0.5466062426567078\n",
      "Epoch:  38 | Step:  12 | Val Loss:  0.46071112155914307\n",
      "Epoch:  38 | Step:  13 | Val Loss:  0.5134979486465454\n",
      "Epoch:  38 | Step:  14 | Val Loss:  0.6115601658821106\n",
      "Epoch:  38 | Step:  15 | Val Loss:  0.48022764921188354\n",
      "Epoch:  38 | Step:  16 | Val Loss:  0.5108614563941956\n",
      "Epoch:  38 | Step:  17 | Val Loss:  0.5646877288818359\n",
      "Epoch:  38 | Step:  18 | Val Loss:  0.7046619057655334\n",
      "Epoch:  38 | Step:  19 | Val Loss:  0.727432370185852\n",
      "Epoch:  38 | Step:  20 | Val Loss:  0.2982889413833618\n",
      "Epoch:  38 | Step:  21 | Val Loss:  0.5544701218605042\n",
      "Epoch:  38 | Step:  22 | Val Loss:  0.5070186853408813\n",
      "Epoch:  38 | Step:  23 | Val Loss:  0.4306773543357849\n",
      "Epoch:  38 | Step:  24 | Val Loss:  0.4700583815574646\n",
      "Epoch:  38 | Step:  25 | Val Loss:  0.5960304737091064\n",
      "Epoch:  38 | Step:  26 | Val Loss:  0.6469327211380005\n",
      "Epoch:  38 | Step:  27 | Val Loss:  0.5358841419219971\n",
      "Epoch:  38 | Step:  28 | Val Loss:  0.5469273328781128\n",
      "Epoch:  38 | Step:  29 | Val Loss:  0.48682016134262085\n",
      "Epoch:  38 | Step:  30 | Val Loss:  0.652681827545166\n",
      "Epoch:  38 | Step:  31 | Val Loss:  0.4725979268550873\n",
      "Epoch:  38 | Step:  32 | Val Loss:  0.4532289505004883\n",
      "Epoch:  38 | Step:  33 | Val Loss:  0.5651414394378662\n",
      "Epoch:  38 | Step:  34 | Val Loss:  0.5109258890151978\n",
      "Epoch:  38 | Step:  35 | Val Loss:  0.2786461114883423\n",
      "Epoch:  38 | Step:  36 | Val Loss:  0.6420881152153015\n",
      "Epoch:  38 | Step:  37 | Val Loss:  0.5483911037445068\n",
      "Epoch:  38 | Step:  38 | Val Loss:  0.5030107498168945\n",
      "Epoch:  38 | Step:  39 | Val Loss:  0.7500683069229126\n",
      "Epoch:  38 | Step:  40 | Val Loss:  0.5792474746704102\n",
      "Epoch:  38 | Step:  41 | Val Loss:  0.6014614701271057\n",
      "Epoch:  38 | Step:  42 | Val Loss:  0.5704976320266724\n",
      "Epoch:  38 | Step:  43 | Val Loss:  0.5096628665924072\n",
      "Epoch:  38 | Step:  44 | Val Loss:  0.40218448638916016\n",
      "Epoch:  38 | Step:  45 | Val Loss:  0.4408995509147644\n",
      "Epoch:  38 | Step:  46 | Val Loss:  0.5941368937492371\n",
      "Epoch:  38 | Step:  47 | Val Loss:  0.507623553276062\n",
      "Epoch:  38 | Step:  48 | Val Loss:  0.49190860986709595\n",
      "Epoch:  38 | Step:  49 | Val Loss:  0.6067006587982178\n",
      "Epoch:  38 | Step:  50 | Val Loss:  0.6247171759605408\n",
      "Epoch:  38 | Step:  51 | Val Loss:  0.617642343044281\n",
      "Epoch:  38 | Step:  52 | Val Loss:  0.586277961730957\n",
      "Epoch:  38 | Step:  53 | Val Loss:  0.5800273418426514\n",
      "Epoch:  38 | Step:  54 | Val Loss:  0.4866117835044861\n",
      "Epoch:  38 | Step:  55 | Val Loss:  0.45915576815605164\n",
      "Epoch:  38 | Step:  56 | Val Loss:  0.4821512997150421\n",
      "Epoch:  38 | Step:  57 | Val Loss:  0.524476170539856\n",
      "Epoch:  38 | Step:  58 | Val Loss:  0.3810788094997406\n",
      "Epoch:  38 | Step:  59 | Val Loss:  0.6604495048522949\n",
      "Epoch:  38 | Step:  60 | Val Loss:  0.4538223147392273\n",
      "Epoch:  38 | Step:  61 | Val Loss:  0.5374282002449036\n",
      "Epoch:  38 | Step:  62 | Val Loss:  0.5406829118728638\n",
      "Epoch:  38 | Step:  63 | Val Loss:  0.4445253014564514\n",
      "Epoch:  38 | Step:  64 | Val Loss:  0.5430172681808472\n",
      "Epoch:  38 | Step:  65 | Val Loss:  0.43743884563446045\n",
      "Epoch:  38 | Step:  66 | Val Loss:  0.26866355538368225\n",
      "Epoch:  38 | Step:  67 | Val Loss:  0.572240948677063\n",
      "Epoch:  38 | Step:  68 | Val Loss:  0.6182575821876526\n",
      "Epoch:  38 | Step:  69 | Val Loss:  0.6808828711509705\n",
      "Epoch:  38 | Step:  70 | Val Loss:  0.3884916305541992\n",
      "Epoch:  38 | Step:  71 | Val Loss:  0.372070848941803\n",
      "Epoch:  38 | Step:  72 | Val Loss:  0.5731184482574463\n",
      "Epoch:  38 | Step:  73 | Val Loss:  0.4128647446632385\n",
      "Epoch:  38 | Step:  74 | Val Loss:  0.5358116626739502\n",
      "Epoch:  38 | Step:  75 | Val Loss:  0.4484426975250244\n",
      "Epoch:  38 | Step:  76 | Val Loss:  0.36882656812667847\n",
      "Epoch:  38 | Step:  77 | Val Loss:  0.4122081696987152\n",
      "Epoch:  38 | Step:  78 | Val Loss:  0.44431203603744507\n",
      "Epoch:  38 | Step:  79 | Val Loss:  0.6195106506347656\n",
      "Epoch:  38 | Step:  80 | Val Loss:  0.6127498149871826\n",
      "Epoch:  38 | Step:  81 | Val Loss:  0.5559015274047852\n",
      "Epoch:  38 | Step:  82 | Val Loss:  0.3975464701652527\n",
      "Epoch:  38 | Step:  83 | Val Loss:  0.6063132286071777\n",
      "Epoch:  38 | Step:  84 | Val Loss:  0.5947296023368835\n",
      "Epoch:  38 | Step:  85 | Val Loss:  0.6265769004821777\n",
      "Epoch:  38 | Step:  86 | Val Loss:  0.7502936124801636\n",
      "Epoch:  38 | Step:  87 | Val Loss:  0.5735138654708862\n",
      "Epoch:  38 | Step:  88 | Val Loss:  0.5781654715538025\n",
      "Epoch:  38 | Step:  89 | Val Loss:  0.5696020126342773\n",
      "Epoch:  38 | Step:  90 | Val Loss:  0.4781818985939026\n",
      "Epoch:  38 | Step:  91 | Val Loss:  0.6325066089630127\n",
      "Epoch:  38 | Step:  92 | Val Loss:  0.47385579347610474\n",
      "Epoch:  38 | Step:  93 | Val Loss:  0.4158731997013092\n",
      "Epoch:  38 | Step:  94 | Val Loss:  0.6294518113136292\n",
      "Epoch:  38 | Step:  95 | Val Loss:  0.3918151259422302\n",
      "Epoch:  38 | Step:  96 | Val Loss:  0.576508641242981\n",
      "Epoch:  38 | Step:  97 | Val Loss:  0.5498971343040466\n",
      "Epoch:  38 | Step:  98 | Val Loss:  0.5447016954421997\n",
      "Epoch:  38 | Step:  99 | Val Loss:  0.49910038709640503\n",
      "Epoch:  38 | Step:  100 | Val Loss:  0.4665036201477051\n",
      "Epoch:  38 | Step:  101 | Val Loss:  0.49171340465545654\n",
      "Epoch:  38 | Step:  102 | Val Loss:  0.5031139850616455\n",
      "Epoch:  38 | Step:  103 | Val Loss:  0.3937875032424927\n",
      "Epoch:  38 | Step:  104 | Val Loss:  0.5973543524742126\n",
      "Epoch:  38 | Step:  105 | Val Loss:  0.4923399090766907\n",
      "Epoch:  38 | Step:  106 | Val Loss:  0.5883738994598389\n",
      "Epoch:  38 | Step:  107 | Val Loss:  0.46146780252456665\n",
      "Epoch:  38 | Step:  108 | Val Loss:  0.3979140818119049\n",
      "Epoch:  38 | Step:  109 | Val Loss:  0.45705848932266235\n",
      "Epoch:  38 | Step:  110 | Val Loss:  0.5279297828674316\n",
      "Epoch:  38 | Step:  111 | Val Loss:  0.4449477195739746\n",
      "Epoch:  38 | Step:  112 | Val Loss:  0.4993175268173218\n",
      "Epoch:  38 | Step:  113 | Val Loss:  0.35103270411491394\n",
      "Epoch:  38 | Step:  114 | Val Loss:  0.48621273040771484\n",
      "Epoch:  38 | Step:  115 | Val Loss:  0.5413705706596375\n",
      "Epoch:  38 | Step:  116 | Val Loss:  0.5490220189094543\n",
      "Epoch:  38 | Step:  117 | Val Loss:  0.6503983736038208\n",
      "Epoch:  38 | Step:  118 | Val Loss:  0.4876302480697632\n",
      "Epoch:  38 | Step:  119 | Val Loss:  0.4371170103549957\n",
      "Epoch:  38 | Step:  120 | Val Loss:  0.5717945098876953\n",
      "Epoch:  38 | Step:  121 | Val Loss:  0.5886238217353821\n",
      "Epoch:  38 | Step:  122 | Val Loss:  0.46279817819595337\n",
      "Epoch:  38 | Step:  123 | Val Loss:  0.5697208642959595\n",
      "Epoch:  38 | Step:  124 | Val Loss:  0.44282376766204834\n",
      "Epoch:  38 | Step:  125 | Val Loss:  0.6051563620567322\n",
      "Epoch:  38 | Train Loss:  tensor(0.5042, device='cuda:0') | Val Loss:  tensor(0.5244, device='cuda:0')\n",
      "Epoch:  39 | Step:  500 | Train Loss:  0.39172452688217163\n",
      "Epoch:  39 | Step:  1 | Val Loss:  0.5222207903862\n",
      "Epoch:  39 | Step:  2 | Val Loss:  0.49508485198020935\n",
      "Epoch:  39 | Step:  3 | Val Loss:  0.4992876350879669\n",
      "Epoch:  39 | Step:  4 | Val Loss:  0.45911532640457153\n",
      "Epoch:  39 | Step:  5 | Val Loss:  0.5960060358047485\n",
      "Epoch:  39 | Step:  6 | Val Loss:  0.5761939287185669\n",
      "Epoch:  39 | Step:  7 | Val Loss:  0.49276989698410034\n",
      "Epoch:  39 | Step:  8 | Val Loss:  0.4599355161190033\n",
      "Epoch:  39 | Step:  9 | Val Loss:  0.5009149312973022\n",
      "Epoch:  39 | Step:  10 | Val Loss:  0.5016932487487793\n",
      "Epoch:  39 | Step:  11 | Val Loss:  0.555816650390625\n",
      "Epoch:  39 | Step:  12 | Val Loss:  0.5283149480819702\n",
      "Epoch:  39 | Step:  13 | Val Loss:  0.598748505115509\n",
      "Epoch:  39 | Step:  14 | Val Loss:  0.5523630380630493\n",
      "Epoch:  39 | Step:  15 | Val Loss:  0.6065458059310913\n",
      "Epoch:  39 | Step:  16 | Val Loss:  0.5537957549095154\n",
      "Epoch:  39 | Step:  17 | Val Loss:  0.36951127648353577\n",
      "Epoch:  39 | Step:  18 | Val Loss:  0.7002533078193665\n",
      "Epoch:  39 | Step:  19 | Val Loss:  0.5530766248703003\n",
      "Epoch:  39 | Step:  20 | Val Loss:  0.5190838575363159\n",
      "Epoch:  39 | Step:  21 | Val Loss:  0.5105326175689697\n",
      "Epoch:  39 | Step:  22 | Val Loss:  0.5908223986625671\n",
      "Epoch:  39 | Step:  23 | Val Loss:  0.6229087114334106\n",
      "Epoch:  39 | Step:  24 | Val Loss:  0.5827879905700684\n",
      "Epoch:  39 | Step:  25 | Val Loss:  0.43152210116386414\n",
      "Epoch:  39 | Step:  26 | Val Loss:  0.6925650835037231\n",
      "Epoch:  39 | Step:  27 | Val Loss:  0.3322961628437042\n",
      "Epoch:  39 | Step:  28 | Val Loss:  0.5652756690979004\n",
      "Epoch:  39 | Step:  29 | Val Loss:  0.39970657229423523\n",
      "Epoch:  39 | Step:  30 | Val Loss:  0.5753813982009888\n",
      "Epoch:  39 | Step:  31 | Val Loss:  0.4919772148132324\n",
      "Epoch:  39 | Step:  32 | Val Loss:  0.5367422699928284\n",
      "Epoch:  39 | Step:  33 | Val Loss:  0.5424618721008301\n",
      "Epoch:  39 | Step:  34 | Val Loss:  0.6121112108230591\n",
      "Epoch:  39 | Step:  35 | Val Loss:  0.41085803508758545\n",
      "Epoch:  39 | Step:  36 | Val Loss:  0.42463088035583496\n",
      "Epoch:  39 | Step:  37 | Val Loss:  0.43178611993789673\n",
      "Epoch:  39 | Step:  38 | Val Loss:  0.5543798804283142\n",
      "Epoch:  39 | Step:  39 | Val Loss:  0.5924078226089478\n",
      "Epoch:  39 | Step:  40 | Val Loss:  0.43954503536224365\n",
      "Epoch:  39 | Step:  41 | Val Loss:  0.5869733095169067\n",
      "Epoch:  39 | Step:  42 | Val Loss:  0.530424177646637\n",
      "Epoch:  39 | Step:  43 | Val Loss:  0.34769609570503235\n",
      "Epoch:  39 | Step:  44 | Val Loss:  0.6131263971328735\n",
      "Epoch:  39 | Step:  45 | Val Loss:  0.47236549854278564\n",
      "Epoch:  39 | Step:  46 | Val Loss:  0.47316408157348633\n",
      "Epoch:  39 | Step:  47 | Val Loss:  0.5659220814704895\n",
      "Epoch:  39 | Step:  48 | Val Loss:  0.5157630443572998\n",
      "Epoch:  39 | Step:  49 | Val Loss:  0.42500150203704834\n",
      "Epoch:  39 | Step:  50 | Val Loss:  0.4143369495868683\n",
      "Epoch:  39 | Step:  51 | Val Loss:  0.5479289293289185\n",
      "Epoch:  39 | Step:  52 | Val Loss:  0.498748242855072\n",
      "Epoch:  39 | Step:  53 | Val Loss:  0.34986579418182373\n",
      "Epoch:  39 | Step:  54 | Val Loss:  0.46681010723114014\n",
      "Epoch:  39 | Step:  55 | Val Loss:  0.5601271390914917\n",
      "Epoch:  39 | Step:  56 | Val Loss:  0.6317834258079529\n",
      "Epoch:  39 | Step:  57 | Val Loss:  0.3768298625946045\n",
      "Epoch:  39 | Step:  58 | Val Loss:  0.5890839099884033\n",
      "Epoch:  39 | Step:  59 | Val Loss:  0.6059765815734863\n",
      "Epoch:  39 | Step:  60 | Val Loss:  0.5388848185539246\n",
      "Epoch:  39 | Step:  61 | Val Loss:  0.5164846181869507\n",
      "Epoch:  39 | Step:  62 | Val Loss:  0.4250011742115021\n",
      "Epoch:  39 | Step:  63 | Val Loss:  0.5952296257019043\n",
      "Epoch:  39 | Step:  64 | Val Loss:  0.6849395036697388\n",
      "Epoch:  39 | Step:  65 | Val Loss:  0.5187145471572876\n",
      "Epoch:  39 | Step:  66 | Val Loss:  0.6288973093032837\n",
      "Epoch:  39 | Step:  67 | Val Loss:  0.5108826756477356\n",
      "Epoch:  39 | Step:  68 | Val Loss:  0.6001114845275879\n",
      "Epoch:  39 | Step:  69 | Val Loss:  0.5538737773895264\n",
      "Epoch:  39 | Step:  70 | Val Loss:  0.6003086566925049\n",
      "Epoch:  39 | Step:  71 | Val Loss:  0.42112553119659424\n",
      "Epoch:  39 | Step:  72 | Val Loss:  0.6258703470230103\n",
      "Epoch:  39 | Step:  73 | Val Loss:  0.2890658378601074\n",
      "Epoch:  39 | Step:  74 | Val Loss:  0.4136126637458801\n",
      "Epoch:  39 | Step:  75 | Val Loss:  0.5976274013519287\n",
      "Epoch:  39 | Step:  76 | Val Loss:  0.38292014598846436\n",
      "Epoch:  39 | Step:  77 | Val Loss:  0.679267168045044\n",
      "Epoch:  39 | Step:  78 | Val Loss:  0.391666978597641\n",
      "Epoch:  39 | Step:  79 | Val Loss:  0.7100673913955688\n",
      "Epoch:  39 | Step:  80 | Val Loss:  0.38109952211380005\n",
      "Epoch:  39 | Step:  81 | Val Loss:  0.46738529205322266\n",
      "Epoch:  39 | Step:  82 | Val Loss:  0.5342757701873779\n",
      "Epoch:  39 | Step:  83 | Val Loss:  0.4601460099220276\n",
      "Epoch:  39 | Step:  84 | Val Loss:  0.4495766758918762\n",
      "Epoch:  39 | Step:  85 | Val Loss:  0.6648611426353455\n",
      "Epoch:  39 | Step:  86 | Val Loss:  0.5874010324478149\n",
      "Epoch:  39 | Step:  87 | Val Loss:  0.6082139611244202\n",
      "Epoch:  39 | Step:  88 | Val Loss:  0.49236220121383667\n",
      "Epoch:  39 | Step:  89 | Val Loss:  0.6335247755050659\n",
      "Epoch:  39 | Step:  90 | Val Loss:  0.42744937539100647\n",
      "Epoch:  39 | Step:  91 | Val Loss:  0.5534151792526245\n",
      "Epoch:  39 | Step:  92 | Val Loss:  0.6562879085540771\n",
      "Epoch:  39 | Step:  93 | Val Loss:  0.44047611951828003\n",
      "Epoch:  39 | Step:  94 | Val Loss:  0.6344122886657715\n",
      "Epoch:  39 | Step:  95 | Val Loss:  0.32752346992492676\n",
      "Epoch:  39 | Step:  96 | Val Loss:  0.5028666853904724\n",
      "Epoch:  39 | Step:  97 | Val Loss:  0.4655500650405884\n",
      "Epoch:  39 | Step:  98 | Val Loss:  0.422868549823761\n",
      "Epoch:  39 | Step:  99 | Val Loss:  0.5113794803619385\n",
      "Epoch:  39 | Step:  100 | Val Loss:  0.7345432043075562\n",
      "Epoch:  39 | Step:  101 | Val Loss:  0.5612749457359314\n",
      "Epoch:  39 | Step:  102 | Val Loss:  0.3304891586303711\n",
      "Epoch:  39 | Step:  103 | Val Loss:  0.5145431756973267\n",
      "Epoch:  39 | Step:  104 | Val Loss:  0.44006308913230896\n",
      "Epoch:  39 | Step:  105 | Val Loss:  0.49068376421928406\n",
      "Epoch:  39 | Step:  106 | Val Loss:  0.46027302742004395\n",
      "Epoch:  39 | Step:  107 | Val Loss:  0.4997202754020691\n",
      "Epoch:  39 | Step:  108 | Val Loss:  0.6468814015388489\n",
      "Epoch:  39 | Step:  109 | Val Loss:  0.5561830997467041\n",
      "Epoch:  39 | Step:  110 | Val Loss:  0.5711327791213989\n",
      "Epoch:  39 | Step:  111 | Val Loss:  0.5318922996520996\n",
      "Epoch:  39 | Step:  112 | Val Loss:  0.7845991849899292\n",
      "Epoch:  39 | Step:  113 | Val Loss:  0.5429047346115112\n",
      "Epoch:  39 | Step:  114 | Val Loss:  0.48361122608184814\n",
      "Epoch:  39 | Step:  115 | Val Loss:  0.5319269299507141\n",
      "Epoch:  39 | Step:  116 | Val Loss:  0.5050219893455505\n",
      "Epoch:  39 | Step:  117 | Val Loss:  0.36648479104042053\n",
      "Epoch:  39 | Step:  118 | Val Loss:  0.637018084526062\n",
      "Epoch:  39 | Step:  119 | Val Loss:  0.5324021577835083\n",
      "Epoch:  39 | Step:  120 | Val Loss:  0.4502274990081787\n",
      "Epoch:  39 | Step:  121 | Val Loss:  0.4751628041267395\n",
      "Epoch:  39 | Step:  122 | Val Loss:  0.5124083757400513\n",
      "Epoch:  39 | Step:  123 | Val Loss:  0.603230357170105\n",
      "Epoch:  39 | Step:  124 | Val Loss:  0.48958247900009155\n",
      "Epoch:  39 | Step:  125 | Val Loss:  0.6409027576446533\n",
      "Epoch:  39 | Train Loss:  tensor(0.5034, device='cuda:0') | Val Loss:  tensor(0.5228, device='cuda:0')\n",
      "Epoch:  40 | Step:  500 | Train Loss:  0.4861191213130951\n",
      "Epoch:  40 | Step:  1 | Val Loss:  0.5130796432495117\n",
      "Epoch:  40 | Step:  2 | Val Loss:  0.5989497900009155\n",
      "Epoch:  40 | Step:  3 | Val Loss:  0.5560814142227173\n",
      "Epoch:  40 | Step:  4 | Val Loss:  0.4968549907207489\n",
      "Epoch:  40 | Step:  5 | Val Loss:  0.5056713819503784\n",
      "Epoch:  40 | Step:  6 | Val Loss:  0.44656145572662354\n",
      "Epoch:  40 | Step:  7 | Val Loss:  0.7128629684448242\n",
      "Epoch:  40 | Step:  8 | Val Loss:  0.45510560274124146\n",
      "Epoch:  40 | Step:  9 | Val Loss:  0.5897661447525024\n",
      "Epoch:  40 | Step:  10 | Val Loss:  0.6169148683547974\n",
      "Epoch:  40 | Step:  11 | Val Loss:  0.3339794874191284\n",
      "Epoch:  40 | Step:  12 | Val Loss:  0.4486125707626343\n",
      "Epoch:  40 | Step:  13 | Val Loss:  0.6650383472442627\n",
      "Epoch:  40 | Step:  14 | Val Loss:  0.48992010951042175\n",
      "Epoch:  40 | Step:  15 | Val Loss:  0.522066593170166\n",
      "Epoch:  40 | Step:  16 | Val Loss:  0.7081379294395447\n",
      "Epoch:  40 | Step:  17 | Val Loss:  0.4960663318634033\n",
      "Epoch:  40 | Step:  18 | Val Loss:  0.5124480724334717\n",
      "Epoch:  40 | Step:  19 | Val Loss:  0.5050967931747437\n",
      "Epoch:  40 | Step:  20 | Val Loss:  0.4592505097389221\n",
      "Epoch:  40 | Step:  21 | Val Loss:  0.6099504232406616\n",
      "Epoch:  40 | Step:  22 | Val Loss:  0.4836244583129883\n",
      "Epoch:  40 | Step:  23 | Val Loss:  0.5604537129402161\n",
      "Epoch:  40 | Step:  24 | Val Loss:  0.4650604724884033\n",
      "Epoch:  40 | Step:  25 | Val Loss:  0.5340420603752136\n",
      "Epoch:  40 | Step:  26 | Val Loss:  0.4110568165779114\n",
      "Epoch:  40 | Step:  27 | Val Loss:  0.5078533887863159\n",
      "Epoch:  40 | Step:  28 | Val Loss:  0.6516444683074951\n",
      "Epoch:  40 | Step:  29 | Val Loss:  0.46012404561042786\n",
      "Epoch:  40 | Step:  30 | Val Loss:  0.5227035284042358\n",
      "Epoch:  40 | Step:  31 | Val Loss:  0.37872686982154846\n",
      "Epoch:  40 | Step:  32 | Val Loss:  0.43248721957206726\n",
      "Epoch:  40 | Step:  33 | Val Loss:  0.441297322511673\n",
      "Epoch:  40 | Step:  34 | Val Loss:  0.40326929092407227\n",
      "Epoch:  40 | Step:  35 | Val Loss:  0.7032648324966431\n",
      "Epoch:  40 | Step:  36 | Val Loss:  0.5392337441444397\n",
      "Epoch:  40 | Step:  37 | Val Loss:  0.552129864692688\n",
      "Epoch:  40 | Step:  38 | Val Loss:  0.47523730993270874\n",
      "Epoch:  40 | Step:  39 | Val Loss:  0.5963180065155029\n",
      "Epoch:  40 | Step:  40 | Val Loss:  0.4184202551841736\n",
      "Epoch:  40 | Step:  41 | Val Loss:  0.4933801293373108\n",
      "Epoch:  40 | Step:  42 | Val Loss:  0.4679274559020996\n",
      "Epoch:  40 | Step:  43 | Val Loss:  0.6279157400131226\n",
      "Epoch:  40 | Step:  44 | Val Loss:  0.5199716091156006\n",
      "Epoch:  40 | Step:  45 | Val Loss:  0.5678625702857971\n",
      "Epoch:  40 | Step:  46 | Val Loss:  0.5637463331222534\n",
      "Epoch:  40 | Step:  47 | Val Loss:  0.5318017601966858\n",
      "Epoch:  40 | Step:  48 | Val Loss:  0.570379912853241\n",
      "Epoch:  40 | Step:  49 | Val Loss:  0.46810388565063477\n",
      "Epoch:  40 | Step:  50 | Val Loss:  0.4477614760398865\n",
      "Epoch:  40 | Step:  51 | Val Loss:  0.5626184940338135\n",
      "Epoch:  40 | Step:  52 | Val Loss:  0.562971830368042\n",
      "Epoch:  40 | Step:  53 | Val Loss:  0.4744577407836914\n",
      "Epoch:  40 | Step:  54 | Val Loss:  0.6501604914665222\n",
      "Epoch:  40 | Step:  55 | Val Loss:  0.5002145767211914\n",
      "Epoch:  40 | Step:  56 | Val Loss:  0.5605789422988892\n",
      "Epoch:  40 | Step:  57 | Val Loss:  0.5026476383209229\n",
      "Epoch:  40 | Step:  58 | Val Loss:  0.6779700517654419\n",
      "Epoch:  40 | Step:  59 | Val Loss:  0.4254244565963745\n",
      "Epoch:  40 | Step:  60 | Val Loss:  0.40788978338241577\n",
      "Epoch:  40 | Step:  61 | Val Loss:  0.45975401997566223\n",
      "Epoch:  40 | Step:  62 | Val Loss:  0.5348755717277527\n",
      "Epoch:  40 | Step:  63 | Val Loss:  0.6132106184959412\n",
      "Epoch:  40 | Step:  64 | Val Loss:  0.7023391127586365\n",
      "Epoch:  40 | Step:  65 | Val Loss:  0.6077181100845337\n",
      "Epoch:  40 | Step:  66 | Val Loss:  0.45515596866607666\n",
      "Epoch:  40 | Step:  67 | Val Loss:  0.46582597494125366\n",
      "Epoch:  40 | Step:  68 | Val Loss:  0.5043349862098694\n",
      "Epoch:  40 | Step:  69 | Val Loss:  0.2604796886444092\n",
      "Epoch:  40 | Step:  70 | Val Loss:  0.6302319765090942\n",
      "Epoch:  40 | Step:  71 | Val Loss:  0.6230943202972412\n",
      "Epoch:  40 | Step:  72 | Val Loss:  0.3745921850204468\n",
      "Epoch:  40 | Step:  73 | Val Loss:  0.41063404083251953\n",
      "Epoch:  40 | Step:  74 | Val Loss:  0.6215348839759827\n",
      "Epoch:  40 | Step:  75 | Val Loss:  0.5610196590423584\n",
      "Epoch:  40 | Step:  76 | Val Loss:  0.5119633674621582\n",
      "Epoch:  40 | Step:  77 | Val Loss:  0.4573935866355896\n",
      "Epoch:  40 | Step:  78 | Val Loss:  0.5373708009719849\n",
      "Epoch:  40 | Step:  79 | Val Loss:  0.4296281337738037\n",
      "Epoch:  40 | Step:  80 | Val Loss:  0.43563762307167053\n",
      "Epoch:  40 | Step:  81 | Val Loss:  0.6683088541030884\n",
      "Epoch:  40 | Step:  82 | Val Loss:  0.6769441962242126\n",
      "Epoch:  40 | Step:  83 | Val Loss:  0.4288289546966553\n",
      "Epoch:  40 | Step:  84 | Val Loss:  0.5405547022819519\n",
      "Epoch:  40 | Step:  85 | Val Loss:  0.6259121298789978\n",
      "Epoch:  40 | Step:  86 | Val Loss:  0.40008705854415894\n",
      "Epoch:  40 | Step:  87 | Val Loss:  0.40837955474853516\n",
      "Epoch:  40 | Step:  88 | Val Loss:  0.5167500376701355\n",
      "Epoch:  40 | Step:  89 | Val Loss:  0.589288055896759\n",
      "Epoch:  40 | Step:  90 | Val Loss:  0.5820156335830688\n",
      "Epoch:  40 | Step:  91 | Val Loss:  0.3444885015487671\n",
      "Epoch:  40 | Step:  92 | Val Loss:  0.5492932796478271\n",
      "Epoch:  40 | Step:  93 | Val Loss:  0.6643527746200562\n",
      "Epoch:  40 | Step:  94 | Val Loss:  0.5756039619445801\n",
      "Epoch:  40 | Step:  95 | Val Loss:  0.5480941534042358\n",
      "Epoch:  40 | Step:  96 | Val Loss:  0.42662739753723145\n",
      "Epoch:  40 | Step:  97 | Val Loss:  0.6904389262199402\n",
      "Epoch:  40 | Step:  98 | Val Loss:  0.6469182968139648\n",
      "Epoch:  40 | Step:  99 | Val Loss:  0.5047545433044434\n",
      "Epoch:  40 | Step:  100 | Val Loss:  0.5182926654815674\n",
      "Epoch:  40 | Step:  101 | Val Loss:  0.67927086353302\n",
      "Epoch:  40 | Step:  102 | Val Loss:  0.5945340394973755\n",
      "Epoch:  40 | Step:  103 | Val Loss:  0.6050745248794556\n",
      "Epoch:  40 | Step:  104 | Val Loss:  0.46786582469940186\n",
      "Epoch:  40 | Step:  105 | Val Loss:  0.4881279170513153\n",
      "Epoch:  40 | Step:  106 | Val Loss:  0.3681781589984894\n",
      "Epoch:  40 | Step:  107 | Val Loss:  0.41438615322113037\n",
      "Epoch:  40 | Step:  108 | Val Loss:  0.7231365442276001\n",
      "Epoch:  40 | Step:  109 | Val Loss:  0.4461853504180908\n",
      "Epoch:  40 | Step:  110 | Val Loss:  0.4848102033138275\n",
      "Epoch:  40 | Step:  111 | Val Loss:  0.4660970866680145\n",
      "Epoch:  40 | Step:  112 | Val Loss:  0.40879541635513306\n",
      "Epoch:  40 | Step:  113 | Val Loss:  0.6240618824958801\n",
      "Epoch:  40 | Step:  114 | Val Loss:  0.39649996161460876\n",
      "Epoch:  40 | Step:  115 | Val Loss:  0.5278575420379639\n",
      "Epoch:  40 | Step:  116 | Val Loss:  0.5804345607757568\n",
      "Epoch:  40 | Step:  117 | Val Loss:  0.5557148456573486\n",
      "Epoch:  40 | Step:  118 | Val Loss:  0.5554933547973633\n",
      "Epoch:  40 | Step:  119 | Val Loss:  0.4753519892692566\n",
      "Epoch:  40 | Step:  120 | Val Loss:  0.5908442735671997\n",
      "Epoch:  40 | Step:  121 | Val Loss:  0.5039726495742798\n",
      "Epoch:  40 | Step:  122 | Val Loss:  0.478388249874115\n",
      "Epoch:  40 | Step:  123 | Val Loss:  0.359109103679657\n",
      "Epoch:  40 | Step:  124 | Val Loss:  0.45891880989074707\n",
      "Epoch:  40 | Step:  125 | Val Loss:  0.5108678340911865\n",
      "Epoch:  40 | Train Loss:  tensor(0.5029, device='cuda:0') | Val Loss:  tensor(0.5218, device='cuda:0')\n",
      "Epoch:  41 | Step:  500 | Train Loss:  0.569442629814148\n",
      "Epoch:  41 | Step:  1 | Val Loss:  0.531536340713501\n",
      "Epoch:  41 | Step:  2 | Val Loss:  0.4490770101547241\n",
      "Epoch:  41 | Step:  3 | Val Loss:  0.6277836561203003\n",
      "Epoch:  41 | Step:  4 | Val Loss:  0.5629869699478149\n",
      "Epoch:  41 | Step:  5 | Val Loss:  0.4166211783885956\n",
      "Epoch:  41 | Step:  6 | Val Loss:  0.4391903281211853\n",
      "Epoch:  41 | Step:  7 | Val Loss:  0.6291347146034241\n",
      "Epoch:  41 | Step:  8 | Val Loss:  0.4440852403640747\n",
      "Epoch:  41 | Step:  9 | Val Loss:  0.4752436578273773\n",
      "Epoch:  41 | Step:  10 | Val Loss:  0.6868443489074707\n",
      "Epoch:  41 | Step:  11 | Val Loss:  0.5414265394210815\n",
      "Epoch:  41 | Step:  12 | Val Loss:  0.4409503936767578\n",
      "Epoch:  41 | Step:  13 | Val Loss:  0.6103187799453735\n",
      "Epoch:  41 | Step:  14 | Val Loss:  0.6240506172180176\n",
      "Epoch:  41 | Step:  15 | Val Loss:  0.4741896986961365\n",
      "Epoch:  41 | Step:  16 | Val Loss:  0.5982469916343689\n",
      "Epoch:  41 | Step:  17 | Val Loss:  0.5841121077537537\n",
      "Epoch:  41 | Step:  18 | Val Loss:  0.5791980028152466\n",
      "Epoch:  41 | Step:  19 | Val Loss:  0.4076111316680908\n",
      "Epoch:  41 | Step:  20 | Val Loss:  0.6730257272720337\n",
      "Epoch:  41 | Step:  21 | Val Loss:  0.47607147693634033\n",
      "Epoch:  41 | Step:  22 | Val Loss:  0.521397590637207\n",
      "Epoch:  41 | Step:  23 | Val Loss:  0.5202534198760986\n",
      "Epoch:  41 | Step:  24 | Val Loss:  0.48872649669647217\n",
      "Epoch:  41 | Step:  25 | Val Loss:  0.5983046889305115\n",
      "Epoch:  41 | Step:  26 | Val Loss:  0.5021636486053467\n",
      "Epoch:  41 | Step:  27 | Val Loss:  0.5063495635986328\n",
      "Epoch:  41 | Step:  28 | Val Loss:  0.5020341277122498\n",
      "Epoch:  41 | Step:  29 | Val Loss:  0.4997667074203491\n",
      "Epoch:  41 | Step:  30 | Val Loss:  0.3754289746284485\n",
      "Epoch:  41 | Step:  31 | Val Loss:  0.4170985817909241\n",
      "Epoch:  41 | Step:  32 | Val Loss:  0.43521344661712646\n",
      "Epoch:  41 | Step:  33 | Val Loss:  0.4210597574710846\n",
      "Epoch:  41 | Step:  34 | Val Loss:  0.6132455468177795\n",
      "Epoch:  41 | Step:  35 | Val Loss:  0.45780423283576965\n",
      "Epoch:  41 | Step:  36 | Val Loss:  0.5635256767272949\n",
      "Epoch:  41 | Step:  37 | Val Loss:  0.6425392627716064\n",
      "Epoch:  41 | Step:  38 | Val Loss:  0.44455772638320923\n",
      "Epoch:  41 | Step:  39 | Val Loss:  0.6041399240493774\n",
      "Epoch:  41 | Step:  40 | Val Loss:  0.483496755361557\n",
      "Epoch:  41 | Step:  41 | Val Loss:  0.46237003803253174\n",
      "Epoch:  41 | Step:  42 | Val Loss:  0.3153189420700073\n",
      "Epoch:  41 | Step:  43 | Val Loss:  0.6079177856445312\n",
      "Epoch:  41 | Step:  44 | Val Loss:  0.4350861608982086\n",
      "Epoch:  41 | Step:  45 | Val Loss:  0.5570975542068481\n",
      "Epoch:  41 | Step:  46 | Val Loss:  0.579137921333313\n",
      "Epoch:  41 | Step:  47 | Val Loss:  0.4742424786090851\n",
      "Epoch:  41 | Step:  48 | Val Loss:  0.5378332138061523\n",
      "Epoch:  41 | Step:  49 | Val Loss:  0.4347168803215027\n",
      "Epoch:  41 | Step:  50 | Val Loss:  0.5717468857765198\n",
      "Epoch:  41 | Step:  51 | Val Loss:  0.3963732421398163\n",
      "Epoch:  41 | Step:  52 | Val Loss:  0.6603411436080933\n",
      "Epoch:  41 | Step:  53 | Val Loss:  0.6110765933990479\n",
      "Epoch:  41 | Step:  54 | Val Loss:  0.4991247355937958\n",
      "Epoch:  41 | Step:  55 | Val Loss:  0.30398106575012207\n",
      "Epoch:  41 | Step:  56 | Val Loss:  0.45526033639907837\n",
      "Epoch:  41 | Step:  57 | Val Loss:  0.4410949945449829\n",
      "Epoch:  41 | Step:  58 | Val Loss:  0.28532031178474426\n",
      "Epoch:  41 | Step:  59 | Val Loss:  0.7606428861618042\n",
      "Epoch:  41 | Step:  60 | Val Loss:  0.3697535991668701\n",
      "Epoch:  41 | Step:  61 | Val Loss:  0.6251240968704224\n",
      "Epoch:  41 | Step:  62 | Val Loss:  0.4739775061607361\n",
      "Epoch:  41 | Step:  63 | Val Loss:  0.5750082731246948\n",
      "Epoch:  41 | Step:  64 | Val Loss:  0.4258855879306793\n",
      "Epoch:  41 | Step:  65 | Val Loss:  0.5107296705245972\n",
      "Epoch:  41 | Step:  66 | Val Loss:  0.4713623523712158\n",
      "Epoch:  41 | Step:  67 | Val Loss:  0.7320774793624878\n",
      "Epoch:  41 | Step:  68 | Val Loss:  0.6721172332763672\n",
      "Epoch:  41 | Step:  69 | Val Loss:  0.5337358713150024\n",
      "Epoch:  41 | Step:  70 | Val Loss:  0.46607935428619385\n",
      "Epoch:  41 | Step:  71 | Val Loss:  0.4134443402290344\n",
      "Epoch:  41 | Step:  72 | Val Loss:  0.5099762678146362\n",
      "Epoch:  41 | Step:  73 | Val Loss:  0.6049270629882812\n",
      "Epoch:  41 | Step:  74 | Val Loss:  0.4863627552986145\n",
      "Epoch:  41 | Step:  75 | Val Loss:  0.5692989230155945\n",
      "Epoch:  41 | Step:  76 | Val Loss:  0.42018601298332214\n",
      "Epoch:  41 | Step:  77 | Val Loss:  0.5811842679977417\n",
      "Epoch:  41 | Step:  78 | Val Loss:  0.5011534690856934\n",
      "Epoch:  41 | Step:  79 | Val Loss:  0.5069687962532043\n",
      "Epoch:  41 | Step:  80 | Val Loss:  0.6197662353515625\n",
      "Epoch:  41 | Step:  81 | Val Loss:  0.5863296985626221\n",
      "Epoch:  41 | Step:  82 | Val Loss:  0.49910852313041687\n",
      "Epoch:  41 | Step:  83 | Val Loss:  0.5463401675224304\n",
      "Epoch:  41 | Step:  84 | Val Loss:  0.38046935200691223\n",
      "Epoch:  41 | Step:  85 | Val Loss:  0.5774731040000916\n",
      "Epoch:  41 | Step:  86 | Val Loss:  0.41362109780311584\n",
      "Epoch:  41 | Step:  87 | Val Loss:  0.5352701544761658\n",
      "Epoch:  41 | Step:  88 | Val Loss:  0.44283702969551086\n",
      "Epoch:  41 | Step:  89 | Val Loss:  0.5861855745315552\n",
      "Epoch:  41 | Step:  90 | Val Loss:  0.6755935549736023\n",
      "Epoch:  41 | Step:  91 | Val Loss:  0.36253488063812256\n",
      "Epoch:  41 | Step:  92 | Val Loss:  0.45686155557632446\n",
      "Epoch:  41 | Step:  93 | Val Loss:  0.5989941358566284\n",
      "Epoch:  41 | Step:  94 | Val Loss:  0.5368614196777344\n",
      "Epoch:  41 | Step:  95 | Val Loss:  0.5663859844207764\n",
      "Epoch:  41 | Step:  96 | Val Loss:  0.5221349000930786\n",
      "Epoch:  41 | Step:  97 | Val Loss:  0.5558379888534546\n",
      "Epoch:  41 | Step:  98 | Val Loss:  0.4871982932090759\n",
      "Epoch:  41 | Step:  99 | Val Loss:  0.4360677897930145\n",
      "Epoch:  41 | Step:  100 | Val Loss:  0.6875327229499817\n",
      "Epoch:  41 | Step:  101 | Val Loss:  0.5359969139099121\n",
      "Epoch:  41 | Step:  102 | Val Loss:  0.6595094799995422\n",
      "Epoch:  41 | Step:  103 | Val Loss:  0.5849915742874146\n",
      "Epoch:  41 | Step:  104 | Val Loss:  0.5294914245605469\n",
      "Epoch:  41 | Step:  105 | Val Loss:  0.6060592532157898\n",
      "Epoch:  41 | Step:  106 | Val Loss:  0.4677458107471466\n",
      "Epoch:  41 | Step:  107 | Val Loss:  0.47944119572639465\n",
      "Epoch:  41 | Step:  108 | Val Loss:  0.46002763509750366\n",
      "Epoch:  41 | Step:  109 | Val Loss:  0.5020022392272949\n",
      "Epoch:  41 | Step:  110 | Val Loss:  0.5097818970680237\n",
      "Epoch:  41 | Step:  111 | Val Loss:  0.4903441071510315\n",
      "Epoch:  41 | Step:  112 | Val Loss:  0.47492408752441406\n",
      "Epoch:  41 | Step:  113 | Val Loss:  0.49542415142059326\n",
      "Epoch:  41 | Step:  114 | Val Loss:  0.6380167603492737\n",
      "Epoch:  41 | Step:  115 | Val Loss:  0.6504772901535034\n",
      "Epoch:  41 | Step:  116 | Val Loss:  0.47842997312545776\n",
      "Epoch:  41 | Step:  117 | Val Loss:  0.46529921889305115\n",
      "Epoch:  41 | Step:  118 | Val Loss:  0.6632217168807983\n",
      "Epoch:  41 | Step:  119 | Val Loss:  0.5996617078781128\n",
      "Epoch:  41 | Step:  120 | Val Loss:  0.5293539762496948\n",
      "Epoch:  41 | Step:  121 | Val Loss:  0.5519986152648926\n",
      "Epoch:  41 | Step:  122 | Val Loss:  0.5407566428184509\n",
      "Epoch:  41 | Step:  123 | Val Loss:  0.43813011050224304\n",
      "Epoch:  41 | Step:  124 | Val Loss:  0.599895715713501\n",
      "Epoch:  41 | Step:  125 | Val Loss:  0.37497827410697937\n",
      "Epoch:  41 | Train Loss:  tensor(0.5024, device='cuda:0') | Val Loss:  tensor(0.5208, device='cuda:0')\n",
      "Epoch:  42 | Step:  500 | Train Loss:  0.5263854265213013\n",
      "Epoch:  42 | Step:  1 | Val Loss:  0.5735642313957214\n",
      "Epoch:  42 | Step:  2 | Val Loss:  0.40196511149406433\n",
      "Epoch:  42 | Step:  3 | Val Loss:  0.5242694616317749\n",
      "Epoch:  42 | Step:  4 | Val Loss:  0.6728048324584961\n",
      "Epoch:  42 | Step:  5 | Val Loss:  0.5868802070617676\n",
      "Epoch:  42 | Step:  6 | Val Loss:  0.5120247602462769\n",
      "Epoch:  42 | Step:  7 | Val Loss:  0.463914692401886\n",
      "Epoch:  42 | Step:  8 | Val Loss:  0.4872531294822693\n",
      "Epoch:  42 | Step:  9 | Val Loss:  0.44719886779785156\n",
      "Epoch:  42 | Step:  10 | Val Loss:  0.6950068473815918\n",
      "Epoch:  42 | Step:  11 | Val Loss:  0.5906227827072144\n",
      "Epoch:  42 | Step:  12 | Val Loss:  0.5719321370124817\n",
      "Epoch:  42 | Step:  13 | Val Loss:  0.43560394644737244\n",
      "Epoch:  42 | Step:  14 | Val Loss:  0.5877206325531006\n",
      "Epoch:  42 | Step:  15 | Val Loss:  0.36522915959358215\n",
      "Epoch:  42 | Step:  16 | Val Loss:  0.6035753488540649\n",
      "Epoch:  42 | Step:  17 | Val Loss:  0.4279252588748932\n",
      "Epoch:  42 | Step:  18 | Val Loss:  0.3314376473426819\n",
      "Epoch:  42 | Step:  19 | Val Loss:  0.4230339527130127\n",
      "Epoch:  42 | Step:  20 | Val Loss:  0.40061861276626587\n",
      "Epoch:  42 | Step:  21 | Val Loss:  0.4889387786388397\n",
      "Epoch:  42 | Step:  22 | Val Loss:  0.47687190771102905\n",
      "Epoch:  42 | Step:  23 | Val Loss:  0.4823392927646637\n",
      "Epoch:  42 | Step:  24 | Val Loss:  0.6121392846107483\n",
      "Epoch:  42 | Step:  25 | Val Loss:  0.5266556143760681\n",
      "Epoch:  42 | Step:  26 | Val Loss:  0.6241493225097656\n",
      "Epoch:  42 | Step:  27 | Val Loss:  0.5462963581085205\n",
      "Epoch:  42 | Step:  28 | Val Loss:  0.4910559058189392\n",
      "Epoch:  42 | Step:  29 | Val Loss:  0.4481516182422638\n",
      "Epoch:  42 | Step:  30 | Val Loss:  0.47047433257102966\n",
      "Epoch:  42 | Step:  31 | Val Loss:  0.35578179359436035\n",
      "Epoch:  42 | Step:  32 | Val Loss:  0.40694162249565125\n",
      "Epoch:  42 | Step:  33 | Val Loss:  0.3953922390937805\n",
      "Epoch:  42 | Step:  34 | Val Loss:  0.5409179925918579\n",
      "Epoch:  42 | Step:  35 | Val Loss:  0.5538089871406555\n",
      "Epoch:  42 | Step:  36 | Val Loss:  0.42795827984809875\n",
      "Epoch:  42 | Step:  37 | Val Loss:  0.5721595883369446\n",
      "Epoch:  42 | Step:  38 | Val Loss:  0.6648852825164795\n",
      "Epoch:  42 | Step:  39 | Val Loss:  0.5668031573295593\n",
      "Epoch:  42 | Step:  40 | Val Loss:  0.39814096689224243\n",
      "Epoch:  42 | Step:  41 | Val Loss:  0.48122426867485046\n",
      "Epoch:  42 | Step:  42 | Val Loss:  0.5415984392166138\n",
      "Epoch:  42 | Step:  43 | Val Loss:  0.4656307101249695\n",
      "Epoch:  42 | Step:  44 | Val Loss:  0.6408392786979675\n",
      "Epoch:  42 | Step:  45 | Val Loss:  0.39203131198883057\n",
      "Epoch:  42 | Step:  46 | Val Loss:  0.47655174136161804\n",
      "Epoch:  42 | Step:  47 | Val Loss:  0.3737790584564209\n",
      "Epoch:  42 | Step:  48 | Val Loss:  0.6670600771903992\n",
      "Epoch:  42 | Step:  49 | Val Loss:  0.6349822282791138\n",
      "Epoch:  42 | Step:  50 | Val Loss:  0.5236829519271851\n",
      "Epoch:  42 | Step:  51 | Val Loss:  0.45046907663345337\n",
      "Epoch:  42 | Step:  52 | Val Loss:  0.42976564168930054\n",
      "Epoch:  42 | Step:  53 | Val Loss:  0.5774321556091309\n",
      "Epoch:  42 | Step:  54 | Val Loss:  0.6309846043586731\n",
      "Epoch:  42 | Step:  55 | Val Loss:  0.37963759899139404\n",
      "Epoch:  42 | Step:  56 | Val Loss:  0.30213671922683716\n",
      "Epoch:  42 | Step:  57 | Val Loss:  0.5751134157180786\n",
      "Epoch:  42 | Step:  58 | Val Loss:  0.6160660982131958\n",
      "Epoch:  42 | Step:  59 | Val Loss:  0.5518962144851685\n",
      "Epoch:  42 | Step:  60 | Val Loss:  0.7139447331428528\n",
      "Epoch:  42 | Step:  61 | Val Loss:  0.5097969770431519\n",
      "Epoch:  42 | Step:  62 | Val Loss:  0.6077160239219666\n",
      "Epoch:  42 | Step:  63 | Val Loss:  0.5378249287605286\n",
      "Epoch:  42 | Step:  64 | Val Loss:  0.6043946146965027\n",
      "Epoch:  42 | Step:  65 | Val Loss:  0.54212486743927\n",
      "Epoch:  42 | Step:  66 | Val Loss:  0.5112376809120178\n",
      "Epoch:  42 | Step:  67 | Val Loss:  0.501511812210083\n",
      "Epoch:  42 | Step:  68 | Val Loss:  0.5592633485794067\n",
      "Epoch:  42 | Step:  69 | Val Loss:  0.46289288997650146\n",
      "Epoch:  42 | Step:  70 | Val Loss:  0.4641910195350647\n",
      "Epoch:  42 | Step:  71 | Val Loss:  0.6222510933876038\n",
      "Epoch:  42 | Step:  72 | Val Loss:  0.4605274796485901\n",
      "Epoch:  42 | Step:  73 | Val Loss:  0.4986417889595032\n",
      "Epoch:  42 | Step:  74 | Val Loss:  0.521251916885376\n",
      "Epoch:  42 | Step:  75 | Val Loss:  0.601563036441803\n",
      "Epoch:  42 | Step:  76 | Val Loss:  0.6764568090438843\n",
      "Epoch:  42 | Step:  77 | Val Loss:  0.5898804068565369\n",
      "Epoch:  42 | Step:  78 | Val Loss:  0.4293581247329712\n",
      "Epoch:  42 | Step:  79 | Val Loss:  0.4568009376525879\n",
      "Epoch:  42 | Step:  80 | Val Loss:  0.602335512638092\n",
      "Epoch:  42 | Step:  81 | Val Loss:  0.48413926362991333\n",
      "Epoch:  42 | Step:  82 | Val Loss:  0.34464019536972046\n",
      "Epoch:  42 | Step:  83 | Val Loss:  0.5863890051841736\n",
      "Epoch:  42 | Step:  84 | Val Loss:  0.5547820329666138\n",
      "Epoch:  42 | Step:  85 | Val Loss:  0.5142725706100464\n",
      "Epoch:  42 | Step:  86 | Val Loss:  0.3224247694015503\n",
      "Epoch:  42 | Step:  87 | Val Loss:  0.5714766383171082\n",
      "Epoch:  42 | Step:  88 | Val Loss:  0.566162109375\n",
      "Epoch:  42 | Step:  89 | Val Loss:  0.5243077278137207\n",
      "Epoch:  42 | Step:  90 | Val Loss:  0.519821047782898\n",
      "Epoch:  42 | Step:  91 | Val Loss:  0.3667801022529602\n",
      "Epoch:  42 | Step:  92 | Val Loss:  0.5551116466522217\n",
      "Epoch:  42 | Step:  93 | Val Loss:  0.5949547290802002\n",
      "Epoch:  42 | Step:  94 | Val Loss:  0.6011590957641602\n",
      "Epoch:  42 | Step:  95 | Val Loss:  0.709348738193512\n",
      "Epoch:  42 | Step:  96 | Val Loss:  0.5798357129096985\n",
      "Epoch:  42 | Step:  97 | Val Loss:  0.5627933740615845\n",
      "Epoch:  42 | Step:  98 | Val Loss:  0.5805631875991821\n",
      "Epoch:  42 | Step:  99 | Val Loss:  0.5110151171684265\n",
      "Epoch:  42 | Step:  100 | Val Loss:  0.5873919725418091\n",
      "Epoch:  42 | Step:  101 | Val Loss:  0.34840887784957886\n",
      "Epoch:  42 | Step:  102 | Val Loss:  0.6040805578231812\n",
      "Epoch:  42 | Step:  103 | Val Loss:  0.5145441293716431\n",
      "Epoch:  42 | Step:  104 | Val Loss:  0.5219339728355408\n",
      "Epoch:  42 | Step:  105 | Val Loss:  0.43026700615882874\n",
      "Epoch:  42 | Step:  106 | Val Loss:  0.6016048192977905\n",
      "Epoch:  42 | Step:  107 | Val Loss:  0.3120756149291992\n",
      "Epoch:  42 | Step:  108 | Val Loss:  0.5235100388526917\n",
      "Epoch:  42 | Step:  109 | Val Loss:  0.4332842528820038\n",
      "Epoch:  42 | Step:  110 | Val Loss:  0.47214275598526\n",
      "Epoch:  42 | Step:  111 | Val Loss:  0.5977332592010498\n",
      "Epoch:  42 | Step:  112 | Val Loss:  0.351673424243927\n",
      "Epoch:  42 | Step:  113 | Val Loss:  0.6148608922958374\n",
      "Epoch:  42 | Step:  114 | Val Loss:  0.5036760568618774\n",
      "Epoch:  42 | Step:  115 | Val Loss:  0.6791481971740723\n",
      "Epoch:  42 | Step:  116 | Val Loss:  0.4662593901157379\n",
      "Epoch:  42 | Step:  117 | Val Loss:  0.5103306174278259\n",
      "Epoch:  42 | Step:  118 | Val Loss:  0.5317031741142273\n",
      "Epoch:  42 | Step:  119 | Val Loss:  0.5153688788414001\n",
      "Epoch:  42 | Step:  120 | Val Loss:  0.7164428234100342\n",
      "Epoch:  42 | Step:  121 | Val Loss:  0.6405524015426636\n",
      "Epoch:  42 | Step:  122 | Val Loss:  0.6431787014007568\n",
      "Epoch:  42 | Step:  123 | Val Loss:  0.4844754934310913\n",
      "Epoch:  42 | Step:  124 | Val Loss:  0.4170399308204651\n",
      "Epoch:  42 | Step:  125 | Val Loss:  0.6152783632278442\n",
      "Epoch:  42 | Train Loss:  tensor(0.5020, device='cuda:0') | Val Loss:  tensor(0.5202, device='cuda:0')\n",
      "Epoch:  43 | Step:  500 | Train Loss:  0.4488573670387268\n",
      "Epoch:  43 | Step:  1 | Val Loss:  0.5260607004165649\n",
      "Epoch:  43 | Step:  2 | Val Loss:  0.6145074963569641\n",
      "Epoch:  43 | Step:  3 | Val Loss:  0.6358855962753296\n",
      "Epoch:  43 | Step:  4 | Val Loss:  0.33533960580825806\n",
      "Epoch:  43 | Step:  5 | Val Loss:  0.6775510311126709\n",
      "Epoch:  43 | Step:  6 | Val Loss:  0.4643535614013672\n",
      "Epoch:  43 | Step:  7 | Val Loss:  0.4862239360809326\n",
      "Epoch:  43 | Step:  8 | Val Loss:  0.42989280819892883\n",
      "Epoch:  43 | Step:  9 | Val Loss:  0.5319896936416626\n",
      "Epoch:  43 | Step:  10 | Val Loss:  0.6582170128822327\n",
      "Epoch:  43 | Step:  11 | Val Loss:  0.6641860008239746\n",
      "Epoch:  43 | Step:  12 | Val Loss:  0.6282436847686768\n",
      "Epoch:  43 | Step:  13 | Val Loss:  0.4789602756500244\n",
      "Epoch:  43 | Step:  14 | Val Loss:  0.532148003578186\n",
      "Epoch:  43 | Step:  15 | Val Loss:  0.475835382938385\n",
      "Epoch:  43 | Step:  16 | Val Loss:  0.6567471027374268\n",
      "Epoch:  43 | Step:  17 | Val Loss:  0.5174218416213989\n",
      "Epoch:  43 | Step:  18 | Val Loss:  0.5750523805618286\n",
      "Epoch:  43 | Step:  19 | Val Loss:  0.6763326525688171\n",
      "Epoch:  43 | Step:  20 | Val Loss:  0.41790738701820374\n",
      "Epoch:  43 | Step:  21 | Val Loss:  0.3711721897125244\n",
      "Epoch:  43 | Step:  22 | Val Loss:  0.49096256494522095\n",
      "Epoch:  43 | Step:  23 | Val Loss:  0.5697721838951111\n",
      "Epoch:  43 | Step:  24 | Val Loss:  0.44124501943588257\n",
      "Epoch:  43 | Step:  25 | Val Loss:  0.4680997133255005\n",
      "Epoch:  43 | Step:  26 | Val Loss:  0.5435434579849243\n",
      "Epoch:  43 | Step:  27 | Val Loss:  0.5754924416542053\n",
      "Epoch:  43 | Step:  28 | Val Loss:  0.5417009592056274\n",
      "Epoch:  43 | Step:  29 | Val Loss:  0.5512847900390625\n",
      "Epoch:  43 | Step:  30 | Val Loss:  0.6191482543945312\n",
      "Epoch:  43 | Step:  31 | Val Loss:  0.5323367714881897\n",
      "Epoch:  43 | Step:  32 | Val Loss:  0.45239150524139404\n",
      "Epoch:  43 | Step:  33 | Val Loss:  0.5216342210769653\n",
      "Epoch:  43 | Step:  34 | Val Loss:  0.36865362524986267\n",
      "Epoch:  43 | Step:  35 | Val Loss:  0.49291184544563293\n",
      "Epoch:  43 | Step:  36 | Val Loss:  0.5934648513793945\n",
      "Epoch:  43 | Step:  37 | Val Loss:  0.39181947708129883\n",
      "Epoch:  43 | Step:  38 | Val Loss:  0.5191925764083862\n",
      "Epoch:  43 | Step:  39 | Val Loss:  0.38720643520355225\n",
      "Epoch:  43 | Step:  40 | Val Loss:  0.5247927904129028\n",
      "Epoch:  43 | Step:  41 | Val Loss:  0.6183741092681885\n",
      "Epoch:  43 | Step:  42 | Val Loss:  0.4113942086696625\n",
      "Epoch:  43 | Step:  43 | Val Loss:  0.4911901354789734\n",
      "Epoch:  43 | Step:  44 | Val Loss:  0.5087238550186157\n",
      "Epoch:  43 | Step:  45 | Val Loss:  0.5642806887626648\n",
      "Epoch:  43 | Step:  46 | Val Loss:  0.3948451578617096\n",
      "Epoch:  43 | Step:  47 | Val Loss:  0.6717432737350464\n",
      "Epoch:  43 | Step:  48 | Val Loss:  0.5405614376068115\n",
      "Epoch:  43 | Step:  49 | Val Loss:  0.46224647760391235\n",
      "Epoch:  43 | Step:  50 | Val Loss:  0.4117172956466675\n",
      "Epoch:  43 | Step:  51 | Val Loss:  0.5083774328231812\n",
      "Epoch:  43 | Step:  52 | Val Loss:  0.6593873500823975\n",
      "Epoch:  43 | Step:  53 | Val Loss:  0.6443988084793091\n",
      "Epoch:  43 | Step:  54 | Val Loss:  0.5475524663925171\n",
      "Epoch:  43 | Step:  55 | Val Loss:  0.5057828426361084\n",
      "Epoch:  43 | Step:  56 | Val Loss:  0.5098872184753418\n",
      "Epoch:  43 | Step:  57 | Val Loss:  0.5724552273750305\n",
      "Epoch:  43 | Step:  58 | Val Loss:  0.5393800735473633\n",
      "Epoch:  43 | Step:  59 | Val Loss:  0.5628160238265991\n",
      "Epoch:  43 | Step:  60 | Val Loss:  0.40230172872543335\n",
      "Epoch:  43 | Step:  61 | Val Loss:  0.6935207843780518\n",
      "Epoch:  43 | Step:  62 | Val Loss:  0.7089820504188538\n",
      "Epoch:  43 | Step:  63 | Val Loss:  0.5189409255981445\n",
      "Epoch:  43 | Step:  64 | Val Loss:  0.4814426898956299\n",
      "Epoch:  43 | Step:  65 | Val Loss:  0.34771913290023804\n",
      "Epoch:  43 | Step:  66 | Val Loss:  0.3625943958759308\n",
      "Epoch:  43 | Step:  67 | Val Loss:  0.6067169904708862\n",
      "Epoch:  43 | Step:  68 | Val Loss:  0.43022701144218445\n",
      "Epoch:  43 | Step:  69 | Val Loss:  0.4766398072242737\n",
      "Epoch:  43 | Step:  70 | Val Loss:  0.490744948387146\n",
      "Epoch:  43 | Step:  71 | Val Loss:  0.48669105768203735\n",
      "Epoch:  43 | Step:  72 | Val Loss:  0.547275185585022\n",
      "Epoch:  43 | Step:  73 | Val Loss:  0.6669397354125977\n",
      "Epoch:  43 | Step:  74 | Val Loss:  0.7074477076530457\n",
      "Epoch:  43 | Step:  75 | Val Loss:  0.6418135166168213\n",
      "Epoch:  43 | Step:  76 | Val Loss:  0.5589874982833862\n",
      "Epoch:  43 | Step:  77 | Val Loss:  0.6420454382896423\n",
      "Epoch:  43 | Step:  78 | Val Loss:  0.3997306823730469\n",
      "Epoch:  43 | Step:  79 | Val Loss:  0.5368636846542358\n",
      "Epoch:  43 | Step:  80 | Val Loss:  0.41809457540512085\n",
      "Epoch:  43 | Step:  81 | Val Loss:  0.5152478218078613\n",
      "Epoch:  43 | Step:  82 | Val Loss:  0.41675999760627747\n",
      "Epoch:  43 | Step:  83 | Val Loss:  0.7071099281311035\n",
      "Epoch:  43 | Step:  84 | Val Loss:  0.4386865496635437\n",
      "Epoch:  43 | Step:  85 | Val Loss:  0.7443790435791016\n",
      "Epoch:  43 | Step:  86 | Val Loss:  0.5596472024917603\n",
      "Epoch:  43 | Step:  87 | Val Loss:  0.661379873752594\n",
      "Epoch:  43 | Step:  88 | Val Loss:  0.4258432984352112\n",
      "Epoch:  43 | Step:  89 | Val Loss:  0.4668082892894745\n",
      "Epoch:  43 | Step:  90 | Val Loss:  0.4782932996749878\n",
      "Epoch:  43 | Step:  91 | Val Loss:  0.5485928654670715\n",
      "Epoch:  43 | Step:  92 | Val Loss:  0.643091082572937\n",
      "Epoch:  43 | Step:  93 | Val Loss:  0.6472026109695435\n",
      "Epoch:  43 | Step:  94 | Val Loss:  0.5336949825286865\n",
      "Epoch:  43 | Step:  95 | Val Loss:  0.47373560070991516\n",
      "Epoch:  43 | Step:  96 | Val Loss:  0.49067768454551697\n",
      "Epoch:  43 | Step:  97 | Val Loss:  0.4793234169483185\n",
      "Epoch:  43 | Step:  98 | Val Loss:  0.471790611743927\n",
      "Epoch:  43 | Step:  99 | Val Loss:  0.3956668972969055\n",
      "Epoch:  43 | Step:  100 | Val Loss:  0.5362791419029236\n",
      "Epoch:  43 | Step:  101 | Val Loss:  0.4146663248538971\n",
      "Epoch:  43 | Step:  102 | Val Loss:  0.38948333263397217\n",
      "Epoch:  43 | Step:  103 | Val Loss:  0.4377289414405823\n",
      "Epoch:  43 | Step:  104 | Val Loss:  0.551845908164978\n",
      "Epoch:  43 | Step:  105 | Val Loss:  0.470498263835907\n",
      "Epoch:  43 | Step:  106 | Val Loss:  0.4666096568107605\n",
      "Epoch:  43 | Step:  107 | Val Loss:  0.3893992304801941\n",
      "Epoch:  43 | Step:  108 | Val Loss:  0.4727541506290436\n",
      "Epoch:  43 | Step:  109 | Val Loss:  0.397912859916687\n",
      "Epoch:  43 | Step:  110 | Val Loss:  0.33887842297554016\n",
      "Epoch:  43 | Step:  111 | Val Loss:  0.394533634185791\n",
      "Epoch:  43 | Step:  112 | Val Loss:  0.6089411377906799\n",
      "Epoch:  43 | Step:  113 | Val Loss:  0.4729391634464264\n",
      "Epoch:  43 | Step:  114 | Val Loss:  0.48141366243362427\n",
      "Epoch:  43 | Step:  115 | Val Loss:  0.4681459367275238\n",
      "Epoch:  43 | Step:  116 | Val Loss:  0.7162297964096069\n",
      "Epoch:  43 | Step:  117 | Val Loss:  0.6509854793548584\n",
      "Epoch:  43 | Step:  118 | Val Loss:  0.46176546812057495\n",
      "Epoch:  43 | Step:  119 | Val Loss:  0.5781750679016113\n",
      "Epoch:  43 | Step:  120 | Val Loss:  0.5165473222732544\n",
      "Epoch:  43 | Step:  121 | Val Loss:  0.4229266047477722\n",
      "Epoch:  43 | Step:  122 | Val Loss:  0.48677366971969604\n",
      "Epoch:  43 | Step:  123 | Val Loss:  0.521172046661377\n",
      "Epoch:  43 | Step:  124 | Val Loss:  0.48541343212127686\n",
      "Epoch:  43 | Step:  125 | Val Loss:  0.5271560549736023\n",
      "Epoch:  43 | Train Loss:  tensor(0.5017, device='cuda:0') | Val Loss:  tensor(0.5196, device='cuda:0')\n",
      "Epoch:  44 | Step:  500 | Train Loss:  0.37698790431022644\n",
      "Epoch:  44 | Step:  1 | Val Loss:  0.6680095195770264\n",
      "Epoch:  44 | Step:  2 | Val Loss:  0.5279294848442078\n",
      "Epoch:  44 | Step:  3 | Val Loss:  0.5869946479797363\n",
      "Epoch:  44 | Step:  4 | Val Loss:  0.5807328224182129\n",
      "Epoch:  44 | Step:  5 | Val Loss:  0.6936800479888916\n",
      "Epoch:  44 | Step:  6 | Val Loss:  0.6280898451805115\n",
      "Epoch:  44 | Step:  7 | Val Loss:  0.5828585624694824\n",
      "Epoch:  44 | Step:  8 | Val Loss:  0.5118905305862427\n",
      "Epoch:  44 | Step:  9 | Val Loss:  0.47715049982070923\n",
      "Epoch:  44 | Step:  10 | Val Loss:  0.5825242400169373\n",
      "Epoch:  44 | Step:  11 | Val Loss:  0.6407012939453125\n",
      "Epoch:  44 | Step:  12 | Val Loss:  0.3996638059616089\n",
      "Epoch:  44 | Step:  13 | Val Loss:  0.5437972545623779\n",
      "Epoch:  44 | Step:  14 | Val Loss:  0.36360523104667664\n",
      "Epoch:  44 | Step:  15 | Val Loss:  0.5352823734283447\n",
      "Epoch:  44 | Step:  16 | Val Loss:  0.51761794090271\n",
      "Epoch:  44 | Step:  17 | Val Loss:  0.48200973868370056\n",
      "Epoch:  44 | Step:  18 | Val Loss:  0.5647670030593872\n",
      "Epoch:  44 | Step:  19 | Val Loss:  0.5425183176994324\n",
      "Epoch:  44 | Step:  20 | Val Loss:  0.45283955335617065\n",
      "Epoch:  44 | Step:  21 | Val Loss:  0.5930979251861572\n",
      "Epoch:  44 | Step:  22 | Val Loss:  0.44983917474746704\n",
      "Epoch:  44 | Step:  23 | Val Loss:  0.5545589923858643\n",
      "Epoch:  44 | Step:  24 | Val Loss:  0.3488273024559021\n",
      "Epoch:  44 | Step:  25 | Val Loss:  0.5431796312332153\n",
      "Epoch:  44 | Step:  26 | Val Loss:  0.557639479637146\n",
      "Epoch:  44 | Step:  27 | Val Loss:  0.5342664122581482\n",
      "Epoch:  44 | Step:  28 | Val Loss:  0.4798452854156494\n",
      "Epoch:  44 | Step:  29 | Val Loss:  0.4977092146873474\n",
      "Epoch:  44 | Step:  30 | Val Loss:  0.4016614854335785\n",
      "Epoch:  44 | Step:  31 | Val Loss:  0.46345055103302\n",
      "Epoch:  44 | Step:  32 | Val Loss:  0.6188980340957642\n",
      "Epoch:  44 | Step:  33 | Val Loss:  0.5109776258468628\n",
      "Epoch:  44 | Step:  34 | Val Loss:  0.5994161367416382\n",
      "Epoch:  44 | Step:  35 | Val Loss:  0.37234246730804443\n",
      "Epoch:  44 | Step:  36 | Val Loss:  0.4065789580345154\n",
      "Epoch:  44 | Step:  37 | Val Loss:  0.6007678508758545\n",
      "Epoch:  44 | Step:  38 | Val Loss:  0.5078957080841064\n",
      "Epoch:  44 | Step:  39 | Val Loss:  0.6770800948143005\n",
      "Epoch:  44 | Step:  40 | Val Loss:  0.5417419075965881\n",
      "Epoch:  44 | Step:  41 | Val Loss:  0.6467366218566895\n",
      "Epoch:  44 | Step:  42 | Val Loss:  0.5272785425186157\n",
      "Epoch:  44 | Step:  43 | Val Loss:  0.3619734048843384\n",
      "Epoch:  44 | Step:  44 | Val Loss:  0.5484733581542969\n",
      "Epoch:  44 | Step:  45 | Val Loss:  0.44492673873901367\n",
      "Epoch:  44 | Step:  46 | Val Loss:  0.5985020399093628\n",
      "Epoch:  44 | Step:  47 | Val Loss:  0.35765397548675537\n",
      "Epoch:  44 | Step:  48 | Val Loss:  0.47327667474746704\n",
      "Epoch:  44 | Step:  49 | Val Loss:  0.5581409931182861\n",
      "Epoch:  44 | Step:  50 | Val Loss:  0.5437716841697693\n",
      "Epoch:  44 | Step:  51 | Val Loss:  0.6159082055091858\n",
      "Epoch:  44 | Step:  52 | Val Loss:  0.539740264415741\n",
      "Epoch:  44 | Step:  53 | Val Loss:  0.5045806169509888\n",
      "Epoch:  44 | Step:  54 | Val Loss:  0.36131560802459717\n",
      "Epoch:  44 | Step:  55 | Val Loss:  0.45720192790031433\n",
      "Epoch:  44 | Step:  56 | Val Loss:  0.4880020022392273\n",
      "Epoch:  44 | Step:  57 | Val Loss:  0.49997782707214355\n",
      "Epoch:  44 | Step:  58 | Val Loss:  0.6973574161529541\n",
      "Epoch:  44 | Step:  59 | Val Loss:  0.47592413425445557\n",
      "Epoch:  44 | Step:  60 | Val Loss:  0.4991217255592346\n",
      "Epoch:  44 | Step:  61 | Val Loss:  0.40631794929504395\n",
      "Epoch:  44 | Step:  62 | Val Loss:  0.5073710680007935\n",
      "Epoch:  44 | Step:  63 | Val Loss:  0.6002601981163025\n",
      "Epoch:  44 | Step:  64 | Val Loss:  0.4628339409828186\n",
      "Epoch:  44 | Step:  65 | Val Loss:  0.46940821409225464\n",
      "Epoch:  44 | Step:  66 | Val Loss:  0.5278347730636597\n",
      "Epoch:  44 | Step:  67 | Val Loss:  0.38187792897224426\n",
      "Epoch:  44 | Step:  68 | Val Loss:  0.421819269657135\n",
      "Epoch:  44 | Step:  69 | Val Loss:  0.4796665608882904\n",
      "Epoch:  44 | Step:  70 | Val Loss:  0.6042304039001465\n",
      "Epoch:  44 | Step:  71 | Val Loss:  0.5420587062835693\n",
      "Epoch:  44 | Step:  72 | Val Loss:  0.5456975698471069\n",
      "Epoch:  44 | Step:  73 | Val Loss:  0.5827392339706421\n",
      "Epoch:  44 | Step:  74 | Val Loss:  0.6610949039459229\n",
      "Epoch:  44 | Step:  75 | Val Loss:  0.47410911321640015\n",
      "Epoch:  44 | Step:  76 | Val Loss:  0.5708010196685791\n",
      "Epoch:  44 | Step:  77 | Val Loss:  0.49504685401916504\n",
      "Epoch:  44 | Step:  78 | Val Loss:  0.5744790434837341\n",
      "Epoch:  44 | Step:  79 | Val Loss:  0.4725416600704193\n",
      "Epoch:  44 | Step:  80 | Val Loss:  0.5753668546676636\n",
      "Epoch:  44 | Step:  81 | Val Loss:  0.6302413940429688\n",
      "Epoch:  44 | Step:  82 | Val Loss:  0.5683033466339111\n",
      "Epoch:  44 | Step:  83 | Val Loss:  0.4518200159072876\n",
      "Epoch:  44 | Step:  84 | Val Loss:  0.4975152611732483\n",
      "Epoch:  44 | Step:  85 | Val Loss:  0.6134248971939087\n",
      "Epoch:  44 | Step:  86 | Val Loss:  0.5766959190368652\n",
      "Epoch:  44 | Step:  87 | Val Loss:  0.47826874256134033\n",
      "Epoch:  44 | Step:  88 | Val Loss:  0.453267902135849\n",
      "Epoch:  44 | Step:  89 | Val Loss:  0.5001816749572754\n",
      "Epoch:  44 | Step:  90 | Val Loss:  0.5341775417327881\n",
      "Epoch:  44 | Step:  91 | Val Loss:  0.49542108178138733\n",
      "Epoch:  44 | Step:  92 | Val Loss:  0.5617206692695618\n",
      "Epoch:  44 | Step:  93 | Val Loss:  0.48509883880615234\n",
      "Epoch:  44 | Step:  94 | Val Loss:  0.533374547958374\n",
      "Epoch:  44 | Step:  95 | Val Loss:  0.5679153203964233\n",
      "Epoch:  44 | Step:  96 | Val Loss:  0.6037884950637817\n",
      "Epoch:  44 | Step:  97 | Val Loss:  0.6363082528114319\n",
      "Epoch:  44 | Step:  98 | Val Loss:  0.3700655996799469\n",
      "Epoch:  44 | Step:  99 | Val Loss:  0.3187701106071472\n",
      "Epoch:  44 | Step:  100 | Val Loss:  0.4299381375312805\n",
      "Epoch:  44 | Step:  101 | Val Loss:  0.5199256539344788\n",
      "Epoch:  44 | Step:  102 | Val Loss:  0.5497533082962036\n",
      "Epoch:  44 | Step:  103 | Val Loss:  0.5187085866928101\n",
      "Epoch:  44 | Step:  104 | Val Loss:  0.5906184911727905\n",
      "Epoch:  44 | Step:  105 | Val Loss:  0.43569216132164\n",
      "Epoch:  44 | Step:  106 | Val Loss:  0.5093148946762085\n",
      "Epoch:  44 | Step:  107 | Val Loss:  0.5200509428977966\n",
      "Epoch:  44 | Step:  108 | Val Loss:  0.478434681892395\n",
      "Epoch:  44 | Step:  109 | Val Loss:  0.5091487169265747\n",
      "Epoch:  44 | Step:  110 | Val Loss:  0.5421006083488464\n",
      "Epoch:  44 | Step:  111 | Val Loss:  0.546180248260498\n",
      "Epoch:  44 | Step:  112 | Val Loss:  0.41433754563331604\n",
      "Epoch:  44 | Step:  113 | Val Loss:  0.4462052583694458\n",
      "Epoch:  44 | Step:  114 | Val Loss:  0.54289710521698\n",
      "Epoch:  44 | Step:  115 | Val Loss:  0.439588725566864\n",
      "Epoch:  44 | Step:  116 | Val Loss:  0.5571510791778564\n",
      "Epoch:  44 | Step:  117 | Val Loss:  0.5711613893508911\n",
      "Epoch:  44 | Step:  118 | Val Loss:  0.3854185938835144\n",
      "Epoch:  44 | Step:  119 | Val Loss:  0.4554242193698883\n",
      "Epoch:  44 | Step:  120 | Val Loss:  0.49659645557403564\n",
      "Epoch:  44 | Step:  121 | Val Loss:  0.5442405939102173\n",
      "Epoch:  44 | Step:  122 | Val Loss:  0.6056567430496216\n",
      "Epoch:  44 | Step:  123 | Val Loss:  0.591285765171051\n",
      "Epoch:  44 | Step:  124 | Val Loss:  0.5280380845069885\n",
      "Epoch:  44 | Step:  125 | Val Loss:  0.5812336802482605\n",
      "Epoch:  44 | Train Loss:  tensor(0.5014, device='cuda:0') | Val Loss:  tensor(0.5191, device='cuda:0')\n",
      "Epoch:  45 | Step:  500 | Train Loss:  0.5805153846740723\n",
      "Epoch:  45 | Step:  1 | Val Loss:  0.47041699290275574\n",
      "Epoch:  45 | Step:  2 | Val Loss:  0.6388572454452515\n",
      "Epoch:  45 | Step:  3 | Val Loss:  0.5611109733581543\n",
      "Epoch:  45 | Step:  4 | Val Loss:  0.47078296542167664\n",
      "Epoch:  45 | Step:  5 | Val Loss:  0.551409125328064\n",
      "Epoch:  45 | Step:  6 | Val Loss:  0.5131512880325317\n",
      "Epoch:  45 | Step:  7 | Val Loss:  0.44182711839675903\n",
      "Epoch:  45 | Step:  8 | Val Loss:  0.7756393551826477\n",
      "Epoch:  45 | Step:  9 | Val Loss:  0.5336796641349792\n",
      "Epoch:  45 | Step:  10 | Val Loss:  0.41710278391838074\n",
      "Epoch:  45 | Step:  11 | Val Loss:  0.6562218070030212\n",
      "Epoch:  45 | Step:  12 | Val Loss:  0.5344578623771667\n",
      "Epoch:  45 | Step:  13 | Val Loss:  0.5485060811042786\n",
      "Epoch:  45 | Step:  14 | Val Loss:  0.4529334306716919\n",
      "Epoch:  45 | Step:  15 | Val Loss:  0.5258464813232422\n",
      "Epoch:  45 | Step:  16 | Val Loss:  0.6122899055480957\n",
      "Epoch:  45 | Step:  17 | Val Loss:  0.49638959765434265\n",
      "Epoch:  45 | Step:  18 | Val Loss:  0.6206830739974976\n",
      "Epoch:  45 | Step:  19 | Val Loss:  0.4524899125099182\n",
      "Epoch:  45 | Step:  20 | Val Loss:  0.5506927967071533\n",
      "Epoch:  45 | Step:  21 | Val Loss:  0.5053597688674927\n",
      "Epoch:  45 | Step:  22 | Val Loss:  0.4647715389728546\n",
      "Epoch:  45 | Step:  23 | Val Loss:  0.43216970562934875\n",
      "Epoch:  45 | Step:  24 | Val Loss:  0.5587761402130127\n",
      "Epoch:  45 | Step:  25 | Val Loss:  0.5576026439666748\n",
      "Epoch:  45 | Step:  26 | Val Loss:  0.45786339044570923\n",
      "Epoch:  45 | Step:  27 | Val Loss:  0.5655396580696106\n",
      "Epoch:  45 | Step:  28 | Val Loss:  0.4516446590423584\n",
      "Epoch:  45 | Step:  29 | Val Loss:  0.580894947052002\n",
      "Epoch:  45 | Step:  30 | Val Loss:  0.39302653074264526\n",
      "Epoch:  45 | Step:  31 | Val Loss:  0.6605792045593262\n",
      "Epoch:  45 | Step:  32 | Val Loss:  0.5143958926200867\n",
      "Epoch:  45 | Step:  33 | Val Loss:  0.661827802658081\n",
      "Epoch:  45 | Step:  34 | Val Loss:  0.46518754959106445\n",
      "Epoch:  45 | Step:  35 | Val Loss:  0.6357760429382324\n",
      "Epoch:  45 | Step:  36 | Val Loss:  0.5705137848854065\n",
      "Epoch:  45 | Step:  37 | Val Loss:  0.34456866979599\n",
      "Epoch:  45 | Step:  38 | Val Loss:  0.4323772192001343\n",
      "Epoch:  45 | Step:  39 | Val Loss:  0.38024452328681946\n",
      "Epoch:  45 | Step:  40 | Val Loss:  0.4655100703239441\n",
      "Epoch:  45 | Step:  41 | Val Loss:  0.581493616104126\n",
      "Epoch:  45 | Step:  42 | Val Loss:  0.4970477223396301\n",
      "Epoch:  45 | Step:  43 | Val Loss:  0.43101051449775696\n",
      "Epoch:  45 | Step:  44 | Val Loss:  0.5048887133598328\n",
      "Epoch:  45 | Step:  45 | Val Loss:  0.503493070602417\n",
      "Epoch:  45 | Step:  46 | Val Loss:  0.5076329112052917\n",
      "Epoch:  45 | Step:  47 | Val Loss:  0.4595337510108948\n",
      "Epoch:  45 | Step:  48 | Val Loss:  0.5802324414253235\n",
      "Epoch:  45 | Step:  49 | Val Loss:  0.5456080436706543\n",
      "Epoch:  45 | Step:  50 | Val Loss:  0.35349738597869873\n",
      "Epoch:  45 | Step:  51 | Val Loss:  0.45609337091445923\n",
      "Epoch:  45 | Step:  52 | Val Loss:  0.5137647390365601\n",
      "Epoch:  45 | Step:  53 | Val Loss:  0.49138343334198\n",
      "Epoch:  45 | Step:  54 | Val Loss:  0.4430886507034302\n",
      "Epoch:  45 | Step:  55 | Val Loss:  0.5520011186599731\n",
      "Epoch:  45 | Step:  56 | Val Loss:  0.5744757056236267\n",
      "Epoch:  45 | Step:  57 | Val Loss:  0.5041875839233398\n",
      "Epoch:  45 | Step:  58 | Val Loss:  0.4756203293800354\n",
      "Epoch:  45 | Step:  59 | Val Loss:  0.4395935535430908\n",
      "Epoch:  45 | Step:  60 | Val Loss:  0.48958730697631836\n",
      "Epoch:  45 | Step:  61 | Val Loss:  0.589869499206543\n",
      "Epoch:  45 | Step:  62 | Val Loss:  0.5444898009300232\n",
      "Epoch:  45 | Step:  63 | Val Loss:  0.49518346786499023\n",
      "Epoch:  45 | Step:  64 | Val Loss:  0.4759921431541443\n",
      "Epoch:  45 | Step:  65 | Val Loss:  0.6091898679733276\n",
      "Epoch:  45 | Step:  66 | Val Loss:  0.5960344076156616\n",
      "Epoch:  45 | Step:  67 | Val Loss:  0.37911635637283325\n",
      "Epoch:  45 | Step:  68 | Val Loss:  0.5359511971473694\n",
      "Epoch:  45 | Step:  69 | Val Loss:  0.5666508674621582\n",
      "Epoch:  45 | Step:  70 | Val Loss:  0.654059648513794\n",
      "Epoch:  45 | Step:  71 | Val Loss:  0.5323137640953064\n",
      "Epoch:  45 | Step:  72 | Val Loss:  0.5795156955718994\n",
      "Epoch:  45 | Step:  73 | Val Loss:  0.4989509582519531\n",
      "Epoch:  45 | Step:  74 | Val Loss:  0.39408010244369507\n",
      "Epoch:  45 | Step:  75 | Val Loss:  0.5304700136184692\n",
      "Epoch:  45 | Step:  76 | Val Loss:  0.48200154304504395\n",
      "Epoch:  45 | Step:  77 | Val Loss:  0.5448004007339478\n",
      "Epoch:  45 | Step:  78 | Val Loss:  0.5820456743240356\n",
      "Epoch:  45 | Step:  79 | Val Loss:  0.3332395851612091\n",
      "Epoch:  45 | Step:  80 | Val Loss:  0.6767279505729675\n",
      "Epoch:  45 | Step:  81 | Val Loss:  0.39851194620132446\n",
      "Epoch:  45 | Step:  82 | Val Loss:  0.26458728313446045\n",
      "Epoch:  45 | Step:  83 | Val Loss:  0.5145351886749268\n",
      "Epoch:  45 | Step:  84 | Val Loss:  0.516046404838562\n",
      "Epoch:  45 | Step:  85 | Val Loss:  0.5159243941307068\n",
      "Epoch:  45 | Step:  86 | Val Loss:  0.6130439043045044\n",
      "Epoch:  45 | Step:  87 | Val Loss:  0.558547854423523\n",
      "Epoch:  45 | Step:  88 | Val Loss:  0.35024863481521606\n",
      "Epoch:  45 | Step:  89 | Val Loss:  0.43730783462524414\n",
      "Epoch:  45 | Step:  90 | Val Loss:  0.582683265209198\n",
      "Epoch:  45 | Step:  91 | Val Loss:  0.5231465697288513\n",
      "Epoch:  45 | Step:  92 | Val Loss:  0.6259545087814331\n",
      "Epoch:  45 | Step:  93 | Val Loss:  0.7111709117889404\n",
      "Epoch:  45 | Step:  94 | Val Loss:  0.41602396965026855\n",
      "Epoch:  45 | Step:  95 | Val Loss:  0.5536067485809326\n",
      "Epoch:  45 | Step:  96 | Val Loss:  0.5353509187698364\n",
      "Epoch:  45 | Step:  97 | Val Loss:  0.6446488499641418\n",
      "Epoch:  45 | Step:  98 | Val Loss:  0.35300540924072266\n",
      "Epoch:  45 | Step:  99 | Val Loss:  0.6838533878326416\n",
      "Epoch:  45 | Step:  100 | Val Loss:  0.47161731123924255\n",
      "Epoch:  45 | Step:  101 | Val Loss:  0.489337682723999\n",
      "Epoch:  45 | Step:  102 | Val Loss:  0.7069687843322754\n",
      "Epoch:  45 | Step:  103 | Val Loss:  0.5826979875564575\n",
      "Epoch:  45 | Step:  104 | Val Loss:  0.6623519659042358\n",
      "Epoch:  45 | Step:  105 | Val Loss:  0.5877885818481445\n",
      "Epoch:  45 | Step:  106 | Val Loss:  0.5176637172698975\n",
      "Epoch:  45 | Step:  107 | Val Loss:  0.4550436735153198\n",
      "Epoch:  45 | Step:  108 | Val Loss:  0.5161834955215454\n",
      "Epoch:  45 | Step:  109 | Val Loss:  0.5322504043579102\n",
      "Epoch:  45 | Step:  110 | Val Loss:  0.6126896739006042\n",
      "Epoch:  45 | Step:  111 | Val Loss:  0.40904414653778076\n",
      "Epoch:  45 | Step:  112 | Val Loss:  0.49289247393608093\n",
      "Epoch:  45 | Step:  113 | Val Loss:  0.4201067090034485\n",
      "Epoch:  45 | Step:  114 | Val Loss:  0.42397981882095337\n",
      "Epoch:  45 | Step:  115 | Val Loss:  0.593340277671814\n",
      "Epoch:  45 | Step:  116 | Val Loss:  0.5627400279045105\n",
      "Epoch:  45 | Step:  117 | Val Loss:  0.5025709867477417\n",
      "Epoch:  45 | Step:  118 | Val Loss:  0.5993959903717041\n",
      "Epoch:  45 | Step:  119 | Val Loss:  0.6184461116790771\n",
      "Epoch:  45 | Step:  120 | Val Loss:  0.4249541461467743\n",
      "Epoch:  45 | Step:  121 | Val Loss:  0.6297411918640137\n",
      "Epoch:  45 | Step:  122 | Val Loss:  0.5329224467277527\n",
      "Epoch:  45 | Step:  123 | Val Loss:  0.4475797414779663\n",
      "Epoch:  45 | Step:  124 | Val Loss:  0.42690515518188477\n",
      "Epoch:  45 | Step:  125 | Val Loss:  0.39407485723495483\n",
      "Epoch:  45 | Train Loss:  tensor(0.5012, device='cuda:0') | Val Loss:  tensor(0.5187, device='cuda:0')\n",
      "Epoch:  46 | Step:  500 | Train Loss:  0.4575180411338806\n",
      "Epoch:  46 | Step:  1 | Val Loss:  0.5427954792976379\n",
      "Epoch:  46 | Step:  2 | Val Loss:  0.47001904249191284\n",
      "Epoch:  46 | Step:  3 | Val Loss:  0.36884796619415283\n",
      "Epoch:  46 | Step:  4 | Val Loss:  0.4685367941856384\n",
      "Epoch:  46 | Step:  5 | Val Loss:  0.4403024911880493\n",
      "Epoch:  46 | Step:  6 | Val Loss:  0.5538944602012634\n",
      "Epoch:  46 | Step:  7 | Val Loss:  0.5302514433860779\n",
      "Epoch:  46 | Step:  8 | Val Loss:  0.4537211060523987\n",
      "Epoch:  46 | Step:  9 | Val Loss:  0.5069276094436646\n",
      "Epoch:  46 | Step:  10 | Val Loss:  0.7156712412834167\n",
      "Epoch:  46 | Step:  11 | Val Loss:  0.6523478031158447\n",
      "Epoch:  46 | Step:  12 | Val Loss:  0.5823346376419067\n",
      "Epoch:  46 | Step:  13 | Val Loss:  0.515551745891571\n",
      "Epoch:  46 | Step:  14 | Val Loss:  0.39572954177856445\n",
      "Epoch:  46 | Step:  15 | Val Loss:  0.5528883337974548\n",
      "Epoch:  46 | Step:  16 | Val Loss:  0.4966435432434082\n",
      "Epoch:  46 | Step:  17 | Val Loss:  0.8148206472396851\n",
      "Epoch:  46 | Step:  18 | Val Loss:  0.4874280095100403\n",
      "Epoch:  46 | Step:  19 | Val Loss:  0.5934654474258423\n",
      "Epoch:  46 | Step:  20 | Val Loss:  0.33326455950737\n",
      "Epoch:  46 | Step:  21 | Val Loss:  0.4183211922645569\n",
      "Epoch:  46 | Step:  22 | Val Loss:  0.41133397817611694\n",
      "Epoch:  46 | Step:  23 | Val Loss:  0.6219639778137207\n",
      "Epoch:  46 | Step:  24 | Val Loss:  0.4705232083797455\n",
      "Epoch:  46 | Step:  25 | Val Loss:  0.5417417287826538\n",
      "Epoch:  46 | Step:  26 | Val Loss:  0.6611725091934204\n",
      "Epoch:  46 | Step:  27 | Val Loss:  0.46605515480041504\n",
      "Epoch:  46 | Step:  28 | Val Loss:  0.4760938286781311\n",
      "Epoch:  46 | Step:  29 | Val Loss:  0.5544131994247437\n",
      "Epoch:  46 | Step:  30 | Val Loss:  0.4289819896221161\n",
      "Epoch:  46 | Step:  31 | Val Loss:  0.4918360114097595\n",
      "Epoch:  46 | Step:  32 | Val Loss:  0.4993343949317932\n",
      "Epoch:  46 | Step:  33 | Val Loss:  0.5777421593666077\n",
      "Epoch:  46 | Step:  34 | Val Loss:  0.6390697360038757\n",
      "Epoch:  46 | Step:  35 | Val Loss:  0.5661879777908325\n",
      "Epoch:  46 | Step:  36 | Val Loss:  0.5502455234527588\n",
      "Epoch:  46 | Step:  37 | Val Loss:  0.4209098517894745\n",
      "Epoch:  46 | Step:  38 | Val Loss:  0.47278401255607605\n",
      "Epoch:  46 | Step:  39 | Val Loss:  0.5263817310333252\n",
      "Epoch:  46 | Step:  40 | Val Loss:  0.49149614572525024\n",
      "Epoch:  46 | Step:  41 | Val Loss:  0.6237910985946655\n",
      "Epoch:  46 | Step:  42 | Val Loss:  0.5490748882293701\n",
      "Epoch:  46 | Step:  43 | Val Loss:  0.4398253560066223\n",
      "Epoch:  46 | Step:  44 | Val Loss:  0.5675196051597595\n",
      "Epoch:  46 | Step:  45 | Val Loss:  0.5224612355232239\n",
      "Epoch:  46 | Step:  46 | Val Loss:  0.6966598033905029\n",
      "Epoch:  46 | Step:  47 | Val Loss:  0.38044893741607666\n",
      "Epoch:  46 | Step:  48 | Val Loss:  0.663277268409729\n",
      "Epoch:  46 | Step:  49 | Val Loss:  0.5393921136856079\n",
      "Epoch:  46 | Step:  50 | Val Loss:  0.5170729160308838\n",
      "Epoch:  46 | Step:  51 | Val Loss:  0.37125808000564575\n",
      "Epoch:  46 | Step:  52 | Val Loss:  0.546582818031311\n",
      "Epoch:  46 | Step:  53 | Val Loss:  0.541204035282135\n",
      "Epoch:  46 | Step:  54 | Val Loss:  0.5678342580795288\n",
      "Epoch:  46 | Step:  55 | Val Loss:  0.5012573599815369\n",
      "Epoch:  46 | Step:  56 | Val Loss:  0.6900972127914429\n",
      "Epoch:  46 | Step:  57 | Val Loss:  0.5974951982498169\n",
      "Epoch:  46 | Step:  58 | Val Loss:  0.39112526178359985\n",
      "Epoch:  46 | Step:  59 | Val Loss:  0.5354502201080322\n",
      "Epoch:  46 | Step:  60 | Val Loss:  0.5057345628738403\n",
      "Epoch:  46 | Step:  61 | Val Loss:  0.45523518323898315\n",
      "Epoch:  46 | Step:  62 | Val Loss:  0.4532696306705475\n",
      "Epoch:  46 | Step:  63 | Val Loss:  0.4435248374938965\n",
      "Epoch:  46 | Step:  64 | Val Loss:  0.46793392300605774\n",
      "Epoch:  46 | Step:  65 | Val Loss:  0.45181500911712646\n",
      "Epoch:  46 | Step:  66 | Val Loss:  0.542082667350769\n",
      "Epoch:  46 | Step:  67 | Val Loss:  0.39750856161117554\n",
      "Epoch:  46 | Step:  68 | Val Loss:  0.6066222786903381\n",
      "Epoch:  46 | Step:  69 | Val Loss:  0.513195276260376\n",
      "Epoch:  46 | Step:  70 | Val Loss:  0.45038479566574097\n",
      "Epoch:  46 | Step:  71 | Val Loss:  0.5655367374420166\n",
      "Epoch:  46 | Step:  72 | Val Loss:  0.6205172538757324\n",
      "Epoch:  46 | Step:  73 | Val Loss:  0.6424217224121094\n",
      "Epoch:  46 | Step:  74 | Val Loss:  0.48683828115463257\n",
      "Epoch:  46 | Step:  75 | Val Loss:  0.430492639541626\n",
      "Epoch:  46 | Step:  76 | Val Loss:  0.4218558371067047\n",
      "Epoch:  46 | Step:  77 | Val Loss:  0.4875835180282593\n",
      "Epoch:  46 | Step:  78 | Val Loss:  0.36361560225486755\n",
      "Epoch:  46 | Step:  79 | Val Loss:  0.4352011978626251\n",
      "Epoch:  46 | Step:  80 | Val Loss:  0.451755166053772\n",
      "Epoch:  46 | Step:  81 | Val Loss:  0.3986409604549408\n",
      "Epoch:  46 | Step:  82 | Val Loss:  0.37053078413009644\n",
      "Epoch:  46 | Step:  83 | Val Loss:  0.5628176331520081\n",
      "Epoch:  46 | Step:  84 | Val Loss:  0.48527050018310547\n",
      "Epoch:  46 | Step:  85 | Val Loss:  0.38692033290863037\n",
      "Epoch:  46 | Step:  86 | Val Loss:  0.6020479202270508\n",
      "Epoch:  46 | Step:  87 | Val Loss:  0.6289100050926208\n",
      "Epoch:  46 | Step:  88 | Val Loss:  0.4530891180038452\n",
      "Epoch:  46 | Step:  89 | Val Loss:  0.5373109579086304\n",
      "Epoch:  46 | Step:  90 | Val Loss:  0.4278734624385834\n",
      "Epoch:  46 | Step:  91 | Val Loss:  0.491752564907074\n",
      "Epoch:  46 | Step:  92 | Val Loss:  0.4220980405807495\n",
      "Epoch:  46 | Step:  93 | Val Loss:  0.571375846862793\n",
      "Epoch:  46 | Step:  94 | Val Loss:  0.5689082145690918\n",
      "Epoch:  46 | Step:  95 | Val Loss:  0.5952757596969604\n",
      "Epoch:  46 | Step:  96 | Val Loss:  0.5916091203689575\n",
      "Epoch:  46 | Step:  97 | Val Loss:  0.4702194929122925\n",
      "Epoch:  46 | Step:  98 | Val Loss:  0.6581884622573853\n",
      "Epoch:  46 | Step:  99 | Val Loss:  0.6511930227279663\n",
      "Epoch:  46 | Step:  100 | Val Loss:  0.5081746578216553\n",
      "Epoch:  46 | Step:  101 | Val Loss:  0.6045205593109131\n",
      "Epoch:  46 | Step:  102 | Val Loss:  0.6070927977561951\n",
      "Epoch:  46 | Step:  103 | Val Loss:  0.504436731338501\n",
      "Epoch:  46 | Step:  104 | Val Loss:  0.514888346195221\n",
      "Epoch:  46 | Step:  105 | Val Loss:  0.5665550231933594\n",
      "Epoch:  46 | Step:  106 | Val Loss:  0.4863472580909729\n",
      "Epoch:  46 | Step:  107 | Val Loss:  0.6409550905227661\n",
      "Epoch:  46 | Step:  108 | Val Loss:  0.3686135411262512\n",
      "Epoch:  46 | Step:  109 | Val Loss:  0.496715784072876\n",
      "Epoch:  46 | Step:  110 | Val Loss:  0.44744735956192017\n",
      "Epoch:  46 | Step:  111 | Val Loss:  0.4581229090690613\n",
      "Epoch:  46 | Step:  112 | Val Loss:  0.47341299057006836\n",
      "Epoch:  46 | Step:  113 | Val Loss:  0.7418103218078613\n",
      "Epoch:  46 | Step:  114 | Val Loss:  0.6809191703796387\n",
      "Epoch:  46 | Step:  115 | Val Loss:  0.4664120674133301\n",
      "Epoch:  46 | Step:  116 | Val Loss:  0.6007608771324158\n",
      "Epoch:  46 | Step:  117 | Val Loss:  0.369232177734375\n",
      "Epoch:  46 | Step:  118 | Val Loss:  0.5820493698120117\n",
      "Epoch:  46 | Step:  119 | Val Loss:  0.6682134866714478\n",
      "Epoch:  46 | Step:  120 | Val Loss:  0.43979784846305847\n",
      "Epoch:  46 | Step:  121 | Val Loss:  0.45075124502182007\n",
      "Epoch:  46 | Step:  122 | Val Loss:  0.5184907913208008\n",
      "Epoch:  46 | Step:  123 | Val Loss:  0.4592881202697754\n",
      "Epoch:  46 | Step:  124 | Val Loss:  0.5901076197624207\n",
      "Epoch:  46 | Step:  125 | Val Loss:  0.461391419172287\n",
      "Epoch:  46 | Train Loss:  tensor(0.5011, device='cuda:0') | Val Loss:  tensor(0.5182, device='cuda:0')\n",
      "Epoch:  47 | Step:  500 | Train Loss:  0.4294472932815552\n",
      "Epoch:  47 | Step:  1 | Val Loss:  0.4270431697368622\n",
      "Epoch:  47 | Step:  2 | Val Loss:  0.5217785239219666\n",
      "Epoch:  47 | Step:  3 | Val Loss:  0.6050513982772827\n",
      "Epoch:  47 | Step:  4 | Val Loss:  0.5590369701385498\n",
      "Epoch:  47 | Step:  5 | Val Loss:  0.4106396436691284\n",
      "Epoch:  47 | Step:  6 | Val Loss:  0.5086343884468079\n",
      "Epoch:  47 | Step:  7 | Val Loss:  0.4182259440422058\n",
      "Epoch:  47 | Step:  8 | Val Loss:  0.36837145686149597\n",
      "Epoch:  47 | Step:  9 | Val Loss:  0.5137206315994263\n",
      "Epoch:  47 | Step:  10 | Val Loss:  0.49916261434555054\n",
      "Epoch:  47 | Step:  11 | Val Loss:  0.468877911567688\n",
      "Epoch:  47 | Step:  12 | Val Loss:  0.5902997255325317\n",
      "Epoch:  47 | Step:  13 | Val Loss:  0.46822330355644226\n",
      "Epoch:  47 | Step:  14 | Val Loss:  0.5160599946975708\n",
      "Epoch:  47 | Step:  15 | Val Loss:  0.5801256895065308\n",
      "Epoch:  47 | Step:  16 | Val Loss:  0.5225752592086792\n",
      "Epoch:  47 | Step:  17 | Val Loss:  0.468681275844574\n",
      "Epoch:  47 | Step:  18 | Val Loss:  0.580691397190094\n",
      "Epoch:  47 | Step:  19 | Val Loss:  0.4476417899131775\n",
      "Epoch:  47 | Step:  20 | Val Loss:  0.45132923126220703\n",
      "Epoch:  47 | Step:  21 | Val Loss:  0.4565543830394745\n",
      "Epoch:  47 | Step:  22 | Val Loss:  0.5048702359199524\n",
      "Epoch:  47 | Step:  23 | Val Loss:  0.3635537028312683\n",
      "Epoch:  47 | Step:  24 | Val Loss:  0.7108034491539001\n",
      "Epoch:  47 | Step:  25 | Val Loss:  0.3879578709602356\n",
      "Epoch:  47 | Step:  26 | Val Loss:  0.48464107513427734\n",
      "Epoch:  47 | Step:  27 | Val Loss:  0.40647411346435547\n",
      "Epoch:  47 | Step:  28 | Val Loss:  0.43524324893951416\n",
      "Epoch:  47 | Step:  29 | Val Loss:  0.5674391984939575\n",
      "Epoch:  47 | Step:  30 | Val Loss:  0.5541977882385254\n",
      "Epoch:  47 | Step:  31 | Val Loss:  0.5283057689666748\n",
      "Epoch:  47 | Step:  32 | Val Loss:  0.6890133023262024\n",
      "Epoch:  47 | Step:  33 | Val Loss:  0.4942516088485718\n",
      "Epoch:  47 | Step:  34 | Val Loss:  0.4812639653682709\n",
      "Epoch:  47 | Step:  35 | Val Loss:  0.3431912660598755\n",
      "Epoch:  47 | Step:  36 | Val Loss:  0.5530619621276855\n",
      "Epoch:  47 | Step:  37 | Val Loss:  0.5396506190299988\n",
      "Epoch:  47 | Step:  38 | Val Loss:  0.3441162705421448\n",
      "Epoch:  47 | Step:  39 | Val Loss:  0.4427223801612854\n",
      "Epoch:  47 | Step:  40 | Val Loss:  0.537226140499115\n",
      "Epoch:  47 | Step:  41 | Val Loss:  0.4472649097442627\n",
      "Epoch:  47 | Step:  42 | Val Loss:  0.5821709632873535\n",
      "Epoch:  47 | Step:  43 | Val Loss:  0.5685721635818481\n",
      "Epoch:  47 | Step:  44 | Val Loss:  0.6019263863563538\n",
      "Epoch:  47 | Step:  45 | Val Loss:  0.6019198894500732\n",
      "Epoch:  47 | Step:  46 | Val Loss:  0.618396520614624\n",
      "Epoch:  47 | Step:  47 | Val Loss:  0.6175310611724854\n",
      "Epoch:  47 | Step:  48 | Val Loss:  0.5659447908401489\n",
      "Epoch:  47 | Step:  49 | Val Loss:  0.6976981163024902\n",
      "Epoch:  47 | Step:  50 | Val Loss:  0.5882105827331543\n",
      "Epoch:  47 | Step:  51 | Val Loss:  0.49401599168777466\n",
      "Epoch:  47 | Step:  52 | Val Loss:  0.503345787525177\n",
      "Epoch:  47 | Step:  53 | Val Loss:  0.3082472085952759\n",
      "Epoch:  47 | Step:  54 | Val Loss:  0.6127005815505981\n",
      "Epoch:  47 | Step:  55 | Val Loss:  0.602594792842865\n",
      "Epoch:  47 | Step:  56 | Val Loss:  0.6415693163871765\n",
      "Epoch:  47 | Step:  57 | Val Loss:  0.6184382438659668\n",
      "Epoch:  47 | Step:  58 | Val Loss:  0.5362975597381592\n",
      "Epoch:  47 | Step:  59 | Val Loss:  0.4382520914077759\n",
      "Epoch:  47 | Step:  60 | Val Loss:  0.7458414435386658\n",
      "Epoch:  47 | Step:  61 | Val Loss:  0.6663894057273865\n",
      "Epoch:  47 | Step:  62 | Val Loss:  0.4124879539012909\n",
      "Epoch:  47 | Step:  63 | Val Loss:  0.467696875333786\n",
      "Epoch:  47 | Step:  64 | Val Loss:  0.41240495443344116\n",
      "Epoch:  47 | Step:  65 | Val Loss:  0.47009190917015076\n",
      "Epoch:  47 | Step:  66 | Val Loss:  0.5663813352584839\n",
      "Epoch:  47 | Step:  67 | Val Loss:  0.4527797996997833\n",
      "Epoch:  47 | Step:  68 | Val Loss:  0.5000402331352234\n",
      "Epoch:  47 | Step:  69 | Val Loss:  0.42403414845466614\n",
      "Epoch:  47 | Step:  70 | Val Loss:  0.4800439774990082\n",
      "Epoch:  47 | Step:  71 | Val Loss:  0.6292476654052734\n",
      "Epoch:  47 | Step:  72 | Val Loss:  0.6406528949737549\n",
      "Epoch:  47 | Step:  73 | Val Loss:  0.544478178024292\n",
      "Epoch:  47 | Step:  74 | Val Loss:  0.4224507808685303\n",
      "Epoch:  47 | Step:  75 | Val Loss:  0.5534572005271912\n",
      "Epoch:  47 | Step:  76 | Val Loss:  0.6330113410949707\n",
      "Epoch:  47 | Step:  77 | Val Loss:  0.47659361362457275\n",
      "Epoch:  47 | Step:  78 | Val Loss:  0.4359129071235657\n",
      "Epoch:  47 | Step:  79 | Val Loss:  0.5518447160720825\n",
      "Epoch:  47 | Step:  80 | Val Loss:  0.5562108755111694\n",
      "Epoch:  47 | Step:  81 | Val Loss:  0.44968676567077637\n",
      "Epoch:  47 | Step:  82 | Val Loss:  0.44220441579818726\n",
      "Epoch:  47 | Step:  83 | Val Loss:  0.4054363965988159\n",
      "Epoch:  47 | Step:  84 | Val Loss:  0.5230552554130554\n",
      "Epoch:  47 | Step:  85 | Val Loss:  0.6624898314476013\n",
      "Epoch:  47 | Step:  86 | Val Loss:  0.6353671550750732\n",
      "Epoch:  47 | Step:  87 | Val Loss:  0.40825578570365906\n",
      "Epoch:  47 | Step:  88 | Val Loss:  0.6359381675720215\n",
      "Epoch:  47 | Step:  89 | Val Loss:  0.5224888324737549\n",
      "Epoch:  47 | Step:  90 | Val Loss:  0.5901108980178833\n",
      "Epoch:  47 | Step:  91 | Val Loss:  0.6039938926696777\n",
      "Epoch:  47 | Step:  92 | Val Loss:  0.4404065012931824\n",
      "Epoch:  47 | Step:  93 | Val Loss:  0.5103711485862732\n",
      "Epoch:  47 | Step:  94 | Val Loss:  0.5799360275268555\n",
      "Epoch:  47 | Step:  95 | Val Loss:  0.44049108028411865\n",
      "Epoch:  47 | Step:  96 | Val Loss:  0.5163658261299133\n",
      "Epoch:  47 | Step:  97 | Val Loss:  0.3562546968460083\n",
      "Epoch:  47 | Step:  98 | Val Loss:  0.42470401525497437\n",
      "Epoch:  47 | Step:  99 | Val Loss:  0.4465518295764923\n",
      "Epoch:  47 | Step:  100 | Val Loss:  0.43449774384498596\n",
      "Epoch:  47 | Step:  101 | Val Loss:  0.6173622608184814\n",
      "Epoch:  47 | Step:  102 | Val Loss:  0.5450210571289062\n",
      "Epoch:  47 | Step:  103 | Val Loss:  0.6203131675720215\n",
      "Epoch:  47 | Step:  104 | Val Loss:  0.4787619709968567\n",
      "Epoch:  47 | Step:  105 | Val Loss:  0.456300288438797\n",
      "Epoch:  47 | Step:  106 | Val Loss:  0.6740850210189819\n",
      "Epoch:  47 | Step:  107 | Val Loss:  0.6086392402648926\n",
      "Epoch:  47 | Step:  108 | Val Loss:  0.47289156913757324\n",
      "Epoch:  47 | Step:  109 | Val Loss:  0.5017075538635254\n",
      "Epoch:  47 | Step:  110 | Val Loss:  0.5731692314147949\n",
      "Epoch:  47 | Step:  111 | Val Loss:  0.48742473125457764\n",
      "Epoch:  47 | Step:  112 | Val Loss:  0.425567626953125\n",
      "Epoch:  47 | Step:  113 | Val Loss:  0.44861724972724915\n",
      "Epoch:  47 | Step:  114 | Val Loss:  0.42606860399246216\n",
      "Epoch:  47 | Step:  115 | Val Loss:  0.6076261401176453\n",
      "Epoch:  47 | Step:  116 | Val Loss:  0.5894221067428589\n",
      "Epoch:  47 | Step:  117 | Val Loss:  0.5968165993690491\n",
      "Epoch:  47 | Step:  118 | Val Loss:  0.5455098152160645\n",
      "Epoch:  47 | Step:  119 | Val Loss:  0.5461634397506714\n",
      "Epoch:  47 | Step:  120 | Val Loss:  0.5876463651657104\n",
      "Epoch:  47 | Step:  121 | Val Loss:  0.5929943323135376\n",
      "Epoch:  47 | Step:  122 | Val Loss:  0.5102553367614746\n",
      "Epoch:  47 | Step:  123 | Val Loss:  0.33806943893432617\n",
      "Epoch:  47 | Step:  124 | Val Loss:  0.5026171803474426\n",
      "Epoch:  47 | Step:  125 | Val Loss:  0.5311412811279297\n",
      "Epoch:  47 | Train Loss:  tensor(0.5009, device='cuda:0') | Val Loss:  tensor(0.5180, device='cuda:0')\n",
      "Epoch:  48 | Step:  500 | Train Loss:  0.5413281917572021\n",
      "Epoch:  48 | Step:  1 | Val Loss:  0.5011196136474609\n",
      "Epoch:  48 | Step:  2 | Val Loss:  0.5359117984771729\n",
      "Epoch:  48 | Step:  3 | Val Loss:  0.5892541408538818\n",
      "Epoch:  48 | Step:  4 | Val Loss:  0.5835756063461304\n",
      "Epoch:  48 | Step:  5 | Val Loss:  0.5257681608200073\n",
      "Epoch:  48 | Step:  6 | Val Loss:  0.45270541310310364\n",
      "Epoch:  48 | Step:  7 | Val Loss:  0.41574323177337646\n",
      "Epoch:  48 | Step:  8 | Val Loss:  0.5929828882217407\n",
      "Epoch:  48 | Step:  9 | Val Loss:  0.49587246775627136\n",
      "Epoch:  48 | Step:  10 | Val Loss:  0.5210201740264893\n",
      "Epoch:  48 | Step:  11 | Val Loss:  0.5726129412651062\n",
      "Epoch:  48 | Step:  12 | Val Loss:  0.5864871740341187\n",
      "Epoch:  48 | Step:  13 | Val Loss:  0.5814421772956848\n",
      "Epoch:  48 | Step:  14 | Val Loss:  0.5814671516418457\n",
      "Epoch:  48 | Step:  15 | Val Loss:  0.519347608089447\n",
      "Epoch:  48 | Step:  16 | Val Loss:  0.39821791648864746\n",
      "Epoch:  48 | Step:  17 | Val Loss:  0.5217851400375366\n",
      "Epoch:  48 | Step:  18 | Val Loss:  0.5779975652694702\n",
      "Epoch:  48 | Step:  19 | Val Loss:  0.48462122678756714\n",
      "Epoch:  48 | Step:  20 | Val Loss:  0.4608383774757385\n",
      "Epoch:  48 | Step:  21 | Val Loss:  0.4122031629085541\n",
      "Epoch:  48 | Step:  22 | Val Loss:  0.43782275915145874\n",
      "Epoch:  48 | Step:  23 | Val Loss:  0.5444368124008179\n",
      "Epoch:  48 | Step:  24 | Val Loss:  0.5416908264160156\n",
      "Epoch:  48 | Step:  25 | Val Loss:  0.4472880959510803\n",
      "Epoch:  48 | Step:  26 | Val Loss:  0.49738064408302307\n",
      "Epoch:  48 | Step:  27 | Val Loss:  0.4372202157974243\n",
      "Epoch:  48 | Step:  28 | Val Loss:  0.6460213661193848\n",
      "Epoch:  48 | Step:  29 | Val Loss:  0.4733729064464569\n",
      "Epoch:  48 | Step:  30 | Val Loss:  0.6314127445220947\n",
      "Epoch:  48 | Step:  31 | Val Loss:  0.5228722095489502\n",
      "Epoch:  48 | Step:  32 | Val Loss:  0.49772536754608154\n",
      "Epoch:  48 | Step:  33 | Val Loss:  0.5451250076293945\n",
      "Epoch:  48 | Step:  34 | Val Loss:  0.5677760243415833\n",
      "Epoch:  48 | Step:  35 | Val Loss:  0.6301416158676147\n",
      "Epoch:  48 | Step:  36 | Val Loss:  0.59568190574646\n",
      "Epoch:  48 | Step:  37 | Val Loss:  0.5562305450439453\n",
      "Epoch:  48 | Step:  38 | Val Loss:  0.46275731921195984\n",
      "Epoch:  48 | Step:  39 | Val Loss:  0.49168702960014343\n",
      "Epoch:  48 | Step:  40 | Val Loss:  0.4769342541694641\n",
      "Epoch:  48 | Step:  41 | Val Loss:  0.6333785057067871\n",
      "Epoch:  48 | Step:  42 | Val Loss:  0.4742332398891449\n",
      "Epoch:  48 | Step:  43 | Val Loss:  0.47195684909820557\n",
      "Epoch:  48 | Step:  44 | Val Loss:  0.6532865762710571\n",
      "Epoch:  48 | Step:  45 | Val Loss:  0.325374037027359\n",
      "Epoch:  48 | Step:  46 | Val Loss:  0.49179607629776\n",
      "Epoch:  48 | Step:  47 | Val Loss:  0.5984290242195129\n",
      "Epoch:  48 | Step:  48 | Val Loss:  0.6526187658309937\n",
      "Epoch:  48 | Step:  49 | Val Loss:  0.5362500548362732\n",
      "Epoch:  48 | Step:  50 | Val Loss:  0.4555492699146271\n",
      "Epoch:  48 | Step:  51 | Val Loss:  0.6123688220977783\n",
      "Epoch:  48 | Step:  52 | Val Loss:  0.4527968168258667\n",
      "Epoch:  48 | Step:  53 | Val Loss:  0.48789000511169434\n",
      "Epoch:  48 | Step:  54 | Val Loss:  0.5678357481956482\n",
      "Epoch:  48 | Step:  55 | Val Loss:  0.5308374166488647\n",
      "Epoch:  48 | Step:  56 | Val Loss:  0.5459334850311279\n",
      "Epoch:  48 | Step:  57 | Val Loss:  0.5416010022163391\n",
      "Epoch:  48 | Step:  58 | Val Loss:  0.425107479095459\n",
      "Epoch:  48 | Step:  59 | Val Loss:  0.509774923324585\n",
      "Epoch:  48 | Step:  60 | Val Loss:  0.5273910164833069\n",
      "Epoch:  48 | Step:  61 | Val Loss:  0.6351810693740845\n",
      "Epoch:  48 | Step:  62 | Val Loss:  0.4526025652885437\n",
      "Epoch:  48 | Step:  63 | Val Loss:  0.5449211597442627\n",
      "Epoch:  48 | Step:  64 | Val Loss:  0.31935393810272217\n",
      "Epoch:  48 | Step:  65 | Val Loss:  0.5685341954231262\n",
      "Epoch:  48 | Step:  66 | Val Loss:  0.6051578521728516\n",
      "Epoch:  48 | Step:  67 | Val Loss:  0.4086112380027771\n",
      "Epoch:  48 | Step:  68 | Val Loss:  0.3910903334617615\n",
      "Epoch:  48 | Step:  69 | Val Loss:  0.5211549401283264\n",
      "Epoch:  48 | Step:  70 | Val Loss:  0.6291550993919373\n",
      "Epoch:  48 | Step:  71 | Val Loss:  0.5245376825332642\n",
      "Epoch:  48 | Step:  72 | Val Loss:  0.4166348874568939\n",
      "Epoch:  48 | Step:  73 | Val Loss:  0.6034719944000244\n",
      "Epoch:  48 | Step:  74 | Val Loss:  0.31026777625083923\n",
      "Epoch:  48 | Step:  75 | Val Loss:  0.5950879454612732\n",
      "Epoch:  48 | Step:  76 | Val Loss:  0.4664279818534851\n",
      "Epoch:  48 | Step:  77 | Val Loss:  0.5352472066879272\n",
      "Epoch:  48 | Step:  78 | Val Loss:  0.6340861916542053\n",
      "Epoch:  48 | Step:  79 | Val Loss:  0.4580747187137604\n",
      "Epoch:  48 | Step:  80 | Val Loss:  0.5396019220352173\n",
      "Epoch:  48 | Step:  81 | Val Loss:  0.2386212944984436\n",
      "Epoch:  48 | Step:  82 | Val Loss:  0.539390504360199\n",
      "Epoch:  48 | Step:  83 | Val Loss:  0.4468938112258911\n",
      "Epoch:  48 | Step:  84 | Val Loss:  0.4560594856739044\n",
      "Epoch:  48 | Step:  85 | Val Loss:  0.5716465711593628\n",
      "Epoch:  48 | Step:  86 | Val Loss:  0.6688804030418396\n",
      "Epoch:  48 | Step:  87 | Val Loss:  0.5612514615058899\n",
      "Epoch:  48 | Step:  88 | Val Loss:  0.6495776176452637\n",
      "Epoch:  48 | Step:  89 | Val Loss:  0.5847086310386658\n",
      "Epoch:  48 | Step:  90 | Val Loss:  0.49691617488861084\n",
      "Epoch:  48 | Step:  91 | Val Loss:  0.5269691944122314\n",
      "Epoch:  48 | Step:  92 | Val Loss:  0.5367138385772705\n",
      "Epoch:  48 | Step:  93 | Val Loss:  0.5316587686538696\n",
      "Epoch:  48 | Step:  94 | Val Loss:  0.5102117657661438\n",
      "Epoch:  48 | Step:  95 | Val Loss:  0.6509097814559937\n",
      "Epoch:  48 | Step:  96 | Val Loss:  0.5669459700584412\n",
      "Epoch:  48 | Step:  97 | Val Loss:  0.4282779097557068\n",
      "Epoch:  48 | Step:  98 | Val Loss:  0.5446099042892456\n",
      "Epoch:  48 | Step:  99 | Val Loss:  0.46106821298599243\n",
      "Epoch:  48 | Step:  100 | Val Loss:  0.2979830801486969\n",
      "Epoch:  48 | Step:  101 | Val Loss:  0.4854702651500702\n",
      "Epoch:  48 | Step:  102 | Val Loss:  0.505524218082428\n",
      "Epoch:  48 | Step:  103 | Val Loss:  0.6494360566139221\n",
      "Epoch:  48 | Step:  104 | Val Loss:  0.45953965187072754\n",
      "Epoch:  48 | Step:  105 | Val Loss:  0.5807473063468933\n",
      "Epoch:  48 | Step:  106 | Val Loss:  0.5440200567245483\n",
      "Epoch:  48 | Step:  107 | Val Loss:  0.6442088484764099\n",
      "Epoch:  48 | Step:  108 | Val Loss:  0.5587105751037598\n",
      "Epoch:  48 | Step:  109 | Val Loss:  0.5354034900665283\n",
      "Epoch:  48 | Step:  110 | Val Loss:  0.5347418785095215\n",
      "Epoch:  48 | Step:  111 | Val Loss:  0.39849621057510376\n",
      "Epoch:  48 | Step:  112 | Val Loss:  0.42891228199005127\n",
      "Epoch:  48 | Step:  113 | Val Loss:  0.5124781727790833\n",
      "Epoch:  48 | Step:  114 | Val Loss:  0.4392346143722534\n",
      "Epoch:  48 | Step:  115 | Val Loss:  0.40435147285461426\n",
      "Epoch:  48 | Step:  116 | Val Loss:  0.47011056542396545\n",
      "Epoch:  48 | Step:  117 | Val Loss:  0.5641241669654846\n",
      "Epoch:  48 | Step:  118 | Val Loss:  0.5554348230361938\n",
      "Epoch:  48 | Step:  119 | Val Loss:  0.3520074486732483\n",
      "Epoch:  48 | Step:  120 | Val Loss:  0.6559598445892334\n",
      "Epoch:  48 | Step:  121 | Val Loss:  0.41860154271125793\n",
      "Epoch:  48 | Step:  122 | Val Loss:  0.4995386600494385\n",
      "Epoch:  48 | Step:  123 | Val Loss:  0.5637238025665283\n",
      "Epoch:  48 | Step:  124 | Val Loss:  0.6040757894515991\n",
      "Epoch:  48 | Step:  125 | Val Loss:  0.5060014724731445\n",
      "Epoch:  48 | Train Loss:  tensor(0.5007, device='cuda:0') | Val Loss:  tensor(0.5177, device='cuda:0')\n",
      "Epoch:  49 | Step:  500 | Train Loss:  0.5245646238327026\n",
      "Epoch:  49 | Step:  1 | Val Loss:  0.5490572452545166\n",
      "Epoch:  49 | Step:  2 | Val Loss:  0.5364406108856201\n",
      "Epoch:  49 | Step:  3 | Val Loss:  0.47840994596481323\n",
      "Epoch:  49 | Step:  4 | Val Loss:  0.5458418726921082\n",
      "Epoch:  49 | Step:  5 | Val Loss:  0.5560171008110046\n",
      "Epoch:  49 | Step:  6 | Val Loss:  0.41377338767051697\n",
      "Epoch:  49 | Step:  7 | Val Loss:  0.5583439469337463\n",
      "Epoch:  49 | Step:  8 | Val Loss:  0.5165193676948547\n",
      "Epoch:  49 | Step:  9 | Val Loss:  0.6418415307998657\n",
      "Epoch:  49 | Step:  10 | Val Loss:  0.4852357804775238\n",
      "Epoch:  49 | Step:  11 | Val Loss:  0.46543774008750916\n",
      "Epoch:  49 | Step:  12 | Val Loss:  0.5114738941192627\n",
      "Epoch:  49 | Step:  13 | Val Loss:  0.5057440996170044\n",
      "Epoch:  49 | Step:  14 | Val Loss:  0.6437129378318787\n",
      "Epoch:  49 | Step:  15 | Val Loss:  0.3967275619506836\n",
      "Epoch:  49 | Step:  16 | Val Loss:  0.45820513367652893\n",
      "Epoch:  49 | Step:  17 | Val Loss:  0.3686370253562927\n",
      "Epoch:  49 | Step:  18 | Val Loss:  0.5719641447067261\n",
      "Epoch:  49 | Step:  19 | Val Loss:  0.3975861668586731\n",
      "Epoch:  49 | Step:  20 | Val Loss:  0.3318406343460083\n",
      "Epoch:  49 | Step:  21 | Val Loss:  0.5346404910087585\n",
      "Epoch:  49 | Step:  22 | Val Loss:  0.5534353256225586\n",
      "Epoch:  49 | Step:  23 | Val Loss:  0.4520982503890991\n",
      "Epoch:  49 | Step:  24 | Val Loss:  0.5368387699127197\n",
      "Epoch:  49 | Step:  25 | Val Loss:  0.3945309817790985\n",
      "Epoch:  49 | Step:  26 | Val Loss:  0.5481020212173462\n",
      "Epoch:  49 | Step:  27 | Val Loss:  0.436893492937088\n",
      "Epoch:  49 | Step:  28 | Val Loss:  0.5216963887214661\n",
      "Epoch:  49 | Step:  29 | Val Loss:  0.6227072477340698\n",
      "Epoch:  49 | Step:  30 | Val Loss:  0.3383820056915283\n",
      "Epoch:  49 | Step:  31 | Val Loss:  0.4946172833442688\n",
      "Epoch:  49 | Step:  32 | Val Loss:  0.3988773226737976\n",
      "Epoch:  49 | Step:  33 | Val Loss:  0.6568852066993713\n",
      "Epoch:  49 | Step:  34 | Val Loss:  0.3351227045059204\n",
      "Epoch:  49 | Step:  35 | Val Loss:  0.4971659481525421\n",
      "Epoch:  49 | Step:  36 | Val Loss:  0.5100646615028381\n",
      "Epoch:  49 | Step:  37 | Val Loss:  0.7318630218505859\n",
      "Epoch:  49 | Step:  38 | Val Loss:  0.739380955696106\n",
      "Epoch:  49 | Step:  39 | Val Loss:  0.55874103307724\n",
      "Epoch:  49 | Step:  40 | Val Loss:  0.5649317502975464\n",
      "Epoch:  49 | Step:  41 | Val Loss:  0.5548974275588989\n",
      "Epoch:  49 | Step:  42 | Val Loss:  0.3689398169517517\n",
      "Epoch:  49 | Step:  43 | Val Loss:  0.6864362955093384\n",
      "Epoch:  49 | Step:  44 | Val Loss:  0.5486855506896973\n",
      "Epoch:  49 | Step:  45 | Val Loss:  0.467593252658844\n",
      "Epoch:  49 | Step:  46 | Val Loss:  0.5044649839401245\n",
      "Epoch:  49 | Step:  47 | Val Loss:  0.7154877781867981\n",
      "Epoch:  49 | Step:  48 | Val Loss:  0.5506448149681091\n",
      "Epoch:  49 | Step:  49 | Val Loss:  0.5893429517745972\n",
      "Epoch:  49 | Step:  50 | Val Loss:  0.6061674356460571\n",
      "Epoch:  49 | Step:  51 | Val Loss:  0.5578993558883667\n",
      "Epoch:  49 | Step:  52 | Val Loss:  0.39900651574134827\n",
      "Epoch:  49 | Step:  53 | Val Loss:  0.4008238613605499\n",
      "Epoch:  49 | Step:  54 | Val Loss:  0.5756046772003174\n",
      "Epoch:  49 | Step:  55 | Val Loss:  0.6605827808380127\n",
      "Epoch:  49 | Step:  56 | Val Loss:  0.5257363319396973\n",
      "Epoch:  49 | Step:  57 | Val Loss:  0.45415520668029785\n",
      "Epoch:  49 | Step:  58 | Val Loss:  0.6119507551193237\n",
      "Epoch:  49 | Step:  59 | Val Loss:  0.5016396045684814\n",
      "Epoch:  49 | Step:  60 | Val Loss:  0.48653894662857056\n",
      "Epoch:  49 | Step:  61 | Val Loss:  0.6876047253608704\n",
      "Epoch:  49 | Step:  62 | Val Loss:  0.57892906665802\n",
      "Epoch:  49 | Step:  63 | Val Loss:  0.4950787425041199\n",
      "Epoch:  49 | Step:  64 | Val Loss:  0.579524040222168\n",
      "Epoch:  49 | Step:  65 | Val Loss:  0.56805020570755\n",
      "Epoch:  49 | Step:  66 | Val Loss:  0.34051430225372314\n",
      "Epoch:  49 | Step:  67 | Val Loss:  0.5433598756790161\n",
      "Epoch:  49 | Step:  68 | Val Loss:  0.5010556578636169\n",
      "Epoch:  49 | Step:  69 | Val Loss:  0.486042320728302\n",
      "Epoch:  49 | Step:  70 | Val Loss:  0.5177377462387085\n",
      "Epoch:  49 | Step:  71 | Val Loss:  0.36959224939346313\n",
      "Epoch:  49 | Step:  72 | Val Loss:  0.3679392635822296\n",
      "Epoch:  49 | Step:  73 | Val Loss:  0.576740026473999\n",
      "Epoch:  49 | Step:  74 | Val Loss:  0.3995266854763031\n",
      "Epoch:  49 | Step:  75 | Val Loss:  0.5226784944534302\n",
      "Epoch:  49 | Step:  76 | Val Loss:  0.4095357656478882\n",
      "Epoch:  49 | Step:  77 | Val Loss:  0.5510693192481995\n",
      "Epoch:  49 | Step:  78 | Val Loss:  0.49312692880630493\n",
      "Epoch:  49 | Step:  79 | Val Loss:  0.498725950717926\n",
      "Epoch:  49 | Step:  80 | Val Loss:  0.5471137166023254\n",
      "Epoch:  49 | Step:  81 | Val Loss:  0.5872960090637207\n",
      "Epoch:  49 | Step:  82 | Val Loss:  0.6009862422943115\n",
      "Epoch:  49 | Step:  83 | Val Loss:  0.44721540808677673\n",
      "Epoch:  49 | Step:  84 | Val Loss:  0.41329044103622437\n",
      "Epoch:  49 | Step:  85 | Val Loss:  0.5873278379440308\n",
      "Epoch:  49 | Step:  86 | Val Loss:  0.36310699582099915\n",
      "Epoch:  49 | Step:  87 | Val Loss:  0.5807592868804932\n",
      "Epoch:  49 | Step:  88 | Val Loss:  0.4775782823562622\n",
      "Epoch:  49 | Step:  89 | Val Loss:  0.49803823232650757\n",
      "Epoch:  49 | Step:  90 | Val Loss:  0.5883920192718506\n",
      "Epoch:  49 | Step:  91 | Val Loss:  0.4707280397415161\n",
      "Epoch:  49 | Step:  92 | Val Loss:  0.3946479558944702\n",
      "Epoch:  49 | Step:  93 | Val Loss:  0.3711244463920593\n",
      "Epoch:  49 | Step:  94 | Val Loss:  0.6061375141143799\n",
      "Epoch:  49 | Step:  95 | Val Loss:  0.6181924343109131\n",
      "Epoch:  49 | Step:  96 | Val Loss:  0.5261450409889221\n",
      "Epoch:  49 | Step:  97 | Val Loss:  0.4149913787841797\n",
      "Epoch:  49 | Step:  98 | Val Loss:  0.3183537721633911\n",
      "Epoch:  49 | Step:  99 | Val Loss:  0.5278487205505371\n",
      "Epoch:  49 | Step:  100 | Val Loss:  0.4739319682121277\n",
      "Epoch:  49 | Step:  101 | Val Loss:  0.6315014362335205\n",
      "Epoch:  49 | Step:  102 | Val Loss:  0.6746484041213989\n",
      "Epoch:  49 | Step:  103 | Val Loss:  0.41737475991249084\n",
      "Epoch:  49 | Step:  104 | Val Loss:  0.6459923386573792\n",
      "Epoch:  49 | Step:  105 | Val Loss:  0.6713537573814392\n",
      "Epoch:  49 | Step:  106 | Val Loss:  0.5545144081115723\n",
      "Epoch:  49 | Step:  107 | Val Loss:  0.5943881869316101\n",
      "Epoch:  49 | Step:  108 | Val Loss:  0.4878658950328827\n",
      "Epoch:  49 | Step:  109 | Val Loss:  0.5769921541213989\n",
      "Epoch:  49 | Step:  110 | Val Loss:  0.5004350543022156\n",
      "Epoch:  49 | Step:  111 | Val Loss:  0.4419836103916168\n",
      "Epoch:  49 | Step:  112 | Val Loss:  0.5678850412368774\n",
      "Epoch:  49 | Step:  113 | Val Loss:  0.47476887702941895\n",
      "Epoch:  49 | Step:  114 | Val Loss:  0.44316670298576355\n",
      "Epoch:  49 | Step:  115 | Val Loss:  0.5285166501998901\n",
      "Epoch:  49 | Step:  116 | Val Loss:  0.6431818604469299\n",
      "Epoch:  49 | Step:  117 | Val Loss:  0.51445472240448\n",
      "Epoch:  49 | Step:  118 | Val Loss:  0.5466065406799316\n",
      "Epoch:  49 | Step:  119 | Val Loss:  0.5307475924491882\n",
      "Epoch:  49 | Step:  120 | Val Loss:  0.5880284309387207\n",
      "Epoch:  49 | Step:  121 | Val Loss:  0.4314414858818054\n",
      "Epoch:  49 | Step:  122 | Val Loss:  0.5425309538841248\n",
      "Epoch:  49 | Step:  123 | Val Loss:  0.556679368019104\n",
      "Epoch:  49 | Step:  124 | Val Loss:  0.42610839009284973\n",
      "Epoch:  49 | Step:  125 | Val Loss:  0.6558713912963867\n",
      "Epoch:  49 | Train Loss:  tensor(0.5007, device='cuda:0') | Val Loss:  tensor(0.5174, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "train_losses =  torch.zeros(len(train_loader))\n",
    "val_losses = torch.zeros(len(val_loader))\n",
    "wandb.init(\n",
    "    project='GRU-From-Scratch'\n",
    ")\n",
    "for epoch in range(ModelArgs.epoch):\n",
    "    \n",
    "    count = 0\n",
    "    for X, y in train_loader:\n",
    "        y_pred = model(X)\n",
    "        # print(y_pred.shape)\n",
    "        loss = criterion(y_pred, y)\n",
    "        train_losses[count] = loss.item()\n",
    "        # print(\"Loss: \", loss.item())\n",
    "        \n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        # if(count != 0):\n",
    "        #     total_norm_before = torch.norm(\n",
    "        #             torch.stack([torch.norm(p.grad.detach(), 2) for p in model.parameters()]), 2\n",
    "        #         )\n",
    "\n",
    "        #     torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        #         # Compute gradient norms after clipping\n",
    "        #     total_norm_after = torch.norm(\n",
    "        #             torch.stack([torch.norm(p.grad.detach(), 2) for p in model.parameters()]), 2\n",
    "        #         )\n",
    "                \n",
    "        #     # if(device  == 0 and step !=0):\n",
    "        #     print(f\"Gradient Norm Before Clipping: {total_norm_before.item():.4f}\")\n",
    "        #     print(f\"Gradient Norm After Clipping: {total_norm_after.item():.4f}\")\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "        \n",
    "    #     wandb.log({\n",
    "    #   \"Train Loss\": loss.item(),\n",
    "    #   \"Val Loss\": loss.item(),\n",
    "    #   \"step\": count  \n",
    "    # })\n",
    "    # count = 0\n",
    "    print(\"Epoch: \", epoch, \"|\", \"Step: \", count, \"|\", \"Train Loss: \", loss.item())\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    \n",
    "    for X, y in val_loader:\n",
    "        y_pred = model(X)\n",
    "        # print(y_pred.shape)\n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        # print(\"Loss: \", loss.item())\n",
    "        val_losses[count] = loss.item()\n",
    "        \n",
    "        # optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        count += 1\n",
    "    #     wandb.log({\n",
    "    #   \"Train Loss\": loss.item(),\n",
    "    #   \"Val Loss\": loss.item(),\n",
    "    #   \"step\": count  \n",
    "    # })\n",
    "        print(\"Epoch: \", epoch, \"|\", \"Step: \", count, \"|\", \"Val Loss: \", loss.item())\n",
    "    model.train()\n",
    "    wandb.log({\n",
    "      \"Train Loss\": train_losses.mean(),\n",
    "      \"Val Loss\": val_losses.mean(),\n",
    "      \"epoch\": epoch  \n",
    "    })\n",
    "    print(\"Epoch: \", epoch, \"|\", \"Train Loss: \", train_losses.mean(),  \"|\", \"Val Loss: \", val_losses.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
