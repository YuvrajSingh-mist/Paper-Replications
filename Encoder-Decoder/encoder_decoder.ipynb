{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch==2.3.0 torchtext==0.18.0\nimport torch\nimport torch.nn as nn","metadata":{"id":"h8C4I4M0AbEr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cb9f500f-4206-480a-aa75-4ad0391361df","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\n# !wandb login\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"API_KEY\")\nwandb.login(key=secret_value_0)","metadata":{"id":"j4n_YxBTAbEs","colab":{"base_uri":"https://localhost:8080/"},"outputId":"238ab5e6-ca61-4638-b9e6-0e016fc56254","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from dataclasses import dataclass\n\n\n@dataclass\nclass ModelArgs:\n    device = 'cuda'\n    no_of_neurons = 256\n    block_size = 32\n    batch_size = 32\n    en_vocab_size = None\n    de_vocab_size = None\n    dropout = 0.1\n    epoch = 10\n    max_lr = 1e-4\n    embedding_dims = 1000\n    num_layers = 2","metadata":{"id":"sxJrSlI2AbEt","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python -m spacy download de_core_news_sm","metadata":{"id":"IYbXtTsAy5-2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7717bef8-2adc-4589-f5b5-9372e157ee7f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if torch.cuda.is_available():\n    ModelArgs.device = 'cuda'\n    torch.set_default_device('cuda')\nelse:\n\n    torch.set_default_device('cpu')\n    ModelArgs.device='cpu'\n\nif torch.cuda.is_available():\n  torch.set_default_device(ModelArgs.device)","metadata":{"id":"9V85nJT8uLw5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python -m spacy download en_core_web_sm","metadata":{"id":"QCNnhvz0y7qp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bec6fe8d-d2a1-4125-c89f-c27383c2823f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torchtext.data.utils import get_tokenizer\nfrom collections import Counter\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torchtext.utils import download_from_url, extract_archive\nimport io\nfrom torch.utils.data import DataLoader, Dataset\n\n# Download and extract data\nurl_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\ntrain_urls = ('train.de.gz', 'train.en.gz')\nval_urls = ('val.de.gz', 'val.en.gz')\ntest_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n\ntrain_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\nval_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\ntest_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n\n# Load SpaCy tokenizers\nde_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\nen_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n\n# Build vocabulary\ndef build_vocab(filepath, tokenizer):\n    counter = Counter()\n    with io.open(filepath, encoding=\"utf8\") as f:\n        for string_ in f:\n            counter.update(tokenizer(string_))\n    vocab = build_vocab_from_iterator(\n        [counter.keys()],\n        specials=['<unk>', '<bos>', '<eos>']\n    )\n    vocab.set_default_index(vocab['<unk>'])\n    return vocab\n\nde_vocab = build_vocab(train_filepaths[0], de_tokenizer)\nModelArgs.de_vocab_size = len(de_vocab) + 1\nen_vocab = build_vocab(train_filepaths[1], en_tokenizer)\nModelArgs.en_vocab_size = len(en_vocab) + 1\n\n\ndef data_process(filepaths):\n    raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n    raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n    data = []\n\n    # Get the indices for <bos> and <eos> tokens\n    de_bos_idx = de_vocab['<bos>']\n    de_eos_idx = de_vocab['<eos>']\n    en_bos_idx = en_vocab['<bos>']\n    en_eos_idx = en_vocab['<eos>']\n\n    for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n        # Tokenize and convert to indices\n        de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)], dtype=torch.long)\n        en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)], dtype=torch.long)\n\n        # Add <bos> and <eos> tokens\n        # de_tensor_ = torch.cat([torch.tensor([de_bos_idx]), de_tensor_, torch.tensor([de_eos_idx])])\n        en_tensor_ = torch.cat([torch.tensor([en_bos_idx]), en_tensor_, torch.tensor([en_eos_idx])])\n\n        # Flip the German tensor (if required)\n        # de_tensor_ = torch.flip(de_tensor_, dims=[0])\n\n        # Append to data\n        data.append((de_tensor_, en_tensor_))\n\n    return data\n\ndef data_process_flip(filepaths):\n    raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n    raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n    data = []\n\n    # Get the indices for <bos> and <eos> tokens\n    de_bos_idx = de_vocab['<bos>']\n    de_eos_idx = de_vocab['<eos>']\n    en_bos_idx = en_vocab['<bos>']\n    en_eos_idx = en_vocab['<eos>']\n\n    for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n        # Tokenize and convert to indices\n        de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)], dtype=torch.long)\n        en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)], dtype=torch.long)\n\n        # Add <bos> and <eos> tokens\n        # de_tensor_ = torch.cat([torch.tensor([de_bos_idx]), de_tensor_, torch.tensor([de_eos_idx])])\n        en_tensor_ = torch.cat([torch.tensor([en_bos_idx]), en_tensor_, torch.tensor([en_eos_idx])])\n\n        # Flip the German tensor (if required)\n        de_tensor_ = torch.flip(de_tensor_, dims=[0])\n\n        # Append to data\n        data.append((de_tensor_, en_tensor_))\n\n    return data\n\ntrain_data = data_process_flip(train_filepaths)\nval_data = data_process(val_filepaths)\ntest_data = data_process(test_filepaths)\n\n# Create a custom Dataset class\nclass TranslationDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n# Create Dataset instances\ntrain_dataset = TranslationDataset(train_data)\nval_dataset = TranslationDataset(val_data)\ntest_dataset = TranslationDataset(test_data)\n\nfrom torch.nn.utils.rnn import pad_sequence\n\ndef collate_fn(batch, block_size=32):\n    \"\"\"\n    Collate function to pad or truncate sequences to a fixed block size.\n\n    Args:\n        batch: A list of tuples (de_tensor, en_tensor).\n        block_size: The fixed length to pad or truncate sequences to.\n\n    Returns:\n        de_batch: Padded/truncated German sequences (batch_size, block_size).\n        en_batch: Padded/truncated English sequences (batch_size, block_size).\n    \"\"\"\n    de_batch, en_batch = zip(*batch)\n\n    # Function to pad or truncate a sequence to the block size\n    def pad_or_truncate(sequence, block_size, pad_value):\n        if len(sequence) > block_size:\n            # Truncate the sequence if it's longer than block_size\n            return sequence[:block_size]\n        else:\n            # Pad the sequence if it's shorter than block_size\n            padding_length = block_size - len(sequence)\n            return torch.cat([sequence, torch.full((padding_length,), pad_value, dtype=sequence.dtype)])\n\n    # Pad or truncate each sequence in the batch\n    de_batch = [pad_or_truncate(seq, block_size, de_vocab['<pad>']) for seq in de_batch]\n    en_batch = [pad_or_truncate(seq, block_size, en_vocab['<pad>']) for seq in en_batch]\n\n    # Stack the sequences into a single tensor\n    de_batch = torch.stack(de_batch)\n    en_batch = torch.stack(en_batch)\n\n    return de_batch, en_batch\n\ngenerator = torch.Generator(device=ModelArgs.device)\n\n\n# Create DataLoader instances\nbatch_size = ModelArgs.batch_size\ntrain_loader = DataLoader(train_dataset, generator=generator, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)\nval_loader = DataLoader(val_dataset, generator=generator, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, drop_last=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, drop_last=False)\n\n# Example usage\nfor de_batch, en_batch in train_loader:\n    print(f\"German batch shape: {de_batch.shape}\")\n    print(f\"English batch shape: {en_batch.shape}\")\n    break","metadata":{"id":"7h-HLa2_xRah","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9970b3eb-8795-4ae1-d14b-8368cb671c85","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# train_data","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1aId84g24ytU","outputId":"6cd4022c-93bd-4c30-a7e4-72abff5baf2e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"sBD_OADeAbEt","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ModelArgs.en_vocab_size","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZpUmGZmHUZ0n","outputId":"9a6986a6-208a-42ed-d8d3-6b81f3387a1f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x = torch.randn((ModelArgs.batch_size, 1, ModelArgs.embedding_dims))\nx1 = torch.randn((ModelArgs.batch_size, ModelArgs.block_size, ModelArgs.embedding_dims))\nprint(torch.cat([x, x1], dim=1).shape)","metadata":{"id":"hE9tqRQcAbEu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2b0460ec-465e-4968-8f44-dbc0ac02fee7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass InputGate(nn.Module):\n    def __init__(self, device, no_of_neurons):\n        super().__init__()\n        self.it = nn.Linear(in_features= ModelArgs.no_of_neurons + ModelArgs.embedding_dims, out_features=no_of_neurons, device=device, dtype=torch.float32)\n        self.ct_bar = nn.Linear(in_features=ModelArgs.no_of_neurons + ModelArgs.embedding_dims, out_features=no_of_neurons, device=device, dtype=torch.float32)\n\n    def forward(self, x, ht_1):\n        x = torch.cat([x, ht_1], dim=1)\n        _it = torch.nn.functional.sigmoid(self.it(x))\n        _ct_bar = torch.nn.functional.tanh(self.ct_bar(x))\n        # out = torch.nn.functional.sigmoid(self.linear(x))\n        return _it, _ct_bar","metadata":{"id":"NAPYL-KeAbEw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class OutputGate(nn.Module):\n    def __init__(self, device, no_of_neurons) -> None:\n        super().__init__()\n        self.linear = nn.Linear(in_features= ModelArgs.no_of_neurons + ModelArgs.embedding_dims, out_features=no_of_neurons, device=device, dtype=torch.float32)\n    def forward(self, x, ht_1):\n        x = torch.cat([x, ht_1], dim=1)\n        out = torch.nn.functional.sigmoid(self.linear(x))\n        return out","metadata":{"id":"MxAPFK3wAbEw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ForgetGate(nn.Module):\n\n    def __init__(self, device, no_of_neurons):\n        super().__init__()\n        self.linear = nn.Linear(in_features=ModelArgs.no_of_neurons + ModelArgs.embedding_dims, out_features=no_of_neurons, device=device, dtype=torch.float32)\n\n    def forward(self, x, ht_1):\n        # print(\"Forgot: \", x.shape)\n        # print(\"Forget: \", ht_1.shape)\n        x = torch.cat([x, ht_1], dim=1)\n        out = torch.nn.functional.sigmoid(self.linear(x))\n        return out","metadata":{"id":"i-NW4ePRAbEw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LSTMBlock(nn.Module):\n    def __init__(self, device, no_of_neurons):\n        super().__init__()\n        self.ip = InputGate(device=device, no_of_neurons=no_of_neurons)\n        self.op = OutputGate(device=device, no_of_neurons=no_of_neurons)\n        self.forget = ForgetGate(device=device, no_of_neurons=no_of_neurons)\n        self.no_of_neurons = no_of_neurons\n        self.device=device\n        self.linear_layer = nn.Linear(in_features=ModelArgs.no_of_neurons, out_features=ModelArgs.embedding_dims, device=device)\n    def forward(self, x, ht_1=None, outputs=None, embeds=None):\n        # print(\"Block: \", x.shape)\n        # print(\"Block: \", ht_1.shape)\n        # print(\"Block: \", ct_1.shape)\n\n        if(ht_1 is None):\n          ht_1 = torch.randn( (x.shape[0], self.no_of_neurons), device=self.device, requires_grad=True, dtype=torch.float32)\n\n        ct_1 = torch.randn((x.shape[0], self.no_of_neurons),device=self.device, requires_grad=True, dtype=torch.float32)\n        seq_len = x.shape[1]\n        # ht_1 = ht_1.unsqueeze(-1)\n        if(outputs == None):\n            # print(\"New\")\n            # print(x)\n            outputs = []\n            for t in range(seq_len):\n                # print(\"Block: \", x.shape)\n                # print(\"Block: \", ht_1.shape)\n                # print(\"Block: \", ct_1.shape)\n                xt = x[:, t, :]\n                # print(xt.shape)\n                ft = self.forget(xt, ht_1) * ct_1\n                it, ct_bar = self.ip(xt , ht_1)\n                ct_bar_prime = it * ct_bar\n                ct = ft * ct_1 + ct_bar_prime\n                ht = self.op(xt, ht_1) * torch.nn.functional.tanh(ct)\n                outputs.append(ht)\n            return ht, ct, torch.stack(outputs, dim=1)\n\n        elif(outputs is not None):\n          # print(\"Other\")\n          new_output = []\n          for t in range(seq_len):\n                # print(\"Block: \", x.shape)\n                # print(\"Block: \", ht_1.shape)\n                # print(\"Block: \", ct_1.shape)\n                xt = outputs[:, t, :]\n                # print(\"Shape: \", xt.shape)\n                xt = self.linear_layer(xt)\n                # print(\"After: \", xt.shape)\n                ft = self.forget(xt, ht_1) * ct_1\n                it, ct_bar = self.ip(xt , ht_1)\n                ct_bar_prime = it * ct_bar\n                ct = ft * ct_1 + ct_bar_prime\n                ht = self.op(xt, ht_1) * torch.nn.functional.tanh(ct)\n                new_output.append(ht)\n          return ht, ct, torch.stack(new_output, dim=1)","metadata":{"id":"8D1FnUKOAbEx","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass EmbeddingTable_en(nn.Module):\n  def __init__(self, device):\n    super().__init__()\n\n    self.embed_en = nn.Embedding(num_embeddings=ModelArgs.en_vocab_size, embedding_dim=ModelArgs.embedding_dims, device=device)\n\n  def forward(self, x):\n    return self.embed_en(x)","metadata":{"id":"Y8M7hip-BGi1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nclass EmbeddingTable_de(nn.Module):\n  def __init__(self, device):\n    super().__init__()\n\n    self.embed_de =  nn.Embedding(num_embeddings=ModelArgs.de_vocab_size, embedding_dim=ModelArgs.embedding_dims, device=device)\n\n  def forward(self, x):\n    # print('Indie: ', x)\n    return self.embed_de(x)","metadata":{"id":"VjEUjJhWKhkM","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, device, no_of_neurons, out_features):\n        super().__init__()\n        self.block1 = LSTMBlock(device=device, no_of_neurons=no_of_neurons)\n        # self.block2 = LSTMBlock(device=device, no_of_neurons=no_of_neurons)\n        # self.embeds_table_en = EmbeddingTable_en(device=device)\n        self.embeds_table_de = EmbeddingTable_de(device=device)\n        # self.ht_1 = torch.randn(ModelArgs.batch_size, no_of_neurons, device=device, requires_grad=True, dtype=torch.float32)\n        # self.ct_1 = torch.randn(ModelArgs.batch_size, no_of_neurons,device=device, requires_grad=True, dtype=torch.float32)\n        # self.output = nn.Linear(in_features=ModelArgs.no_of_neurons, out_features=out_features, device=device, dtype=torch.float32)\n        self.dropout = nn.Dropout(p=ModelArgs.dropout)\n        # self.embedding = nn.Embedding()\n\n    def forward(self, x, outputs=None, initial=None):\n        # x =\n        # print(\"LSTM: \",x.shape)\n        # print(\"LSTM: \", self.ht_1.shape)\n        # print(\"LSTM: \", self.ct_1.shape)\n        # if(encoder):\n        if(initial is not None and initial is True):\n          x = self.embeds_table_de(x)\n        # print(x.shape)\n        # elif(decoder):\n        #   x = self.embeds_table_en(x)\n        if(outputs is not None):\n          # print(outputs)\n          ht, ct, outputs = self.block1(x, outputs=outputs)\n        elif(outputs is None):\n          ht, ct, outputs = self.block1(x)\n        # print(ht.shape)\n        # print(ct.shape)\n        # ht, ct = self.block2(x, ht, ct)\n        # ht = self.dropout(ht)\n        # print(\"Aft: \", outputs.shape)\n        # out = self.output(ht)\n        return  ht, ct, outputs, self.embeds_table_de","metadata":{"id":"yONjsNpKAbEx","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, device, no_of_neurons, out_features):\n        super().__init__()\n        self.block1 = LSTMBlock(device=device, no_of_neurons=no_of_neurons)\n        # self.block2 = LSTMBlock(device=device, no_of_neurons=no_of_neurons)\n        self.embeds_table_en = EmbeddingTable_en(device=device)\n        # self.embeds_table_de = EmbeddingTable_de(device=device)\n        # self.ht_1 = torch.randn(ModelArgs.batch_size, no_of_neurons, device=device, requires_grad=True, dtype=torch.float32)\n        # self.ct_1 = torch.randn(ModelArgs.batch_size, no_of_neurons,device=device, requires_grad=True, dtype=torch.float32)\n        self.output = nn.Linear(in_features=ModelArgs.no_of_neurons, out_features=ModelArgs.en_vocab_size, device=device, dtype=torch.float32)\n        self.dropout = nn.Dropout(p=ModelArgs.dropout)\n        # self.embedding = nn.Embedding()\n\n    def forward(self, x, ctx=None, inf=None, embeds=None, initial=None, outputs=None):\n        # x =\n        # print(\"LSTM: \",x.shape)\n        # print(\"LSTM: \", self.ht_1.shape)\n        # print(\"LSTM: \", self.ct_1.shape)\n        # if(encoder):\n        #   x = self.embeds_table_de(x)\n          # print(x.shape)\n        # elif(decoder):\n        if(inf is not True and initial is True):\n          x = self.embeds_table_en(x)\n        if(inf is True):\n          # print(\"Before: \", x.shape)\n          x = embeds(x)\n          # print(\"After: \", x.shape)\n        ht, ct, outputs = self.block1(x, ctx, outputs=outputs)\n        # print(ht.shape)\n        # print(ct.shape)\n        # ht, ct = self.block2(x, ht, ct)\n        out = self.dropout(outputs)\n        # print(\"After: \", outputs.shape)\n        out = self.output(out)\n        return  out, outputs","metadata":{"id":"HaOMDN53MtyC","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass Seq2Seq(nn.Module):\n\n    def __init__(self, device, no_of_neurons, out_features):\n        super().__init__()\n\n        # self.encoder = Encoder(device, no_of_neurons, out_features)\n        # self.decoder = Decoder(device, no_of_neurons, out_features)\n        self.encoders = nn.ModuleList(Encoder(device, no_of_neurons, out_features) for _  in range(ModelArgs.num_layers))\n        self.decoders = nn.ModuleList(Decoder(device, no_of_neurons, out_features) for x in range(ModelArgs.num_layers))\n\n    def forward(self, x, y=None, inf=None):\n\n        count = 0\n        for i in self.encoders:\n          if(count == 0):\n            ht_encoder, ct_encoder,outputs_encoder, embeds_de = i(x, initial=True)\n            # x = ht_encoder\n          else:\n            ht_encoder, ct_encoder,outputs_encoder, embeds_de = i(x, outputs=outputs_encoder)\n            # x = ht_encoder\n          count += 1\n\n        res = None\n        count = 0\n        if(y is not None and inf==False):\n          for i in self.decoders:\n\n            # print(\"Hiii\")\n            if(count == 0):\n              y , outputs = i(y, ht_encoder, inf, embeds_de, True)\n              # res = x\n            else:\n              y, outputs = i(y, ht_encoder, inf, embeds_de, outputs=outputs)\n              # res = x\n            # return res\n          # elif(y is not None and inf==False):\n            # print(\"Here\")\n            # res = self.decoder(y, ht_encoder)\n            # return res\n            count += 1\n          return y\n\n\n        elif(inf==True and y is None):\n          x_init = x\n          count = 0\n          for i in self.decoders:\n\n            if(count == 0):\n              x, outputs = i(x, ht_encoder, inf, embeds_de, True)\n            # res = x\n\n            else:\n              x, outputs = i(x_init, ht_encoder, inf, embeds_de, outputs=outputs)\n\n            count += 1\n          return x","metadata":{"id":"FdhO0rZpAbEx","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Seq2Seq(device=ModelArgs.device, no_of_neurons=ModelArgs.no_of_neurons, out_features=1)\nmodel = model.to(ModelArgs.device)","metadata":{"id":"xFj17tN-AbEx","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torchinfo\n\nfrom torchinfo import summary\n\n# x = torch.randint(0, 100, (ModelArgs.batch_size,ModelArgs.block_size))  # Random integer between 0 and 100\nx,y = next(iter(train_loader))\nx = x.to(ModelArgs.device)\ny = y.to(ModelArgs.device)\n\nsummary(model=model,\n        input_data=[x,y, False],\n        # input_size=(ModelArgs.batch_size, ModelArgs.block_size, ModelArgs.embeddings_dims),\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"])\n","metadata":{"id":"djd0vnbzAbEx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"db9a8217-2593-4588-808b-12430f01265e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\n# x,y = next(iter(train_loader))\n# x.shape","metadata":{"id":"YzWzrztJ6_-J","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from andrej karapathy github\nimport torch.nn.functional as F\ndef topk_sampling(model, prompt, tokenizer, device, max_length=50, top_k=50, temperature=1.0):\n\n    # input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n\n    input_ids = torch.tensor([de_vocab[token] for token in de_tokenizer(prompt)]).unsqueeze(0)\n    oov = []\n    generated_text = \"\"\n    for _ in range(max_length):\n        with torch.no_grad():\n            outputs = model(input_ids, None, True)\n            logits = outputs[:, -1, :]\n\n            probs = F.softmax(logits, dim=-1)\n\n            # Top-k filtering\n            top_k_probs, top_k_indices = torch.topk(probs, top_k, dim=-1)\n#\n            # Apply temperature scaling\n            # probs = probs / temperature\n\n            # Sample from top-k\n            next_token = torch.multinomial(top_k_probs, num_samples=1)\n\n            # generated_tokens.append(next_token.item())\n\n            xcol = torch.gather(top_k_indices, -1, next_token)\n            # xcol = torch.argmax(probs, dim=-1)\n\n            # if(xcol == '<eos>'):\n            #   break\n            # print(xcol.shape)\n            # print(input_ids.shape)\n            # print(xcol.shape)\n            input_ids = torch.cat([input_ids, xcol], dim=-1) #1 because is it the dimension of the sequence\n    # print(input_ids)\n    count = 0\n    de_len = torch.tensor([de_vocab[token] for token in de_tokenizer(prompt)])\n    for i in input_ids[0]:\n      # print(de_len.shape)\n      if(count > de_len.shape[0]):\n      # print(i)\n      # try:\n        if(en_vocab.vocab.get_itos()[i] == '<eos>'):\n          print(\"Done\")\n          break\n        token = en_vocab.vocab.get_itos()[i]\n        generated_text += token\n\n        generated_text += ' '\n      # except:\n        # oov.append(i)\n      else:\n        count += 1\n\n    return generated_text","metadata":{"id":"ealRKUoh2ez8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=ModelArgs.max_lr)","metadata":{"id":"Pd5iUNSoAbEx","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.train()\ntrain_losses =  torch.zeros(len(train_loader))\nval_losses = torch.zeros(len(val_loader))\nwandb.init(\n    project='Encoder_decoder-From-Scratch'\n)\nfor epoch in range(ModelArgs.epoch):\n\n    count = 0\n    for de, en in train_loader:\n        logits = model(de, en, False)\n        # print(logits.shape)\n\n        batch_size, block_size, vocab = logits.shape\n        # print(\"Va: \", vocab)\n        logits = logits.view(batch_size*block_size, vocab)\n        targets = en.view(batch_size * block_size)\n        # print(\"HiiiL \", en.shape)\n        # print(\"HiiiT \", logits.shape)\n        loss = criterion(logits, targets)\n        train_losses[count] = loss.item()\n        # print(\"Loss: \", loss.item())\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        count += 1\n        # print(count)\n       \n    # count = 0\n    model.eval()\n    count = 0\n    for de, en in val_loader:\n        logits = model(de, en, False)\n        # print(logits.shape)\n        batch_size, block_size, vocab = logits.shape\n\n        logits = logits.view(batch_size*block_size, vocab)\n        # print(\"Va: \", vocab)\n        targets = en.view(batch_size * block_size)\n        loss = criterion(logits, targets)\n\n        # print(\"Loss: \", loss.item())\n        val_losses[count] = loss.item()\n\n        # optimizer.zero_grad()\n        # loss.backward()\n        # optimizer.step()\n        count += 1\n      \n    # print(\"eval\")\n    generated_text = topk_sampling(model, 'Ich fahre heute mit dem Rad zur Schule', de_tokenizer, device=ModelArgs.device, max_length=50, top_k=50, temperature=1.0)\n\n    print(generated_text)\n  \n\n    model.train()\n    wandb.log({\n      \"Train Loss\": train_losses.mean(),\n      \"Val Loss\": val_losses.mean(),\n      \"epoch\": epoch\n    })\n    print(\"Epoch: \", epoch, \"|\", \"Train Loss: \", train_losses.mean(),  \"|\", \"Val Loss: \", val_losses.mean())\n","metadata":{"id":"BE6dFrZvAbEx","colab":{"base_uri":"https://localhost:8080/","height":89},"outputId":"6a35eaaa-b1f0-4a61-d169-88241d76693b","trusted":true,"execution":{"execution_failed":"2025-03-05T13:59:00.428Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_loader)","metadata":{"id":"jZag0Wb0AbEx","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ModelArgs.en_vocab_size","metadata":{"id":"Cru3ytJir_f6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"en_vocab.vocab.get_itos()[1]","metadata":{"id":"qYNXkxsRNN9r","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"oov","metadata":{"id":"SZwrSH7y0F9V","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"[de_vocab[token] for token in de_tokenizer('Ich fahre heute mit dem Rad zur Schule')]","metadata":{"id":"YHemt6HEnylp","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"4Mtq_4E2oITX","trusted":true},"outputs":[],"execution_count":null}]}