{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch==2.3.0 torchtext==0.18.0\nimport torch\nimport torch.nn as nn","metadata":{"id":"h8C4I4M0AbEr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cb9f500f-4206-480a-aa75-4ad0391361df","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:01:20.654790Z","iopub.execute_input":"2025-03-05T14:01:20.655238Z","iopub.status.idle":"2025-03-05T14:01:25.474740Z","shell.execute_reply.started":"2025-03-05T14:01:20.655196Z","shell.execute_reply":"2025-03-05T14:01:25.474009Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.10/dist-packages (2.3.0)\nRequirement already satisfied: torchtext==0.18.0 in /usr/local/lib/python3.10/dist-packages (0.18.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2024.12.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.105)\nRequirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2.3.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.18.0) (4.67.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.18.0) (2.32.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.18.0) (1.26.4)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.6.85)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.18.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.18.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.18.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.18.0) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.18.0) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.18.0) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.18.0) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.18.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.18.0) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.18.0) (2025.1.31)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0) (1.3.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchtext==0.18.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchtext==0.18.0) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchtext==0.18.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchtext==0.18.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchtext==0.18.0) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import wandb\n# !wandb login\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"API_KEY\")\nwandb.login(key=secret_value_0)","metadata":{"id":"j4n_YxBTAbEs","colab":{"base_uri":"https://localhost:8080/"},"outputId":"238ab5e6-ca61-4638-b9e6-0e016fc56254","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:01:25.475826Z","iopub.execute_input":"2025-03-05T14:01:25.476199Z","iopub.status.idle":"2025-03-05T14:01:33.269446Z","shell.execute_reply.started":"2025-03-05T14:01:25.476160Z","shell.execute_reply":"2025-03-05T14:01:33.268721Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrajceo2031\u001b[0m (\u001b[33mrentio\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"from dataclasses import dataclass\n\n\n@dataclass\nclass ModelArgs:\n    device = 'cuda'\n    no_of_neurons = 512\n    block_size = 128\n    batch_size = 128\n    en_vocab_size = None\n    de_vocab_size = None\n    dropout = 0.1\n    epoch = 10\n    max_lr = 1e-4\n    embedding_dims = 1000\n    num_layers = 4","metadata":{"id":"sxJrSlI2AbEt","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:01:33.270769Z","iopub.execute_input":"2025-03-05T14:01:33.271134Z","iopub.status.idle":"2025-03-05T14:01:33.275700Z","shell.execute_reply.started":"2025-03-05T14:01:33.271113Z","shell.execute_reply":"2025-03-05T14:01:33.274853Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!python -m spacy download de_core_news_sm","metadata":{"id":"IYbXtTsAy5-2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7717bef8-2adc-4589-f5b5-9372e157ee7f","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:01:33.277162Z","iopub.execute_input":"2025-03-05T14:01:33.277568Z","iopub.status.idle":"2025-03-05T14:01:43.504566Z","shell.execute_reply.started":"2025-03-05T14:01:33.277544Z","shell.execute_reply":"2025-03-05T14:01:43.503282Z"}},"outputs":[{"name":"stdout","text":"Collecting de-core-news-sm==3.7.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.7.0) (3.7.5)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.11)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.5)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.5.0)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.15.1)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.67.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.11.0a2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (75.1.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (24.2)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.5.0)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.4)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.29.0)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2025.1.31)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.5)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (13.9.4)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.20.0)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (7.0.5)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.2)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.19.1)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.19.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.2)\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('de_core_news_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"if torch.cuda.is_available():\n    ModelArgs.device = 'cuda'\n    torch.set_default_device('cuda')\nelse:\n\n    torch.set_default_device('cpu')\n    ModelArgs.device='cpu'\n\nif torch.cuda.is_available():\n  torch.set_default_device(ModelArgs.device)","metadata":{"id":"9V85nJT8uLw5","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:01:43.505850Z","iopub.execute_input":"2025-03-05T14:01:43.506164Z","iopub.status.idle":"2025-03-05T14:01:43.557402Z","shell.execute_reply.started":"2025-03-05T14:01:43.506122Z","shell.execute_reply":"2025-03-05T14:01:43.556434Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"!python -m spacy download en_core_web_sm","metadata":{"id":"QCNnhvz0y7qp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bec6fe8d-d2a1-4125-c89f-c27383c2823f","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:01:43.558569Z","iopub.execute_input":"2025-03-05T14:01:43.558867Z","iopub.status.idle":"2025-03-05T14:01:53.258994Z","shell.execute_reply.started":"2025-03-05T14:01:43.558839Z","shell.execute_reply":"2025-03-05T14:01:53.258163Z"}},"outputs":[{"name":"stdout","text":"Collecting en-core-web-sm==3.7.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.11)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.0)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.11.0a2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.29.0)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.1.31)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.19.1)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch\nfrom torchtext.data.utils import get_tokenizer\nfrom collections import Counter\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torchtext.utils import download_from_url, extract_archive\nimport io\nfrom torch.utils.data import DataLoader, Dataset\n\n# Download and extract data\nurl_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\ntrain_urls = ('train.de.gz', 'train.en.gz')\nval_urls = ('val.de.gz', 'val.en.gz')\ntest_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n\ntrain_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\nval_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\ntest_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n\n# Load SpaCy tokenizers\nde_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\nen_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n\n# Build vocabulary\ndef build_vocab(filepath, tokenizer):\n    counter = Counter()\n    with io.open(filepath, encoding=\"utf8\") as f:\n        for string_ in f:\n            counter.update(tokenizer(string_))\n    vocab = build_vocab_from_iterator(\n        [counter.keys()],\n        specials=['<unk>', '<bos>', '<eos>']\n    )\n    vocab.set_default_index(vocab['<unk>'])\n    return vocab\n\nde_vocab = build_vocab(train_filepaths[0], de_tokenizer)\nModelArgs.de_vocab_size = len(de_vocab) + 1\nen_vocab = build_vocab(train_filepaths[1], en_tokenizer)\nModelArgs.en_vocab_size = len(en_vocab) + 1\n\n\ndef data_process(filepaths):\n    raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n    raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n    data = []\n\n    # Get the indices for <bos> and <eos> tokens\n    de_bos_idx = de_vocab['<bos>']\n    de_eos_idx = de_vocab['<eos>']\n    en_bos_idx = en_vocab['<bos>']\n    en_eos_idx = en_vocab['<eos>']\n\n    for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n        # Tokenize and convert to indices\n        de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)], dtype=torch.long)\n        en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)], dtype=torch.long)\n\n        # Add <bos> and <eos> tokens\n        # de_tensor_ = torch.cat([torch.tensor([de_bos_idx]), de_tensor_, torch.tensor([de_eos_idx])])\n        en_tensor_ = torch.cat([torch.tensor([en_bos_idx]), en_tensor_, torch.tensor([en_eos_idx])])\n\n        # Flip the German tensor (if required)\n        # de_tensor_ = torch.flip(de_tensor_, dims=[0])\n\n        # Append to data\n        data.append((de_tensor_, en_tensor_))\n\n    return data\n\ndef data_process_flip(filepaths):\n    raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n    raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n    data = []\n\n    # Get the indices for <bos> and <eos> tokens\n    de_bos_idx = de_vocab['<bos>']\n    de_eos_idx = de_vocab['<eos>']\n    en_bos_idx = en_vocab['<bos>']\n    en_eos_idx = en_vocab['<eos>']\n\n    for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n        # Tokenize and convert to indices\n        de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)], dtype=torch.long)\n        en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)], dtype=torch.long)\n\n        # Add <bos> and <eos> tokens\n        # de_tensor_ = torch.cat([torch.tensor([de_bos_idx]), de_tensor_, torch.tensor([de_eos_idx])])\n        en_tensor_ = torch.cat([torch.tensor([en_bos_idx]), en_tensor_, torch.tensor([en_eos_idx])])\n\n        # Flip the German tensor (if required)\n        de_tensor_ = torch.flip(de_tensor_, dims=[0])\n\n        # Append to data\n        data.append((de_tensor_, en_tensor_))\n\n    return data\n\ntrain_data = data_process_flip(train_filepaths)\nval_data = data_process(val_filepaths)\ntest_data = data_process(test_filepaths)\n\n# Create a custom Dataset class\nclass TranslationDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n# Create Dataset instances\ntrain_dataset = TranslationDataset(train_data)\nval_dataset = TranslationDataset(val_data)\ntest_dataset = TranslationDataset(test_data)\n\nfrom torch.nn.utils.rnn import pad_sequence\n\ndef collate_fn(batch, block_size=32):\n    \"\"\"\n    Collate function to pad or truncate sequences to a fixed block size.\n\n    Args:\n        batch: A list of tuples (de_tensor, en_tensor).\n        block_size: The fixed length to pad or truncate sequences to.\n\n    Returns:\n        de_batch: Padded/truncated German sequences (batch_size, block_size).\n        en_batch: Padded/truncated English sequences (batch_size, block_size).\n    \"\"\"\n    de_batch, en_batch = zip(*batch)\n\n    # Function to pad or truncate a sequence to the block size\n    def pad_or_truncate(sequence, block_size, pad_value):\n        if len(sequence) > block_size:\n            # Truncate the sequence if it's longer than block_size\n            return sequence[:block_size]\n        else:\n            # Pad the sequence if it's shorter than block_size\n            padding_length = block_size - len(sequence)\n            return torch.cat([sequence, torch.full((padding_length,), pad_value, dtype=sequence.dtype)])\n\n    # Pad or truncate each sequence in the batch\n    de_batch = [pad_or_truncate(seq, block_size, de_vocab['<pad>']) for seq in de_batch]\n    en_batch = [pad_or_truncate(seq, block_size, en_vocab['<pad>']) for seq in en_batch]\n\n    # Stack the sequences into a single tensor\n    de_batch = torch.stack(de_batch)\n    en_batch = torch.stack(en_batch)\n\n    return de_batch, en_batch\n\ngenerator = torch.Generator(device=ModelArgs.device)\n\n\n# Create DataLoader instances\nbatch_size = ModelArgs.batch_size\ntrain_loader = DataLoader(train_dataset, generator=generator, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)\nval_loader = DataLoader(val_dataset, generator=generator, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, drop_last=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, drop_last=False)\n\n# Example usage\nfor de_batch, en_batch in train_loader:\n    print(f\"German batch shape: {de_batch.shape}\")\n    print(f\"English batch shape: {en_batch.shape}\")\n    break","metadata":{"id":"7h-HLa2_xRah","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9970b3eb-8795-4ae1-d14b-8368cb671c85","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:01:53.260263Z","iopub.execute_input":"2025-03-05T14:01:53.260597Z","iopub.status.idle":"2025-03-05T14:02:10.011514Z","shell.execute_reply.started":"2025-03-05T14:01:53.260558Z","shell.execute_reply":"2025-03-05T14:02:10.010519Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \nTorchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n/usr/local/lib/python3.10/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \nTorchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n/usr/local/lib/python3.10/dist-packages/torchtext/utils.py:4: UserWarning: \n/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \nTorchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n","output_type":"stream"},{"name":"stdout","text":"German batch shape: torch.Size([128, 32])\nEnglish batch shape: torch.Size([128, 32])\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"\n# train_data","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1aId84g24ytU","outputId":"6cd4022c-93bd-4c30-a7e4-72abff5baf2e","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:02:10.014068Z","iopub.execute_input":"2025-03-05T14:02:10.014503Z","iopub.status.idle":"2025-03-05T14:02:10.018018Z","shell.execute_reply.started":"2025-03-05T14:02:10.014480Z","shell.execute_reply":"2025-03-05T14:02:10.017293Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"","metadata":{"id":"sBD_OADeAbEt","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ModelArgs.en_vocab_size","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZpUmGZmHUZ0n","outputId":"9a6986a6-208a-42ed-d8d3-6b81f3387a1f","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:02:10.019233Z","iopub.execute_input":"2025-03-05T14:02:10.019434Z","iopub.status.idle":"2025-03-05T14:02:10.034415Z","shell.execute_reply.started":"2025-03-05T14:02:10.019416Z","shell.execute_reply":"2025-03-05T14:02:10.033704Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"10838"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"x = torch.randn((ModelArgs.batch_size, 1, ModelArgs.embedding_dims))\nx1 = torch.randn((ModelArgs.batch_size, ModelArgs.block_size, ModelArgs.embedding_dims))\nprint(torch.cat([x, x1], dim=1).shape)","metadata":{"id":"hE9tqRQcAbEu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2b0460ec-465e-4968-8f44-dbc0ac02fee7","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:02:10.035076Z","iopub.execute_input":"2025-03-05T14:02:10.035348Z","iopub.status.idle":"2025-03-05T14:02:10.051286Z","shell.execute_reply.started":"2025-03-05T14:02:10.035327Z","shell.execute_reply":"2025-03-05T14:02:10.050466Z"}},"outputs":[{"name":"stdout","text":"torch.Size([128, 129, 1000])\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"\nclass InputGate(nn.Module):\n    def __init__(self, device, no_of_neurons):\n        super().__init__()\n        self.it = nn.Linear(in_features= ModelArgs.no_of_neurons + ModelArgs.embedding_dims, out_features=no_of_neurons, device=device, dtype=torch.float32)\n        self.ct_bar = nn.Linear(in_features=ModelArgs.no_of_neurons + ModelArgs.embedding_dims, out_features=no_of_neurons, device=device, dtype=torch.float32)\n\n    def forward(self, x, ht_1):\n        x = torch.cat([x, ht_1], dim=1)\n        _it = torch.nn.functional.sigmoid(self.it(x))\n        _ct_bar = torch.nn.functional.tanh(self.ct_bar(x))\n        # out = torch.nn.functional.sigmoid(self.linear(x))\n        return _it, _ct_bar","metadata":{"id":"NAPYL-KeAbEw","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:02:10.052270Z","iopub.execute_input":"2025-03-05T14:02:10.052564Z","iopub.status.idle":"2025-03-05T14:02:10.062559Z","shell.execute_reply.started":"2025-03-05T14:02:10.052535Z","shell.execute_reply":"2025-03-05T14:02:10.061806Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class OutputGate(nn.Module):\n    def __init__(self, device, no_of_neurons) -> None:\n        super().__init__()\n        self.linear = nn.Linear(in_features= ModelArgs.no_of_neurons + ModelArgs.embedding_dims, out_features=no_of_neurons, device=device, dtype=torch.float32)\n    def forward(self, x, ht_1):\n        x = torch.cat([x, ht_1], dim=1)\n        out = torch.nn.functional.sigmoid(self.linear(x))\n        return out","metadata":{"id":"MxAPFK3wAbEw","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:02:10.063321Z","iopub.execute_input":"2025-03-05T14:02:10.063670Z","iopub.status.idle":"2025-03-05T14:02:10.078920Z","shell.execute_reply.started":"2025-03-05T14:02:10.063638Z","shell.execute_reply":"2025-03-05T14:02:10.078107Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class ForgetGate(nn.Module):\n\n    def __init__(self, device, no_of_neurons):\n        super().__init__()\n        self.linear = nn.Linear(in_features=ModelArgs.no_of_neurons + ModelArgs.embedding_dims, out_features=no_of_neurons, device=device, dtype=torch.float32)\n\n    def forward(self, x, ht_1):\n        # print(\"Forgot: \", x.shape)\n        # print(\"Forget: \", ht_1.shape)\n        x = torch.cat([x, ht_1], dim=1)\n        out = torch.nn.functional.sigmoid(self.linear(x))\n        return out","metadata":{"id":"i-NW4ePRAbEw","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:02:10.079765Z","iopub.execute_input":"2025-03-05T14:02:10.080114Z","iopub.status.idle":"2025-03-05T14:02:10.093997Z","shell.execute_reply.started":"2025-03-05T14:02:10.080067Z","shell.execute_reply":"2025-03-05T14:02:10.093231Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class LSTMBlock(nn.Module):\n    def __init__(self, device, no_of_neurons):\n        super().__init__()\n        self.ip = InputGate(device=device, no_of_neurons=no_of_neurons)\n        self.op = OutputGate(device=device, no_of_neurons=no_of_neurons)\n        self.forget = ForgetGate(device=device, no_of_neurons=no_of_neurons)\n        self.no_of_neurons = no_of_neurons\n        self.device=device\n        self.linear_layer = nn.Linear(in_features=ModelArgs.no_of_neurons, out_features=ModelArgs.embedding_dims, device=device)\n    def forward(self, x, ht_1=None, outputs=None, embeds=None):\n        # print(\"Block: \", x.shape)\n        # print(\"Block: \", ht_1.shape)\n        # print(\"Block: \", ct_1.shape)\n\n        if(ht_1 is None):\n          ht_1 = torch.randn( (x.shape[0], self.no_of_neurons), device=self.device, requires_grad=True, dtype=torch.float32)\n\n        ct_1 = torch.randn((x.shape[0], self.no_of_neurons),device=self.device, requires_grad=True, dtype=torch.float32)\n        seq_len = x.shape[1]\n        # ht_1 = ht_1.unsqueeze(-1)\n        if(outputs == None):\n            # print(\"New\")\n            # print(x)\n            outputs = []\n            for t in range(seq_len):\n                # print(\"Block: \", x.shape)\n                # print(\"Block: \", ht_1.shape)\n                # print(\"Block: \", ct_1.shape)\n                xt = x[:, t, :]\n                # print(xt.shape)\n                ft = self.forget(xt, ht_1) * ct_1\n                it, ct_bar = self.ip(xt , ht_1)\n                ct_bar_prime = it * ct_bar\n                ct = ft * ct_1 + ct_bar_prime\n                ht = self.op(xt, ht_1) * torch.nn.functional.tanh(ct)\n                outputs.append(ht)\n            return ht, ct, torch.stack(outputs, dim=1)\n\n        elif(outputs is not None):\n          # print(\"Other\")\n          new_output = []\n          for t in range(seq_len):\n                # print(\"Block: \", x.shape)\n                # print(\"Block: \", ht_1.shape)\n                # print(\"Block: \", ct_1.shape)\n                xt = outputs[:, t, :]\n                # print(\"Shape: \", xt.shape)\n                xt = self.linear_layer(xt)\n                # print(\"After: \", xt.shape)\n                ft = self.forget(xt, ht_1) * ct_1\n                it, ct_bar = self.ip(xt , ht_1)\n                ct_bar_prime = it * ct_bar\n                ct = ft * ct_1 + ct_bar_prime\n                ht = self.op(xt, ht_1) * torch.nn.functional.tanh(ct)\n                new_output.append(ht)\n          return ht, ct, torch.stack(new_output, dim=1)","metadata":{"id":"8D1FnUKOAbEx","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:02:10.094784Z","iopub.execute_input":"2025-03-05T14:02:10.095054Z","iopub.status.idle":"2025-03-05T14:02:10.109893Z","shell.execute_reply.started":"2025-03-05T14:02:10.095034Z","shell.execute_reply":"2025-03-05T14:02:10.109163Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"\nclass EmbeddingTable_en(nn.Module):\n  def __init__(self, device):\n    super().__init__()\n\n    self.embed_en = nn.Embedding(num_embeddings=ModelArgs.en_vocab_size, embedding_dim=ModelArgs.embedding_dims, device=device)\n\n  def forward(self, x):\n    return self.embed_en(x)","metadata":{"id":"Y8M7hip-BGi1","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:02:10.110671Z","iopub.execute_input":"2025-03-05T14:02:10.110965Z","iopub.status.idle":"2025-03-05T14:02:10.126901Z","shell.execute_reply.started":"2025-03-05T14:02:10.110934Z","shell.execute_reply":"2025-03-05T14:02:10.126178Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"\n\nclass EmbeddingTable_de(nn.Module):\n  def __init__(self, device):\n    super().__init__()\n\n    self.embed_de =  nn.Embedding(num_embeddings=ModelArgs.de_vocab_size, embedding_dim=ModelArgs.embedding_dims, device=device)\n\n  def forward(self, x):\n    # print('Indie: ', x)\n    return self.embed_de(x)","metadata":{"id":"VjEUjJhWKhkM","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:02:10.127725Z","iopub.execute_input":"2025-03-05T14:02:10.128040Z","iopub.status.idle":"2025-03-05T14:02:10.141531Z","shell.execute_reply.started":"2025-03-05T14:02:10.128010Z","shell.execute_reply":"2025-03-05T14:02:10.140899Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, device, no_of_neurons, out_features):\n        super().__init__()\n        self.block1 = LSTMBlock(device=device, no_of_neurons=no_of_neurons)\n        # self.block2 = LSTMBlock(device=device, no_of_neurons=no_of_neurons)\n        # self.embeds_table_en = EmbeddingTable_en(device=device)\n        self.embeds_table_de = EmbeddingTable_de(device=device)\n        # self.ht_1 = torch.randn(ModelArgs.batch_size, no_of_neurons, device=device, requires_grad=True, dtype=torch.float32)\n        # self.ct_1 = torch.randn(ModelArgs.batch_size, no_of_neurons,device=device, requires_grad=True, dtype=torch.float32)\n        # self.output = nn.Linear(in_features=ModelArgs.no_of_neurons, out_features=out_features, device=device, dtype=torch.float32)\n        self.dropout = nn.Dropout(p=ModelArgs.dropout)\n        # self.embedding = nn.Embedding()\n\n    def forward(self, x, outputs=None, initial=None):\n        # x =\n        # print(\"LSTM: \",x.shape)\n        # print(\"LSTM: \", self.ht_1.shape)\n        # print(\"LSTM: \", self.ct_1.shape)\n        # if(encoder):\n        if(initial is not None and initial is True):\n          x = self.embeds_table_de(x)\n        # print(x.shape)\n        # elif(decoder):\n        #   x = self.embeds_table_en(x)\n        if(outputs is not None):\n          # print(outputs)\n          ht, ct, outputs = self.block1(x, outputs=outputs)\n        elif(outputs is None):\n          ht, ct, outputs = self.block1(x)\n        # print(ht.shape)\n        # print(ct.shape)\n        # ht, ct = self.block2(x, ht, ct)\n        # ht = self.dropout(ht)\n        # print(\"Aft: \", outputs.shape)\n        # out = self.output(ht)\n        return  ht, ct, outputs, self.embeds_table_de","metadata":{"id":"yONjsNpKAbEx","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:02:10.142428Z","iopub.execute_input":"2025-03-05T14:02:10.142714Z","iopub.status.idle":"2025-03-05T14:02:10.157203Z","shell.execute_reply.started":"2025-03-05T14:02:10.142693Z","shell.execute_reply":"2025-03-05T14:02:10.156335Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, device, no_of_neurons, out_features):\n        super().__init__()\n        self.block1 = LSTMBlock(device=device, no_of_neurons=no_of_neurons)\n        # self.block2 = LSTMBlock(device=device, no_of_neurons=no_of_neurons)\n        self.embeds_table_en = EmbeddingTable_en(device=device)\n        # self.embeds_table_de = EmbeddingTable_de(device=device)\n        # self.ht_1 = torch.randn(ModelArgs.batch_size, no_of_neurons, device=device, requires_grad=True, dtype=torch.float32)\n        # self.ct_1 = torch.randn(ModelArgs.batch_size, no_of_neurons,device=device, requires_grad=True, dtype=torch.float32)\n        self.output = nn.Linear(in_features=ModelArgs.no_of_neurons, out_features=ModelArgs.en_vocab_size, device=device, dtype=torch.float32)\n        self.dropout = nn.Dropout(p=ModelArgs.dropout)\n        # self.embedding = nn.Embedding()\n\n    def forward(self, x, ctx=None, inf=None, embeds=None, initial=None, outputs=None):\n        # x =\n        # print(\"LSTM: \",x.shape)\n        # print(\"LSTM: \", self.ht_1.shape)\n        # print(\"LSTM: \", self.ct_1.shape)\n        # if(encoder):\n        #   x = self.embeds_table_de(x)\n          # print(x.shape)\n        # elif(decoder):\n        if(inf is not True and initial is True):\n          x = self.embeds_table_en(x)\n        if(inf is True):\n          # print(\"Before: \", x.shape)\n          x = embeds(x)\n          # print(\"After: \", x.shape)\n        ht, ct, outputs = self.block1(x, ctx, outputs=outputs)\n        # print(ht.shape)\n        # print(ct.shape)\n        # ht, ct = self.block2(x, ht, ct)\n        out = self.dropout(outputs)\n        # print(\"After: \", outputs.shape)\n        out = self.output(out)\n        return  out, outputs","metadata":{"id":"HaOMDN53MtyC","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:02:10.157966Z","iopub.execute_input":"2025-03-05T14:02:10.158395Z","iopub.status.idle":"2025-03-05T14:02:10.175385Z","shell.execute_reply.started":"2025-03-05T14:02:10.158364Z","shell.execute_reply":"2025-03-05T14:02:10.174720Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"\nclass Seq2Seq(nn.Module):\n\n    def __init__(self, device, no_of_neurons, out_features):\n        super().__init__()\n\n        # self.encoder = Encoder(device, no_of_neurons, out_features)\n        # self.decoder = Decoder(device, no_of_neurons, out_features)\n        self.encoders = nn.ModuleList(Encoder(device, no_of_neurons, out_features) for _  in range(ModelArgs.num_layers))\n        self.decoders = nn.ModuleList(Decoder(device, no_of_neurons, out_features) for x in range(ModelArgs.num_layers))\n\n    def forward(self, x, y=None, inf=None):\n\n        count = 0\n        for i in self.encoders:\n          if(count == 0):\n            ht_encoder, ct_encoder,outputs_encoder, embeds_de = i(x, initial=True)\n            # x = ht_encoder\n          else:\n            ht_encoder, ct_encoder,outputs_encoder, embeds_de = i(x, outputs=outputs_encoder)\n            # x = ht_encoder\n          count += 1\n\n        res = None\n        count = 0\n        if(y is not None and inf==False):\n          for i in self.decoders:\n\n            # print(\"Hiii\")\n            if(count == 0):\n              y , outputs = i(y, ht_encoder, inf, embeds_de, True, outputs=outputs_encoder)\n              # res = x\n            else:\n              y, outputs = i(y, ht_encoder, inf, embeds_de, outputs=outputs)\n              # res = x\n            # return res\n          # elif(y is not None and inf==False):\n            # print(\"Here\")\n            # res = self.decoder(y, ht_encoder)\n            # return res\n            count += 1\n          return y\n\n\n        elif(inf==True and y is None):\n          x_init = x\n          count = 0\n          for i in self.decoders:\n\n            if(count == 0):\n              x, outputs = i(x, ht_encoder, inf, embeds_de, True)\n            # res = x\n\n            else:\n              x, outputs = i(x_init, ht_encoder, inf, embeds_de, outputs=outputs)\n\n            count += 1\n          return x","metadata":{"id":"FdhO0rZpAbEx","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:02:47.901798Z","iopub.execute_input":"2025-03-05T14:02:47.902151Z","iopub.status.idle":"2025-03-05T14:02:47.909875Z","shell.execute_reply.started":"2025-03-05T14:02:47.902117Z","shell.execute_reply":"2025-03-05T14:02:47.909110Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"model = Seq2Seq(device=ModelArgs.device, no_of_neurons=ModelArgs.no_of_neurons, out_features=1)\nmodel = model.to(ModelArgs.device)","metadata":{"id":"xFj17tN-AbEx","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:02:48.637643Z","iopub.execute_input":"2025-03-05T14:02:48.637949Z","iopub.status.idle":"2025-03-05T14:02:48.661801Z","shell.execute_reply.started":"2025-03-05T14:02:48.637923Z","shell.execute_reply":"2025-03-05T14:02:48.661153Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"!pip install torchinfo\n\nfrom torchinfo import summary\n\n# x = torch.randint(0, 100, (ModelArgs.batch_size,ModelArgs.block_size))  # Random integer between 0 and 100\nx,y = next(iter(train_loader))\nx = x.to(ModelArgs.device)\ny = y.to(ModelArgs.device)\n\nsummary(model=model,\n        input_data=[x,y, False],\n        # input_size=(ModelArgs.batch_size, ModelArgs.block_size, ModelArgs.embeddings_dims),\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"])\n","metadata":{"id":"djd0vnbzAbEx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"db9a8217-2593-4588-808b-12430f01265e","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:02:49.063467Z","iopub.execute_input":"2025-03-05T14:02:49.063705Z","iopub.status.idle":"2025-03-05T14:02:53.087343Z","shell.execute_reply.started":"2025-03-05T14:02:49.063684Z","shell.execute_reply":"2025-03-05T14:02:53.086448Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"=======================================================================================================================================\nLayer (type (var_name))                                 Input Shape          Output Shape         Param #              Trainable\n=======================================================================================================================================\nSeq2Seq (Seq2Seq)                                       [128, 32]            [128, 32, 10838]     --                   True\n├─ModuleList (encoders)                                 --                   --                   --                   True\n│    └─Encoder (0)                                      [128, 32]            [128, 512]           --                   True\n│    │    └─EmbeddingTable_de (embeds_table_de)         [128, 32]            [128, 32, 1000]      19,215,000           True\n│    │    └─LSTMBlock (block1)                          [128, 32, 1000]      [128, 512]           3,611,624            True\n│    └─Encoder (1)                                      [128, 32]            [128, 512]           19,215,000           True\n│    │    └─LSTMBlock (block1)                          [128, 32]            [128, 512]           3,611,624            True\n│    └─Encoder (2)                                      [128, 32]            [128, 512]           19,215,000           True\n│    │    └─LSTMBlock (block1)                          [128, 32]            [128, 512]           3,611,624            True\n│    └─Encoder (3)                                      [128, 32]            [128, 512]           19,215,000           True\n│    │    └─LSTMBlock (block1)                          [128, 32]            [128, 512]           3,611,624            True\n├─ModuleList (decoders)                                 --                   --                   --                   True\n│    └─Decoder (0)                                      [128, 32]            [128, 32, 10838]     --                   True\n│    │    └─EmbeddingTable_en (embeds_table_en)         [128, 32]            [128, 32, 1000]      10,838,000           True\n│    │    └─LSTMBlock (block1)                          [128, 32, 1000]      [128, 512]           3,611,624            True\n│    │    └─Dropout (dropout)                           [128, 32, 512]       [128, 32, 512]       --                   --\n│    │    └─Linear (output)                             [128, 32, 512]       [128, 32, 10838]     5,559,894            True\n│    └─Decoder (1)                                      [128, 32, 10838]     [128, 32, 10838]     10,838,000           True\n│    │    └─LSTMBlock (block1)                          [128, 32, 10838]     [128, 512]           3,611,624            True\n│    │    └─Dropout (dropout)                           [128, 32, 512]       [128, 32, 512]       --                   --\n│    │    └─Linear (output)                             [128, 32, 512]       [128, 32, 10838]     5,559,894            True\n│    └─Decoder (2)                                      [128, 32, 10838]     [128, 32, 10838]     10,838,000           True\n│    │    └─LSTMBlock (block1)                          [128, 32, 10838]     [128, 512]           3,611,624            True\n│    │    └─Dropout (dropout)                           [128, 32, 512]       [128, 32, 512]       --                   --\n│    │    └─Linear (output)                             [128, 32, 512]       [128, 32, 10838]     5,559,894            True\n│    └─Decoder (3)                                      [128, 32, 10838]     [128, 32, 10838]     10,838,000           True\n│    │    └─LSTMBlock (block1)                          [128, 32, 10838]     [128, 512]           3,611,624            True\n│    │    └─Dropout (dropout)                           [128, 32, 512]       [128, 32, 512]       --                   --\n│    │    └─Linear (output)                             [128, 32, 512]       [128, 32, 10838]     5,559,894            True\n=======================================================================================================================================\nTotal params: 171,344,568\nTrainable params: 171,344,568\nNon-trainable params: 0\nTotal mult-adds (G): 122.94\n=======================================================================================================================================\nInput size (MB): 0.07\nForward/backward pass size (MB): 2252.34\nParams size (MB): 322.69\nEstimated Total Size (MB): 2575.10\n======================================================================================================================================="},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"\n\n\n# x,y = next(iter(train_loader))\n# x.shape","metadata":{"id":"YzWzrztJ6_-J","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:02:53.088441Z","iopub.execute_input":"2025-03-05T14:02:53.088742Z","iopub.status.idle":"2025-03-05T14:02:53.092161Z","shell.execute_reply.started":"2025-03-05T14:02:53.088715Z","shell.execute_reply":"2025-03-05T14:02:53.091286Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# from andrej karapathy github\nimport torch.nn.functional as F\ndef topk_sampling(model, prompt, tokenizer, device, max_length=50, top_k=50, temperature=1.0):\n\n    # input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n\n    input_ids = torch.tensor([de_vocab[token] for token in de_tokenizer(prompt)]).unsqueeze(0)\n    oov = []\n    generated_text = \"\"\n    for _ in range(max_length):\n        with torch.no_grad():\n            outputs = model(input_ids, None, True)\n            logits = outputs[:, -1, :]\n\n            probs = F.softmax(logits, dim=-1)\n\n            # Top-k filtering\n            top_k_probs, top_k_indices = torch.topk(probs, top_k, dim=-1)\n#\n            # Apply temperature scaling\n            # probs = probs / temperature\n\n            # Sample from top-k\n            next_token = torch.multinomial(top_k_probs, num_samples=1)\n\n            # generated_tokens.append(next_token.item())\n\n            xcol = torch.gather(top_k_indices, -1, next_token)\n            # xcol = torch.argmax(probs, dim=-1)\n\n            # if(xcol == '<eos>'):\n            #   break\n            # print(xcol.shape)\n            # print(input_ids.shape)\n            # print(xcol.shape)\n            input_ids = torch.cat([input_ids, xcol], dim=-1) #1 because is it the dimension of the sequence\n    # print(input_ids)\n    count = 0\n    de_len = torch.tensor([de_vocab[token] for token in de_tokenizer(prompt)])\n    for i in input_ids[0]:\n      # print(de_len.shape)\n      if(count > de_len.shape[0]):\n      # print(i)\n      # try:\n        if(en_vocab.vocab.get_itos()[i] == '<eos>'):\n          print(\"Done\")\n          break\n        token = en_vocab.vocab.get_itos()[i]\n        generated_text += token\n\n        generated_text += ' '\n      # except:\n        # oov.append(i)\n      else:\n        count += 1\n\n    return generated_text","metadata":{"id":"ealRKUoh2ez8","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:02:53.093341Z","iopub.execute_input":"2025-03-05T14:02:53.093577Z","iopub.status.idle":"2025-03-05T14:02:53.109597Z","shell.execute_reply.started":"2025-03-05T14:02:53.093558Z","shell.execute_reply":"2025-03-05T14:02:53.108695Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=ModelArgs.max_lr)","metadata":{"id":"Pd5iUNSoAbEx","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:03:00.357719Z","iopub.execute_input":"2025-03-05T14:03:00.358040Z","iopub.status.idle":"2025-03-05T14:03:01.618205Z","shell.execute_reply.started":"2025-03-05T14:03:00.358015Z","shell.execute_reply":"2025-03-05T14:03:01.617184Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"model.train()\ntrain_losses =  torch.zeros(len(train_loader))\nval_losses = torch.zeros(len(val_loader))\nwandb.init(\n    project='Encoder_decoder-From-Scratch'\n)\nfor epoch in range(ModelArgs.epoch):\n\n    count = 0\n    for de, en in train_loader:\n        logits = model(de, en, False)\n        # print(logits.shape)\n\n        batch_size, block_size, vocab = logits.shape\n        # print(\"Va: \", vocab)\n        logits = logits.view(batch_size*block_size, vocab)\n        targets = en.view(batch_size * block_size)\n        # print(\"HiiiL \", en.shape)\n        # print(\"HiiiT \", logits.shape)\n        loss = criterion(logits, targets)\n        train_losses[count] = loss.item()\n        # print(\"Loss: \", loss.item())\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        count += 1\n        # print(count)\n       \n    # count = 0\n    model.eval()\n    count = 0\n    for de, en in val_loader:\n        logits = model(de, en, False)\n        # print(logits.shape)\n        batch_size, block_size, vocab = logits.shape\n\n        logits = logits.view(batch_size*block_size, vocab)\n        # print(\"Va: \", vocab)\n        targets = en.view(batch_size * block_size)\n        loss = criterion(logits, targets)\n\n        # print(\"Loss: \", loss.item())\n        val_losses[count] = loss.item()\n\n        # optimizer.zero_grad()\n        # loss.backward()\n        # optimizer.step()\n        count += 1\n      \n    # print(\"eval\")\n    generated_text = topk_sampling(model, 'Ich fahre heute mit dem Rad zur Schule', de_tokenizer, device=ModelArgs.device, max_length=50, top_k=50, temperature=1.0)\n\n    print(generated_text)\n  \n\n    model.train()\n    wandb.log({\n      \"Train Loss\": train_losses.mean(),\n      \"Val Loss\": val_losses.mean(),\n      \"epoch\": epoch\n    })\n    print(\"Epoch: \", epoch, \"|\", \"Train Loss: \", train_losses.mean(),  \"|\", \"Val Loss: \", val_losses.mean())\n","metadata":{"id":"BE6dFrZvAbEx","colab":{"base_uri":"https://localhost:8080/","height":89},"outputId":"6a35eaaa-b1f0-4a61-d169-88241d76693b","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:03:01.880159Z","iopub.execute_input":"2025-03-05T14:03:01.880730Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250305_140301-hf1mplwc</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/rentio/Encoder_decoder-From-Scratch/runs/hf1mplwc' target=\"_blank\">decent-disco-25</a></strong> to <a href='https://wandb.ai/rentio/Encoder_decoder-From-Scratch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/rentio/Encoder_decoder-From-Scratch' target=\"_blank\">https://wandb.ai/rentio/Encoder_decoder-From-Scratch</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/rentio/Encoder_decoder-From-Scratch/runs/hf1mplwc' target=\"_blank\">https://wandb.ai/rentio/Encoder_decoder-From-Scratch/runs/hf1mplwc</a>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"len(train_loader)","metadata":{"id":"jZag0Wb0AbEx","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ModelArgs.en_vocab_size","metadata":{"id":"Cru3ytJir_f6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"en_vocab.vocab.get_itos()[1]","metadata":{"id":"qYNXkxsRNN9r","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"oov","metadata":{"id":"SZwrSH7y0F9V","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"[de_vocab[token] for token in de_tokenizer('Ich fahre heute mit dem Rad zur Schule')]","metadata":{"id":"YHemt6HEnylp","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"4Mtq_4E2oITX","trusted":true},"outputs":[],"execution_count":null}]}