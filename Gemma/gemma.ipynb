{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from torchtune.modules import RMSNorm\n",
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    #Hyperparameters\n",
    "\n",
    "    block_size = 64\n",
    "    batch_size = 64\n",
    "    embeddings_dims = 768\n",
    "    attn_dropout = 0.1\n",
    "    no_of_heads = 8 #IMP needs to be thoroughly calculated\n",
    "    dropout = 0.1\n",
    "    epochs = 100\n",
    "    max_lr = 2.5e-4\n",
    "    no_of_decoder_layers = 6 #IMP needs to be thoroughly calculated\n",
    "    weight_decay_optim = 0.1\n",
    "    beta_1 = 0.9\n",
    "    beta_2 = 0.95\n",
    "    device = 'cuda'\n",
    "    no_kv_heads = 2\n",
    "    vocab_size = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-15 20:26:54--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.5’\n",
      "\n",
      "input.txt.5         100%[===================>]   1.06M  2.13MB/s    in 0.5s    \n",
      "\n",
      "2024-08-15 20:26:55 (2.13 MB/s) - ‘input.txt.5’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Collab setup\n",
    "data_path = Path('data')\n",
    "data_path.mkdir(exist_ok=True)\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "!cp input.txt data/input.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-15 20:26:55--  https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 278779 (272K) [text/plain]\n",
      "Saving to: ‘botchan.txt.5’\n",
      "\n",
      "botchan.txt.5       100%[===================>] 272.25K  --.-KB/s    in 0.09s   \n",
      "\n",
      "2024-08-15 20:26:56 (2.89 MB/s) - ‘botchan.txt.5’ saved [278779/278779]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
    "!cp botchan.txt data/botchan.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets\n",
    "\n",
    "# Using tinyshakespeare\n",
    "\n",
    "with open('data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=botchan.txt --model_prefix=m --vocab_size=2000\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: botchan.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=144687\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 16112 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 9165\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 9165 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3923 obj=8.71845 num_tokens=20449 num_tokens/piece=5.21259\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3921 obj=8.66278 num_tokens=20450 num_tokens/piece=5.21551\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2940 obj=8.96602 num_tokens=22797 num_tokens/piece=7.75408\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2940 obj=8.88624 num_tokens=22798 num_tokens/piece=7.75442\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2205 obj=9.2744 num_tokens=25530 num_tokens/piece=11.5782\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2205 obj=9.18615 num_tokens=25533 num_tokens/piece=11.5796\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2200 obj=9.18811 num_tokens=25552 num_tokens/piece=11.6145\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2200 obj=9.18747 num_tokens=25552 num_tokens/piece=11.6145\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=2000')\n",
    "\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(sp.encode_as_ids(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - ModelArgs.block_size, (ModelArgs.batch_size,))\n",
    "    x = torch.stack([data[i:i+ModelArgs.block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+ModelArgs.block_size+1] for i in ix])\n",
    "    x, y = x.to(ModelArgs.device), y.to(ModelArgs.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims\n",
    "    ):  \n",
    "        super().__init__()\n",
    "        self.rmsnorm_layer = RMSNorm(dim=embeddings_dims)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.rmsnorm_layer(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class RotaryEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        batch_size: int = ModelArgs.batch_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeddings_dims = embeddings_dims\n",
    "        self.block_size = block_size\n",
    "        self.batch_size = batch_size\n",
    "        self.theta = 0  \n",
    "\n",
    "        \n",
    "    # def init_matrix(self, seq_len):\n",
    "    #         self.matrix = torch.zeros((seq_len, self.embeddings_dims, self.embeddings_dims), dtype=torch.float32, device=ModelArgs.device, requires_grad=False)\n",
    "    #         for pos in range(seq_len):\n",
    "    #             for j in range(1, self.embeddings_dims // 2):\n",
    "    #                 self.theta = 10000 ** (-2*(pos-1) / self.embeddings_dims)\n",
    "    #                 self.matrix[pos, 2*j + 1, 2*j + 1] = np.cos((pos*self.theta))\n",
    "    #                 self.matrix[pos, 2*j + 1, j + 1] = -np.sin((pos* self.theta))\n",
    "    #                 self.matrix[pos, 2*j , 2*j ] = -np.cos((pos* self.theta))\n",
    "    #                 self.matrix[pos, 2*j + 1, 2*j + 1] = np.sin((pos* self.theta))\n",
    "    #         return self.matrix\n",
    "    \n",
    "    def init_matrix(self, seq_len):\n",
    "        self.matrix = torch.zeros((seq_len, self.embeddings_dims, self.embeddings_dims), device=ModelArgs.device, requires_grad=False)\n",
    "        \n",
    "        positions = torch.arange(seq_len, device=ModelArgs.device).unsqueeze(1)\n",
    "        # dims = torch.arange(1, self.embeddings_dims // 2, device=ModelArgs.device, dtype=torch.float32)\n",
    "        theta = 10000 ** (-2 * (positions - 1) / self.embeddings_dims)\n",
    "        angles = positions * theta\n",
    "        \n",
    "        cos_angles = torch.cos(angles)\n",
    "        sin_angles = torch.sin(angles)\n",
    "        \n",
    "        indices = torch.arange(self.embeddings_dims, device=ModelArgs.device)\n",
    "        # print(indices)\n",
    "        # print(indices.shape)\n",
    "        # print(indices[::2])\n",
    "        even_indices = indices[::2]\n",
    "        odd_indices = indices[1::2]\n",
    "        \n",
    "        self.matrix[:, even_indices, even_indices] = cos_angles\n",
    "        self.matrix[:, odd_indices, odd_indices] = sin_angles\n",
    "        self.matrix[:, odd_indices, even_indices] = -sin_angles\n",
    "        self.matrix[:, even_indices, odd_indices] = cos_angles\n",
    "        \n",
    "        return self.matrix\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B,T,C = x.shape\n",
    "        # print(\"MATRIX:\",x)\n",
    "        if(x > self.block_size):\n",
    "            matrix = self.init_matrix(x)\n",
    "            return matrix\n",
    "        else:\n",
    "            matrix = self.init_matrix(self.block_size)\n",
    "            \n",
    "            return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryAttentionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "        no_of_heads: int = ModelArgs.no_of_heads,\n",
    "        attn_dropout: int = ModelArgs.attn_dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.head_size = embeddings_dims // no_of_heads\n",
    "        self.query = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=ModelArgs.device, bias=False)\n",
    "        self.key = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=ModelArgs.device, bias=False)\n",
    "        self.value = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=ModelArgs.device, bias=False)\n",
    "        self.rotary_matrix = RotaryEmbeddings(embeddings_dims=embeddings_dims)\n",
    "        self.dropout = nn.Dropout(p = attn_dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # print(x.shape)\n",
    "        batch, block_size, embeddings_dims = x.shape\n",
    "        query = self.query(x)\n",
    "        # print(query)\n",
    "        key = self.key(x)\n",
    "        values = self.value(x)\n",
    "        matrix = self.rotary_matrix(block_size)\n",
    "        \n",
    "        # print(matrix.shape)\n",
    "        # print(query.shape)\n",
    "        masked = torch.tril(torch.ones((block_size, block_size), device=ModelArgs.device, requires_grad=False))\n",
    "        rotary_query = matrix @ query.permute(1,2,0) # (B,T, C,C) @ (B,T,C) -> (B,C,T) = (B,T,C,T)\n",
    "        rotary_key = matrix @ key.permute(1,2,0)  #  (B,T, C,C  ) @ (B,T,C) -> (B,C,T) = (B,T,C,T)\n",
    "        weights = rotary_query.permute(2,0,1) @ rotary_key.permute(2,0,1).transpose(-2, -1)#(B,T,C,T) @ (B,T,C,T) = (T,C,C,T)\n",
    "        weights_masked = weights.masked_fill(masked == 0, float('-inf'))\n",
    "        scaled_weights = weights_masked / (torch.sqrt(torch.tensor(key.shape[-1])))\n",
    "        scaled_weights = F.softmax(scaled_weights, dim=-1)\n",
    "        value = scaled_weights @ values\n",
    "        out = self.dropout(value)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MQA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        no_of_kv_heads: int = ModelArgs.no_of_heads,\n",
    "        no_of_heads: int = ModelArgs.no_of_heads\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.no_of_kv_heads = no_of_kv_heads\n",
    "        self.no_of_q_heads = no_of_heads // no_of_kv_heads\n",
    "        self.head_size = embeddings_dims // self.no_of_q_heads\n",
    "        self.rotary_matrix = RotaryEmbeddings(embeddings_dims=embeddings_dims)\n",
    "        # self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=ModelArgs.device, bias=False)\n",
    "        self.key = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=ModelArgs.device,  bias=False)\n",
    "        self.value = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=ModelArgs.device, bias=False)\n",
    "        self.dropout = nn.Dropout(p = ModelArgs.attn_dropout)\n",
    "        self.linear_layer = nn.Linear(in_features=4 * embeddings_dims, out_features=embeddings_dims, device=ModelArgs.device, bias=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def scaled_dot_product(self, q, k, v, block_size, matrix):\n",
    "\n",
    "            masked = torch.tril(torch.ones((block_size, block_size), device=ModelArgs.device, requires_grad=False))\n",
    "            rotary_query = matrix @ q.permute(1,2,0) # (B,T, C,C) @ (B,T,C) -> (B,C,T) = (B,T,C,T)\n",
    "            rotary_key = matrix @ k.permute(1,2,0)  #  (B,T, C,C  ) @ (B,T,C) -> (B,C,T) = (B,T,C,T)\n",
    "            weights = rotary_query.permute(2,0,1) @ rotary_key.permute(2,0,1).transpose(-2, -1)#(B,T,C,T) @ (B,T,C,T) = (T,C,C,T)\n",
    "            weights_masked = weights.masked_fill(masked == 0, float('-inf'))\n",
    "            scaled_weights = weights_masked / (torch.sqrt(torch.tensor(k.shape[-1])))\n",
    "            scaled_weights = F.softmax(scaled_weights, dim=-1)\n",
    "            value = scaled_weights @ v\n",
    "            out = self.dropout(value)\n",
    "            return value\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # print(\"MQA: \", x.shape)\n",
    "        batch, block_size, embeddings_dims = x.shape\n",
    "        multi_query = nn.ModuleList([nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=ModelArgs.device, bias=False) for _ in range(self.no_of_q_heads)])\n",
    "        # query = self.query(x)\n",
    "        matrix = self.rotary_matrix(block_size)\n",
    "            \n",
    "\n",
    "        key = self.key(x)\n",
    "        values = self.value(x)\n",
    "\n",
    "        multi_query_concat = torch.cat([self.scaled_dot_product(query(x), key, values, block_size, matrix) for query in multi_query], dim=-1)\n",
    "  \n",
    "        \n",
    "        linear_layer= self.linear_layer(multi_query_concat)\n",
    "        out = self.dropout(linear_layer)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeGLU(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_layer1 = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=ModelArgs.device, bias=False)\n",
    "        self.linear_layer2 = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=ModelArgs.device, bias=False)\n",
    "        self.linear_layer3 = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=ModelArgs.device, bias=False)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        gelu_res = nn.functional.gelu(self.linear_layer1(x))\n",
    "        x_V = self.linear_layer2(x)\n",
    "        res = torch.mul(gelu_res, x_V)\n",
    "        out = self.linear_layer3(res)\n",
    "        return out\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self,\n",
    "                  embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "                  block_size: int = ModelArgs.block_size,\n",
    "                  vocab_size: int = ModelArgs.vocab_size,\n",
    "                   dropout = ModelArgs.dropout\n",
    "                 \n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=ModelArgs.device)\n",
    "        self.gglu = GeGLU(block_size=block_size, embeddings_dims=embeddings_dims)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.gglu(x)\n",
    "        x = self.linear_layer(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "                dropout = ModelArgs.dropout,\n",
    "                block_size: int = ModelArgs.block_size,\n",
    "                vocab_size: int = ModelArgs.vocab_size,\n",
    "                 \n",
    "                 ) :\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.feedforward_network = FFN(embeddings_dims=embeddings_dims, block_size=block_size, vocab_size=vocab_size)\n",
    "        self.mqa = MQA(embeddings_dims=embeddings_dims, block_size=block_size, no_of_kv_heads=ModelArgs.no_kv_heads, no_of_heads=ModelArgs.no_of_heads)\n",
    "        # self.norm = Normalization(embeddings_dims=embeddings_dims)\n",
    "        self.norm = Normalization(embeddings_dims=embeddings_dims)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.norm(x + self.mqa(x))\n",
    "        x = self.norm(x + self.feedforward_network(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gemma(nn.Module):\n",
    "    def __init__(self, \n",
    "                  embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "                  no_of_decoder_layers: int = ModelArgs.no_of_decoder_layers,\n",
    "                  block_size: int = ModelArgs.block_size,\n",
    "                  vocab_size: int = ModelArgs.vocab_size,\n",
    "                  dropout = ModelArgs.dropout\n",
    "                 \n",
    "                 ) :\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embeddings_dims, device=ModelArgs.device)\n",
    "        self.decoder = nn.Sequential(*[DecoderLayer(embeddings_dims=embeddings_dims, block_size=block_size, vocab_size=vocab_size, dropout=dropout) for _ in range(no_of_decoder_layers)])\n",
    "        self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=vocab_size, device=ModelArgs.device)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        self.norm = Normalization(embeddings_dims)\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.decoder(x)\n",
    "        # x = self.norm(x)\n",
    "        x = self.linear_layer(x)\n",
    "        # out = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "ModelArgs.device = device\n",
    "model = Gemma(embeddings_dims=ModelArgs.embeddings_dims, block_size=ModelArgs.block_size, vocab_size=ModelArgs.vocab_size, dropout=ModelArgs.dropout)\n",
    "model = model.to(ModelArgs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=======================================================================================================================================\n",
       "Layer (type (var_name))                                 Input Shape          Output Shape         Param #              Trainable\n",
       "=======================================================================================================================================\n",
       "Gemma (Gemma)                                           [64, 64]             [64, 64, 2000]       768                  True\n",
       "├─Embedding (embeddings)                                [64, 64]             [64, 64, 768]        1,536,000            True\n",
       "├─Dropout (dropout)                                     [64, 64, 768]        [64, 64, 768]        --                   --\n",
       "├─Sequential (decoder)                                  [64, 64, 768]        [64, 64, 768]        --                   True\n",
       "│    └─DecoderLayer (0)                                 [64, 64, 768]        [64, 64, 768]        --                   True\n",
       "│    │    └─MQA (mqa)                                   [64, 64, 768]        [64, 64, 768]        3,538,944            True\n",
       "│    │    └─Normalization (norm)                        [64, 64, 768]        [64, 64, 768]        768                  True\n",
       "│    │    └─FFN (feedforward_network)                   [64, 64, 768]        [64, 64, 768]        2,360,064            True\n",
       "│    │    └─Normalization (norm)                        [64, 64, 768]        [64, 64, 768]        (recursive)          True\n",
       "│    └─DecoderLayer (1)                                 [64, 64, 768]        [64, 64, 768]        --                   True\n",
       "│    │    └─MQA (mqa)                                   [64, 64, 768]        [64, 64, 768]        3,538,944            True\n",
       "│    │    └─Normalization (norm)                        [64, 64, 768]        [64, 64, 768]        768                  True\n",
       "│    │    └─FFN (feedforward_network)                   [64, 64, 768]        [64, 64, 768]        2,360,064            True\n",
       "│    │    └─Normalization (norm)                        [64, 64, 768]        [64, 64, 768]        (recursive)          True\n",
       "│    └─DecoderLayer (2)                                 [64, 64, 768]        [64, 64, 768]        --                   True\n",
       "│    │    └─MQA (mqa)                                   [64, 64, 768]        [64, 64, 768]        3,538,944            True\n",
       "│    │    └─Normalization (norm)                        [64, 64, 768]        [64, 64, 768]        768                  True\n",
       "│    │    └─FFN (feedforward_network)                   [64, 64, 768]        [64, 64, 768]        2,360,064            True\n",
       "│    │    └─Normalization (norm)                        [64, 64, 768]        [64, 64, 768]        (recursive)          True\n",
       "│    └─DecoderLayer (3)                                 [64, 64, 768]        [64, 64, 768]        --                   True\n",
       "│    │    └─MQA (mqa)                                   [64, 64, 768]        [64, 64, 768]        3,538,944            True\n",
       "│    │    └─Normalization (norm)                        [64, 64, 768]        [64, 64, 768]        768                  True\n",
       "│    │    └─FFN (feedforward_network)                   [64, 64, 768]        [64, 64, 768]        2,360,064            True\n",
       "│    │    └─Normalization (norm)                        [64, 64, 768]        [64, 64, 768]        (recursive)          True\n",
       "│    └─DecoderLayer (4)                                 [64, 64, 768]        [64, 64, 768]        --                   True\n",
       "│    │    └─MQA (mqa)                                   [64, 64, 768]        [64, 64, 768]        3,538,944            True\n",
       "│    │    └─Normalization (norm)                        [64, 64, 768]        [64, 64, 768]        768                  True\n",
       "│    │    └─FFN (feedforward_network)                   [64, 64, 768]        [64, 64, 768]        2,360,064            True\n",
       "│    │    └─Normalization (norm)                        [64, 64, 768]        [64, 64, 768]        (recursive)          True\n",
       "│    └─DecoderLayer (5)                                 [64, 64, 768]        [64, 64, 768]        --                   True\n",
       "│    │    └─MQA (mqa)                                   [64, 64, 768]        [64, 64, 768]        3,538,944            True\n",
       "│    │    └─Normalization (norm)                        [64, 64, 768]        [64, 64, 768]        768                  True\n",
       "│    │    └─FFN (feedforward_network)                   [64, 64, 768]        [64, 64, 768]        2,360,064            True\n",
       "│    │    └─Normalization (norm)                        [64, 64, 768]        [64, 64, 768]        (recursive)          True\n",
       "├─Linear (linear_layer)                                 [64, 64, 768]        [64, 64, 2000]       1,538,000            True\n",
       "=======================================================================================================================================\n",
       "Total params: 38,473,424\n",
       "Trainable params: 38,473,424\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 2.46\n",
       "=======================================================================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 1449.66\n",
       "Params size (MB): 153.89\n",
       "Estimated Total Size (MB): 1603.58\n",
       "======================================================================================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printing a summary of the architecture\n",
    "from torchinfo import summary\n",
    "idx, targets = get_batch('test')\n",
    "# idx = idx.to(device)\n",
    "summary(model=model,\n",
    "        input_data=idx,\n",
    "        # input_size=(ModelArgs.batch_size, ModelArgs.block_size, ModelArgs.embeddings_dims),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer setup and scheduler steup\n",
    "\n",
    "optimizer = torch.optim.AdamW(weight_decay=ModelArgs.weight_decay_optim, params=model.parameters(), lr=ModelArgs.max_lr, betas=(ModelArgs.beta_1, ModelArgs.beta_2))\n",
    "\n",
    "total_steps = 5000\n",
    "eval_iters = 100\n",
    "# lr_scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max= total_steps - initial_iters)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            idx, targets = get_batch(split=split)\n",
    "            logits = model(idx)\n",
    "            batch_size, block_size, embeddings_dims = logits.shape\n",
    "            logits = logits.view(batch_size*block_size, embeddings_dims) # Total tokens(words) => batch_size * block_size\n",
    "            targets = targets.view(batch_size * block_size)\n",
    "            loss = nn.functional.cross_entropy(logits, targets)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 40/5000 [01:45<4:05:50,  2.97s/it]"
     ]
    }
   ],
   "source": [
    "#Train the  model\n",
    "from tqdm import tqdm\n",
    "\n",
    "model.train()\n",
    "for step in tqdm(range(total_steps)):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if (step  % eval_iters == 0 and step != 0) or step == total_steps - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        # torch.save(model.state_dict(), 'weights/Gemma1_32M_steps_%d.pth' % (step))\n",
    "\n",
    "    idx, targets = get_batch(split='train')\n",
    "    logits = model(idx)\n",
    "    batch_size, block_size, embeddings_dims = logits.shape\n",
    "    logits = logits.view(batch_size*block_size, embeddings_dims)\n",
    "    targets = targets.view(batch_size * block_size)\n",
    "    loss = nn.functional.cross_entropy(logits, targets)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
