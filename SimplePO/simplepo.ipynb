{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "!pip install datasets\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "            # entity = 'rajceo2031',\n",
    "                        project = 'GPTJ-DPO',\n",
    "                        # config = CFG,\n",
    "                        # save_code = True,\n",
    "                        #group = 'ANN',\n",
    "                        #job_type = 'train'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "\n",
    "batch_size = 128\n",
    "beta = 2\n",
    "max_lr = 2e-5\n",
    "gamma = 1.2\n",
    "min_lr = 0.1 * max_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomLRScheduler:\n",
    "    def __init__(self, optimizer, warmup_iters, lr_decay_iters, min_lr, max_lr):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_iters = warmup_iters\n",
    "        self.lr_decay_iters = lr_decay_iters\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.it = 0\n",
    "        self._last_lr = [max_lr]  # Initialize with max_lr (matching PyTorch convention)\n",
    "        \n",
    "    def step(self):\n",
    "        \n",
    "        self._last_lr = [self._get_lr()]  # Store as list\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = self._last_lr[0]\n",
    "        self.it += 1\n",
    "\n",
    "    def get_last_lr(self):\n",
    "        return self._last_lr  # Returns list to match PyTorch convention\n",
    "    \n",
    "    def _get_lr(self):\n",
    "\n",
    "        # cycle = math.floor(1 + self.it / (2 * self.warmup_iters))\n",
    "        # x = abs(self.it / self.warmup_iters - 2 * cycle + 1)\n",
    "        # return self.min_lr + (self.max_lr - self.min_lr) * max(0, (1 - x))\n",
    "        \n",
    "        # 1) linear warmup for warmup_iters steps\n",
    "        if self.it < self.warmup_iters:\n",
    "            return self.max_lr * (self.it + 1) / (self.warmup_iters + 1)\n",
    "        # 2) if it > lr_decay_iters, return min learning rate\n",
    "        if self.it > self.lr_decay_iters:\n",
    "            return self.min_lr\n",
    "        # 3) in between, use cosine decay down to min learning rate\n",
    "        decay_ratio = (self.it - self.warmup_iters) / (self.lr_decay_iters - self.warmup_iters)\n",
    "        assert 0 <= decay_ratio <= 1\n",
    "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) \n",
    "        return self.min_lr + coeff * (self.max_lr - self.min_lr)\n",
    "    \n",
    "    def state_dict(self):\n",
    "        return {\n",
    "            'warmup_iters': self.warmup_iters,\n",
    "            'lr_decay_iters': self.lr_decay_iters,\n",
    "            'min_lr': self.min_lr,\n",
    "            'max_lr': self.max_lr,\n",
    "            'it': self.it\n",
    "        }\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.warmup_iters = state_dict['warmup_iters']\n",
    "        self.lr_decay_iters = state_dict['lr_decay_iters']\n",
    "        self.min_lr = state_dict['min_lr']\n",
    "        self.max_lr = state_dict['max_lr']\n",
    "        self.it = state_dict['it']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePO:\n",
    "  def __init__(self, sft_model, device, beta, gamma, tokenizer):\n",
    "\n",
    "\n",
    "    self.sft_model = sft_model\n",
    "    self.device=device\n",
    "    self.beta = beta\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "    self.gamma = gamma\n",
    "\n",
    "  def SimplePOloss(self, datapoint):\n",
    "\n",
    "\n",
    "\n",
    "    self.win_prompt = datapoint['chosen']\n",
    "    self.lose_prompt = datapoint['rejected']\n",
    "\n",
    "    self.win_log_sft = torch.nn.functional.log_softmax(self.sft_model(**self.win_prompt).logits, dim=-1)\n",
    "    self.win_log_sft = torch.gather(self.win_log_sft, -1, self.win_prompt['input_ids'].unsqueeze(-1)).squeeze(-1) #Why gather? Because its not token level stuff we care about but sequence level. Hence, we will sum up the probs of every token to get seq level but we don't want to do it for attention maksed tokens too. Hence we we will use gather() to get the ids and multiply the probs by the masked out tokens indexes.\n",
    "    self.win_log_sft = self.win_log_sft * (self.win_prompt['attention_mask'])\n",
    "    self.win_log_sft = self.win_log_sft.sum(dim=-1)\n",
    "\n",
    "\n",
    "    self.lose_log_sft = torch.nn.functional.log_softmax(self.sft_model(**self.lose_prompt).logits, dim=-1)\n",
    "    self.lose_log_sft = torch.gather(self.lose_log_sft, -1, self.lose_prompt['input_ids'].unsqueeze(-1)).squeeze(-1) #Why gather? Because its not token level stuff we care about but sequence level. Hence, we will sum up the probs of every token to get seq level but we don't want to do it for attention maksed tokens too. Hence we we will use gather() to get the ids and multiply the probs by the masked out tokens indexes.\n",
    "    self.lose_log_sft = self.lose_log_sft * (self.lose_prompt['attention_mask'])\n",
    "    self.lose_log_sft = self.lose_log_sft.sum(dim=-1)\n",
    "\n",
    "\n",
    "    self.avg_log_win = self.win_log_sft.mean()\n",
    "    self.avg_log_lose = self.lose_log_sft.mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    self.diff1 = (self.win_log_sft / self.avg_log_win) - (self.lose_log_sft / self.avg_log_lose)\n",
    "\n",
    "    self.final = -nn.functional.logsigmoid(self.beta * (self.diff1) - self.gamma).mean() \n",
    "\n",
    "    # sft_model.train()\n",
    "    return self.final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda:0'\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\", token=HF_TOKEN, device_map=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\", token=HF_TOKEN, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "train_dataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train\", token=HF_TOKEN)\n",
    "val_dataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"test\", token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpo_collate_fn_merged_prompt(batch):\n",
    "\n",
    "    merged_chosen_prompts = []\n",
    "    merged_rejected_prompts = []\n",
    "\n",
    "    for sample in batch:\n",
    "\n",
    "        # print(sample)\n",
    "\n",
    "        # Extract and merge chosen response\n",
    "        prompt = sample['prompt']\n",
    "        chosen_data = sample['chosen']\n",
    "        chosen_data = \"Instruction: \" + prompt + \"\\n\" + \"Output: \" + chosen_data[1]['content'] + \"\\n\"\n",
    "        # Extract and merge rejected response\n",
    "        rejected_data = sample['rejected']\n",
    "        rejected_data =  \"Instruction: \" + prompt + \"\\n\" + \"Output: \" + rejected_data[1]['content'] + \"\\n\"\n",
    "\n",
    "        # print(chosen_data)\n",
    "        # print(rejected_data)\n",
    "        merged_chosen_prompts.append(chosen_data)\n",
    "\n",
    "\n",
    "        merged_rejected_prompts.append(rejected_data)\n",
    "\n",
    "    tokenized_win_prompt = tokenizer(merged_chosen_prompts, max_length = 1024, padding='max_length', truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    tokenized_lose_prompt = tokenizer(merged_rejected_prompts, max_length = 1024, truncation=True, padding='max_length', return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "        # 'prompt': prompts, # Still return original prompts for potential use\n",
    "        'chosen': tokenized_win_prompt, # List of merged prompt-chosen texts\n",
    "        'rejected': tokenized_lose_prompt # List of merged prompt-rejected texts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=dpo_collate_fn_merged_prompt)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=dpo_collate_fn_merged_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optimizer setup and scheduler steup\n",
    "sft_model.train()\n",
    "\n",
    "val_iterator = iter(val_loader)\n",
    "train_itertaor = iter(train_loader)\n",
    "\n",
    "\n",
    "warmup_iters = 0.1 * len(train_itertaor)\n",
    "lr_decay_iters = len(train_itertaor) \n",
    "optimizer = torch.optim.AdamW(sft_model.parameters(), lr=max_lr)\n",
    "scheduler = CustomLRScheduler(optimizer, warmup_iters, lr_decay_iters, min_lr, max_lr)\n",
    "\n",
    "epoch = 1\n",
    "eval_iters = 20\n",
    "\n",
    "simplepo_loss = SimplePO(sft_model, device, beta, gamma, tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def estimate_loss():\n",
    "    loader = None\n",
    "    out = {}\n",
    "    sft_model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        if(split == 'train'):\n",
    "            loader = train_itertaor\n",
    "\n",
    "        elif (split == 'val'):\n",
    "            loader = val_iterator\n",
    "\n",
    "        \n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "\n",
    "            datapoint = next(loader)\n",
    "\n",
    "\n",
    "            loss = simplepo_loss.SimplePOloss(datapoint)\n",
    "\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    sft_model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Train the  model\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in epoch:\n",
    "    for step in tqdm(range(len(train_iterator))):\n",
    "\n",
    "\n",
    "        if (step  % eval_iters == 0 and step != 0) or step == len(train_iterator) - 1:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"epoch {epoch}, step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"step\": step,\n",
    "                \"training_loss\": losses['train'],\n",
    "                \"val_loss\": losses['val']\n",
    "            })\n",
    "\n",
    "        text  = next(train_iterator)\n",
    "\n",
    "\n",
    "        loss = simplepo_loss.SimplePOloss(text)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
