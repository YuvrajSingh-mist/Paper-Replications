{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IxeBhneWYdgj","outputId":"5af9843c-de1a-4ef3-e95d-7fc8b05c2955","trusted":true},"outputs":[],"source":["!pip install datasets\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"orUGwehKYiDD","trusted":true},"outputs":[],"source":["\n","from datasets import Audio\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e4b2df23-1b2f-4ca2-9968-9bbd2972779d","_uuid":"4b54e35b-02bb-44b4-adca-a297c3081d36","collapsed":false,"id":"Pw7f2ghccuoK","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["\n","import torch\n","from torch import nn\n","from pathlib import Path\n","import random\n","\n","\n","# from tokenizers import Tokenizer\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8e81d8cb-2c43-4550-a8f8-09815ff2a1ab","_uuid":"fe2c17e3-b1bf-4b93-8760-de26e849e84f","colab":{"base_uri":"https://localhost:8080/"},"collapsed":false,"id":"LwR5_uvTcuoL","jupyter":{"outputs_hidden":false},"outputId":"bed91316-5875-4007-d7e8-121a4702cf5e","trusted":true},"outputs":[],"source":["\n","# import re\n","HF_TOKEN = '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'\n","\n","from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", token=HF_TOKEN)\n","tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_UrQBMMqqHMV","outputId":"488f0c63-21e6-43b0-8130-34989fccd95c","trusted":true},"outputs":[],"source":["import wandb\n","\n","from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","secret_value_0 = user_secrets.get_secret(\"API_KEY\")\n","\n","wandb.login(key=secret_value_0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D7AP219KJzTs","trusted":true},"outputs":[],"source":["#Hyperparameters\n","epochs=10\n","block_size = 40\n","batch_size = 128\n","# src_vocab_size = None\n","tgt_vocab_size = 32768\n","# phenome_embeddings_dims = 512\n","embeddings_dims = 288\n","# prenet_encoder_embeddings_dims = 512\n","attn_dropout = 0.1\n","no_of_heads = 6 #IMP needs to be thoroughly calculated\n","dropout = 0.1\n","# epochs = 3\n","max_lr = 6e-4\n","no_of_decoder_layers = 6 #IMP needs to be thoroughly calculated\n","attn_dropout = 0.1\n","weight_decay_optim = 0.1\n","log_mel_features = 80\n","kernel_size = 5\n","stride = (2,10)\n","sr = 16000\n","device= 'cuda:0'\n","SAMPLING_RATE=480000\n","N_MELS = 80  # 80-channel Mel spectrogram\n","WINDOW_DURATION = 0.050  # 25 milliseconds\n","STRIDE_DURATION = 0.0125  # 10 milliseconds\n","max_t = 16000\n","n_channels = max_t\n","clip = 1.0\n","# embeddings_dims_decoder = 256"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5ac94789-1d5e-4ace-88f3-25b5413fb78b","_uuid":"92a80d68-b0eb-4f1d-b988-7f190e0b934a","collapsed":false,"id":"TmPkI_UEpvor","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["torch.set_default_device(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1777fa9a-7686-4b62-93c1-c31c2ccf73c1","_uuid":"e08f94e6-dae5-42f3-a285-c151086b8f1b","colab":{"base_uri":"https://localhost:8080/"},"collapsed":false,"id":"IME1Ls95Y3gl","jupyter":{"outputs_hidden":false},"outputId":"8cabce1c-54b1-41d4-bb78-af3ba79d8265","trusted":true},"outputs":[],"source":["\n","!pip install datasets\n","from tabnanny import verbose\n","from datasets import load_dataset\n","\n","gs = load_dataset(\"keithito/lj_speech\", token=HF_TOKEN)\n","\n","\n","print(gs)\n","\n","\n","audio_input = gs['train'][0][\"audio\"]\n","transcription = gs[\"train\"][0][\"text\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0c018b51-6161-49d9-995e-21cf09d1391f","_uuid":"027b3fe3-8e01-40c4-99de-1a1b12d93453","colab":{"base_uri":"https://localhost:8080/","height":180,"referenced_widgets":["30c7d02180c749669ab0c725040c83bd","70f08b4090204d4cb2e0bc2c08e9f21a","b6578fa751084e79872f27c8ec7447c3","44fca81bee7e4d178e60529a953ac4fb","01fe5a96c7b9468dbd6abd83d58b31d0","d987d59ac37046d88ed73dcc0aa7b2a7","dcd607401ec44598912d3e934cfec4e7","74b4ac2926e842848815ac6c45173572","5c3b0cddb8ff4d54a52ffc8c417185e9","98d4d8c0785f4177b7bf79bb029bfb33","3e91345376394605941d9f031995ff10","1de74b76d5364fdbb230ef3a0587b7e8","96924505eb4344aea814c59efb60fccf","374f1467fcc34b2683985c33e5aff04a","78d0266852f64e3bb6b0d201a78b1dac","35bc1ad50a6b45a297cbf80af4d77de4","fdd2a49adbb84d08b6c2df0f24a315bc","dde00aa4837b4b4b872e0e555ca639b0","5a2fa1d2392d42ab87aeadd9101a3c94","d53d4ba3aa4d4aee9eb34fbdd72b0afc","c45794bc2dc24d1bad98ecc6bb58ce43","5a2404992981486897018299f8ea9cdf","85767c5f479b4c6ca901fda615d7afaa","a6a14773edbb4da5a0db88907d72836e","431809753f044eef9bd4bded81a686b1","54788d0eaac944e6a1dbcf0d4e6fdc24","74942eb68cfa4cfebce1dd922c906eb1","143e0faa317e44f6821ffdef7a5f6931","55b5b881943e45c5b21d2a4b9a7df1ca","2f045124f238406cb58a77013ae254c5","7d592f1aa4904cb6989f55760c8daa9d","aff7d4f4d930451fb9d659b4e26034ca","3b8f4b197a84464cb31d59b0e038ce1c","b5c49554e5774d54884867e0800c6716","171bcaedc1034ad883e8c0ae00632999","c486b16024824cf584fa8a24516404e8","e7d4a4f0a1354b3e9164bf4a31cdff15","3f7d7b3642784218bf1f3e0f72f2809f","056becbc86134ff993f62136c636f263","6c41650eecf24e55a5affda617eac687","e5278fd04d9d40b1b907759338420356","504d6c72038241fbbad2732ef3d5c75f","c3f2ef601e1f477ea16d4be5c6743952","07315cca04584ff584df3f0f925dc21e"]},"collapsed":false,"id":"cRV1EOlVY3gm","jupyter":{"outputs_hidden":false},"outputId":"2c3b7151-9173-4631-e32d-60115f61816b","trusted":true},"outputs":[],"source":["MAX_DURATION_IN_SECONDS = 30\n","\n","gs = gs['train'].train_test_split(test_size=0.2)\n","# print(dataset)\n","# train_data, val_data = dataset['train'], dataset['test']\n","\n","import librosa\n","from tqdm import tqdm\n","def is_audio_length_in_range(input_length):\n","    return input_length < MAX_DURATION_IN_SECONDS\n","\n","train_new_column = []\n","\n","for x in tqdm(range(len(gs['train']))):\n","    train_new_column.append(librosa.get_duration(path=gs['train'][x]['audio']['path']))\n","\n","gs_ = gs['train'].add_column(\"duration\", train_new_column)\n","\n","\n","gs_ = gs_.filter(is_audio_length_in_range, input_columns=[\"duration\"])\n","\n","truncated_gs_train = gs_\n","# truncated_gs_train = gs_.remove_columns([\"duration\"])\n","# truncated_gs\n","\n","\n","\n","val_new_column = []\n","# new_column = [librosa.get_duration(path=x) ]]\n","for x in tqdm(range(len(gs['test']))):\n","    val_new_column.append(librosa.get_duration(path=gs['test'][x]['audio']['path']))\n","\n","gs_ = gs['test'].add_column(\"duration\", val_new_column)\n","\n","\n","gs_ = gs_.filter(is_audio_length_in_range, input_columns=[\"duration\"])\n","\n","truncated_gs_val = gs_\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"68c3295a-6eef-4874-acaf-73f9279fad98","_uuid":"d988499f-ad5b-47cf-bd9e-257395584889","colab":{"base_uri":"https://localhost:8080/"},"collapsed":false,"id":"6NZ9Hbp5q1to","jupyter":{"outputs_hidden":false},"outputId":"9f433b61-7593-4506-a572-47a0531ee336","trusted":true},"outputs":[],"source":["\n","import numpy as np\n","\n","\n","\n","train_outputs = []\n","train_texts = []\n","train_duration = []\n","val_outputs = []\n","val_texts = []\n","val_duration = []\n","count = 0\n","\n","for i in tqdm(range(len(truncated_gs_train))):\n","\n","\n","  train_texts.append(truncated_gs_train[i]['normalized_text'])\n","  train_duration.append(truncated_gs_train[i]['duration'])\n","  train_outputs.append(truncated_gs_train[i]['audio']['array'])\n","  count += 1\n","\n","count = 0\n","val_outputs = []\n","val_texts = []\n","for i in tqdm(range(len(truncated_gs_val))):\n","\n","  val_outputs.append(truncated_gs_val[i]['audio']['array'])\n","  val_texts.append(truncated_gs_val[i]['text'])\n","  val_duration.append(truncated_gs_val[i]['duration'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wFUv_NELDtXW","outputId":"cd95a1fd-c580-4c26-a5e5-71f18f8ba39d","trusted":true},"outputs":[],"source":["len(val_outputs[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"db1f5e13-91c6-4d24-8d31-325796a8a0fd","_uuid":"b456c1a6-6703-4809-a1fb-fc077a5dc8e3","collapsed":false,"id":"z2aGf6_7xe9S","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# import math\n","import re\n","# print(round(random.random(), 1))\n","class TTSDataset(Dataset):\n","\n","  def __init__(self, outputs, texts, duration):\n","\n","    self.data = outputs\n","    self.texts = texts\n","    self.max_t = max_t\n","    self.duration = duration\n","\n","    self.min_duration = 4.0\n","    self.max_duration = 30.0\n","\n","  def __len__(self):\n","    return len(self.data)\n","\n","\n","  def pad_to_max_t(self, spectrogram, max_t):\n","    # print(spectrogram.shape)\n","    t = len(spectrogram)\n","\n","    if t < max_t:\n","        # Pad with zeros\n","        pad_width = ((0, max_t - t))\n","        spectrogram = np.pad(spectrogram, pad_width, mode='constant')\n","    else:\n","      spectrogram = spectrogram[:max_t]\n","\n","\n","    return spectrogram\n","\n","  def clean(self, desc):\n","    # Use regex to remove anything between < and >\n","    cleaned_text = re.sub(r'<[^>]*>', '', desc)\n","    return cleaned_text\n","\n","  def __getitem__(self, idx):\n","\n","\n","      audio_array = []\n","\n","      audio_array = self.pad_to_max_t(self.data[idx], self.max_t)\n","\n","\n","      audio_array = torch.tensor(audio_array, dtype=torch.float32)\n","\n","\n","      # text = self.clean(self.texts[idx])\n","      text = self.texts[idx]\n","      text = text.lower()\n","      # text = SOT  + 'en' + transcribe +  text + EOT\n","      # text += '[EOS]'\n","      tokenized_text = tokenizer(text, truncation=True, padding='max_length', max_length=block_size, return_tensors='pt')\n","\n","    \n","      tokenized_text['labels'] = tokenized_text['input_ids'].clone()\n","      tokenized_text['labels'][: , :-1] = tokenized_text['input_ids'][:1 , 1:]\n","      tokenized_text['labels'][: , -1] = tokenizer.eos_token_id\n","\n","      tokenized_text_x = tokenized_text['input_ids'].squeeze(0)\n","      tokenized_text_y = tokenized_text['labels'].squeeze(0)\n","\n","      # print(tokenized_text.shape)\n","      return audio_array.unsqueeze(0), tokenized_text_x, tokenized_text_y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BWiwc8a5Qkhe","trusted":true},"outputs":[],"source":["def collate_fn(batch):\n","    text = []\n","    audio_array = []\n","    labels_list = []\n","    stop = []\n","    for audio_arr, text_dict in batch:\n","\n","        # spectrograms.append(spec)\n","\n","        # stop.append(stop_token)\n","        audio_array.append(audio_arr)\n","        labels_list.append(text_dict['input_ids'])\n","        # text.append(text_dict)\n","\n","    # 3. Stack tensors\n","    # text = torch.stack(text)\n","    audio_array = torch.stack(audio_array)\n","    labels = torch.stack(labels_list)\n","    # stop = torch.stack(stop)\n","    # 4. Return in proper format\n","    return {\n","        # 'text': text,\n","        'input_ids': audio_array,\n","        'labels': labels,\n","        # \"stop_tokens\": stop\n","    }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"78e27c8d-e3cd-434c-b2c6-9d16b79eb5be","_uuid":"5ca81223-5c7e-43e5-83b2-7b00a3ecee7f","colab":{"base_uri":"https://localhost:8080/"},"collapsed":false,"id":"2FJvDV-4psOo","jupyter":{"outputs_hidden":false},"outputId":"959c35d3-905e-409b-f1b4-c93611bca529","trusted":true},"outputs":[],"source":["\n","torch.autograd.set_detect_anomaly(True)  # Add at the start of training"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d26cf6eb-55b2-484c-bc9b-10952ea5c977","_uuid":"8db84c70-d655-4b7c-8c00-9e195d240a7d","collapsed":false,"id":"2tvMuBSOynPy","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["\n","\n","shuffle = True\n","\n","train_dataset = TTSDataset(train_outputs, train_texts, train_duration)\n","val_dataset = TTSDataset(val_outputs, val_texts, val_duration)\n","\n","generator = torch.Generator(device=device)\n","# \n","train_dataloader = DataLoader(\n","\n","    train_dataset,\n","    batch_size=batch_size,\n","    generator=generator,\n","    shuffle=shuffle,\n","     drop_last=True,\n","    # collate_fn=collate_fn\n",")\n","val_dataloader = DataLoader(\n","    val_dataset,\n","    batch_size=batch_size,\n","\n","    generator=generator,\n","    drop_last=True ,\n","    shuffle=False,\n","    # collate_fn = collate_fn\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j6t9Pb275ZpE","trusted":true},"outputs":[],"source":["\n","\n","# Text embeddings\n","class TgtTextEmbeddings(nn.Module):\n","    def __init__(\n","        self,\n","        vocab_size = tgt_vocab_size,\n","        embeddings_dims = embeddings_dims\n","    ):\n","        super().__init__()\n","        self.embeddings_table = nn.Embedding(num_embeddings = tgt_vocab_size, embedding_dim=embeddings_dims, device=device) #Just a look up table to convert the toekns_ids to some numbers\n","        # nn.init.normal_(self.embeddings_table.weight.data, mean=0, std=0.02)\n","\n","    def forward(self, x):\n","        return self.embeddings_table(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lmrP6_qEcmi2","trusted":true},"outputs":[],"source":["class RotaryEmbeddings(nn.Module):\n","    def __init__(\n","        self,\n","         device,\n","        embeddings_dims: int = embeddings_dims,\n","        block_size: int = block_size,\n","        batch_size: int = batch_size\n","    ):\n","        super().__init__()\n","\n","        self.embeddings_dims = embeddings_dims\n","        self.block_size = block_size\n","        self.batch_size = batch_size\n","        self.theta = 0\n","        self.device=device\n","\n","    def apply_rope(self, seq):\n","        batch_size, seq_len, embeds_dims = seq.shape\n","        # print(seq.shape)\n","        # print(self.embeddings_dims)\n","        # self.matrix = torch.zeros((seq_len, self.embeddings_dims, self.embeddings_dims), dtype=torch.float32,  requires_grad=False,  device = self.device)\n","        token_idx = torch.arange(0 , seq_len, dtype=torch.float32,  device = self.device).unsqueeze(1)\n","        positions = torch.arange(0 , embeds_dims, 2, dtype=torch.float32,  device = self.device).unsqueeze(0)\n","        # dims = torch.arange(1, self.embeddings_dims // 2,  dtype=torch.float32)\n","        theta = 10000 ** (-2 * (positions) / embeds_dims)\n","        angles = token_idx * theta\n","        angles = angles.expand(seq_len, -1) # because this thing needs to be applied to every sequence in the batch but with embeds dims halved\n","        x_reshaped = seq.view(batch_size, seq_len, embeds_dims // 2, 2)\n","\n","        cos_angles = torch.cos(angles)\n","        sin_angles = torch.sin(angles)\n","      \n","\n","        out = torch.stack([x_reshaped[..., 0]*cos_angles - (x_reshaped[...,1] * sin_angles), x_reshaped[...,1] * cos_angles + x_reshaped[..., 0] * sin_angles], dim=-1)\n","        out = out.view(batch_size, seq_len, embeds_dims)\n","        return out\n","\n","    def forward(self, x):\n","  \n","        res = self.apply_rope(x)\n","        return res\n","        # else:\n","            # return self.x_reshaped\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NKMyGQLTc3vO","trusted":true},"outputs":[],"source":["class Swish(nn.Module):\n","    def __init__(\n","        self,\n","        block_size: int = block_size,\n","        embeddings_dims: int = embeddings_dims,\n","        device = device\n","    ):\n","        super().__init__()\n","\n","        self.sig = torch.nn.Sigmoid()\n","\n","\n","    def forward(self, x):\n","        swish = x * self.sig(x)\n","\n","        return swish\n","\n","\n","\n","class MLPBlock(nn.Module):\n","    def __init__(\n","        self,\n","        block_size: int = block_size,\n","        embeddings_dims: int = embeddings_dims,\n","        device = device\n","    ):\n","        super().__init__()\n","        self.hidden_dims = embeddings_dims * 2\n","        # self.hidden_dims = int(2 * ( 4 * embeddings_dims) / 3)\n","\n","        self.swish = Swish(block_size=block_size, embeddings_dims=embeddings_dims, device=device)\n","        self.linear_layer1 = nn.Linear(in_features=embeddings_dims, out_features=self.hidden_dims,  bias=False, device = device)\n","        self.linear_layer2 = nn.Linear(in_features=embeddings_dims, out_features=self.hidden_dims,  bias=False, device = device)\n","        self.linear_layer3 = nn.Linear(in_features=self.hidden_dims, out_features=embeddings_dims,  bias=False, device = device)\n","\n","\n","\n","\n","    def forward(self, x):\n","        swish_res = self.swish(self.linear_layer1(x))\n","        x_V = self.linear_layer2(x)\n","        res = torch.mul(swish_res, x_V)\n","        out = self.linear_layer3(res)\n","        return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0e795d31-2801-446a-807d-0ea3bebb6e3d","_uuid":"ae3226bb-4414-4840-8da7-73db14ad8271","collapsed":false,"id":"REUDHWrWcuoN","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["\n","#Layer Normalization\n","\n","class LayerNormalization(nn.Module):\n","    def __init__(\n","        self,\n","        embeddings_dims = embeddings_dims\n","    ):\n","        super().__init__()\n","        self.norm = nn.LayerNorm(normalized_shape=embeddings_dims)\n","    def forward(self, x):\n","\n","        return self.norm(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"50c3f233-f3ea-44cc-bccf-10aa13fc35d5","_uuid":"bd1f493a-c5eb-4821-938b-1fd4baf55549","collapsed":false,"id":"cf0Jf_7UcuoN","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["\n","\n","class MaskedAttentionHead(nn.Module):\n","    def __init__(\n","        self,\n","        attn_dropout = attn_dropout,\n","        embeddings_dims = embeddings_dims,\n","        no_of_heads = no_of_heads,\n","    ):\n","        super().__init__()\n","        # print(embeddings_dims)\n","        self.head_size = embeddings_dims // no_of_heads\n","        self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device, bias=False)\n","        self.keys = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,device=device, bias=False)\n","        self.values = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device,bias=False)\n","        self.dropout = nn.Dropout(p = attn_dropout)\n","\n","        self.rotary = RotaryEmbeddings(device=device, embeddings_dims=self.head_size, block_size=block_size, batch_size=batch_size)\n","    def forward(self, x):\n","        # print(x.shape)\n","        batch, block_size, embd_dims = x.shape\n","        k = self.keys(x)\n","        q = self.query(x)\n","        v = self.values(x)\n","        q = self.rotary(q)\n","        k = self.rotary(k)\n","        masked_table = torch.tril(torch.ones(block_size, block_size, device=device))\n","        weights = q @ torch.transpose(k, dim0=-2, dim1=-1) * (k.shape[-1] ** -0.5)\n","        masked_values = weights.masked_fill(masked_table[: block_size, : block_size] == 0, float('-inf'))\n","        weights_normalized = nn.functional.softmax(masked_values, dim=-1) #Normalize along the embeddings dimension for all the tokens\n","        weights_normalized = self.dropout(weights_normalized)\n","        out = weights_normalized @ v\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aee629bd-84ad-4f0c-a0e7-9e3070e10081","_uuid":"a252848c-a0b1-4c08-8a42-e2851e642a06","collapsed":false,"id":"OUFERSL2u8LT","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["\n","\n","\n","class MaskedMHA(nn.Module):\n","    def __init__(\n","        self,\n","        attn_dropout = attn_dropout,\n","        embeddings_dims = embeddings_dims,\n","        no_of_heads = no_of_heads,\n","    ):\n","        super().__init__()\n","        self.heads = nn.ModuleList([MaskedAttentionHead(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads) for _ in range(no_of_heads)])\n","        self.dropout = nn.Dropout(p = attn_dropout)\n","        self.linear = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=device, bias=False) # 12 (no of heads) * (batch_size) 64 = 768 -> gives out the text embeddings\n","\n","    def forward(self, x):\n","        concat = torch.cat([head(x) for head in self.heads], dim=-1)\n","        linear_layer = self.linear(concat)\n","        out = self.dropout(linear_layer)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"76ee1dab-07cf-4ae4-ac9a-3c134b9dc41d","_uuid":"0bdd11ba-b65a-4dbe-9b09-25d24c2b3ab6","collapsed":false,"id":"oGGyyF4pjHyd","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["\n","#Single Attention Head\n","\n","class CrossAttentionHead(nn.Module):\n","    def __init__(\n","        self,\n","        attn_dropout = attn_dropout,\n","        embeddings_dims = embeddings_dims,\n","        no_of_heads = no_of_heads,\n","    ):\n","        super().__init__()\n","        self.head_size = embeddings_dims // no_of_heads\n","        self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device, bias=False)\n","        self.keys = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,device=device, bias=False)\n","        self.values = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device,bias=False)\n","        self.dropout = nn.Dropout(p = attn_dropout)\n","\n","\n","    def forward(self, query, key, value, mask=None):\n","\n","\n","        batch, block_size, embd_dims = query.shape\n","        q = self.query(query)\n","        k = self.keys(key)\n","        v = self.values(value)\n","\n","\n","\n","        masked_table = torch.tril(torch.ones(block_size, block_size, device=device))\n","        weights = q @ torch.transpose(k, dim0=-2, dim1=-1) * (k.shape[-1] ** -0.5)\n","        masked_values = weights.masked_fill(masked_table[: block_size, : block_size] == 0, float('-inf'))\n","        weights_normalized = nn.functional.softmax(masked_values, dim=-1) #Normalize along the embeddings dimension for all the tokens\n","        weights_normalized = self.dropout(weights_normalized)\n","        out = weights_normalized @ v\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bcde57b5-61a0-4d56-aa5a-5fbbc2f2a9bd","_uuid":"af55eb4d-6d72-465f-8692-4777da91e56f","collapsed":false,"id":"U5NmszzcjHyf","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["\n","\n","\n","#Single Attention Head\n","\n","class FullAttentionHead(nn.Module):\n","    def __init__(\n","        self,\n","        attn_dropout = attn_dropout,\n","        embeddings_dims = embeddings_dims,\n","        no_of_heads = no_of_heads,\n","    ):\n","        super().__init__()\n","        self.head_size = embeddings_dims // no_of_heads\n","        self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device, bias=False)\n","        self.keys = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,device=device, bias=False)\n","        self.values = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device,bias=False)\n","        self.dropout = nn.Dropout(p = attn_dropout)\n","        self.rotary = RotaryEmbeddings(device=device, embeddings_dims=self.head_size, block_size=block_size, batch_size=batch_size)\n","\n","    def forward(self, x, mask=None):\n","        # batch, block_size, embd_dims = x.shape\n","        k = self.keys(x)\n","        q = self.query(x)\n","        v = self.values(x)\n","        k = self.rotary(k)\n","        q = self.rotary(q)\n","        # masked_table = torch.tril(torch.ones(block_size, block_size, device=device))\n","        weights = q @ torch.transpose(k, dim0=-2, dim1=-1) * (k.shape[-1] ** -0.5)\n","        if(mask != None):\n","            mask = mask.unsqueeze(1)\n","            masked_values = weights.masked_fill(mask == 0, float('-inf'))\n","            weights_normalized = nn.functional.softmax(masked_values, dim=-1) #Normalize along the embeddings dimension for all the tokens\n","            # weights_normalized = self.dropout(weights_normalized)\n","            out = weights_normalized @ v\n","            out = self.dropout(out)\n","            return out\n","        else:\n","            weights_normalized = nn.functional.softmax(weights, dim=-1) #Normalize along the embeddings dimension for all the tokens\n","            # weights_normalized = self.dropout(weights_normalized)\n","            out = weights_normalized @ v\n","            out = self.dropout(out)\n","            return out"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8099497c-1f19-4d53-8965-ae8f153335be","_uuid":"50fee96b-7dae-4da2-951a-b06330293a3e","collapsed":false,"id":"v_BB7r7kqmOc","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["\n","class FullMHA(nn.Module):\n","    def __init__(\n","        self,\n","        attn_dropout = attn_dropout,\n","        embeddings_dims = embeddings_dims,\n","        no_of_heads = no_of_heads,\n","    ):\n","        super().__init__()\n","        self.heads = nn.ModuleList([FullAttentionHead(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads) for _ in range(no_of_heads)])\n","        self.dropout = nn.Dropout(p = attn_dropout)\n","        self.linear = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=device, bias=False) # 12 (no of heads) * (batch_size) 64 = 768 -> gives out the text embeddings\n","\n","    def forward(self, x, mask=None):\n","        concat = torch.cat([head(x, mask) for head in self.heads], dim=-1)\n","        linear_layer = self.linear(concat)\n","        out = self.dropout(linear_layer)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"06ee6670-5428-4e5a-867e-cdd98cddc2aa","_uuid":"3627fa0f-bd35-4dab-9451-03a2b716732b","collapsed":false,"id":"TTwRkBzcvE-_","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["\n","\n","class CrossMHA(nn.Module):\n","    def __init__(\n","        self,\n","        attn_dropout = attn_dropout,\n","        embeddings_dims = embeddings_dims,\n","        no_of_heads = no_of_heads,\n","    ):\n","        super().__init__()\n","        self.heads = nn.ModuleList([CrossAttentionHead(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads) for _ in range(no_of_heads)])\n","        self.dropout = nn.Dropout(p = attn_dropout)\n","        self.linear = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=device, bias=False)\n","\n","    def forward(self, value, key, x, mask=None):\n","        concat = torch.cat([head(x, key, value,  mask) for head in self.heads], dim=-1)\n","        linear_layer = self.linear(concat)\n","        out = self.dropout(linear_layer)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2e095386-2ee5-4a3c-83bb-cad96cb26746","_uuid":"2c290085-c1fe-4c64-9bab-baa448c3bae5","collapsed":false,"id":"s9rJzO_XcuoO","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Decoder Block\n","\n","class TransformerDecoderBlock(nn.Module):\n","    def __init__(\n","        self,\n","        attn_dropout = attn_dropout,\n","        embeddings_dims = embeddings_dims,\n","        no_of_heads = no_of_heads,\n","        dropout = dropout,\n","        # vocab_size = vocab_size\n","    ):\n","        super().__init__()\n","\n","        self.cross = CrossMHA(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads)\n","        self.masked = MaskedMHA(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads)\n","        self.layer_norm1 = LayerNormalization(embeddings_dims)\n","        self.layer_norm2 = LayerNormalization(embeddings_dims)\n","        # self.layer_norm3 = LayerNormalization(embeddings_dims=embeddings_dims)\n","        self.layer_norm4 = LayerNormalization(embeddings_dims)\n","        self.mlp_block = MLPBlock(block_size=block_size, embeddings_dims=embeddings_dims)\n","        self.rope = RotaryEmbeddings(device=device, embeddings_dims=embeddings_dims, block_size=block_size, batch_size=batch_size)\n","\n","    def forward(self, key, value, x, mask=None):\n","        # print(x.shape)\n","        # x = self.rope(x)\n","        x = self.layer_norm1(x + self.masked(x)) #Very important step -> Layer Norm on input and then passes it to the subsequent blocks\n","        # print(x.shape)\n","        x = self.layer_norm2(x + self.cross(value, key, x, mask)) #Very important step\n","        # print(x.shape)\n","        # x = x + self.mha(self.layer_norm1(x))  #Very important step -> Layer Norm on input and then passes it to the subsequent blocks\n","        x = self.layer_norm4(x + self.mlp_block(x)) #Very important step\n","        # print(x.shape)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"63e5400a-b5d2-4ef7-8f30-b1bc0e233820","_uuid":"05fd4f49-cc4b-474c-9a9c-3d5191eefa43","collapsed":false,"id":"KGh8ujQJcuoO","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Decoder Block\n","\n","class DecoderModel(nn.Module):\n","    def __init__(\n","        self,\n","        attn_dropout = attn_dropout,\n","        embeddings_dims = embeddings_dims,\n","        no_of_heads = no_of_heads,\n","        block_size = block_size,\n","        dropout = dropout,\n","        no_of_decoder_layers = no_of_decoder_layers,\n","        # vocab_size = vocab_size\n","    ):\n","        super().__init__()\n","\n","\n","\n","\n","        # self.tgt_text_embds = TgtTextEmbeddings(vocab_size=tgt_vocab_size, embeddings_dims=embeddings_dims)\n","        # self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=tgt_vocab_size, device=device, bias=False) # Takes in logits of dimensions- embeds_dims and converts it into dimension of vocab_size (logits in range of vocab_size)\n","        # self.layer_norm = LayerNormalization(embeddings_dims=embeddings_dims)\n","        self.decoder_layers = nn.ModuleList([TransformerDecoderBlock(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads, dropout=dropout) for _ in range(no_of_decoder_layers)])\n","        self.apply(self._init_weights)\n","        # self.positional_embeddings_tgt = nn.Parameter(torch.randn(1, block_size, embeddings_dims, device=device), requires_grad=True) #To give positional embeddings to each token of the input text, hence num_embeddings=block_size\n","        # self.positional_embeddings_tgt = TgTPositionEmbeddings()\n","        # self.scaled_factor = nn.Parameter(torch.ones(1, N_MELS, embeddings_dims), requires_grad=True)\n","        # torch.nn.init.normal_(self.positional_embeddings_tgt, mean=0.0, std=0.02)\n","\n","        # out = self.decoder_layers(query, key, x)\n","        # Loop through each decoder layer\n","    def _init_weights(self, module):  #Weight Initialization\n","            if isinstance(module, nn.Linear):\n","                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","                if module.bias is not None:\n","                    torch.nn.init.zeros_(module.bias)\n","            elif isinstance(module, nn.Embedding):\n","                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","    def forward(self, key, value, x, mask):\n","        # x = self.tgt_text_embds(x)\n","        # print(x.shape)\n","        # x = x + self.scaled_factor * self.positional_embeddings_tgt(x)\n","        # print(x.shape)\n","        for decoder_layer in self.decoder_layers:\n","            x = decoder_layer(key, value, x, mask)\n","        # x = self.layer_norm(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5327372f-0db3-4ef1-a59c-1757b6edfc62","_uuid":"1e94a41d-aaa3-4e74-a02f-fb0edbfa9118","collapsed":false,"id":"A3SgKrC-jHyd","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["\n","\n","#Encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2a3683d1-303c-41a2-9cc6-b7e053fde34c","_uuid":"3ecc13cd-e38a-4e1c-af9c-10f8e84b02d7","collapsed":false,"id":"v6mbbO3yp-gh","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["\n","\n","\n","\n","\n","class TransformerEncoderBlock(nn.Module):\n","    def __init__(\n","        self,\n","        attn_dropout = attn_dropout,\n","        embeddings_dims = embeddings_dims,\n","        no_of_heads = no_of_heads,\n","        dropout = dropout,\n","        mask=None\n","    ):\n","        super().__init__()\n","\n","        self.mha = FullMHA(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads)\n","        self.layer_norm1 = LayerNormalization(embeddings_dims)\n","        self.layer_norm2 = LayerNormalization(embeddings_dims)\n","        self.mlp_block = MLPBlock(block_size=block_size, embeddings_dims=embeddings_dims)\n","        # self.rope = RotaryEmbeddings(device=device, embeddings_dims=embeddings_dims, block_size=block_size, batch_size=batch_size)\n","\n","    def forward(self, x, mask=None):\n","        # x = self.rope(x)/\n","        x = self.layer_norm1(x + self.mha(x, mask))\n","        x = self.layer_norm2(x + self.mlp_block(x))\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2fa82426-e551-497d-8247-051a55f74a3c","_uuid":"39475274-a828-4151-9556-fa4bf30d4455","collapsed":false,"id":"HxW0pvnV12Ms","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["\n","\n","\n","\n","class EncoderModel(nn.Module):\n","    def __init__(\n","        self,\n","        attn_dropout = attn_dropout,\n","        embeddings_dims = embeddings_dims,\n","        no_of_heads = no_of_heads,\n","        block_size = block_size,\n","        dropout = dropout,\n","        no_of_decoder_layers = no_of_decoder_layers,\n","        # vocab_size = vocab_size\n","    ):\n","        super().__init__()\n","\n","        # self.positional_embeddings_src = nn.Parameter(torch.randn(1, block_size, embeddings_dims, device=device), requires_grad=True) #To give positional embeddings to each token of the input text, hence num_embeddings=block_size\n","        # self.prenet_enc = PrenetEncoder()\n","        # self.pos_embeds = nn.Parameter(torch.randn(1, block_size, embeddings_dims, device=device), requires_grad=True)\n","        # self.trainable_factor = nn.Parameter(torch.ones(1, block_size, embeddings_dims, device=device), requires_grad=True)\n","        self.conv1 = nn.Conv1d(in_channels=1, out_channels=embeddings_dims, kernel_size=127, stride=64, device=device)\n","        self.conv3 = nn.Conv1d(in_channels=embeddings_dims, out_channels=embeddings_dims, kernel_size=3, stride=2, device=device)\n","        self.conv2 = nn.Conv1d(in_channels=embeddings_dims, out_channels=embeddings_dims, kernel_size=7, stride=3, device=device)\n","\n","        # self.positional_embeddings_src = SrcPositionEmbeddings()\n","        # self.src_text_embeds = nn.Embedding(num_embeddings=src_vocab_size, embedding_dim=embeddings_dims, device=device)\n","        # self.src_text_embeds = SrcTextEmbeddings(vocab_size=src_vocab_size, embeddings_dims=embeddings_dims)\n","        self.encoder_layers = nn.ModuleList([TransformerEncoderBlock(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads, dropout=dropout) for _ in range(no_of_decoder_layers)])\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, module):  #Weight Initialization\n","            if isinstance(module, nn.Linear):\n","                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","                if module.bias is not None:\n","                    torch.nn.init.zeros_(module.bias)\n","            elif isinstance(module, nn.Embedding):\n","                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","    def forward(self, x, mask):\n","        # x = x.unsqueeze(1)\n","        # x = x.transpose(1,2).contiguous()\n","        # print(\"x here: \", x.shape)\n","        x = self.conv1(x)\n","        x = torch.nn.functional.tanh(x)\n","        x = self.conv2(x)\n","        x = torch.nn.functional.gelu(x)\n","        x = self.conv3(x)\n","        x = torch.nn.functional.gelu(x)\n","        # print(\"now: \", x.shape)\n","        # x = self.src_text_embeds(x)\n","        # x = x + self.positional_embeddings_src(x)\n","        # print(self.positional_embeddings_src.shape)\n","        # x = x.transpose(1, 2).contiguous()\n","        # x = self.prenet_enc(x)\n","        x = x.permute(0, 2, 1)\n","        # print(x.shape)\n","        # print(self.positional_embeddings_src(x).shape)\n","        # x = x + self.trainable_factor * self.positional_embeddings_src(x)\n","        # print(x)\n","        # print(x.shape)\n","        # Loop through each encoder layer\\\n","        for encoder_layer in self.encoder_layers:\n","            x = encoder_layer(x, mask)\n","        # print(\"enc: \", x.shape)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a6d4d773-0ed2-4059-ab2b-683692ce2c32","_uuid":"6f0e32f9-14f2-4b65-a106-cab2051ad125","collapsed":false,"id":"2UWijIFl2Ykd","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["\n","\n","\n","\n","\n","class TTS(nn.Module):\n","    def __init__(\n","        self,\n","\n","    ):\n","        super().__init__()\n","\n","        self.encoder = EncoderModel()\n","        self.decoder = DecoderModel()\n","        # self.postnet = PostNet()\n","        # self.pos = PositionalEmbeddings()\n","        self.tgt_text_embds = TgtTextEmbeddings(vocab_size=tgt_vocab_size, embeddings_dims=embeddings_dims)\n","        # self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=device, bias=False) # Takes in logits of dimensions- embeds_dims and converts it into dimension of vocab_size (logits in range of vocab_size)\n","        # self.src_text_embeds = SrcTextEmbeddings(vocab_size=src_vocab_size, embeddings_dims=embeddings_dims)\n","        # self.stop_layer = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=device, bias=False)\n","        # self.prenet_dec = PrenetDecoder()\n","        self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=tgt_vocab_size, device=device, bias=False)\n","\n","    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n","        # x = self.src_text_embeds(src)\n","        x = self.encoder(src, src_mask)\n","        # print(\"hgere\" , tgt.shape)\n","        # y = self.prenet_dec(tgt)\n","        y = self.tgt_text_embds(tgt)\n","        # print(x.shape)\n","        y = self.decoder(x, x, y, tgt_mask)\n","        # print(y.shape)\n","\n","        out = self.linear_layer(y)\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"17dd187f-c195-44a0-990c-c6d17c90227e","_uuid":"e407283d-b631-479c-9bae-014aab7a0a3d","collapsed":false,"id":"ntIaQj1U3pFX","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["#Instantiating the model\n","model = TTS()\n","# model = torch.compile(model)\n","# model = model.to(device)\n","model = model.to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d92d3629-d59c-4361-9572-8e0075329808","_uuid":"5061d499-36a9-427b-90ad-eead117d5a51","colab":{"base_uri":"https://localhost:8080/"},"collapsed":false,"id":"yOXtmG-lcuoO","jupyter":{"outputs_hidden":false},"outputId":"320e49fa-241f-40d8-9d38-60e82600d10d","trusted":true},"outputs":[],"source":["\n","\n","\n","!pip install torchinfo\n","from torchinfo import summary\n","\n","spec, X, y = next(iter(train_dataloader))\n","# print(data)\n","# tgt_mask = torch.randint(1, tgt_vocab_size, (batch_size, block_size)).to(device)  #\n","spec1 = spec.to(device)\n","text  = X.to(device)\n","\n","summary(model=model,\n","        input_data=(spec1, text),\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"97226398-cb33-48d6-9abc-f5a9c549b5c5","_uuid":"ee98acba-6992-4b1e-8cc1-3eda3997ae5b","collapsed":false,"id":"LH95cJEvcuoO","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["\n","# # Optimizer setup and scheduler steup\n","# out = {\"Train\": None, \"val\": None}\n","optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, betas = (0.9, 0.95), weight_decay = weight_decay_optim)\n","\n","\n","# loss_fn = nn.MSELoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"53e031da-134a-4271-8879-b012f262ccc6","_uuid":"7860931f-c816-4796-b4c5-265e3df9bf57","colab":{"base_uri":"https://localhost:8080/"},"collapsed":false,"id":"bbvONdUTWmvL","jupyter":{"outputs_hidden":false},"outputId":"db9c9a45-a1ce-4849-921d-3892e3ffec55","trusted":true},"outputs":[],"source":["\n","\n","\n","torch.set_float32_matmul_precision('high')\n","\n","# scaler = torch.amp.GradScaler(enabled=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"58095701-5ad6-4bd6-bc30-0b1422af2a93","_uuid":"6d9f8456-d5da-49d1-8211-c97897782909","collapsed":false,"id":"MdhqasdjpsOr","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["\n","\n","def _save_snapshot(model, optimizer, scheduler, epoch, step):\n","    snapshot = {\n","        \"MODEL_STATE\": model.state_dict(),\n","        \"OPTIMIZER_STATE\": optimizer.state_dict(),\n","        # \"SCHEDULER_STATE\": scheduler.state_dict(),\n","        \"EPOCHS_RUN\": epoch,\n","        \"STEP_RUN\": step\n","    }\n","    torch.save(snapshot, f\"snapshot_{step}.pt\")\n","    print(f\"Epoch: {epoch} | Step: {step} | Snapshot saved.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6f951a8f-3f05-4b6a-9080-99a81e5925b0","_uuid":"9f8791fb-3c63-4871-8402-fbfb2a649e89","collapsed":false,"id":"wBo4MmRopsOr","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# !pip install torchtriton"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2fc4d2f7-40d4-4733-8e32-65c38f198b5e","_uuid":"5582f78b-b57d-484e-bfa5-66a2688a6e21","collapsed":false,"id":"EpyoWzdxpsOr","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["save_chechpoint_iter = 50\n","total_iters = 20000\n","eval_iters = 50\n","eval_check = 100\n","warmup_iters = 2048\n","min_lr = 0.1 * max_lr\n","lr_decay_iters = 20000\n","total_batch_size = 524288\n","micro_batch_size = batch_size\n","gradient_accumulation_steps = 32"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"20e917d1-9770-456a-99dd-5e9c33d4edda","_uuid":"a0929ebe-c656-48ba-b479-e08113f3113e","colab":{"base_uri":"https://localhost:8080/"},"collapsed":false,"id":"oATkWQfApsOs","jupyter":{"outputs_hidden":false},"outputId":"7f807743-d441-4dce-a612-bcc54011ba45","trusted":true},"outputs":[],"source":["model.eval()\n","world_size = torch.cuda.device_count()\n","@torch.inference_mode()\n","def estimate_loss(val_loader, val_iterator, device):\n","    out = {}\n","    # train_loader = prepare_dataset('train', ModelArgs.batch_size)\n","\n","    # val_loader_iterator = iter(val_loader)\n","    loader = None\n","    epoch_loss = None\n","    epoch_losses = []\n","    # print(\"Starting the eval...\")\n","    for split in ['val']:\n","        print(f\"Starting with {split} evaluation...\")\n","        # losses = torch.zeros(ModelArgs.val_epochs)\n","        # if(split == 'train'):\n","        #         loader = train_loader\n","        # if(split == 'val'):\n","        #         loader = val_loader\n","        for step in range(eval_check):\n","            try:\n","                X, x, y = next(val_iterator)\n","            except StopIteration:\n","                val_loader_iterator = iter(val_loader)\n","                X, x, y = next(val_loader_iterator)\n","\n","            # tgt_mask = torch.randint(1, tgt_vocab_size, (batch_size, block_size)).to(device)  #\n","            total_loss = 0\n","            # loader.sampler.set_epoch(step)\n","            total_batches = 0\n","            # batch = next(val_loader_iterator)\n","            # for batch in loader:  # Loop through DataLoader batches\n","            # idx = batch['input_ids']\n","            # targets = batch['labels']\n","            spec = X.to(device)\n","\n","            idx = x.to(device)\n","            targets = y.to(device)\n","            # with torch.autocast(device_type=device, dtype=torch.float16):\n","\n","            logits = model(spec, idx)\n","            batch_size, block_size, embeddings_dims = logits.shape\n","            logits = logits.view(batch_size * block_size, embeddings_dims)  # Flatten tokens\n","            targets = targets.view(batch_size * block_size)\n","\n","            loss = torch.nn.functional.cross_entropy(logits, targets, ignore_index=tokenizer.pad_token_id)\n","\n","            total_loss += loss.item()\n","            total_batches += 1\n","\n","        # Compute mean loss for this epoch\n","        epoch_loss = total_loss / total_batches if total_batches > 0 else 0.0\n","        epoch_losses.append(epoch_loss)\n","\n","            # print(f\"Epoch {epoch + 1}/{ModelArgs.val_epochs}: Loss = {epoch_loss:.4f}\")\n","\n","        # Compute mean loss across all evaluation epochs\n","        out[split] = sum(epoch_losses) / len(epoch_losses) if epoch_losses else 0.0\n","        epoch_loss = None\n","        epoch_losses = []\n","\n","    model.train()\n","    return out\n","\n","# model = model.to(rank)\n","model.train()\n","count = 0\n","\n","# train_dataloader = prepare_dataset('train', device, ModelArgs.batch_size)\n","# val_loader= prepare_dataset('val', device, ModelArgs.batch_size)\n","# for step in tqdm(range(total_iters)):\n","# for epoch in range(ModelArgs.epochs):\n","    # torch.cuda.synchronize()\n","\n","# train_dataloader.sampler.set_epoch(epoch)\n","\n","# val_loader.sampler.set_epoch(epoch)\n","print(\"Loaders ready both\")\n","epochs = epochs\n","\n","# train_step_iterator = range(len(train_dataloader))\n","# if device == 0:  # Only create progress bar on rank 0\n","#   train_step_iterator = tqdm(train_step_iterator, desc=\"Training Progress\", position=0, leave=True)\n","\n","    # Print progress on rank 0\n","train_loader_length = 0\n","train_data_iterator = iter(train_dataloader)\n","val_data_iterator = iter(val_dataloader)\n","token_count = 0\n","if(device == 0):\n","    train_loader_length = len(train_dataloader)\n","    # print(\"Total batches: \", train_loader_length)\n","# print(\"Length of : \", len(train_dataloader))\n","# print(\"Length of val: \", len(val_loader))\n","# for  step, batch in enumerate(train_dataloader):"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5fd44e9d-01f8-4e2b-9130-4640fdab7419","_uuid":"c8abd50d-6173-4aeb-8219-b6396120af54","collapsed":false,"id":"R9rFUrIlpsOs","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["\n","def find_unused_parameters(model):\n","    unused = []\n","    for name, param in model.named_parameters():\n","        if param.grad is None:\n","\n","            unused.append(name)\n","    return unused"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"39ba72e2-ffe8-4acf-afa3-dbc3077e3e2a","_uuid":"1cdf81be-4ae9-43f2-b3c5-304c73e13883","collapsed":false,"id":"AMnlZeKTpsOt","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["\n","\n","\n","\n","import math\n","def get_lr(it):\n","    # 1) linear warmup for warmup_iters steps\n","    if it < warmup_iters:\n","        return max_lr * (it + 1) / (warmup_iters + 1)\n","    # 2) if it > lr_decay_iters, return min learning rate\n","    if it > lr_decay_iters:\n","        return min_lr\n","    # 3) in between, use cosine decay down to min learning rate\n","    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n","    assert 0 <= decay_ratio <= 1\n","    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n","    return min_lr + coeff * (max_lr - min_lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"317d13ff-dc6c-4965-adfe-d28c4bd6b40d","_uuid":"5feb1c4e-332c-4b97-8952-4646e07467a6","colab":{"base_uri":"https://localhost:8080/"},"collapsed":false,"id":"O7-thMpYpsOt","jupyter":{"outputs_hidden":false},"outputId":"b7e488fb-aecf-4603-9198-3a5de1664823","trusted":true},"outputs":[],"source":["\n","#data = nex\n","\n","# data = next(iter(train_dataloader))\n","# print(data[0].shape)\n","for key, value, val in train_dataloader:\n","#     # if(key == 'stop_tokens'):\n","#     #   print(value)\n","#     #   break\n","    print(val.shape)\n","    print(key.shape, value.shape)\n","    break\n","# print(data)\n","# tgt_mask = torch.randint(1, tgt_vocab_size, (batch_size, block_size)).to(device)  #"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"44ced50a-b954-4b04-8bc2-25b7c2601b9c","_uuid":"600fec50-d0e3-4593-a316-5c25e586c4f2","colab":{"base_uri":"https://localhost:8080/","height":191},"collapsed":false,"execution":{"execution_failed":"2025-03-28T12:41:30.761Z"},"id":"nPrSPPu8cuoO","jupyter":{"outputs_hidden":false},"outputId":"73e617ef-15bc-4585-f77f-5014f62ab325","trusted":true},"outputs":[],"source":["model.train()\n","train_losses =  torch.zeros(len(train_dataloader))\n","val_losses = torch.zeros(len(val_dataloader))\n","wandb.init(\n","    project='Moonshine-From-Scratch'\n",")\n","step = 0\n","for step in tqdm(range(total_iters)):\n","        # print(\"Dataloader things: \", batch)\n","        # print(\"Total batches: \", len(train_dataloader))\n","\n","\n","        # if(device == 0):\n","            # if(step % 100 == 0):\n","        #     if(step == train_loader_length):\n","        #       break\n","        print(\"Step : \", step, \"/\", total_iters)\n","        print('Total batches: ', len(train_dataloader))\n","        print(\"Total gradient accumulation steps: \", gradient_accumulation_steps)\n","                # print(\"Total tokens processed: \", token_count)\n","\n","        # all_gpus_avg_train_loss = None\n","        # all_gpus_avg_val_loss = None\n","        # every once in a while evaluate the loss on train and val sets\n","        if (step  % eval_iters == 0 and step != 0) or step == total_iters - 1:\n","            losses = estimate_loss( val_dataloader, val_data_iterator, 'cuda')\n","            # avg_train_loss = losses['train']\n","            avg_val_loss = losses['val']\n","            # print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","            # if device == 0:  # Only print on main process\n","            print(f\"[GPU {device}] | Step: {step} / {total_iters} | Val Loss: {losses['val']:.4f}\")\n","            # print(f\"[GPU {device}] | Epoch {epoch}/{ModelArgs.epochs}| |Step: {step} | Train Loss: {losses['train']:.4f}\")\n","                # print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","                # Log training loss more frequently\n","                # Aggregate average loss across all GPUs\n","            # avg_train_loss = torch.Tensor([losses['train']]).to(device)\n","            avg_val_loss = torch.Tensor([losses['val']]).to(device)\n","            # torch.distributed.reduce(avg_train_loss, dst=0, op=torch.distributed.ReduceOp.SUM)\n","            # torch.distributed.reduce(avg_val_loss, dst=0, op=torch.distributed.ReduceOp.SUM)\n","\n","            # if device == 0:\n","                # all_gpus_avg_train_loss = avg_train_loss / world_size\n","                # print(f\"All_GPUs_Train_losses: {all_gpus_avg_train_loss.item():.4f}\")\n","            all_gpus_avg_val_loss = avg_val_loss / world_size\n","            print(f\"All_GPUs_Val_losses: {all_gpus_avg_val_loss.item():.4f}\")\n","\n","            # if device == 0:\n","\n","                # writer.add_scalar(\"All_GPUs_Train_losses\", all_gpus_avg_train_loss.item(), global_step=step)\n","                # writer.add_scalar(\"All_GPUs_Val_losses\", all_gpus_avg_val_loss.item(), global_step=step)\n","                # writer.add_scalar(\"training_step_loss\", losses['train'], global_step=step)\n","                # writer.add_scalar(\"val_step_loss\", losses['val'], global_step=step)\n","                # writer.add_scalar(\"GPU\", device, global_step=step)\n","                # writer.add_scalar(\"Epoch\", epoch, global_step=step)\n","\n","            wandb.log({\n","                    # \"Learning Rate\": optimizer.param_groups[0]['lr'],\n","                    # \"All_GPUs_Train_losses\": all_gpus_avg_train_loss,\n","                    \"All_GPUs_Val_losses\": all_gpus_avg_val_loss,\n","                    # \"training_step_loss\": losses['train'],\n","                    \"val_step_loss\": losses['val'],\n","                    # \"Step\": step,\n","                    # \"Epoch\": epoch\n","                })\n","\n","\n","\n","        #Loading a checkpoint\n","        # if(os.path.exists('snapshot.pt')):\n","        #    model, optimizer =  _load_snapshot(model=model, optimizer=optimizer, epoch=epoch, step=step, snapshot_path='snapshot.pt')\n","\n","        # if(step % save_chechpoint_iter == 0 and device == 0 and step != 0):\n","\n","        #     _save_snapshot(epoch=epoch, model=model, optimizer=optimizer, step=step)\n","\n","        if step % save_chechpoint_iter == 0 and device == 0 and step != 0:\n","            print(f\"Saving the model checkpoint for step: {step}\")\n","            _save_snapshot(model, optimizer, None, None, step)\n","\n","        accumulated_loss = 0.0\n","\n","\n","        optimizer.zero_grad(set_to_none=True)\n","        # for micro_step in range(gradient_accumulation_steps):\n","        try:\n","            spec, idx, y = next(train_data_iterator)\n","        except StopIteration:\n","            train_data_iterator = iter(train_dataloader)\n","            spec, idx, y = next(train_data_iterator)\n","        spec = spec.to(device)\n","        y = y.to(device)\n","        idx = idx.to(device)\n","\n","            # tgt_mask = torch.randint(1, tgt_vocab_size, (batch_size, block_size)).to(device)  #\n","            # print(batch)\n","            # batch = next(train_data_iterator)\n","            # print(batch)\n","            # batch = {k: v.to(self.local_rank) for k, v in batch.items()}\n","            # idx = batch['input_ids'].to(device)\n","            # idx, targets = get_batch(split='train')\n","            # print(f\"Starting the train step: {step}...\")\n","            # for idx, targets in train_loader:\n","            # idx, targets = next(iter(train_loader))\n","\n","            # print(\"Idx: \", idx)\n","            # print(\"Targets: \", targets)\n","\n","            # idx = idx.to(device)\n","            # print(\"Idx: \", idx)\n","            # print(\"Targets: \", targets)\n","            # targets = batch['labels'].to(device)\n","            # token_count += len(idx)\n","            # with torch.autocast(device_type=device, dtype=torch.float16):\n","        logits = model(spec, idx)\n","        batch_size, block_size, embeddings_dims = logits.shape\n","        # print(logits.shape)\n","        # print(targets)\n","        logits = logits.view(batch_size*block_size, embeddings_dims)\n","        # print(\"OK\")\n","        targets = y.view(batch_size * block_size)\n","        # print(\"OK2\")\n","        loss = nn.functional.cross_entropy(logits, targets, ignore_index=tokenizer.pad_token_id)\n","\n","        # loss = loss / gradient_accumulation_steps #IDK why div is done here specifically? Maybe think of it in terms of a very big batch being processed and there is need for equal important of each mini batch for the overall big batch\n","        # accumulated_loss += loss.detach()\n","\n","        # model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1) # so that we dont synchronize the gradient everytime across the GPU devices\n","        loss.backward()\n","        # print(\"loss: \", loss.item())\n","            # Check for unused parameters\n","        unused_params = find_unused_parameters(model)\n","        if unused_params:\n","            print(f\"Unused parameters: {unused_params}\")\n","    # break\n","\n","        # if(device == 0):\n","        # if(micro_step % 10 == 0):\n","        # #     if(step == train_loader_length):\n","        # #       break\n","\n","        #         print(\"Micro Batch : \", micro_step)\n","        #         # print(\"Step : \", step, \"/\", total_iters)\n","        #         print('Total batches: ', len(train_dataloader))\n","        #         print(\"Total gradient accumulation steps: \", gradient_accumulation_steps)\n","        #         print(\"Total tokens processed: \", token_count)\n","        # count += 1\n","\n","        lr = get_lr(step)\n","        for params in optimizer.param_groups:\n","            params['lr'] = lr\n","\n","\n","\n","        # Compute gradient norms before clipping\n","        if(clip != 0.0):\n","            # scaler.unscale_(optimizer) #To avoid underflow\n","            total_norm_before = torch.norm(\n","                torch.stack([torch.norm(p.grad.detach(), 2) for p in model.parameters()]), 2\n","            )\n","\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\n","\n","            # Compute gradient norms after clipping\n","            total_norm_after = torch.norm(\n","                torch.stack([torch.norm(p.grad.detach(), 2) for p in model.parameters()]), 2\n","            )\n","\n","            if(device  == 0 and step !=0):\n","                print(f\"Gradient Norm Before Clipping: {total_norm_before.item():.4f}\")\n","                print(f\"Gradient Norm After Clipping: {total_norm_after.item():.4f}\")\n","\n","        optimizer.step()\n","        # scaler.update()\n","\n","        # optimizer.step()\n","        # new_scheduler.step()\n","        # print(accumulated_loss)\n","        # torch.cuda.synchronize()\n","        # torch.distributed.reduce(loss, dst=0, op=torch.distributed.ReduceOp.SUM)\n","        # if(device == 0):\n","        wandb.log({\n","                    \"Learning Rate\": lr,\n","                    \"All_GPUs_Train_losses\": loss.item(),\n","                    # \"All_GPUs_Val_losses\": all_gpus_avg_val_loss,\n","                    # \"training_step_loss\": losses['train'],\n","                    # \"val_step_loss\": losses['val'],\n","                    # \"Step\": step,\n","                    # \"Epoch\": epoch\n","\n","                })\n","\n","\n","        # model.train()\n","        # wandb.log({\n","        #   \"Train Loss\": train_losses.mean(),\n","        #   \"Val Loss\": val_losses.mean(),\n","        #   # \"epoch\": epoch\n","        # })\n","        # print(\"Epoch: \", epoch, \"|\", \"Train Loss: \", train_losses.mean(),  \"|\", \"Val Loss: \", val_losses.mean())"]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"01fe5a96c7b9468dbd6abd83d58b31d0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"056becbc86134ff993f62136c636f263":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07315cca04584ff584df3f0f925dc21e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"143e0faa317e44f6821ffdef7a5f6931":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"171bcaedc1034ad883e8c0ae00632999":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_056becbc86134ff993f62136c636f263","placeholder":"​","style":"IPY_MODEL_6c41650eecf24e55a5affda617eac687","value":"Filter: 100%"}},"1de74b76d5364fdbb230ef3a0587b7e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_96924505eb4344aea814c59efb60fccf","IPY_MODEL_374f1467fcc34b2683985c33e5aff04a","IPY_MODEL_78d0266852f64e3bb6b0d201a78b1dac"],"layout":"IPY_MODEL_35bc1ad50a6b45a297cbf80af4d77de4"}},"2f045124f238406cb58a77013ae254c5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30c7d02180c749669ab0c725040c83bd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_70f08b4090204d4cb2e0bc2c08e9f21a","IPY_MODEL_b6578fa751084e79872f27c8ec7447c3","IPY_MODEL_44fca81bee7e4d178e60529a953ac4fb"],"layout":"IPY_MODEL_01fe5a96c7b9468dbd6abd83d58b31d0"}},"35bc1ad50a6b45a297cbf80af4d77de4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"374f1467fcc34b2683985c33e5aff04a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a2fa1d2392d42ab87aeadd9101a3c94","max":10480,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d53d4ba3aa4d4aee9eb34fbdd72b0afc","value":10480}},"3b8f4b197a84464cb31d59b0e038ce1c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3e91345376394605941d9f031995ff10":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f7d7b3642784218bf1f3e0f72f2809f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"431809753f044eef9bd4bded81a686b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f045124f238406cb58a77013ae254c5","max":2620,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7d592f1aa4904cb6989f55760c8daa9d","value":2620}},"44fca81bee7e4d178e60529a953ac4fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98d4d8c0785f4177b7bf79bb029bfb33","placeholder":"​","style":"IPY_MODEL_3e91345376394605941d9f031995ff10","value":" 10480/10480 [00:01&lt;00:00, 5740.63 examples/s]"}},"504d6c72038241fbbad2732ef3d5c75f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"54788d0eaac944e6a1dbcf0d4e6fdc24":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aff7d4f4d930451fb9d659b4e26034ca","placeholder":"​","style":"IPY_MODEL_3b8f4b197a84464cb31d59b0e038ce1c","value":" 2620/2620 [00:00&lt;00:00, 13954.45 examples/s]"}},"55b5b881943e45c5b21d2a4b9a7df1ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a2404992981486897018299f8ea9cdf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a2fa1d2392d42ab87aeadd9101a3c94":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c3b0cddb8ff4d54a52ffc8c417185e9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6c41650eecf24e55a5affda617eac687":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"70f08b4090204d4cb2e0bc2c08e9f21a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d987d59ac37046d88ed73dcc0aa7b2a7","placeholder":"​","style":"IPY_MODEL_dcd607401ec44598912d3e934cfec4e7","value":"Flattening the indices: 100%"}},"74942eb68cfa4cfebce1dd922c906eb1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74b4ac2926e842848815ac6c45173572":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78d0266852f64e3bb6b0d201a78b1dac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c45794bc2dc24d1bad98ecc6bb58ce43","placeholder":"​","style":"IPY_MODEL_5a2404992981486897018299f8ea9cdf","value":" 10480/10480 [00:00&lt;00:00, 136193.05 examples/s]"}},"7d592f1aa4904cb6989f55760c8daa9d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"85767c5f479b4c6ca901fda615d7afaa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a6a14773edbb4da5a0db88907d72836e","IPY_MODEL_431809753f044eef9bd4bded81a686b1","IPY_MODEL_54788d0eaac944e6a1dbcf0d4e6fdc24"],"layout":"IPY_MODEL_74942eb68cfa4cfebce1dd922c906eb1"}},"96924505eb4344aea814c59efb60fccf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fdd2a49adbb84d08b6c2df0f24a315bc","placeholder":"​","style":"IPY_MODEL_dde00aa4837b4b4b872e0e555ca639b0","value":"Filter: 100%"}},"98d4d8c0785f4177b7bf79bb029bfb33":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6a14773edbb4da5a0db88907d72836e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_143e0faa317e44f6821ffdef7a5f6931","placeholder":"​","style":"IPY_MODEL_55b5b881943e45c5b21d2a4b9a7df1ca","value":"Flattening the indices: 100%"}},"aff7d4f4d930451fb9d659b4e26034ca":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5c49554e5774d54884867e0800c6716":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_171bcaedc1034ad883e8c0ae00632999","IPY_MODEL_c486b16024824cf584fa8a24516404e8","IPY_MODEL_e7d4a4f0a1354b3e9164bf4a31cdff15"],"layout":"IPY_MODEL_3f7d7b3642784218bf1f3e0f72f2809f"}},"b6578fa751084e79872f27c8ec7447c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_74b4ac2926e842848815ac6c45173572","max":10480,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5c3b0cddb8ff4d54a52ffc8c417185e9","value":10480}},"c3f2ef601e1f477ea16d4be5c6743952":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c45794bc2dc24d1bad98ecc6bb58ce43":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c486b16024824cf584fa8a24516404e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5278fd04d9d40b1b907759338420356","max":2620,"min":0,"orientation":"horizontal","style":"IPY_MODEL_504d6c72038241fbbad2732ef3d5c75f","value":2620}},"d53d4ba3aa4d4aee9eb34fbdd72b0afc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d987d59ac37046d88ed73dcc0aa7b2a7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dcd607401ec44598912d3e934cfec4e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dde00aa4837b4b4b872e0e555ca639b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e5278fd04d9d40b1b907759338420356":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7d4a4f0a1354b3e9164bf4a31cdff15":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3f2ef601e1f477ea16d4be5c6743952","placeholder":"​","style":"IPY_MODEL_07315cca04584ff584df3f0f925dc21e","value":" 2620/2620 [00:00&lt;00:00, 68128.19 examples/s]"}},"fdd2a49adbb84d08b6c2df0f24a315bc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":4}
